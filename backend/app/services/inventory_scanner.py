"""Inventory scanner service for complete cloud resource scanning.

This service scans ALL cloud resources (not just orphans) to provide
cost intelligence and optimization recommendations.
"""

import structlog
from datetime import datetime, timedelta
from typing import Any

from app.providers.base import AllCloudResourceData

logger = structlog.get_logger()


def safe_get_value(obj: Any, default: Any = None) -> Any:
    """
    Safely extract .value from Azure SDK Enum objects.

    Azure SDK inconsistently returns Enum objects (with .value property) or
    strings directly depending on API version and region. This helper handles both cases.

    Args:
        obj: Azure SDK object (Enum or string)
        default: Default value if obj is None

    Returns:
        The actual value (obj.value if Enum, obj if string, default if None)

    Example:
        >>> safe_get_value(OperatingSystemTypes.LINUX)  # Enum
        'Linux'
        >>> safe_get_value("Linux")  # String
        'Linux'
        >>> safe_get_value(None, "Unknown")
        'Unknown'
    """
    if obj is None:
        return default
    return obj.value if hasattr(obj, 'value') else obj


class AWSInventoryScanner:
    """AWS-specific inventory scanner for cost intelligence."""

    def __init__(self, provider: Any) -> None:
        """
        Initialize AWS inventory scanner.

        Args:
            provider: AWS provider instance with authenticated session
        """
        self.provider = provider
        self.session = provider.session

    async def scan_ec2_instances(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL EC2 instances (running, stopped, etc.) for cost intelligence.

        Unlike orphan detection, this returns ALL instances with utilization metrics
        and optimization recommendations.

        Args:
            region: AWS region to scan

        Returns:
            List of all EC2 instance resources
        """
        logger.info("inventory.scan_ec2_start", region=region)
        all_instances: list[AllCloudResourceData] = []

        try:
            async with self.session.client("ec2", region_name=region) as ec2:
                # Describe ALL instances (no filters)
                response = await ec2.describe_instances()

                for reservation in response.get("Reservations", []):
                    for instance in reservation.get("Instances", []):
                        instance_id = instance["InstanceId"]
                        instance_type = instance["InstanceType"]
                        state = instance["State"]["Name"]

                        # Extract instance name from tags
                        instance_name = None
                        tags = {}
                        for tag in instance.get("Tags", []):
                            tags[tag["Key"]] = tag["Value"]
                            if tag["Key"] == "Name":
                                instance_name = tag["Value"]

                        # Get CloudWatch metrics (last 14 days)
                        cpu_util = await self._get_cpu_utilization(instance_id, region)
                        network_in = await self._get_network_in(instance_id, region)

                        # Calculate monthly cost
                        monthly_cost = self._calculate_ec2_monthly_cost(
                            instance_type, state
                        )

                        # Determine utilization status
                        utilization_status = self._determine_utilization_status(
                            cpu_util, state
                        )

                        # Calculate optimization score and recommendations
                        (
                            is_optimizable,
                            optimization_score,
                            optimization_priority,
                            potential_savings,
                            recommendations,
                        ) = self._calculate_ec2_optimization(
                            instance,
                            cpu_util,
                            monthly_cost,
                            state,
                        )

                        # Check if instance is also detected as orphan
                        is_orphan = state == "stopped" or (
                            state == "running" and cpu_util < 5.0
                        )

                        # Create resource data
                        resource = AllCloudResourceData(
                            resource_type="ec2_instance",
                            resource_id=instance_id,
                            resource_name=instance_name,
                            region=region,
                            estimated_monthly_cost=monthly_cost,
                            resource_metadata={
                                "instance_type": instance_type,
                                "state": state,
                                "availability_zone": instance.get(
                                    "Placement", {}
                                ).get("AvailabilityZone"),
                                "launch_time": instance.get("LaunchTime").isoformat()
                                if instance.get("LaunchTime")
                                else None,
                                "platform": instance.get("Platform", "linux"),
                                "vpc_id": instance.get("VpcId"),
                                "subnet_id": instance.get("SubnetId"),
                            },
                            currency="USD",
                            utilization_status=utilization_status,
                            cpu_utilization_percent=cpu_util,
                            memory_utilization_percent=None,  # TODO: Fetch from CloudWatch agent
                            network_utilization_mbps=network_in,
                            is_optimizable=is_optimizable,
                            optimization_priority=optimization_priority,
                            optimization_score=optimization_score,
                            potential_monthly_savings=potential_savings,
                            optimization_recommendations=recommendations,
                            tags=tags,
                            resource_status=state,
                            is_orphan=is_orphan,
                            created_at_cloud=instance.get("LaunchTime"),
                            last_used_at=None,  # TODO: Estimate from CloudWatch
                        )

                        all_instances.append(resource)

                logger.info(
                    "inventory.scan_ec2_complete",
                    region=region,
                    total_instances=len(all_instances),
                )

        except Exception as e:
            logger.error(
                "inventory.scan_ec2_error",
                region=region,
                error=str(e),
                exc_info=True,
            )
            raise

        return all_instances

    async def scan_ebs_volumes(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL EBS volumes (attached + unattached) for cost intelligence.

        Unlike orphan detection, this returns ALL volumes with I/O metrics
        and optimization recommendations.

        Args:
            region: AWS region to scan

        Returns:
            List of all EBS volume resources
        """
        logger.info("inventory.scan_ebs_start", region=region)
        all_volumes: list[AllCloudResourceData] = []

        try:
            async with self.session.client("ec2", region_name=region) as ec2:
                # Describe ALL volumes (no filters)
                response = await ec2.describe_volumes()

                for volume in response.get("Volumes", []):
                    volume_id = volume["VolumeId"]
                    volume_type = volume.get("VolumeType", "gp3")
                    state = volume["State"]
                    size_gb = volume["Size"]
                    iops = volume.get("Iops")
                    throughput = volume.get("Throughput")

                    # Extract volume name from tags
                    volume_name = None
                    tags = {}
                    for tag in volume.get("Tags", []):
                        tags[tag["Key"]] = tag["Value"]
                        if tag["Key"] == "Name":
                            volume_name = tag["Value"]

                    # Get CloudWatch metrics (last 14 days)
                    read_ops = await self._get_volume_read_ops(volume_id, region)
                    write_ops = await self._get_volume_write_ops(volume_id, region)

                    # Calculate monthly cost
                    monthly_cost = self._calculate_ebs_monthly_cost(
                        volume_type, size_gb, iops, throughput, region
                    )

                    # Determine utilization status
                    if state == "available":
                        utilization_status = "idle"
                    elif read_ops + write_ops == 0:
                        utilization_status = "idle"
                    elif read_ops + write_ops < 1000:  # < 1000 ops/day
                        utilization_status = "low"
                    elif read_ops + write_ops < 10000:  # < 10K ops/day
                        utilization_status = "medium"
                    else:
                        utilization_status = "high"

                    # Calculate optimization score and recommendations
                    (
                        is_optimizable,
                        optimization_score,
                        optimization_priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_ebs_optimization(
                        volume,
                        read_ops,
                        write_ops,
                        monthly_cost,
                        state,
                    )

                    # Check if volume is also detected as orphan
                    is_orphan = state == "available" or (
                        len(volume.get("Attachments", [])) > 0 and read_ops + write_ops == 0
                    )

                    # Get attachment info
                    attachments = volume.get("Attachments", [])
                    attached_to = None
                    if attachments:
                        attached_to = attachments[0].get("InstanceId")

                    # Create resource data
                    resource = AllCloudResourceData(
                        resource_type="ebs_volume",
                        resource_id=volume_id,
                        resource_name=volume_name,
                        region=region,
                        estimated_monthly_cost=monthly_cost,
                        resource_metadata={
                            "volume_type": volume_type,
                            "state": state,
                            "size_gb": size_gb,
                            "iops": iops,
                            "throughput": throughput,
                            "availability_zone": volume.get("AvailabilityZone"),
                            "encrypted": volume.get("Encrypted", False),
                            "multi_attach_enabled": volume.get("MultiAttachEnabled", False),
                            "attached_to": attached_to,
                            "attachment_count": len(attachments),
                        },
                        currency="USD",
                        utilization_status=utilization_status,
                        cpu_utilization_percent=None,
                        memory_utilization_percent=None,
                        storage_utilization_percent=None,
                        network_utilization_mbps=None,
                        is_optimizable=is_optimizable,
                        optimization_priority=optimization_priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations,
                        tags=tags,
                        resource_status=state,
                        is_orphan=is_orphan,
                        created_at_cloud=volume.get("CreateTime"),
                        last_used_at=None,  # TODO: Estimate from CloudWatch metrics
                    )

                    all_volumes.append(resource)

                logger.info(
                    "inventory.scan_ebs_complete",
                    region=region,
                    total_volumes=len(all_volumes),
                )

        except Exception as e:
            logger.error(
                "inventory.scan_ebs_error",
                region=region,
                error=str(e),
                exc_info=True,
            )
            raise

        return all_volumes

    async def scan_elastic_ips(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Elastic IPs (associated + unassociated) for cost intelligence.

        Unlike orphan detection, this returns ALL Elastic IPs with
        utilization status and optimization recommendations.

        Args:
            region: AWS region to scan

        Returns:
            List of all Elastic IP resources
        """
        logger.info("inventory.scan_elastic_ips_start", region=region)
        all_eips: list[AllCloudResourceData] = []

        try:
            async with self.session.client("ec2", region_name=region) as ec2:
                # Describe ALL Elastic IPs (no filters)
                response = await ec2.describe_addresses()

                for eip in response.get("Addresses", []):
                    allocation_id = eip.get("AllocationId", "N/A")
                    public_ip = eip.get("PublicIp", "N/A")
                    association_id = eip.get("AssociationId")
                    instance_id = eip.get("InstanceId")
                    network_interface_id = eip.get("NetworkInterfaceId")

                    # Extract EIP name from tags
                    eip_name = None
                    tags = {}
                    for tag in eip.get("Tags", []):
                        tags[tag["Key"]] = tag["Value"]
                        if tag["Key"] == "Name":
                            eip_name = tag["Value"]

                    # Determine if EIP is associated (attached to resource)
                    is_associated = bool(association_id or instance_id or network_interface_id)

                    # Calculate monthly cost
                    # Unassociated EIPs cost $3.60/month, associated are free
                    if is_associated:
                        monthly_cost = 0.0
                        utilization_status = "active"
                        state = "associated"
                    else:
                        # Use dynamic pricing service with fallback
                        try:
                            eip_price = await self.pricing_service.get_aws_price("elastic_ip", region)
                            monthly_cost = eip_price if eip_price else 3.60
                        except Exception:
                            monthly_cost = 3.60
                        utilization_status = "idle"
                        state = "unassociated"

                    # Calculate optimization score and recommendations
                    recommendations = []
                    is_optimizable = False
                    optimization_score = 0
                    potential_savings = 0.0
                    priority = "none"

                    if not is_associated:
                        # Unassociated EIP - critical priority
                        is_optimizable = True
                        optimization_score = 95
                        priority = "critical"
                        potential_savings = monthly_cost
                        recommendations.append({
                            "action": "Release unassociated Elastic IP",
                            "details": f"EIP is not associated with any resource. Release to save ${potential_savings:.2f}/month",
                            "priority": "critical",
                        })
                    elif network_interface_id and not instance_id:
                        # Associated with ENI but not instance - might be detached
                        is_optimizable = True
                        optimization_score = 70
                        priority = "high"
                        potential_savings = monthly_cost
                        recommendations.append({
                            "action": "Review EIP on detached ENI",
                            "details": "EIP is attached to ENI but not directly to instance. Verify if ENI is in use",
                            "priority": "high",
                        })

                    # Check if EIP is also detected as orphan
                    is_orphan = not is_associated

                    # Create resource data
                    resource = AllCloudResourceData(
                        resource_type="elastic_ip",
                        resource_id=allocation_id,
                        resource_name=eip_name,
                        region=region,
                        estimated_monthly_cost=monthly_cost,
                        resource_metadata={
                            "public_ip": public_ip,
                            "allocation_id": allocation_id,
                            "association_id": association_id,
                            "instance_id": instance_id,
                            "network_interface_id": network_interface_id,
                            "domain": eip.get("Domain", "vpc"),
                            "private_ip_address": eip.get("PrivateIpAddress"),
                            "is_associated": is_associated,
                        },
                        currency="USD",
                        utilization_status=utilization_status,
                        cpu_utilization_percent=None,
                        memory_utilization_percent=None,
                        storage_utilization_percent=None,
                        network_utilization_mbps=None,
                        is_optimizable=is_optimizable,
                        optimization_priority=priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations,
                        tags=tags,
                        resource_status=state,
                        is_orphan=is_orphan,
                        created_at_cloud=None,  # EIPs don't have creation time in API
                        last_used_at=None,
                    )

                    all_eips.append(resource)

                logger.info(
                    "inventory.scan_elastic_ips_complete",
                    region=region,
                    total_eips=len(all_eips),
                )

        except Exception as e:
            logger.error(
                "inventory.scan_elastic_ips_error",
                region=region,
                error=str(e),
                exc_info=True,
            )
            raise

        return all_eips

    async def scan_aws_load_balancers(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS Load Balancers (ALB, NLB, GLB, CLB) for cost intelligence.

        Unlike orphan detection, this returns ALL load balancers with
        utilization status and optimization recommendations.

        Args:
            region: AWS region to scan

        Returns:
            List of all load balancer resources
        """
        logger.info("inventory.scan_aws_lb_start", region=region)
        all_lbs: list[AllCloudResourceData] = []

        try:
            # ================================================================
            # Scan Application/Network/Gateway Load Balancers (ELBv2)
            # ================================================================
            async with self.session.client("elbv2", region_name=region) as elbv2:
                response = await elbv2.describe_load_balancers()

                for lb in response.get("LoadBalancers", []):
                    lb_arn = lb["LoadBalancerArn"]
                    lb_name = lb["LoadBalancerName"]
                    lb_type = lb["Type"]  # 'application', 'network', or 'gateway'
                    lb_state = lb["State"]["Code"]  # 'active', 'provisioning', 'failed'
                    created_at = lb["CreatedTime"]

                    # Extract tags
                    tags = {}
                    try:
                        tags_response = await elbv2.describe_tags(ResourceArns=[lb_arn])
                        for tag_desc in tags_response.get("TagDescriptions", []):
                            for tag in tag_desc.get("Tags", []):
                                tags[tag["Key"]] = tag["Value"]
                    except Exception:
                        pass

                    # Get listeners count
                    try:
                        listeners_response = await elbv2.describe_listeners(
                            LoadBalancerArn=lb_arn
                        )
                        listener_count = len(listeners_response.get("Listeners", []))
                    except Exception:
                        listener_count = 0

                    # Get target groups for this LB
                    try:
                        tg_response = await elbv2.describe_target_groups(
                            LoadBalancerArn=lb_arn
                        )
                        target_groups = tg_response.get("TargetGroups", [])
                        target_group_count = len(target_groups)
                    except Exception:
                        target_groups = []
                        target_group_count = 0

                    # Get target health
                    healthy_target_count = 0
                    total_target_count = 0
                    for tg in target_groups:
                        tg_arn = tg["TargetGroupArn"]
                        try:
                            health_response = await elbv2.describe_target_health(
                                TargetGroupArn=tg_arn
                            )
                            targets = health_response.get("TargetHealthDescriptions", [])
                            total_target_count += len(targets)
                            for target in targets:
                                if target["TargetHealth"]["State"] == "healthy":
                                    healthy_target_count += 1
                        except Exception:
                            pass

                    # Calculate monthly cost
                    monthly_cost = self._calculate_alb_monthly_cost(lb_type, region)

                    # Determine utilization status
                    if lb_state != "active":
                        utilization_status = "idle"
                    elif listener_count == 0 or target_group_count == 0:
                        utilization_status = "idle"
                    elif total_target_count > 0 and healthy_target_count == 0:
                        utilization_status = "idle"
                    elif healthy_target_count > 0:
                        utilization_status = "active"
                    else:
                        utilization_status = "unknown"

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        optimization_priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_alb_optimization(
                        lb,
                        listener_count,
                        target_group_count,
                        healthy_target_count,
                        total_target_count,
                        monthly_cost,
                        lb_type,
                    )

                    # Check if LB is orphan
                    is_orphan = (
                        listener_count == 0
                        or target_group_count == 0
                        or (total_target_count > 0 and healthy_target_count == 0)
                    )

                    # Create resource data
                    resource = AllCloudResourceData(
                        resource_type="load_balancer",
                        resource_id=lb_arn,
                        resource_name=lb_name,
                        region=region,
                        estimated_monthly_cost=monthly_cost,
                        resource_metadata={
                            "lb_type": lb_type,
                            "lb_state": lb_state,
                            "dns_name": lb.get("DNSName"),
                            "scheme": lb.get("Scheme", "internet-facing"),
                            "vpc_id": lb.get("VpcId"),
                            "availability_zones": [
                                az.get("ZoneName") for az in lb.get("AvailabilityZones", [])
                            ],
                            "listener_count": listener_count,
                            "target_group_count": target_group_count,
                            "healthy_target_count": healthy_target_count,
                            "total_target_count": total_target_count,
                        },
                        currency="USD",
                        utilization_status=utilization_status,
                        cpu_utilization_percent=None,
                        memory_utilization_percent=None,
                        storage_utilization_percent=None,
                        network_utilization_mbps=None,
                        is_optimizable=is_optimizable,
                        optimization_priority=optimization_priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations,
                        tags=tags,
                        resource_status=lb_state,
                        is_orphan=is_orphan,
                        created_at_cloud=created_at,
                        last_used_at=None,
                    )

                    all_lbs.append(resource)

            # ================================================================
            # Scan Classic Load Balancers (ELB)
            # ================================================================
            async with self.session.client("elb", region_name=region) as elb:
                response = await elb.describe_load_balancers()

                for lb in response.get("LoadBalancerDescriptions", []):
                    lb_name = lb["LoadBalancerName"]
                    created_at = lb.get("CreatedTime")
                    dns_name = lb.get("DNSName")
                    scheme = lb.get("Scheme", "internet-facing")
                    vpc_id = lb.get("VPCId")
                    availability_zones = lb.get("AvailabilityZones", [])

                    # Extract tags
                    tags = {}
                    try:
                        tags_response = await elb.describe_tags(
                            LoadBalancerNames=[lb_name]
                        )
                        for tag_desc in tags_response.get("TagDescriptions", []):
                            for tag in tag_desc.get("Tags", []):
                                tags[tag["Key"]] = tag["Value"]
                    except Exception:
                        pass

                    # Get listeners count
                    listeners = lb.get("ListenerDescriptions", [])
                    listener_count = len(listeners)

                    # Get registered instances (CLB doesn't use target groups)
                    instances = lb.get("Instances", [])
                    total_target_count = len(instances)

                    # Get instance health
                    healthy_target_count = 0
                    if instances:
                        try:
                            health_response = await elb.describe_instance_health(
                                LoadBalancerName=lb_name
                            )
                            for instance_state in health_response.get("InstanceStates", []):
                                if instance_state["State"] == "InService":
                                    healthy_target_count += 1
                        except Exception:
                            pass

                    # Calculate monthly cost for CLB
                    monthly_cost = self._calculate_alb_monthly_cost("classic", region)

                    # Determine utilization status
                    if listener_count == 0 or total_target_count == 0:
                        utilization_status = "idle"
                    elif total_target_count > 0 and healthy_target_count == 0:
                        utilization_status = "idle"
                    elif healthy_target_count > 0:
                        utilization_status = "active"
                    else:
                        utilization_status = "unknown"

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        optimization_priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_alb_optimization(
                        {"LoadBalancerName": lb_name, "Name": lb_name},
                        listener_count,
                        0,  # CLB doesn't have target groups
                        healthy_target_count,
                        total_target_count,
                        monthly_cost,
                        "classic",
                    )

                    # Check if CLB is orphan
                    is_orphan = (
                        listener_count == 0
                        or total_target_count == 0
                        or (total_target_count > 0 and healthy_target_count == 0)
                    )

                    # Create resource data
                    resource = AllCloudResourceData(
                        resource_type="load_balancer",
                        resource_id=lb_name,  # CLB uses name as ID
                        resource_name=lb_name,
                        region=region,
                        estimated_monthly_cost=monthly_cost,
                        resource_metadata={
                            "lb_type": "classic",
                            "lb_state": "active",  # CLB doesn't have state field
                            "dns_name": dns_name,
                            "scheme": scheme,
                            "vpc_id": vpc_id,
                            "availability_zones": availability_zones,
                            "listener_count": listener_count,
                            "target_group_count": 0,  # CLB doesn't use target groups
                            "healthy_target_count": healthy_target_count,
                            "total_target_count": total_target_count,
                        },
                        currency="USD",
                        utilization_status=utilization_status,
                        cpu_utilization_percent=None,
                        memory_utilization_percent=None,
                        storage_utilization_percent=None,
                        network_utilization_mbps=None,
                        is_optimizable=is_optimizable,
                        optimization_priority=optimization_priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations,
                        tags=tags,
                        resource_status="active",
                        is_orphan=is_orphan,
                        created_at_cloud=created_at,
                        last_used_at=None,
                    )

                    all_lbs.append(resource)

            logger.info(
                "inventory.scan_aws_lb_complete",
                region=region,
                total_lbs=len(all_lbs),
            )

        except Exception as e:
            logger.error(
                "inventory.scan_aws_lb_error",
                region=region,
                error=str(e),
                exc_info=True,
            )
            raise

        return all_lbs

    async def scan_ebs_snapshots(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL EBS snapshots for cost intelligence.

        Unlike orphan detection, this returns ALL snapshots owned by account
        with optimization recommendations.

        Args:
            region: AWS region to scan

        Returns:
            List of all EBS snapshot resources
        """
        logger.info("inventory.scan_ebs_snapshots_start", region=region)
        all_snapshots: list[AllCloudResourceData] = []

        try:
            async with self.session.client("ec2", region_name=region) as ec2:
                async with self.session.client("sts", region_name=region) as sts:
                    # Get account ID
                    identity = await sts.get_caller_identity()
                    account_id = identity["Account"]

                # Describe all snapshots owned by this account
                response = await ec2.describe_snapshots(OwnerIds=[account_id])
                snapshots = response.get("Snapshots", [])

                # Get all volumes to check if snapshot source still exists
                volumes_response = await ec2.describe_volumes()
                volume_ids = {vol["VolumeId"] for vol in volumes_response.get("Volumes", [])}

                # Count snapshots per volume
                snapshot_counts: dict[str, int] = {}
                for snap in snapshots:
                    vol_id = snap.get("VolumeId")
                    if vol_id:
                        snapshot_counts[vol_id] = snapshot_counts.get(vol_id, 0) + 1

                for snapshot in snapshots:
                    snapshot_id = snapshot["SnapshotId"]
                    volume_id = snapshot.get("VolumeId")
                    size_gb = snapshot["VolumeSize"]
                    start_time = snapshot["StartTime"]
                    description = snapshot.get("Description", "")
                    state = snapshot["State"]
                    encrypted = snapshot.get("Encrypted", False)

                    # Calculate age
                    age_days = (datetime.now(start_time.tzinfo) - start_time).days

                    # Check if volume still exists
                    volume_exists = volume_id in volume_ids if volume_id else False
                    is_orphaned = not volume_exists and age_days > 90

                    # Get snapshot count for this volume
                    snapshot_count_for_volume = snapshot_counts.get(volume_id, 1) if volume_id else 1

                    # Calculate monthly cost
                    monthly_cost = self._calculate_ebs_snapshot_monthly_cost(size_gb, region)

                    # Extract tags
                    tags = {}
                    for tag in snapshot.get("Tags", []):
                        tags[tag["Key"]] = tag["Value"]

                    # Extract name from tags
                    snapshot_name = tags.get("Name")

                    # Calculate optimization (reuse existing function)
                    (
                        is_optimizable,
                        optimization_score,
                        optimization_priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_snapshot_optimization(
                        snapshot,
                        age_days,
                        size_gb,
                        is_orphaned,
                        snapshot_count_for_volume,
                        True,  # AWS snapshots are incremental by default
                        monthly_cost,
                    )

                    # Determine utilization status
                    if age_days > 365:
                        utilization_status = "idle"
                    elif is_orphaned:
                        utilization_status = "idle"
                    elif snapshot_count_for_volume > 10:
                        utilization_status = "low"
                    else:
                        utilization_status = "active"

                    # Create resource data
                    resource = AllCloudResourceData(
                        resource_type="ebs_snapshot",
                        resource_id=snapshot_id,
                        resource_name=snapshot_name,
                        region=region,
                        estimated_monthly_cost=monthly_cost,
                        resource_metadata={
                            "snapshot_id": snapshot_id,
                            "volume_id": volume_id,
                            "volume_exists": volume_exists,
                            "size_gb": size_gb,
                            "start_time": start_time.isoformat(),
                            "age_days": age_days,
                            "description": description,
                            "state": state,
                            "encrypted": encrypted,
                            "progress": snapshot.get("Progress", "100%"),
                            "snapshot_count_for_volume": snapshot_count_for_volume,
                        },
                        currency="USD",
                        utilization_status=utilization_status,
                        cpu_utilization_percent=None,
                        memory_utilization_percent=None,
                        storage_utilization_percent=None,
                        network_utilization_mbps=None,
                        is_optimizable=is_optimizable,
                        optimization_priority=optimization_priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations,
                        tags=tags,
                        resource_status=state,
                        is_orphan=is_orphaned,
                        created_at_cloud=start_time,
                        last_used_at=None,
                    )

                    all_snapshots.append(resource)

                logger.info(
                    "inventory.scan_ebs_snapshots_complete",
                    region=region,
                    total_snapshots=len(all_snapshots),
                )

        except Exception as e:
            logger.error(
                "inventory.scan_ebs_snapshots_error",
                region=region,
                error=str(e),
                exc_info=True,
            )
            raise

        return all_snapshots

    async def scan_aws_nat_gateways(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS NAT Gateways for cost intelligence.

        Unlike orphan detection, this returns ALL NAT Gateways with
        utilization metrics and optimization recommendations.

        Args:
            region: AWS region to scan

        Returns:
            List of all NAT Gateway resources
        """
        logger.info("inventory.scan_aws_nat_gateways_start", region=region)
        all_nat_gateways: list[AllCloudResourceData] = []

        try:
            async with self.session.client("ec2", region_name=region) as ec2:
                # Describe all NAT Gateways
                response = await ec2.describe_nat_gateways()
                nat_gateways = response.get("NatGateways", [])

                async with self.session.client("cloudwatch", region_name=region) as cloudwatch:
                    for nat in nat_gateways:
                        nat_gateway_id = nat["NatGatewayId"]
                        state = nat["State"]  # 'pending', 'available', 'deleting', 'deleted', 'failed'
                        vpc_id = nat.get("VpcId")
                        subnet_id = nat.get("SubnetId")
                        created_at = nat.get("CreateTime")

                        # Get Elastic IP addresses
                        nat_gateway_addresses = nat.get("NatGatewayAddresses", [])
                        elastic_ip_count = len(nat_gateway_addresses)
                        public_ip = None
                        private_ip = None
                        if nat_gateway_addresses:
                            public_ip = nat_gateway_addresses[0].get("PublicIp")
                            private_ip = nat_gateway_addresses[0].get("PrivateIp")

                        # Get CloudWatch metrics for bytes processed (last 30 days)
                        bytes_processed = 0.0
                        try:
                            end_time = datetime.utcnow()
                            start_time = end_time - timedelta(days=30)

                            # BytesOutToDestination metric
                            metrics_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/NATGateway",
                                MetricName="BytesOutToDestination",
                                Dimensions=[{"Name": "NatGatewayId", "Value": nat_gateway_id}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=86400 * 30,  # 30 days
                                Statistics=["Sum"],
                            )

                            datapoints = metrics_response.get("Datapoints", [])
                            if datapoints:
                                bytes_processed = datapoints[0].get("Sum", 0.0)
                        except Exception:
                            # CloudWatch metrics may not be available, continue with 0
                            pass

                        # Calculate data processed in GB
                        data_processed_gb = bytes_processed / (1024 ** 3)

                        # Calculate monthly cost
                        monthly_cost = self._calculate_aws_nat_monthly_cost(
                            data_processed_gb, region
                        )

                        # Determine utilization status
                        if state == "failed":
                            utilization_status = "idle"
                        elif state == "deleted" or state == "deleting":
                            utilization_status = "idle"
                        elif bytes_processed == 0:
                            utilization_status = "idle"
                        elif data_processed_gb < 10:
                            utilization_status = "low"
                        elif data_processed_gb < 100:
                            utilization_status = "medium"
                        else:
                            utilization_status = "high"

                        # Calculate optimization
                        (
                            is_optimizable,
                            optimization_score,
                            optimization_priority,
                            potential_savings,
                            recommendations,
                        ) = self._calculate_aws_nat_optimization(
                            nat,
                            bytes_processed,
                            monthly_cost,
                            state,
                        )

                        # Check if NAT Gateway is orphan (failed or no traffic)
                        is_orphan = state == "failed" or (
                            state == "available" and bytes_processed == 0
                        )

                        # Extract tags
                        tags = {}
                        for tag in nat.get("Tags", []):
                            tags[tag["Key"]] = tag["Value"]

                        # Extract name from tags
                        nat_gateway_name = tags.get("Name")

                        # Calculate network utilization in Mbps (approximate)
                        # bytes_processed is for 30 days, convert to Mbps
                        network_mbps = None
                        if bytes_processed > 0:
                            bytes_per_second = bytes_processed / (30 * 24 * 3600)
                            network_mbps = (bytes_per_second * 8) / (1024 * 1024)  # Convert to Mbps

                        # Create resource data
                        resource = AllCloudResourceData(
                            resource_type="nat_gateway",
                            resource_id=nat_gateway_id,
                            resource_name=nat_gateway_name,
                            region=region,
                            estimated_monthly_cost=monthly_cost,
                            resource_metadata={
                                "nat_gateway_id": nat_gateway_id,
                                "state": state,
                                "vpc_id": vpc_id,
                                "subnet_id": subnet_id,
                                "elastic_ip_count": elastic_ip_count,
                                "public_ip": public_ip,
                                "private_ip": private_ip,
                                "data_processed_gb": round(data_processed_gb, 2),
                                "bytes_processed": int(bytes_processed),
                            },
                            currency="USD",
                            utilization_status=utilization_status,
                            cpu_utilization_percent=None,
                            memory_utilization_percent=None,
                            storage_utilization_percent=None,
                            network_utilization_mbps=network_mbps,
                            is_optimizable=is_optimizable,
                            optimization_priority=optimization_priority,
                            optimization_score=optimization_score,
                            potential_monthly_savings=potential_savings,
                            optimization_recommendations=recommendations,
                            tags=tags,
                            resource_status=state,
                            is_orphan=is_orphan,
                            created_at_cloud=created_at,
                            last_used_at=None,
                        )

                        all_nat_gateways.append(resource)

                logger.info(
                    "inventory.scan_aws_nat_gateways_complete",
                    region=region,
                    total_nat_gateways=len(all_nat_gateways),
                )

        except Exception as e:
            logger.error(
                "inventory.scan_aws_nat_gateways_error",
                region=region,
                error=str(e),
                exc_info=True,
            )
            raise

        return all_nat_gateways

    async def scan_rds_instances(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL RDS instances for cost intelligence.

        Args:
            region: AWS region to scan

        Returns:
            List of all RDS instance resources
        """
        logger.info("inventory.scan_rds_start", region=region)
        all_rds: list[AllCloudResourceData] = []

        try:
            async with self.session.client("rds", region_name=region) as rds:
                response = await rds.describe_db_instances()

                for db_instance in response.get("DBInstances", []):
                    db_identifier = db_instance["DBInstanceIdentifier"]
                    db_instance_class = db_instance["DBInstanceClass"]
                    db_engine = db_instance["Engine"]
                    status = db_instance["DBInstanceStatus"]

                    # Get CloudWatch metrics
                    cpu_util = await self._get_rds_cpu_utilization(
                        db_identifier, region
                    )
                    connections = await self._get_rds_connections(
                        db_identifier, region
                    )

                    # Calculate monthly cost
                    monthly_cost = self._calculate_rds_monthly_cost(
                        db_instance_class, db_engine, status
                    )

                    # Determine utilization
                    utilization_status = self._determine_utilization_status(
                        cpu_util, status
                    )

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        optimization_priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_rds_optimization(
                        db_instance,
                        cpu_util,
                        connections,
                        monthly_cost,
                        status,
                    )

                    # Check if RDS is orphan (stopped or very low activity)
                    is_orphan = status == "stopped" or (
                        status == "available" and cpu_util < 1.0 and connections < 1
                    )

                    # Extract tags
                    tags = {}
                    tag_list = db_instance.get("TagList", [])
                    for tag in tag_list:
                        tags[tag["Key"]] = tag["Value"]

                    resource = AllCloudResourceData(
                        resource_type="rds_instance",
                        resource_id=db_identifier,
                        resource_name=db_identifier,
                        region=region,
                        estimated_monthly_cost=monthly_cost,
                        resource_metadata={
                            "db_instance_class": db_instance_class,
                            "engine": db_engine,
                            "engine_version": db_instance.get("EngineVersion"),
                            "status": status,
                            "allocated_storage": db_instance.get("AllocatedStorage"),
                            "storage_type": db_instance.get("StorageType"),
                            "multi_az": db_instance.get("MultiAZ", False),
                            "availability_zone": db_instance.get("AvailabilityZone"),
                            "instance_create_time": db_instance.get(
                                "InstanceCreateTime"
                            ).isoformat()
                            if db_instance.get("InstanceCreateTime")
                            else None,
                        },
                        currency="USD",
                        utilization_status=utilization_status,
                        cpu_utilization_percent=cpu_util,
                        network_utilization_mbps=None,
                        is_optimizable=is_optimizable,
                        optimization_priority=optimization_priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations,
                        tags=tags,
                        resource_status=status,
                        is_orphan=is_orphan,
                        created_at_cloud=db_instance.get("InstanceCreateTime"),
                        last_used_at=None,
                    )

                    all_rds.append(resource)

                logger.info(
                    "inventory.scan_rds_complete",
                    region=region,
                    total_rds=len(all_rds),
                )

        except Exception as e:
            logger.error(
                "inventory.scan_rds_error",
                region=region,
                error=str(e),
                exc_info=True,
            )
            raise

        return all_rds

    async def scan_s3_buckets(self) -> list[AllCloudResourceData]:
        """
        Scan ALL S3 buckets for cost intelligence.

        Note: S3 is global, so this is called once per account (not per region).

        Returns:
            List of all S3 bucket resources
        """
        logger.info("inventory.scan_s3_start")
        all_buckets: list[AllCloudResourceData] = []

        try:
            async with self.session.client("s3") as s3:
                response = await s3.list_buckets()

                for bucket in response.get("Buckets", []):
                    bucket_name = bucket["Name"]

                    # Get bucket region
                    try:
                        location_response = await s3.get_bucket_location(
                            Bucket=bucket_name
                        )
                        region = location_response.get("LocationConstraint") or "us-east-1"
                    except Exception:
                        region = "us-east-1"

                    # Get bucket size and object count (from CloudWatch metrics)
                    bucket_size_gb, object_count = await self._get_s3_bucket_size(
                        bucket_name, region
                    )

                    # Calculate monthly cost (storage + requests)
                    monthly_cost = self._calculate_s3_monthly_cost(
                        bucket_size_gb, region
                    )

                    # Determine if bucket is empty or rarely used
                    is_empty = bucket_size_gb < 0.001  # Less than 1 MB
                    utilization_status = "idle" if is_empty else "medium"

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        optimization_priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_s3_optimization(
                        bucket_name,
                        bucket_size_gb,
                        object_count,
                        monthly_cost,
                        region,
                    )

                    # Check if S3 bucket is orphan (empty for >90 days)
                    is_orphan = is_empty

                    # Get tags
                    tags = {}
                    try:
                        tag_response = await s3.get_bucket_tagging(Bucket=bucket_name)
                        for tag in tag_response.get("TagSet", []):
                            tags[tag["Key"]] = tag["Value"]
                    except Exception:
                        pass  # Bucket may not have tags

                    resource = AllCloudResourceData(
                        resource_type="s3_bucket",
                        resource_id=bucket_name,
                        resource_name=bucket_name,
                        region=region,
                        estimated_monthly_cost=monthly_cost,
                        resource_metadata={
                            "bucket_size_gb": bucket_size_gb,
                            "object_count": object_count,
                            "creation_date": bucket["CreationDate"].isoformat()
                            if bucket.get("CreationDate")
                            else None,
                        },
                        currency="USD",
                        utilization_status=utilization_status,
                        storage_utilization_percent=(
                            min(bucket_size_gb / 100, 100) if bucket_size_gb else 0
                        ),
                        is_optimizable=is_optimizable,
                        optimization_priority=optimization_priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations,
                        tags=tags,
                        resource_status="active",
                        is_orphan=is_orphan,
                        created_at_cloud=bucket.get("CreationDate"),
                        last_used_at=None,
                    )

                    all_buckets.append(resource)

                logger.info(
                    "inventory.scan_s3_complete",
                    total_buckets=len(all_buckets),
                )

        except Exception as e:
            logger.error(
                "inventory.scan_s3_error",
                error=str(e),
                exc_info=True,
            )
            raise

        return all_buckets

    # ========== Helper Methods ==========

    async def _get_cpu_utilization(
        self, instance_id: str, region: str
    ) -> float:
        """Get average CPU utilization from CloudWatch (last 14 days)."""
        try:
            async with self.session.client("cloudwatch", region_name=region) as cw:
                end_time = datetime.utcnow()
                start_time = end_time - timedelta(days=14)

                response = await cw.get_metric_statistics(
                    Namespace="AWS/EC2",
                    MetricName="CPUUtilization",
                    Dimensions=[{"Name": "InstanceId", "Value": instance_id}],
                    StartTime=start_time,
                    EndTime=end_time,
                    Period=86400,  # 1 day
                    Statistics=["Average"],
                )

                datapoints = response.get("Datapoints", [])
                if not datapoints:
                    return 0.0

                avg_cpu = sum(dp["Average"] for dp in datapoints) / len(datapoints)
                return round(avg_cpu, 2)

        except Exception as e:
            logger.warning(
                "cloudwatch.cpu_fetch_error",
                instance_id=instance_id,
                error=str(e),
            )
            return 0.0

    async def _get_network_in(
        self, instance_id: str, region: str
    ) -> float | None:
        """Get average network in (Mbps) from CloudWatch."""
        try:
            async with self.session.client("cloudwatch", region_name=region) as cw:
                end_time = datetime.utcnow()
                start_time = end_time - timedelta(days=14)

                response = await cw.get_metric_statistics(
                    Namespace="AWS/EC2",
                    MetricName="NetworkIn",
                    Dimensions=[{"Name": "InstanceId", "Value": instance_id}],
                    StartTime=start_time,
                    EndTime=end_time,
                    Period=86400,
                    Statistics=["Average"],
                )

                datapoints = response.get("Datapoints", [])
                if not datapoints:
                    return None

                # Convert bytes to Mbps (average over period)
                avg_bytes = sum(dp["Average"] for dp in datapoints) / len(datapoints)
                mbps = (avg_bytes * 8) / (1024 * 1024)  # bytes to Mbps
                return round(mbps, 2)

        except Exception:
            return None

    async def _get_rds_cpu_utilization(
        self, db_identifier: str, region: str
    ) -> float:
        """Get average RDS CPU utilization from CloudWatch."""
        try:
            async with self.session.client("cloudwatch", region_name=region) as cw:
                end_time = datetime.utcnow()
                start_time = end_time - timedelta(days=14)

                response = await cw.get_metric_statistics(
                    Namespace="AWS/RDS",
                    MetricName="CPUUtilization",
                    Dimensions=[{"Name": "DBInstanceIdentifier", "Value": db_identifier}],
                    StartTime=start_time,
                    EndTime=end_time,
                    Period=86400,
                    Statistics=["Average"],
                )

                datapoints = response.get("Datapoints", [])
                if not datapoints:
                    return 0.0

                avg_cpu = sum(dp["Average"] for dp in datapoints) / len(datapoints)
                return round(avg_cpu, 2)

        except Exception:
            return 0.0

    async def _get_rds_connections(
        self, db_identifier: str, region: str
    ) -> float:
        """Get average RDS database connections from CloudWatch."""
        try:
            async with self.session.client("cloudwatch", region_name=region) as cw:
                end_time = datetime.utcnow()
                start_time = end_time - timedelta(days=14)

                response = await cw.get_metric_statistics(
                    Namespace="AWS/RDS",
                    MetricName="DatabaseConnections",
                    Dimensions=[{"Name": "DBInstanceIdentifier", "Value": db_identifier}],
                    StartTime=start_time,
                    EndTime=end_time,
                    Period=86400,
                    Statistics=["Average"],
                )

                datapoints = response.get("Datapoints", [])
                if not datapoints:
                    return 0.0

                avg_conn = sum(dp["Average"] for dp in datapoints) / len(datapoints)
                return round(avg_conn, 2)

        except Exception:
            return 0.0

    async def _get_s3_bucket_size(
        self, bucket_name: str, region: str
    ) -> tuple[float, int]:
        """Get S3 bucket size (GB) and object count from CloudWatch."""
        try:
            async with self.session.client("cloudwatch", region_name=region) as cw:
                end_time = datetime.utcnow()
                start_time = end_time - timedelta(days=2)

                # Get bucket size
                size_response = await cw.get_metric_statistics(
                    Namespace="AWS/S3",
                    MetricName="BucketSizeBytes",
                    Dimensions=[
                        {"Name": "BucketName", "Value": bucket_name},
                        {"Name": "StorageType", "Value": "StandardStorage"},
                    ],
                    StartTime=start_time,
                    EndTime=end_time,
                    Period=86400,
                    Statistics=["Average"],
                )

                size_datapoints = size_response.get("Datapoints", [])
                size_bytes = size_datapoints[0]["Average"] if size_datapoints else 0
                size_gb = size_bytes / (1024**3)

                # Get object count
                count_response = await cw.get_metric_statistics(
                    Namespace="AWS/S3",
                    MetricName="NumberOfObjects",
                    Dimensions=[
                        {"Name": "BucketName", "Value": bucket_name},
                        {"Name": "StorageType", "Value": "AllStorageTypes"},
                    ],
                    StartTime=start_time,
                    EndTime=end_time,
                    Period=86400,
                    Statistics=["Average"],
                )

                count_datapoints = count_response.get("Datapoints", [])
                object_count = int(count_datapoints[0]["Average"]) if count_datapoints else 0

                return round(size_gb, 3), object_count

        except Exception:
            return 0.0, 0

    def _calculate_ec2_monthly_cost(
        self, instance_type: str, state: str
    ) -> float:
        """
        Calculate estimated monthly cost for EC2 instance.

        Simplified pricing - in production, use AWS Pricing API.
        """
        # Hardcoded prices per hour (us-east-1, on-demand)
        INSTANCE_PRICES = {
            "t2.micro": 0.0116,
            "t2.small": 0.023,
            "t2.medium": 0.0464,
            "t3.micro": 0.0104,
            "t3.small": 0.0208,
            "t3.medium": 0.0416,
            "m5.large": 0.096,
            "m5.xlarge": 0.192,
            "c5.large": 0.085,
            "r5.large": 0.126,
        }

        hourly_rate = INSTANCE_PRICES.get(instance_type, 0.10)  # Default $0.10/hr

        if state == "stopped":
            # Stopped instances still incur EBS costs (~$0.10/GB/month)
            # Estimate 30GB EBS volume
            return 30 * 0.10
        else:
            # Running instances: hourly rate * 730 hours/month
            return hourly_rate * 730

    def _calculate_rds_monthly_cost(
        self, db_instance_class: str, engine: str, status: str
    ) -> float:
        """Calculate estimated monthly cost for RDS instance."""
        # Simplified RDS pricing
        RDS_PRICES = {
            "db.t2.micro": 0.017,
            "db.t3.micro": 0.016,
            "db.t3.small": 0.032,
            "db.t3.medium": 0.064,
            "db.m5.large": 0.17,
            "db.r5.large": 0.24,
        }

        hourly_rate = RDS_PRICES.get(db_instance_class, 0.15)

        if status == "stopped":
            # Stopped RDS instances incur storage costs
            return 50 * 0.10  # Estimate 50GB storage
        else:
            return hourly_rate * 730

    def _calculate_s3_monthly_cost(self, size_gb: float, region: str) -> float:
        """Calculate estimated monthly cost for S3 bucket."""
        # S3 Standard pricing: $0.023/GB/month (first 50TB)
        storage_cost = size_gb * 0.023

        # Add request costs (estimate)
        request_cost = 1.0  # Flat $1/month estimate

        return storage_cost + request_cost

    def _determine_utilization_status(
        self, cpu_util: float | None, state: str
    ) -> str:
        """Determine utilization status based on CPU and state."""
        if state in ["stopped", "stopping"]:
            return "idle"
        elif cpu_util is None:
            return "unknown"
        elif cpu_util < 10:
            return "idle"
        elif cpu_util < 30:
            return "low"
        elif cpu_util < 70:
            return "medium"
        else:
            return "high"

    def _calculate_ec2_optimization(
        self,
        instance: dict[str, Any],
        cpu_util: float,
        monthly_cost: float,
        state: str,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Calculate EC2 optimization metrics.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        recommendations = []
        is_optimizable = False
        optimization_score = 0
        potential_savings = 0.0
        priority = "none"

        instance_type = instance["InstanceType"]

        # Scenario 1: Stopped instance (critical)
        if state == "stopped":
            is_optimizable = True
            optimization_score = 80
            priority = "critical"
            potential_savings = monthly_cost * 0.7  # Save 70% (only EBS remains)
            recommendations.append({
                "action": "Terminate or restart stopped instance",
                "details": f"Instance has been stopped. Terminate to save ${potential_savings:.2f}/month",
                "priority": "critical",
            })

        # Scenario 2: Very low CPU (<10%)
        elif cpu_util < 10:
            is_optimizable = True
            optimization_score = 60
            priority = "high"
            # Suggest downgrading to smaller instance
            potential_savings = monthly_cost * 0.5
            recommendations.append({
                "action": "Downgrade instance type",
                "details": f"CPU utilization is {cpu_util}%. Consider t3.micro or t3.small",
                "alternatives": [
                    {"name": "t3.micro", "cost": 7.60, "savings": monthly_cost - 7.60},
                    {"name": "t3.small", "cost": 15.20, "savings": monthly_cost - 15.20},
                ],
                "priority": "high",
            })

        # Scenario 3: Old generation instance
        elif instance_type.startswith(("t2.", "m4.", "c4.", "r4.")):
            is_optimizable = True
            optimization_score = 40
            priority = "medium"
            potential_savings = monthly_cost * 0.2  # 20% savings
            new_type = instance_type.replace("t2.", "t3.").replace("m4.", "m5.").replace("c4.", "c5.").replace("r4.", "r5.")
            recommendations.append({
                "action": "Upgrade to newer generation",
                "details": f"Migrate {instance_type}  {new_type} for better price/performance",
                "priority": "medium",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def _get_volume_read_ops(
        self, volume_id: str, region: str
    ) -> float:
        """Get average volume read operations from CloudWatch (last 14 days)."""
        try:
            async with self.session.client("cloudwatch", region_name=region) as cw:
                end_time = datetime.utcnow()
                start_time = end_time - timedelta(days=14)

                response = await cw.get_metric_statistics(
                    Namespace="AWS/EBS",
                    MetricName="VolumeReadOps",
                    Dimensions=[{"Name": "VolumeId", "Value": volume_id}],
                    StartTime=start_time,
                    EndTime=end_time,
                    Period=86400,  # 1 day
                    Statistics=["Sum"],
                )

                datapoints = response.get("Datapoints", [])
                if not datapoints:
                    return 0.0

                # Calculate average daily operations
                total_ops = sum(dp["Sum"] for dp in datapoints)
                avg_daily_ops = total_ops / len(datapoints) if datapoints else 0.0
                return round(avg_daily_ops, 2)

        except Exception as e:
            logger.warning(
                "cloudwatch.volume_read_ops_error",
                volume_id=volume_id,
                error=str(e),
            )
            return 0.0

    async def _get_volume_write_ops(
        self, volume_id: str, region: str
    ) -> float:
        """Get average volume write operations from CloudWatch (last 14 days)."""
        try:
            async with self.session.client("cloudwatch", region_name=region) as cw:
                end_time = datetime.utcnow()
                start_time = end_time - timedelta(days=14)

                response = await cw.get_metric_statistics(
                    Namespace="AWS/EBS",
                    MetricName="VolumeWriteOps",
                    Dimensions=[{"Name": "VolumeId", "Value": volume_id}],
                    StartTime=start_time,
                    EndTime=end_time,
                    Period=86400,  # 1 day
                    Statistics=["Sum"],
                )

                datapoints = response.get("Datapoints", [])
                if not datapoints:
                    return 0.0

                # Calculate average daily operations
                total_ops = sum(dp["Sum"] for dp in datapoints)
                avg_daily_ops = total_ops / len(datapoints) if datapoints else 0.0
                return round(avg_daily_ops, 2)

        except Exception as e:
            logger.warning(
                "cloudwatch.volume_write_ops_error",
                volume_id=volume_id,
                error=str(e),
            )
            return 0.0

    def _calculate_ebs_monthly_cost(
        self,
        volume_type: str,
        size_gb: int,
        iops: int | None,
        throughput: int | None,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for EBS volume.

        Uses dynamic pricing from pricing service with hardcoded fallback.
        Includes storage, IOPS, and throughput costs.

        Args:
            volume_type: Volume type (gp3, gp2, io2, io1, st1, sc1)
            size_gb: Volume size in GB
            iops: Provisioned IOPS (for io1/io2/gp3)
            throughput: Provisioned throughput in MBps (for gp3/st1)
            region: AWS region

        Returns:
            Estimated monthly cost in USD
        """
        # Base storage cost per GB/month (fallback prices)
        STORAGE_PRICES = {
            "gp3": 0.08,
            "gp2": 0.10,
            "io2": 0.125,
            "io1": 0.125,
            "st1": 0.045,
            "sc1": 0.015,
        }

        storage_price = STORAGE_PRICES.get(volume_type, 0.08)
        storage_cost = size_gb * storage_price

        # Additional IOPS cost (above baseline)
        iops_cost = 0.0
        if volume_type == "gp3" and iops and iops > 3000:
            # gp3: $0.005/IOPS/month above 3,000 baseline
            extra_iops = iops - 3000
            iops_cost = extra_iops * 0.005
        elif volume_type in ["io1", "io2"] and iops:
            # io1/io2: $0.065/IOPS/month
            iops_cost = iops * 0.065

        # Additional throughput cost (above baseline)
        throughput_cost = 0.0
        if volume_type == "gp3" and throughput and throughput > 125:
            # gp3: $0.04/MBps/month above 125 MBps baseline
            extra_throughput = throughput - 125
            throughput_cost = extra_throughput * 0.04

        total_cost = storage_cost + iops_cost + throughput_cost
        return round(total_cost, 2)

    def _calculate_ebs_optimization(
        self,
        volume: dict[str, Any],
        read_ops: float,
        write_ops: float,
        monthly_cost: float,
        state: str,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Calculate EBS volume optimization metrics.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        recommendations = []
        is_optimizable = False
        optimization_score = 0
        potential_savings = 0.0
        priority = "none"

        volume_type = volume.get("VolumeType", "gp3")
        size_gb = volume.get("Size", 0)
        iops = volume.get("Iops")
        attachments = volume.get("Attachments", [])

        # Scenario 1: Unattached volume (critical)
        if state == "available":
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost  # 100% savings
            recommendations.append({
                "action": "Delete unattached volume",
                "details": f"Volume is unattached. Create snapshot if needed, then delete to save ${potential_savings:.2f}/month",
                "priority": "critical",
            })

        # Scenario 2: Attached to stopped instance (high)
        elif attachments and read_ops == 0 and write_ops == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost * 0.9  # Assume snapshot cost is 10%
            recommendations.append({
                "action": "Snapshot and delete",
                "details": f"Volume has no I/O activity. Consider snapshot (${monthly_cost * 0.1:.2f}/month) and delete",
                "priority": "high",
            })

        # Scenario 3: gp2  gp3 migration opportunity (medium)
        elif volume_type == "gp2":
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # gp3 is ~20% cheaper than gp2
            gp3_cost = size_gb * 0.08
            potential_savings = monthly_cost - gp3_cost
            recommendations.append({
                "action": "Migrate gp2  gp3",
                "details": f"Migrate to gp3 for better price/performance. Save ~${potential_savings:.2f}/month",
                "alternatives": [
                    {
                        "name": "gp3",
                        "cost": gp3_cost,
                        "savings": potential_savings,
                        "performance": "Better (20% more baseline throughput)"
                    }
                ],
                "priority": "medium",
            })

        # Scenario 4: Overprovisioned IOPS (medium)
        elif volume_type in ["io1", "io2", "gp3"] and iops:
            # If total daily ops < 10% of provisioned IOPS, it's overprovisioned
            daily_ops = read_ops + write_ops
            if iops > 3000 and daily_ops < (iops * 0.1 * 86400):  # 86400 seconds/day
                is_optimizable = True
                optimization_score = 60
                priority = "medium"

                # Suggest reducing to 3000 IOPS (gp3 baseline)
                if volume_type == "gp3":
                    reduced_iops = 3000
                    iops_savings = (iops - reduced_iops) * 0.005
                else:  # io1/io2
                    reduced_iops = max(3000, int(iops * 0.5))
                    iops_savings = (iops - reduced_iops) * 0.065

                potential_savings = iops_savings
                recommendations.append({
                    "action": "Reduce provisioned IOPS",
                    "details": f"IOPS utilization <10%. Reduce {iops}  {reduced_iops} IOPS to save ${potential_savings:.2f}/month",
                    "priority": "medium",
                })

        # Scenario 5: io2  gp3 downgrade opportunity (low)
        elif volume_type in ["io1", "io2"] and daily_ops < 16000 * 86400:  # 16K IOPS baseline for gp3
            is_optimizable = True
            optimization_score = 40
            priority = "low"
            gp3_cost = size_gb * 0.08
            potential_savings = monthly_cost - gp3_cost
            if potential_savings > 0:
                recommendations.append({
                    "action": "Downgrade to gp3",
                    "details": f"Low IOPS usage. Consider gp3 (3K-16K IOPS) to save ${potential_savings:.2f}/month",
                    "priority": "low",
                })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    def _calculate_alb_monthly_cost(
        self,
        lb_type: str,
        region: str,
        data_processed_gb: float = 0.0,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS Load Balancer.

        AWS Load Balancer pricing:
        - ALB: $22.61/month base + $0.008/LCU-hour (~$5.76/month for 1 LCU-hour average)
        - NLB: $22.61/month base + $0.006/LCU-hour (~$4.32/month for 1 LCU-hour average)
        - CLB: $18.03/month base + $0.008/GB data processed

        Args:
            lb_type: Load balancer type ('application', 'network', 'classic')
            region: AWS region
            data_processed_gb: Data processed in GB (for CLB pricing)

        Returns:
            Estimated monthly cost in USD
        """
        # Base hourly rates * 730 hours/month
        if lb_type == "application":
            base_cost = 22.61
            # Simplified LCU cost: assume 1 LCU-hour average
            # 1 LCU-hour = $0.008/hour * 730 hours = $5.84/month
            lcu_cost = 5.84
            return base_cost + lcu_cost
        elif lb_type == "network":
            base_cost = 22.61
            # 1 LCU-hour for NLB = $0.006/hour * 730 hours = $4.38/month
            lcu_cost = 4.38
            return base_cost + lcu_cost
        elif lb_type == "classic":
            base_cost = 18.03
            # Data processing: $0.008/GB
            data_cost = data_processed_gb * 0.008
            return base_cost + data_cost
        else:
            # Default to ALB pricing
            return 28.45

    def _calculate_alb_optimization(
        self,
        lb: dict[str, Any],
        listener_count: int,
        target_group_count: int,
        healthy_target_count: int,
        total_target_count: int,
        monthly_cost: float,
        lb_type: str,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Calculate AWS Load Balancer optimization metrics.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        recommendations = []
        is_optimizable = False
        optimization_score = 0
        potential_savings = 0.0
        priority = "none"

        lb_name = lb.get("LoadBalancerName") or lb.get("Name", "Unknown")

        # Scenario 1: No listeners configured (Critical)
        if listener_count == 0:
            is_optimizable = True
            optimization_score = 95
            priority = "critical"
            potential_savings = monthly_cost  # 100% savings
            recommendations.append({
                "action": "Delete load balancer",
                "details": f"Load balancer has no listeners configured. Delete to save ${potential_savings:.2f}/month",
                "priority": "critical",
            })

        # Scenario 2: No target groups (Critical)
        elif target_group_count == 0 and lb_type != "classic":
            is_optimizable = True
            optimization_score = 95
            priority = "critical"
            potential_savings = monthly_cost  # 100% savings
            recommendations.append({
                "action": "Delete load balancer",
                "details": f"Load balancer has no target groups. No backends configured. Delete to save ${potential_savings:.2f}/month",
                "priority": "critical",
            })

        # Scenario 3: All targets unhealthy (High)
        elif total_target_count > 0 and healthy_target_count == 0:
            is_optimizable = True
            optimization_score = 85
            priority = "high"
            potential_savings = monthly_cost * 0.9  # Assume 90% savings if deleted
            recommendations.append({
                "action": "Fix or delete load balancer",
                "details": f"All {total_target_count} targets are unhealthy. Fix targets or delete LB to save ~${potential_savings:.2f}/month",
                "priority": "high",
            })

        # Scenario 4: No targets at all for CLB (High)
        elif lb_type == "classic" and total_target_count == 0:
            is_optimizable = True
            optimization_score = 90
            priority = "high"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete Classic Load Balancer",
                "details": f"CLB has no registered instances. Delete to save ${potential_savings:.2f}/month",
                "priority": "high",
            })

        # Scenario 5: CLB migration opportunity (Medium)
        elif lb_type == "classic" and healthy_target_count > 0:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # ALB costs ~$4/month more but offers better features
            alb_cost = 28.45
            potential_savings = -(alb_cost - monthly_cost)  # Negative = additional cost
            recommendations.append({
                "action": "Migrate Classic LB to Application LB",
                "details": f"Upgrade to ALB for better features (HTTP/2, WebSocket, path-based routing). Additional cost: ${abs(potential_savings):.2f}/month",
                "alternatives": [
                    {
                        "name": "Application Load Balancer",
                        "cost": alb_cost,
                        "savings": potential_savings,
                        "benefits": "HTTP/2, WebSocket, path-based routing, host-based routing, better health checks"
                    }
                ],
                "priority": "medium",
            })

        # Scenario 6: Very low utilization (Low)
        # This would require CloudWatch metrics, which we can add later
        # For now, we focus on structural issues

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    def _calculate_ebs_snapshot_monthly_cost(
        self,
        size_gb: int,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for EBS snapshot.

        AWS EBS Snapshot pricing: $0.05/GB/month (standard snapshot storage)

        Args:
            size_gb: Snapshot size in GB
            region: AWS region

        Returns:
            Estimated monthly cost in USD
        """
        # EBS snapshot pricing is simple: $0.05/GB/month
        return size_gb * 0.05

    def _calculate_aws_nat_monthly_cost(
        self,
        data_processed_gb: float,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS NAT Gateway.

        AWS NAT Gateway pricing:
        - Base: $32.40/month ($0.045/hour * 720 hours)
        - Data processing: $0.045/GB

        Args:
            data_processed_gb: Data processed per month in GB
            region: AWS region

        Returns:
            Estimated monthly cost in USD
        """
        base_cost = 32.40  # Base hourly cost * 720 hours/month
        data_cost = data_processed_gb * 0.045  # $0.045/GB
        return base_cost + data_cost

    def _calculate_aws_nat_optimization(
        self,
        nat_gateway: dict[str, Any],
        bytes_processed: float,
        monthly_cost: float,
        state: str,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Calculate AWS NAT Gateway optimization metrics.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        recommendations = []
        is_optimizable = False
        optimization_score = 0
        potential_savings = 0.0
        priority = "none"

        nat_gateway_id = nat_gateway.get("NatGatewayId", "Unknown")
        data_processed_gb = bytes_processed / (1024 ** 3)  # Convert bytes to GB

        # Scenario 1: NAT Gateway in failed state (Critical)
        if state == "failed":
            is_optimizable = True
            optimization_score = 95
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete failed NAT Gateway",
                "details": f"NAT Gateway is in failed state. Delete to save ${potential_savings:.2f}/month",
                "priority": "critical",
            })

        # Scenario 2: Zero traffic for 30+ days (High)
        elif bytes_processed == 0:
            is_optimizable = True
            optimization_score = 85
            priority = "high"
            potential_savings = monthly_cost * 0.9  # Assume 90% savings
            recommendations.append({
                "action": "Delete unused NAT Gateway",
                "details": f"NAT Gateway has no traffic. Delete to save ~${potential_savings:.2f}/month",
                "priority": "high",
            })

        # Scenario 3: Very low traffic (<10GB/month) (Medium)
        elif data_processed_gb < 10:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Suggest using instance with public IP instead
            potential_savings = 32.40  # Save base cost only
            recommendations.append({
                "action": "Consider using EC2 instance with public IP",
                "details": f"Very low traffic ({data_processed_gb:.2f}GB/month). EC2 with public IP may be more cost-effective. Save ~${potential_savings:.2f}/month",
                "alternatives": [
                    {
                        "name": "EC2 with Public IP",
                        "cost": data_processed_gb * 0.01,  # Approximate data transfer cost
                        "savings": potential_savings,
                    }
                ],
                "priority": "medium",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    def _calculate_rds_optimization(
        self,
        db_instance: dict[str, Any],
        cpu_util: float,
        connections: float,
        monthly_cost: float,
        status: str,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """Calculate RDS optimization metrics."""
        recommendations = []
        is_optimizable = False
        optimization_score = 0
        potential_savings = 0.0
        priority = "none"

        # Scenario 1: Stopped RDS
        if status == "stopped":
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost * 0.8
            recommendations.append({
                "action": "Delete or restart stopped RDS instance",
                "details": f"RDS instance is stopped. Terminate to save ${potential_savings:.2f}/month",
                "priority": "critical",
            })

        # Scenario 2: Very low activity
        elif cpu_util < 5 and connections < 2:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = monthly_cost * 0.5
            recommendations.append({
                "action": "Downgrade RDS instance or move to Aurora Serverless",
                "details": f"CPU: {cpu_util}%, Connections: {connections}. Consider smaller instance or serverless",
                "priority": "high",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    def _calculate_s3_optimization(
        self,
        bucket_name: str,
        bucket_size_gb: float,
        object_count: int,
        monthly_cost: float,
        region: str,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """Calculate S3 optimization metrics."""
        recommendations = []
        is_optimizable = False
        optimization_score = 0
        potential_savings = 0.0
        priority = "none"

        # Scenario 1: Empty bucket
        if bucket_size_gb < 0.001:
            is_optimizable = True
            optimization_score = 50
            priority = "low"
            potential_savings = 1.0  # Request costs
            recommendations.append({
                "action": "Delete empty S3 bucket",
                "details": f"Bucket is empty. Delete to save ${potential_savings:.2f}/month",
                "priority": "low",
            })

        # Scenario 2: Large bucket on Standard storage
        elif bucket_size_gb > 1000:  # > 1TB
            is_optimizable = True
            optimization_score = 30
            priority = "medium"
            # Suggest Glacier for archival
            potential_savings = bucket_size_gb * (0.023 - 0.004)  # Standard  Glacier
            recommendations.append({
                "action": "Move to S3 Glacier for archival data",
                "details": f"Bucket size: {bucket_size_gb:.1f}GB. Move infrequently accessed data to Glacier",
                "alternatives": [
                    {"name": "S3 Glacier", "cost": bucket_size_gb * 0.004, "savings": potential_savings},
                ],
                "priority": "medium",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations


class AzureInventoryScanner:
    """Azure-specific inventory scanner for cost intelligence."""

    def __init__(self, provider: Any) -> None:
        """
        Initialize Azure inventory scanner.

        Args:
            provider: Azure provider instance with authenticated credentials
        """
        self.provider = provider
        self.tenant_id = provider.tenant_id
        self.client_id = provider.client_id
        self.client_secret = provider.client_secret
        self.subscription_id = provider.subscription_id
        self.regions = provider.regions or []
        self.resource_groups = provider.resource_groups or []
        self.logger = structlog.get_logger()

    async def scan_virtual_machines(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Virtual Machines for cost intelligence.

        Unlike orphan detection, this returns ALL VMs (running, stopped, deallocated)
        with utilization metrics and optimization recommendations.

        Args:
            region: Azure region to scan

        Returns:
            List of all VM resources
        """
        logger.info("inventory.scan_azure_vms_start", region=region)
        all_vms: list[AllCloudResourceData] = []

        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.compute import ComputeManagementClient

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            compute_client = ComputeManagementClient(credential, self.subscription_id)

            # Get ALL VMs
            vms = list(compute_client.virtual_machines.list_all())

            for vm in vms:
                # Filter by region
                if vm.location != region:
                    continue

                # Filter by resource group (if specified)
                if not self.provider._is_resource_in_scope(vm.id):
                    continue

                # Extract resource group name
                resource_group = vm.id.split('/')[4]

                # Get instance view for power state
                instance_view = compute_client.virtual_machines.instance_view(
                    resource_group_name=resource_group,
                    vm_name=vm.name
                )

                # Determine power state
                power_state = "unknown"
                for status in instance_view.statuses:
                    if status.code and status.code.startswith('PowerState/'):
                        power_state = status.code.split('/')[-1]

                # Extract tags
                tags = vm.tags if vm.tags else {}

                # Get VM metrics (CPU utilization from Azure Monitor)
                cpu_util = await self._get_vm_cpu_utilization(
                    resource_group, vm.name, region
                )
                network_in = await self._get_vm_network_in(
                    resource_group, vm.name, region
                )

                # Calculate monthly cost (VM + disks)
                monthly_cost = self._calculate_vm_monthly_cost(
                    vm.hardware_profile.vm_size if vm.hardware_profile else "Standard_D2s_v3",
                    power_state,
                    vm.storage_profile if vm.storage_profile else None,
                    compute_client,
                    resource_group
                )

                # Determine utilization status
                utilization_status = self._determine_vm_utilization_status(
                    cpu_util, power_state
                )

                # Calculate optimization
                (
                    is_optimizable,
                    optimization_score,
                    optimization_priority,
                    potential_savings,
                    recommendations,
                ) = self._calculate_vm_optimization(
                    vm,
                    power_state,
                    cpu_util,
                    monthly_cost,
                )

                # Check if VM is orphan (deallocated or very low CPU)
                is_orphan = power_state == "deallocated" or (
                    power_state == "running" and cpu_util < 5.0
                )

                # Create resource data
                resource = AllCloudResourceData(
                    resource_type="azure_vm",
                    resource_id=vm.id,
                    resource_name=vm.name,
                    region=region,
                    estimated_monthly_cost=monthly_cost,
                    resource_metadata={
                        "vm_size": vm.hardware_profile.vm_size if vm.hardware_profile else None,
                        "power_state": power_state,
                        "os_type": vm.storage_profile.os_disk.os_type if vm.storage_profile and vm.storage_profile.os_disk else None,
                        "resource_group": resource_group,
                        "availability_zone": vm.zones[0] if vm.zones else None,
                    },
                    currency="USD",
                    utilization_status=utilization_status,
                    cpu_utilization_percent=cpu_util,
                    memory_utilization_percent=None,  # TODO: Fetch from Azure Monitor
                    network_utilization_mbps=network_in,
                    is_optimizable=is_optimizable,
                    optimization_priority=optimization_priority,
                    optimization_score=optimization_score,
                    potential_monthly_savings=potential_savings,
                    optimization_recommendations=recommendations,
                    tags=tags,
                    resource_status=power_state,
                    is_orphan=is_orphan,
                    created_at_cloud=None,  # Azure doesn't provide creation time easily
                    last_used_at=None,
                )

                all_vms.append(resource)

            logger.info(
                "inventory.scan_azure_vms_complete",
                region=region,
                total_vms=len(all_vms),
            )

        except Exception as e:
            logger.error(
                "inventory.scan_azure_vms_error",
                region=region,
                error=str(e),
                exc_info=True,
            )
            raise

        return all_vms

    async def scan_managed_disks(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Managed Disks for cost intelligence.

        Args:
            region: Azure region to scan

        Returns:
            List of all disk resources
        """
        logger.info("inventory.scan_azure_disks_start", region=region)
        all_disks: list[AllCloudResourceData] = []

        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.compute import ComputeManagementClient

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            compute_client = ComputeManagementClient(credential, self.subscription_id)

            # Get ALL disks
            disks = list(compute_client.disks.list())

            for disk in disks:
                # Filter by region
                if disk.location != region:
                    continue

                # Filter by resource group
                if not self.provider._is_resource_in_scope(disk.id):
                    continue

                # Determine if attached or unattached
                is_attached = disk.managed_by is not None
                disk_state = "attached" if is_attached else "unattached"

                # Extract tags
                tags = disk.tags if disk.tags else {}

                # Calculate monthly cost using provider's helper
                monthly_cost = self.provider._calculate_disk_cost(disk)

                # Determine utilization status
                utilization_status = "idle" if not is_attached else "active"

                # Calculate optimization
                (
                    is_optimizable,
                    optimization_score,
                    optimization_priority,
                    potential_savings,
                    recommendations,
                ) = self._calculate_disk_optimization(
                    disk,
                    is_attached,
                    monthly_cost,
                )

                # Check if disk is orphan (unattached)
                is_orphan = not is_attached

                # Create resource data
                resource = AllCloudResourceData(
                    resource_type="azure_managed_disk",
                    resource_id=disk.id,
                    resource_name=disk.name,
                    region=region,
                    estimated_monthly_cost=monthly_cost,
                    resource_metadata={
                        "disk_size_gb": disk.disk_size_gb,
                        "disk_state": disk_state,
                        "sku_name": disk.sku.name if disk.sku else None,
                        "sku_tier": disk.sku.tier if disk.sku else None,
                        "disk_iops": getattr(disk, 'disk_iops_read_write', None),
                        "disk_mbps": getattr(disk, 'disk_mbps_read_write', None),
                        "encryption_type": disk.encryption.type if disk.encryption else None,
                    },
                    currency="USD",
                    utilization_status=utilization_status,
                    cpu_utilization_percent=None,
                    memory_utilization_percent=None,
                    storage_utilization_percent=None,  # TODO: Calculate from IOPS metrics
                    network_utilization_mbps=None,
                    is_optimizable=is_optimizable,
                    optimization_priority=optimization_priority,
                    optimization_score=optimization_score,
                    potential_monthly_savings=potential_savings,
                    optimization_recommendations=recommendations,
                    tags=tags,
                    resource_status=disk_state,
                    is_orphan=is_orphan,
                    created_at_cloud=disk.time_created.replace(tzinfo=None) if hasattr(disk, 'time_created') and disk.time_created else None,
                    last_used_at=None,
                )

                all_disks.append(resource)

            logger.info(
                "inventory.scan_azure_disks_complete",
                region=region,
                total_disks=len(all_disks),
            )

        except Exception as e:
            logger.error(
                "inventory.scan_azure_disks_error",
                region=region,
                error=str(e),
                exc_info=True,
            )
            raise

        return all_disks

    async def scan_public_ips(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Public IPs for cost intelligence.

        Args:
            region: Azure region to scan

        Returns:
            List of all public IP resources
        """
        logger.info("inventory.scan_azure_ips_start", region=region)
        all_ips: list[AllCloudResourceData] = []

        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.network import NetworkManagementClient

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            network_client = NetworkManagementClient(credential, self.subscription_id)

            # Get ALL public IPs
            public_ips = list(network_client.public_ip_addresses.list_all())

            for ip in public_ips:
                # Filter by region
                if ip.location != region:
                    continue

                # Filter by resource group
                if not self.provider._is_resource_in_scope(ip.id):
                    continue

                # Determine if assigned or unassigned
                is_assigned = ip.ip_configuration is not None
                ip_state = "assigned" if is_assigned else "unassigned"

                # Extract tags
                tags = ip.tags if ip.tags else {}

                # Calculate monthly cost
                sku_name = ip.sku.name if ip.sku else "Basic"
                monthly_cost = 3.65 if sku_name == "Basic" else 4.00  # Standard SKU costs more

                # Determine utilization status
                utilization_status = "active" if is_assigned else "idle"

                # Calculate optimization
                (
                    is_optimizable,
                    optimization_score,
                    optimization_priority,
                    potential_savings,
                    recommendations,
                ) = self._calculate_ip_optimization(
                    ip,
                    is_assigned,
                    sku_name,
                    monthly_cost,
                )

                # Check if IP is orphan (unassigned)
                is_orphan = not is_assigned

                # Create resource data
                resource = AllCloudResourceData(
                    resource_type="azure_public_ip",
                    resource_id=ip.id,
                    resource_name=ip.name,
                    region=region,
                    estimated_monthly_cost=monthly_cost,
                    resource_metadata={
                        "ip_address": ip.ip_address,
                        "ip_state": ip_state,
                        "sku_name": sku_name,
                        "allocation_method": safe_get_value(ip.public_ip_allocation_method),
                        "ip_version": safe_get_value(ip.public_ip_address_version),
                        "idle_timeout_minutes": ip.idle_timeout_in_minutes,
                    },
                    currency="USD",
                    utilization_status=utilization_status,
                    cpu_utilization_percent=None,
                    memory_utilization_percent=None,
                    storage_utilization_percent=None,
                    network_utilization_mbps=None,  # TODO: Fetch from Azure Monitor
                    is_optimizable=is_optimizable,
                    optimization_priority=optimization_priority,
                    optimization_score=optimization_score,
                    potential_monthly_savings=potential_savings,
                    optimization_recommendations=recommendations,
                    tags=tags,
                    resource_status=ip_state,
                    is_orphan=is_orphan,
                    created_at_cloud=None,
                    last_used_at=None,
                )

                all_ips.append(resource)

            logger.info(
                "inventory.scan_azure_ips_complete",
                region=region,
                total_ips=len(all_ips),
            )

        except Exception as e:
            logger.error(
                "inventory.scan_azure_ips_error",
                region=region,
                error=str(e),
                exc_info=True,
            )
            raise

        return all_ips

    # Helper methods for Azure Monitor metrics

    async def _get_vm_cpu_utilization(
        self, resource_group: str, vm_name: str, region: str
    ) -> float:
        """
        Get CPU utilization percentage from Azure Monitor (last 14 days average).

        Args:
            resource_group: Resource group name
            vm_name: VM name
            region: Azure region

        Returns:
            Average CPU utilization percentage (0-100)
        """
        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.monitor import MonitorManagementClient
            from datetime import datetime, timedelta

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            monitor_client = MonitorManagementClient(credential, self.subscription_id)

            # Build resource ID
            resource_id = f"/subscriptions/{self.subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.Compute/virtualMachines/{vm_name}"

            # Get metrics for last 14 days
            end_time = datetime.utcnow()
            start_time = end_time - timedelta(days=14)

            metrics_data = monitor_client.metrics.list(
                resource_id,
                timespan=f"{start_time.isoformat()}/{end_time.isoformat()}",
                interval='PT1H',  # 1 hour granularity
                metricnames='Percentage CPU',
                aggregation='Average'
            )

            # Calculate average
            total_cpu = 0.0
            count = 0
            for metric in metrics_data.value:
                for timeseries in metric.timeseries:
                    for data in timeseries.data:
                        if data.average is not None:
                            total_cpu += data.average
                            count += 1

            return total_cpu / count if count > 0 else 0.0

        except Exception as e:
            logger.warning(
                "azure.monitor.cpu_error",
                vm_name=vm_name,
                error=str(e),
            )
            return 0.0  # Default to 0 if metrics unavailable

    async def _get_vm_network_in(
        self, resource_group: str, vm_name: str, region: str
    ) -> float:
        """
        Get network in (MB) from Azure Monitor (last 14 days average).

        Args:
            resource_group: Resource group name
            vm_name: VM name
            region: Azure region

        Returns:
            Average network in MB/s
        """
        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.monitor import MonitorManagementClient
            from datetime import datetime, timedelta

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            monitor_client = MonitorManagementClient(credential, self.subscription_id)

            # Build resource ID
            resource_id = f"/subscriptions/{self.subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.Compute/virtualMachines/{vm_name}"

            # Get metrics for last 14 days
            end_time = datetime.utcnow()
            start_time = end_time - timedelta(days=14)

            metrics_data = monitor_client.metrics.list(
                resource_id,
                timespan=f"{start_time.isoformat()}/{end_time.isoformat()}",
                interval='PT1H',
                metricnames='Network In Total',
                aggregation='Average'
            )

            # Calculate average (convert from bytes to MB/s)
            total_network = 0.0
            count = 0
            for metric in metrics_data.value:
                for timeseries in metric.timeseries:
                    for data in timeseries.data:
                        if data.average is not None:
                            total_network += data.average / (1024 * 1024)  # bytes to MB
                            count += 1

            return total_network / count if count > 0 else 0.0

        except Exception as e:
            logger.warning(
                "azure.monitor.network_error",
                vm_name=vm_name,
                error=str(e),
            )
            return 0.0

    # Helper methods for cost calculation

    def _calculate_vm_monthly_cost(
        self,
        vm_size: str,
        power_state: str,
        storage_profile: Any,
        compute_client: Any,
        resource_group: str,
    ) -> float:
        """
        Calculate monthly cost for Azure VM (VM compute + disks).

        Args:
            vm_size: VM size (e.g., Standard_D2s_v3)
            power_state: VM power state
            storage_profile: VM storage profile
            compute_client: Compute management client
            resource_group: Resource group name

        Returns:
            Estimated monthly cost in USD
        """
        # Hardcoded VM pricing (simplified, can be improved with Azure Retail Prices API)
        vm_pricing = {
            "Standard_B1s": 7.59,
            "Standard_B1ms": 15.18,
            "Standard_B2s": 30.37,
            "Standard_B2ms": 60.74,
            "Standard_D2s_v3": 70.08,
            "Standard_D4s_v3": 140.16,
            "Standard_D8s_v3": 280.32,
            "Standard_D16s_v3": 560.64,
            "Standard_E2s_v3": 88.32,
            "Standard_E4s_v3": 176.64,
            "Standard_F2s_v2": 59.20,
            "Standard_F4s_v2": 118.40,
        }

        # If deallocated, compute cost is $0 (but disks still charge)
        vm_cost = 0.0 if power_state == "deallocated" else vm_pricing.get(vm_size, 70.0)

        # Add disk costs
        disk_cost = 0.0
        if storage_profile:
            # OS disk
            if storage_profile.os_disk and storage_profile.os_disk.managed_disk:
                try:
                    os_disk_id = storage_profile.os_disk.managed_disk.id
                    os_disk_name = os_disk_id.split('/')[-1]
                    os_disk_rg = os_disk_id.split('/')[4]
                    os_disk = compute_client.disks.get(os_disk_rg, os_disk_name)
                    disk_cost += self.provider._calculate_disk_cost(os_disk)
                except Exception:
                    pass

            # Data disks
            if storage_profile.data_disks:
                for data_disk in storage_profile.data_disks:
                    if data_disk.managed_disk:
                        try:
                            disk_id = data_disk.managed_disk.id
                            disk_name = disk_id.split('/')[-1]
                            disk_rg = disk_id.split('/')[4]
                            disk = compute_client.disks.get(disk_rg, disk_name)
                            disk_cost += self.provider._calculate_disk_cost(disk)
                        except Exception:
                            pass

        return vm_cost + disk_cost

    def _determine_vm_utilization_status(self, cpu_util: float, power_state: str) -> str:
        """
        Determine VM utilization status.

        Args:
            cpu_util: CPU utilization percentage
            power_state: VM power state

        Returns:
            Utilization status string
        """
        if power_state == "deallocated":
            return "idle"
        elif cpu_util < 5.0:
            return "underutilized"
        elif cpu_util < 30.0:
            return "low_usage"
        elif cpu_util < 70.0:
            return "moderate_usage"
        else:
            return "high_usage"

    # Helper methods for optimization calculation

    def _calculate_vm_optimization(
        self,
        vm: Any,
        power_state: str,
        cpu_util: float,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Calculate VM optimization score and recommendations.

        IMPORTANT: Orphan resources (deallocated VMs or very low CPU <5%) are NOT "optimizable".
        They are WASTE that should be deleted, not optimized.
        Only non-orphan resources can be optimizable (e.g., low CPU 5-30%).

        Args:
            vm: Azure VM object
            power_state: VM power state
            cpu_util: Average CPU utilization
            monthly_cost: Monthly cost

        Returns:
            Tuple of (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: Deallocated VM  This is an ORPHAN (waste), not optimizable
        # It should be deleted, not optimized. Return no optimization.
        if power_state == "deallocated":
            return False, 0, "none", 0.0, []

        # Scenario 2: Running VM with very low CPU (<5%)  This is also an ORPHAN (waste), not optimizable
        # It should be stopped/deleted, not optimized. Return no optimization.
        elif power_state == "running" and cpu_util < 5.0:
            return False, 0, "none", 0.0, []

        # Scenario 3: Running VM with low CPU (5-30%)  This IS optimizable!
        # These VMs are being used but could be downsized to save costs.
        elif power_state == "running" and cpu_util < 30.0:
            is_optimizable = True
            optimization_score = 40
            priority = "medium"
            potential_savings = monthly_cost * 0.3
            recommendations.append({
                "action": "Consider downsizing this VM",
                "details": f"CPU utilization is {cpu_util:.1f}%. You may be able to use a smaller VM size.",
                "alternatives": [
                    {"name": "Downsize to smaller SKU", "cost": monthly_cost * 0.7, "savings": monthly_cost * 0.3},
                ],
                "priority": "medium",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    def _calculate_disk_optimization(
        self,
        disk: Any,
        is_attached: bool,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Calculate disk optimization score and recommendations.

        Args:
            disk: Azure disk object
            is_attached: Whether disk is attached
            monthly_cost: Monthly cost

        Returns:
            Tuple of (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: Unattached disk (Critical waste)
        if not is_attached:
            is_optimizable = True
            optimization_score = 85
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete this unattached disk",
                "details": "Disk is not attached to any VM. Consider deleting if no longer needed.",
                "alternatives": [
                    {"name": "Delete disk", "cost": 0, "savings": monthly_cost},
                    {"name": "Create snapshot and delete", "cost": monthly_cost * 0.1, "savings": monthly_cost * 0.9},
                ],
                "priority": "critical",
            })

        # Scenario 2: Premium disk when Standard SSD might suffice
        elif is_attached and disk.sku and disk.sku.name.startswith("Premium"):
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = monthly_cost * 0.4
            recommendations.append({
                "action": "Consider downgrading to Standard SSD",
                "details": "Using Premium SSD. Evaluate if Standard SSD performance is sufficient.",
                "alternatives": [
                    {"name": "Downgrade to Standard SSD", "cost": monthly_cost * 0.6, "savings": monthly_cost * 0.4},
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    def _calculate_ip_optimization(
        self,
        ip: Any,
        is_assigned: bool,
        sku_name: str,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Calculate public IP optimization score and recommendations.

        Args:
            ip: Azure public IP object
            is_assigned: Whether IP is assigned
            sku_name: SKU name (Basic or Standard)
            monthly_cost: Monthly cost

        Returns:
            Tuple of (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: Unassigned IP (Critical waste)
        if not is_assigned:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Release this unassigned public IP",
                "details": "Public IP is not assigned to any resource.",
                "alternatives": [
                    {"name": "Release IP", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "critical",
            })

        # Scenario 2: Standard SKU when Basic might suffice
        elif is_assigned and sku_name == "Standard":
            is_optimizable = True
            optimization_score = 20
            priority = "low"
            potential_savings = 0.35  # Difference between Standard and Basic
            recommendations.append({
                "action": "Consider using Basic SKU public IP",
                "details": "Using Standard SKU. Evaluate if Basic SKU is sufficient for your needs.",
                "alternatives": [
                    {"name": "Downgrade to Basic SKU", "cost": 3.65, "savings": 0.35},
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_load_balancers(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Load Balancers for cost intelligence.

        Args:
            region: Azure region to scan

        Returns:
            List of all load balancer resources
        """
        logger.info("inventory.scan_azure_lbs_start", region=region)
        all_lbs: list[AllCloudResourceData] = []

        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.network import NetworkManagementClient

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            network_client = NetworkManagementClient(credential, self.subscription_id)

            # Get ALL load balancers
            load_balancers = list(network_client.load_balancers.list_all())

            for lb in load_balancers:
                # Filter by region
                if lb.location != region:
                    continue

                # Filter by resource group
                if not self.provider._is_resource_in_scope(lb.id):
                    continue

                # Extract resource group
                resource_group = lb.id.split('/')[4]

                # Determine if load balancer is being used
                has_backend_pools = lb.backend_address_pools and len(lb.backend_address_pools) > 0
                has_probes = lb.probes and len(lb.probes) > 0
                has_rules = lb.load_balancing_rules and len(lb.load_balancing_rules) > 0

                # Extract tags
                tags = lb.tags if lb.tags else {}

                # Calculate monthly cost
                sku_name = lb.sku.name if lb.sku else "Basic"
                monthly_cost = 25.55 if sku_name == "Standard" else 18.25

                # Determine utilization status
                if not has_backend_pools and not has_probes:
                    utilization_status = "idle"
                elif not has_rules:
                    utilization_status = "underutilized"
                else:
                    utilization_status = "active"

                # Calculate optimization
                (
                    is_optimizable,
                    optimization_score,
                    optimization_priority,
                    potential_savings,
                    recommendations,
                ) = self._calculate_lb_optimization(
                    lb,
                    has_backend_pools,
                    has_probes,
                    has_rules,
                    sku_name,
                    monthly_cost,
                )

                # Check if LB is orphan (no backend pools)
                is_orphan = not has_backend_pools

                # Create resource data
                resource = AllCloudResourceData(
                    resource_type="azure_load_balancer",
                    resource_id=lb.id,
                    resource_name=lb.name,
                    region=region,
                    estimated_monthly_cost=monthly_cost,
                    resource_metadata={
                        "sku_name": sku_name,
                        "resource_group": resource_group,
                        "backend_pool_count": len(lb.backend_address_pools) if lb.backend_address_pools else 0,
                        "probe_count": len(lb.probes) if lb.probes else 0,
                        "rule_count": len(lb.load_balancing_rules) if lb.load_balancing_rules else 0,
                        "frontend_ip_count": len(lb.frontend_ip_configurations) if lb.frontend_ip_configurations else 0,
                    },
                    currency="USD",
                    utilization_status=utilization_status,
                    cpu_utilization_percent=None,
                    memory_utilization_percent=None,
                    storage_utilization_percent=None,
                    network_utilization_mbps=None,
                    is_optimizable=is_optimizable,
                    optimization_priority=optimization_priority,
                    optimization_score=optimization_score,
                    potential_monthly_savings=potential_savings,
                    optimization_recommendations=recommendations,
                    tags=tags,
                    resource_status="active",
                    is_orphan=is_orphan,
                    created_at_cloud=None,
                    last_used_at=None,
                )

                all_lbs.append(resource)

            logger.info(
                "inventory.scan_azure_lbs_complete",
                region=region,
                total_lbs=len(all_lbs),
            )

        except Exception as e:
            logger.error(
                "inventory.scan_azure_lbs_error",
                region=region,
                error=str(e),
                exc_info=True,
            )
            raise

        return all_lbs

    def _calculate_lb_optimization(
        self,
        lb: Any,
        has_backend_pools: bool,
        has_probes: bool,
        has_rules: bool,
        sku_name: str,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Calculate load balancer optimization score and recommendations.

        Args:
            lb: Azure load balancer object
            has_backend_pools: Whether LB has backend pools
            has_probes: Whether LB has health probes
            has_rules: Whether LB has load balancing rules
            sku_name: SKU name (Basic or Standard)
            monthly_cost: Monthly cost

        Returns:
            Tuple of (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: No backend pools (Critical waste)
        if not has_backend_pools:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete unused load balancer",
                "details": "Load balancer has no backend pools configured. Not serving any traffic.",
                "alternatives": [
                    {"name": "Delete load balancer", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "critical",
            })

        # Scenario 2: No health probes (High priority)
        elif not has_probes:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete load balancer without health probes",
                "details": "Load balancer has backend pools but no health probes. Likely misconfigured or unused.",
                "alternatives": [
                    {"name": "Delete load balancer", "cost": 0, "savings": monthly_cost},
                    {"name": "Configure health probes", "cost": monthly_cost, "savings": 0},
                ],
                "priority": "high",
            })

        # Scenario 3: Standard SKU with low traffic
        elif sku_name == "Standard" and has_backend_pools and has_rules:
            is_optimizable = True
            optimization_score = 40
            priority = "medium"
            potential_savings = 7.30  # Difference between Standard and Basic
            recommendations.append({
                "action": "Consider downgrading to Basic SKU",
                "details": "Using Standard SKU. Evaluate if Basic SKU features are sufficient.",
                "alternatives": [
                    {"name": "Downgrade to Basic SKU", "cost": 18.25, "savings": 7.30},
                ],
                "priority": "medium",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_app_gateways(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Application Gateways (WAF v2) for cost intelligence.

        Detection criteria:
        - Stopped/Deallocated instances (CRITICAL - 90 score)
        - No backend pools configured (CRITICAL - 80 score)
        - Oversized tier (Large when Medium/Small sufficient) (HIGH - 60 score)
        - WAF enabled but no custom rules (MEDIUM - 40 score)

        Args:
            region: Azure region to scan (e.g., 'eastus')

        Returns:
            List of all Application Gateways with optimization recommendations
        """
        logger.info("inventory.scan_azure_app_gateways_start", region=region)
        all_app_gateways: list[AllCloudResourceData] = []

        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.network import NetworkManagementClient

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            network_client = NetworkManagementClient(credential, self.subscription_id)
            app_gateways = list(network_client.application_gateways.list_all())

            logger.info(
                "inventory.azure_app_gateways_fetched",
                region=region,
                total_app_gateways=len(app_gateways)
            )

            for ag in app_gateways:
                # Filter by region
                if ag.location != region:
                    continue

                # Filter by resource group scope
                if not self.provider._is_resource_in_scope(ag.id):
                    continue

                # Detect usage patterns
                is_running = ag.operational_state and ag.operational_state.lower() == "running"
                has_backend_pools = ag.backend_address_pools and len(ag.backend_address_pools) > 0
                has_http_listeners = ag.http_listeners and len(ag.http_listeners) > 0
                has_request_routing_rules = ag.request_routing_rules and len(ag.request_routing_rules) > 0

                # WAF analysis
                has_waf = ag.web_application_firewall_configuration is not None
                waf_rule_count = 0
                if has_waf and ag.web_application_firewall_configuration.rule_set_type:
                    waf_rule_count = len(ag.web_application_firewall_configuration.disabled_rule_groups or [])

                # SKU analysis (tier + size)
                sku_tier = ag.sku.tier if ag.sku else "Standard_v2"
                sku_size = ag.sku.name if ag.sku else "Standard_Medium"

                # Pricing calculation (Azure Application Gateway v2 pricing - US East 2025)
                # SKU cost (per gateway per month)
                sku_cost_map = {
                    "Standard_Small": 142.00,    # Small (2 CU baseline)
                    "Standard_Medium": 372.00,   # Medium (10 CU baseline)
                    "Standard_Large": 745.00,    # Large (50 CU baseline)
                    "WAF_Small": 160.00,         # WAF Small (2 CU baseline + WAF cost)
                    "WAF_Medium": 390.00,        # WAF Medium (10 CU baseline + WAF cost)
                    "WAF_Large": 763.00,         # WAF Large (50 CU baseline + WAF cost)
                }

                monthly_cost = sku_cost_map.get(sku_size, 372.00)  # Default to Medium

                # Capacity units (additional cost beyond baseline)
                capacity = ag.sku.capacity if ag.sku and ag.sku.capacity else 2
                if capacity > 2:  # Beyond baseline 2 CU
                    monthly_cost += (capacity - 2) * 8.76  # $8.76 per CU per month

                # Calculate optimization
                (is_optimizable, optimization_score, optimization_priority,
                 potential_savings, recommendations) = self._calculate_ag_optimization(
                    ag, is_running, has_backend_pools, has_http_listeners,
                    has_request_routing_rules, has_waf, waf_rule_count,
                    sku_tier, sku_size, monthly_cost
                )

                # Build resource metadata
                resource_metadata = {
                    "sku_tier": sku_tier,
                    "sku_size": sku_size,
                    "capacity": capacity,
                    "operational_state": ag.operational_state,
                    "backend_pools_count": len(ag.backend_address_pools) if ag.backend_address_pools else 0,
                    "http_listeners_count": len(ag.http_listeners) if ag.http_listeners else 0,
                    "routing_rules_count": len(ag.request_routing_rules) if ag.request_routing_rules else 0,
                    "waf_enabled": has_waf,
                    "waf_rule_count": waf_rule_count,
                }

                resource = AllCloudResourceData(
                    resource_type="azure_app_gateway",
                    resource_id=ag.id,
                    resource_name=ag.name,
                    region=region,
                    estimated_monthly_cost=round(monthly_cost, 2),
                    currency="USD",
                    is_optimizable=is_optimizable,
                    optimization_priority=optimization_priority,
                    optimization_score=optimization_score,
                    potential_monthly_savings=round(potential_savings, 2),
                    optimization_recommendations=recommendations,
                    resource_metadata=resource_metadata,
                    created_at_cloud=ag.etag,  # Use etag as proxy for creation time
                    last_used_at=None,  # No last-used timestamp available
                    status="active",
                )

                all_app_gateways.append(resource)

            logger.info(
                "inventory.azure_app_gateways_scanned",
                region=region,
                total_scanned=len(all_app_gateways),
                optimizable=sum(1 for r in all_app_gateways if r.is_optimizable)
            )

        except ImportError:
            logger.error(
                "inventory.azure_sdk_missing",
                region=region,
                sdk="azure-mgmt-network",
                message="Install azure-mgmt-network to scan Application Gateways"
            )
        except Exception as e:
            logger.exception(
                "inventory.azure_app_gateways_scan_failed",
                region=region,
                error=str(e)
            )

        return all_app_gateways

    def _calculate_ag_optimization(
        self,
        ag,
        is_running: bool,
        has_backend_pools: bool,
        has_http_listeners: bool,
        has_request_routing_rules: bool,
        has_waf: bool,
        waf_rule_count: int,
        sku_tier: str,
        sku_size: str,
        monthly_cost: float
    ) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate Application Gateway optimization opportunities.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: Stopped or Deallocated (CRITICAL - 90 score)
        if not is_running:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete stopped Application Gateway",
                "details": f"Application Gateway is in '{ag.operational_state}' state and not serving traffic. You're still paying ${monthly_cost}/month.",
                "alternatives": [
                    {"name": "Delete Application Gateway", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "critical",
            })

        # Scenario 2: No backend pools (CRITICAL - 80 score)
        elif not has_backend_pools:
            is_optimizable = True
            optimization_score = 80
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete Application Gateway without backend pools",
                "details": "Application Gateway has no backend pools configured. Not routing any traffic.",
                "alternatives": [
                    {"name": "Delete Application Gateway", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "critical",
            })

        # Scenario 3: No HTTP listeners or routing rules (HIGH - 70 score)
        elif not has_http_listeners or not has_request_routing_rules:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete Application Gateway with no active routing",
                "details": "Application Gateway has backend pools but no HTTP listeners or routing rules. Not accepting traffic.",
                "alternatives": [
                    {"name": "Delete Application Gateway", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "high",
            })

        # Scenario 4: Oversized SKU (HIGH - 60 score)
        elif sku_size in ["Standard_Large", "WAF_Large"] and has_backend_pools:
            is_optimizable = True
            optimization_score = 60
            priority = "high"
            # Savings: Large  Medium (50% reduction)
            medium_cost = 372.00 if "WAF" not in sku_size else 390.00
            potential_savings = monthly_cost - medium_cost
            recommendations.append({
                "action": "Downsize Application Gateway from Large to Medium SKU",
                "details": f"Using Large SKU (${monthly_cost}/mo). Evaluate if Medium SKU is sufficient (${medium_cost}/mo).",
                "alternatives": [
                    {"name": "Downgrade to Medium SKU", "cost": medium_cost, "savings": potential_savings},
                ],
                "priority": "high",
            })

        # Scenario 5: WAF enabled but no custom rules (MEDIUM - 40 score)
        elif has_waf and waf_rule_count == 0 and "WAF" in sku_size:
            is_optimizable = True
            optimization_score = 40
            priority = "medium"
            # Savings: WAF  Standard (WAF adds ~$18/mo overhead)
            standard_cost = monthly_cost - 18.00
            potential_savings = 18.00
            recommendations.append({
                "action": "Consider disabling WAF or switch to Standard SKU",
                "details": f"WAF is enabled but no custom rules configured. Paying extra ${potential_savings}/month for unused feature.",
                "alternatives": [
                    {"name": "Switch to Standard SKU", "cost": standard_cost, "savings": potential_savings},
                ],
                "priority": "medium",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_storage_accounts(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Storage Accounts for cost intelligence.

        Detection criteria:
        - Zero storage used (CRITICAL - 90 score)
        - Premium tier underutilized (<100GB) (HIGH - 70 score)
        - Hot tier with cold access patterns (MEDIUM - 50 score)
        - GRS replication without geo-redundancy need (LOW - 30 score)

        Args:
            region: Azure region to scan (e.g., 'eastus')

        Returns:
            List of all Storage Accounts with optimization recommendations
        """
        logger.info("inventory.scan_azure_storage_accounts_start", region=region)
        all_storage_accounts: list[AllCloudResourceData] = []

        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.storage import StorageManagementClient

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            storage_client = StorageManagementClient(credential, self.subscription_id)
            storage_accounts = list(storage_client.storage_accounts.list())

            logger.info(
                "inventory.azure_storage_accounts_fetched",
                region=region,
                total_storage_accounts=len(storage_accounts)
            )

            for sa in storage_accounts:
                # Filter by region
                if sa.location != region:
                    continue

                # Filter by resource group scope
                if not self.provider._is_resource_in_scope(sa.id):
                    continue

                # Get account properties
                account_kind = sa.kind if sa.kind else "StorageV2"
                sku_name = sa.sku.name if sa.sku else "Standard_LRS"
                sku_tier = sa.sku.tier if sa.sku else "Standard"
                access_tier = sa.access_tier if sa.access_tier else "Hot"

                # Get usage metrics (requires additional API call)
                usage_gb = 0.0
                try:
                    # Get resource group from storage account ID
                    resource_group = sa.id.split("/")[4]
                    usage_metrics = storage_client.storage_accounts.list_account_sas(
                        resource_group_name=resource_group,
                        account_name=sa.name
                    )
                    # Note: Actual usage requires Azure Monitor API, hardcoded for now
                    usage_gb = 50.0  # Placeholder - would need Monitor API
                except Exception:
                    usage_gb = 50.0  # Default assumption

                # Pricing calculation (Azure Storage pricing - US East 2025)
                # Base cost per GB per month
                pricing_map = {
                    # Standard tier
                    "Standard_LRS": 0.0184,      # Locally redundant
                    "Standard_GRS": 0.0368,      # Geo-redundant
                    "Standard_RAGRS": 0.046,     # Read-access geo-redundant
                    "Standard_ZRS": 0.0225,      # Zone-redundant
                    # Premium tier
                    "Premium_LRS": 0.15,         # Premium locally redundant
                    "Premium_ZRS": 0.1875,       # Premium zone-redundant
                }

                price_per_gb = pricing_map.get(sku_name, 0.0184)
                storage_cost = usage_gb * price_per_gb

                # Additional costs (transactions, bandwidth) - estimate 20% overhead
                monthly_cost = storage_cost * 1.2

                # Calculate optimization
                (is_optimizable, optimization_score, optimization_priority,
                 potential_savings, recommendations) = self._calculate_sa_optimization(
                    sa, usage_gb, sku_name, sku_tier, access_tier,
                    account_kind, monthly_cost, price_per_gb
                )

                # Build resource metadata
                resource_metadata = {
                    "sku_name": sku_name,
                    "sku_tier": sku_tier,
                    "account_kind": account_kind,
                    "access_tier": access_tier,
                    "usage_gb": round(usage_gb, 2),
                    "price_per_gb": price_per_gb,
                    "provisioning_state": sa.provisioning_state,
                }

                resource = AllCloudResourceData(
                    resource_type="azure_storage_account",
                    resource_id=sa.id,
                    resource_name=sa.name,
                    region=region,
                    estimated_monthly_cost=round(monthly_cost, 2),
                    currency="USD",
                    is_optimizable=is_optimizable,
                    optimization_priority=optimization_priority,
                    optimization_score=optimization_score,
                    potential_monthly_savings=round(potential_savings, 2),
                    optimization_recommendations=recommendations,
                    resource_metadata=resource_metadata,
                    created_at_cloud=sa.creation_time,
                    last_used_at=None,  # No last-used timestamp available
                    status="active",
                )

                all_storage_accounts.append(resource)

            logger.info(
                "inventory.azure_storage_accounts_scanned",
                region=region,
                total_scanned=len(all_storage_accounts),
                optimizable=sum(1 for r in all_storage_accounts if r.is_optimizable)
            )

        except ImportError:
            logger.error(
                "inventory.azure_sdk_missing",
                region=region,
                sdk="azure-mgmt-storage",
                message="Install azure-mgmt-storage to scan Storage Accounts"
            )
        except Exception as e:
            logger.exception(
                "inventory.azure_storage_accounts_scan_failed",
                region=region,
                error=str(e)
            )

        return all_storage_accounts

    def _calculate_sa_optimization(
        self,
        sa,
        usage_gb: float,
        sku_name: str,
        sku_tier: str,
        access_tier: str,
        account_kind: str,
        monthly_cost: float,
        price_per_gb: float
    ) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate Storage Account optimization opportunities.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: Zero storage used (CRITICAL - 90 score)
        if usage_gb < 0.1:  # Less than 100 MB
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete empty Storage Account",
                "details": f"Storage Account has no data stored (<100 MB). You're paying ${monthly_cost}/month for unused storage.",
                "alternatives": [
                    {"name": "Delete Storage Account", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "critical",
            })

        # Scenario 2: Premium tier underutilized (HIGH - 70 score)
        elif sku_tier == "Premium" and usage_gb < 100:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Savings: Premium  Standard (80% reduction in per-GB cost)
            standard_cost = usage_gb * 0.0184 * 1.2
            potential_savings = monthly_cost - standard_cost
            recommendations.append({
                "action": "Downgrade from Premium to Standard tier",
                "details": f"Using Premium tier (${price_per_gb}/GB) but only {usage_gb}GB stored. Standard tier (${0.0184}/GB) would save ${potential_savings}/month.",
                "alternatives": [
                    {"name": "Downgrade to Standard LRS", "cost": round(standard_cost, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "high",
            })

        # Scenario 3: Hot tier with cold access (MEDIUM - 50 score)
        # Note: Would need actual access metrics from Monitor API
        elif access_tier == "Hot" and usage_gb > 100:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Savings: Hot  Cool (30% reduction in storage cost)
            cool_cost = usage_gb * 0.01 * 1.2
            potential_savings = monthly_cost - cool_cost
            recommendations.append({
                "action": "Consider switching from Hot to Cool access tier",
                "details": f"Using Hot tier for {usage_gb}GB. If access is infrequent, Cool tier could save ${potential_savings}/month.",
                "alternatives": [
                    {"name": "Switch to Cool tier", "cost": round(cool_cost, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "medium",
            })

        # Scenario 4: GRS replication without geo-redundancy need (LOW - 30 score)
        elif "GRS" in sku_name or "RAGRS" in sku_name:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            # Savings: GRS  LRS (50% reduction)
            lrs_cost = usage_gb * 0.0184 * 1.2
            potential_savings = monthly_cost - lrs_cost
            recommendations.append({
                "action": "Evaluate if geo-redundancy is necessary",
                "details": f"Using {sku_name} replication. If geo-redundancy isn't required, switch to LRS to save ${potential_savings}/month.",
                "alternatives": [
                    {"name": "Downgrade to LRS", "cost": round(lrs_cost, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_expressroute_circuits(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure ExpressRoute Circuits for cost intelligence.

        Detection criteria:
        - Not provisioned (CRITICAL - 90 score)
        - No peerings configured (HIGH - 75 score)
        - Premium tier underutilized (MEDIUM - 50 score)
        - Metered data plan with low usage (LOW - 30 score)

        Args:
            region: Azure region to scan (e.g., 'eastus')

        Returns:
            List of all ExpressRoute Circuits with optimization recommendations
        """
        logger.info("inventory.scan_azure_expressroute_start", region=region)
        all_expressroute: list[AllCloudResourceData] = []

        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.network import NetworkManagementClient

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            network_client = NetworkManagementClient(credential, self.subscription_id)
            expressroute_circuits = list(network_client.express_route_circuits.list_all())

            logger.info(
                "inventory.azure_expressroute_fetched",
                region=region,
                total_expressroute=len(expressroute_circuits)
            )

            for er in expressroute_circuits:
                # Filter by region
                if er.location != region:
                    continue

                # Filter by resource group scope
                if not self.provider._is_resource_in_scope(er.id):
                    continue

                # Get circuit properties
                sku_tier = er.sku.tier if er.sku else "Standard"
                sku_family = er.sku.family if er.sku else "MeteredData"
                bandwidth_mbps = er.service_provider_properties.bandwidth_in_mbps if er.service_provider_properties else 50

                # Circuit state
                provisioning_state = er.provisioning_state if er.provisioning_state else "Unknown"
                circuit_provisioning_state = er.circuit_provisioning_state if er.circuit_provisioning_state else "Disabled"

                # Peerings analysis
                has_peerings = er.peerings and len(er.peerings) > 0
                peering_count = len(er.peerings) if er.peerings else 0

                # Pricing calculation (Azure ExpressRoute pricing - US East 2025)
                # Port fee per month
                port_fees = {
                    50: 55.00,     # 50 Mbps
                    100: 120.00,   # 100 Mbps
                    200: 230.00,   # 200 Mbps
                    500: 560.00,   # 500 Mbps
                    1000: 1100.00, # 1 Gbps
                    2000: 2200.00, # 2 Gbps
                    5000: 5500.00, # 5 Gbps
                    10000: 11000.00, # 10 Gbps
                }

                # Find closest bandwidth tier
                port_cost = port_fees.get(bandwidth_mbps, 560.00)  # Default to 500 Mbps
                for tier_mbps, cost in sorted(port_fees.items()):
                    if bandwidth_mbps <= tier_mbps:
                        port_cost = cost
                        break

                # Premium add-on (if Premium tier)
                premium_cost = 1150.00 if sku_tier == "Premium" else 0.0

                # Metered data (if MeteredData family) - estimate $0.025/GB outbound
                # Assume 10% bandwidth utilization for cost estimation
                estimated_gb_outbound = (bandwidth_mbps / 8) * 0.1 * 730 * 3600 / 1024  # GB/month
                metered_cost = estimated_gb_outbound * 0.025 if sku_family == "MeteredData" else 0.0

                monthly_cost = port_cost + premium_cost + metered_cost

                # Calculate optimization
                (is_optimizable, optimization_score, optimization_priority,
                 potential_savings, recommendations) = self._calculate_er_optimization(
                    er, provisioning_state, circuit_provisioning_state, has_peerings,
                    peering_count, sku_tier, sku_family, bandwidth_mbps,
                    monthly_cost, port_cost, premium_cost
                )

                # Build resource metadata
                resource_metadata = {
                    "sku_tier": sku_tier,
                    "sku_family": sku_family,
                    "bandwidth_mbps": bandwidth_mbps,
                    "provisioning_state": provisioning_state,
                    "circuit_provisioning_state": circuit_provisioning_state,
                    "peering_count": peering_count,
                    "service_provider": er.service_provider_properties.service_provider_name if er.service_provider_properties else None,
                }

                resource = AllCloudResourceData(
                    resource_type="azure_expressroute_circuit",
                    resource_id=er.id,
                    resource_name=er.name,
                    region=region,
                    estimated_monthly_cost=round(monthly_cost, 2),
                    currency="USD",
                    is_optimizable=is_optimizable,
                    optimization_priority=optimization_priority,
                    optimization_score=optimization_score,
                    potential_monthly_savings=round(potential_savings, 2),
                    optimization_recommendations=recommendations,
                    resource_metadata=resource_metadata,
                    created_at_cloud=er.etag,  # Use etag as proxy
                    last_used_at=None,  # No last-used timestamp available
                    status="active",
                )

                all_expressroute.append(resource)

            logger.info(
                "inventory.azure_expressroute_scanned",
                region=region,
                total_scanned=len(all_expressroute),
                optimizable=sum(1 for r in all_expressroute if r.is_optimizable)
            )

        except ImportError:
            logger.error(
                "inventory.azure_sdk_missing",
                region=region,
                sdk="azure-mgmt-network",
                message="Install azure-mgmt-network to scan ExpressRoute Circuits"
            )
        except Exception as e:
            logger.exception(
                "inventory.azure_expressroute_scan_failed",
                region=region,
                error=str(e)
            )

        return all_expressroute

    def _calculate_er_optimization(
        self,
        er,
        provisioning_state: str,
        circuit_provisioning_state: str,
        has_peerings: bool,
        peering_count: int,
        sku_tier: str,
        sku_family: str,
        bandwidth_mbps: int,
        monthly_cost: float,
        port_cost: float,
        premium_cost: float
    ) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate ExpressRoute Circuit optimization opportunities.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: Not provisioned (CRITICAL - 90 score)
        if circuit_provisioning_state in ["Disabled", "NotProvisioned"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete unprovisioned ExpressRoute Circuit",
                "details": f"Circuit is '{circuit_provisioning_state}' and not serving traffic. You're paying ${monthly_cost}/month for unused circuit.",
                "alternatives": [
                    {"name": "Delete ExpressRoute Circuit", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "critical",
            })

        # Scenario 2: No peerings configured (HIGH - 75 score)
        elif not has_peerings or peering_count == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete ExpressRoute Circuit without peerings",
                "details": f"Circuit has no peerings configured. Not routing any traffic. Paying ${monthly_cost}/month.",
                "alternatives": [
                    {"name": "Delete ExpressRoute Circuit", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "high",
            })

        # Scenario 3: Premium tier underutilized (MEDIUM - 50 score)
        elif sku_tier == "Premium" and peering_count < 2:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Savings: Premium  Standard ($1150/mo savings)
            standard_cost = monthly_cost - premium_cost
            potential_savings = premium_cost
            recommendations.append({
                "action": "Downgrade from Premium to Standard tier",
                "details": f"Using Premium tier (${premium_cost}/mo add-on) but only {peering_count} peering(s) configured. Standard tier may be sufficient.",
                "alternatives": [
                    {"name": "Downgrade to Standard tier", "cost": round(standard_cost, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "medium",
            })

        # Scenario 4: Oversized bandwidth (MEDIUM - 40 score)
        elif bandwidth_mbps >= 1000 and has_peerings:
            is_optimizable = True
            optimization_score = 40
            priority = "medium"
            # Savings: Downsize bandwidth (estimate 50% reduction)
            lower_tier_cost = port_cost * 0.5
            potential_savings = port_cost - lower_tier_cost
            recommendations.append({
                "action": "Evaluate if lower bandwidth tier is sufficient",
                "details": f"Using {bandwidth_mbps} Mbps circuit (${port_cost}/mo). Review actual usage to see if downsizing is possible.",
                "alternatives": [
                    {"name": "Downsize to lower bandwidth", "cost": round(lower_tier_cost, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "medium",
            })

        # Scenario 5: Metered data with low usage (LOW - 30 score)
        elif sku_family == "MeteredData" and bandwidth_mbps <= 200:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            # Savings: Switch to UnlimitedData (may save on overage charges)
            # Note: UnlimitedData is typically more cost-effective for high usage
            potential_savings = monthly_cost * 0.1  # Estimate 10% savings
            recommendations.append({
                "action": "Evaluate UnlimitedData plan",
                "details": f"Using MeteredData plan on {bandwidth_mbps} Mbps circuit. If outbound data is high, UnlimitedData may be more cost-effective.",
                "alternatives": [
                    {"name": "Switch to UnlimitedData plan", "cost": round(monthly_cost * 0.9, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_disk_snapshots(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Disk Snapshots for cost intelligence.

        Detection criteria:
        - Snapshot trs ancien (>365 jours) (CRITICAL - 90 score)
        - Snapshot orphelin (disque source supprim) (HIGH - 75 score)
        - Snapshots multiples du mme disque (>10) (MEDIUM - 50 score)
        - Snapshot non incrmentiel (LOW - 30 score)

        Args:
            region: Azure region to scan (e.g., 'eastus')

        Returns:
            List of all Disk Snapshots with optimization recommendations
        """
        logger.info("inventory.scan_azure_snapshots_start", region=region)
        all_snapshots: list[AllCloudResourceData] = []

        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.compute import ComputeManagementClient
            from datetime import datetime, timezone

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            compute_client = ComputeManagementClient(credential, self.subscription_id)
            snapshots = list(compute_client.snapshots.list())

            logger.info(
                "inventory.azure_snapshots_fetched",
                region=region,
                total_snapshots=len(snapshots)
            )

            # Get all disks to check for orphaned snapshots
            all_disks = list(compute_client.disks.list())
            disk_ids = {disk.id for disk in all_disks}

            # Group snapshots by source disk
            snapshots_by_disk: dict[str, list] = {}
            for snapshot in snapshots:
                source_id = snapshot.creation_data.source_resource_id if snapshot.creation_data else None
                if source_id:
                    if source_id not in snapshots_by_disk:
                        snapshots_by_disk[source_id] = []
                    snapshots_by_disk[source_id].append(snapshot)

            for snap in snapshots:
                # Filter by region
                if snap.location != region:
                    continue

                # Filter by resource group scope
                if not self.provider._is_resource_in_scope(snap.id):
                    continue

                # Snapshot properties
                sku_name = snap.sku.name if snap.sku else "Standard_LRS"
                disk_size_gb = snap.disk_size_gb if snap.disk_size_gb else 128
                incremental = snap.incremental if hasattr(snap, 'incremental') else False

                # Calculate age
                time_created = snap.time_created if snap.time_created else datetime.now(timezone.utc)
                age_days = (datetime.now(timezone.utc) - time_created).days

                # Check if orphaned (source disk deleted)
                source_disk_id = snap.creation_data.source_resource_id if snap.creation_data else None
                is_orphaned = source_disk_id and source_disk_id not in disk_ids

                # Count snapshots from same source disk
                snapshot_count_for_disk = len(snapshots_by_disk.get(source_disk_id, [])) if source_disk_id else 1

                # Pricing calculation (Azure Snapshot pricing - US East 2025)
                # Price per GB per month
                pricing_map = {
                    "Standard_LRS": 0.05,      # Standard HDD snapshots
                    "Premium_LRS": 0.10,       # Premium SSD snapshots
                    "StandardSSD_LRS": 0.065,  # Standard SSD snapshots
                }

                price_per_gb = pricing_map.get(sku_name, 0.05)
                monthly_cost = disk_size_gb * price_per_gb

                # Calculate optimization
                (is_optimizable, optimization_score, optimization_priority,
                 potential_savings, recommendations) = self._calculate_snapshot_optimization(
                    snap, age_days, disk_size_gb, is_orphaned,
                    snapshot_count_for_disk, incremental, monthly_cost
                )

                # Build resource metadata
                resource_metadata = {
                    "sku_name": sku_name,
                    "disk_size_gb": disk_size_gb,
                    "incremental": incremental,
                    "age_days": age_days,
                    "is_orphaned": is_orphaned,
                    "snapshot_count_for_disk": snapshot_count_for_disk,
                    "source_disk_id": source_disk_id,
                    "provisioning_state": snap.provisioning_state,
                }

                resource = AllCloudResourceData(
                    resource_type="azure_disk_snapshot",
                    resource_id=snap.id,
                    resource_name=snap.name,
                    region=region,
                    estimated_monthly_cost=round(monthly_cost, 2),
                    currency="USD",
                    is_optimizable=is_optimizable,
                    optimization_priority=optimization_priority,
                    optimization_score=optimization_score,
                    potential_monthly_savings=round(potential_savings, 2),
                    optimization_recommendations=recommendations,
                    resource_metadata=resource_metadata,
                    created_at_cloud=time_created,
                    last_used_at=None,  # No last-used timestamp available
                    status="active",
                )

                all_snapshots.append(resource)

            logger.info(
                "inventory.azure_snapshots_scanned",
                region=region,
                total_scanned=len(all_snapshots),
                optimizable=sum(1 for r in all_snapshots if r.is_optimizable)
            )

        except ImportError:
            logger.error(
                "inventory.azure_sdk_missing",
                region=region,
                sdk="azure-mgmt-compute",
                message="Install azure-mgmt-compute to scan Disk Snapshots"
            )
        except Exception as e:
            logger.exception(
                "inventory.azure_snapshots_scan_failed",
                region=region,
                error=str(e)
            )

        return all_snapshots

    def _calculate_snapshot_optimization(
        self,
        snap,
        age_days: int,
        disk_size_gb: int,
        is_orphaned: bool,
        snapshot_count_for_disk: int,
        incremental: bool,
        monthly_cost: float
    ) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate Disk Snapshot optimization opportunities.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: Snapshot trs ancien (>365 jours) (CRITICAL - 90 score)
        if age_days > 365:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete old snapshot (>1 year old)",
                "details": f"Snapshot is {age_days} days old ({disk_size_gb}GB). Consider deleting if no longer needed. Saving ${monthly_cost}/month.",
                "alternatives": [
                    {"name": "Delete snapshot", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "critical",
            })

        # Scenario 2: Snapshot orphelin (disque source supprim) (HIGH - 75 score)
        elif is_orphaned:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete orphaned snapshot (source disk deleted)",
                "details": f"Source disk no longer exists. Snapshot is orphaned ({disk_size_gb}GB). Delete to save ${monthly_cost}/month.",
                "alternatives": [
                    {"name": "Delete orphaned snapshot", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "high",
            })

        # Scenario 3: Snapshots multiples du mme disque (>10) (MEDIUM - 50 score)
        elif snapshot_count_for_disk > 10:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Estimate savings: delete oldest 50% of snapshots
            potential_savings = monthly_cost * 0.5
            recommendations.append({
                "action": f"Reduce number of snapshots ({snapshot_count_for_disk} snapshots)",
                "details": f"Source disk has {snapshot_count_for_disk} snapshots. Consider retention policy to delete old snapshots. Save ~${potential_savings}/month.",
                "alternatives": [
                    {"name": "Delete oldest 50% of snapshots", "cost": round(monthly_cost * 0.5, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "medium",
            })

        # Scenario 4: Snapshot non incrmentiel (LOW - 30 score)
        elif not incremental and disk_size_gb > 128:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            # Savings: incremental snapshots are ~80% cheaper
            potential_savings = monthly_cost * 0.8
            recommendations.append({
                "action": "Switch to incremental snapshots",
                "details": f"Using full snapshot ({disk_size_gb}GB). Incremental snapshots could save ~${potential_savings}/month (80% reduction).",
                "alternatives": [
                    {"name": "Use incremental snapshots", "cost": round(monthly_cost * 0.2, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_nat_gateways(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure NAT Gateways for cost intelligence.

        Detection criteria:
        - Pas de subnet attach (CRITICAL - 90 score)
        - Pas d'IP publiques configures (HIGH - 75 score)
        - Trs faible utilisation (<100GB/mois) (MEDIUM - 50 score)
        - Multiple NAT Gateways dans mme VNet (LOW - 30 score)

        Args:
            region: Azure region to scan (e.g., 'eastus')

        Returns:
            List of all NAT Gateways with optimization recommendations
        """
        logger.info("inventory.scan_azure_nat_gateways_start", region=region)
        all_nat_gateways: list[AllCloudResourceData] = []

        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.network import NetworkManagementClient

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            network_client = NetworkManagementClient(credential, self.subscription_id)
            nat_gateways = list(network_client.nat_gateways.list_all())

            logger.info(
                "inventory.azure_nat_gateways_fetched",
                region=region,
                total_nat_gateways=len(nat_gateways)
            )

            # Get all VNets to count NAT Gateways per VNet
            vnets = list(network_client.virtual_networks.list_all())
            nat_per_vnet: dict[str, int] = {}

            for nat in nat_gateways:
                # Filter by region
                if nat.location != region:
                    continue

                # Filter by resource group scope
                if not self.provider._is_resource_in_scope(nat.id):
                    continue

                # NAT Gateway properties
                has_subnets = nat.subnets and len(nat.subnets) > 0
                subnet_count = len(nat.subnets) if nat.subnets else 0

                has_public_ips = nat.public_ip_addresses and len(nat.public_ip_addresses) > 0
                public_ip_count = len(nat.public_ip_addresses) if nat.public_ip_addresses else 0

                # Estimate outbound data usage (requires Azure Monitor - hardcoded for now)
                estimated_gb_outbound = 500.0  # Placeholder - would need Monitor API

                # Count NAT Gateways in same VNet (for optimization scenario)
                vnet_id = None
                if nat.subnets and len(nat.subnets) > 0:
                    # Extract VNet ID from subnet ID
                    subnet_id = nat.subnets[0].id
                    vnet_id = "/".join(subnet_id.split("/")[:9]) if "/" in subnet_id else None

                if vnet_id:
                    if vnet_id not in nat_per_vnet:
                        nat_per_vnet[vnet_id] = 0
                    nat_per_vnet[vnet_id] += 1

                nat_count_in_vnet = nat_per_vnet.get(vnet_id, 1) if vnet_id else 1

                # Pricing calculation (Azure NAT Gateway pricing - US East 2025)
                # Gateway cost: $32.85/month
                # Outbound data: $0.045/GB
                gateway_cost = 32.85
                outbound_cost = estimated_gb_outbound * 0.045

                monthly_cost = gateway_cost + outbound_cost

                # Calculate optimization
                (is_optimizable, optimization_score, optimization_priority,
                 potential_savings, recommendations) = self._calculate_nat_optimization(
                    nat, has_subnets, subnet_count, has_public_ips,
                    public_ip_count, estimated_gb_outbound, nat_count_in_vnet,
                    monthly_cost, gateway_cost
                )

                # Build resource metadata
                resource_metadata = {
                    "subnet_count": subnet_count,
                    "public_ip_count": public_ip_count,
                    "estimated_gb_outbound": estimated_gb_outbound,
                    "nat_count_in_vnet": nat_count_in_vnet,
                    "vnet_id": vnet_id,
                    "provisioning_state": nat.provisioning_state,
                }

                resource = AllCloudResourceData(
                    resource_type="azure_nat_gateway",
                    resource_id=nat.id,
                    resource_name=nat.name,
                    region=region,
                    estimated_monthly_cost=round(monthly_cost, 2),
                    currency="USD",
                    is_optimizable=is_optimizable,
                    optimization_priority=optimization_priority,
                    optimization_score=optimization_score,
                    potential_monthly_savings=round(potential_savings, 2),
                    optimization_recommendations=recommendations,
                    resource_metadata=resource_metadata,
                    created_at_cloud=nat.etag,  # Use etag as proxy
                    last_used_at=None,  # No last-used timestamp available
                    status="active",
                )

                all_nat_gateways.append(resource)

            logger.info(
                "inventory.azure_nat_gateways_scanned",
                region=region,
                total_scanned=len(all_nat_gateways),
                optimizable=sum(1 for r in all_nat_gateways if r.is_optimizable)
            )

        except ImportError:
            logger.error(
                "inventory.azure_sdk_missing",
                region=region,
                sdk="azure-mgmt-network",
                message="Install azure-mgmt-network to scan NAT Gateways"
            )
        except Exception as e:
            logger.exception(
                "inventory.azure_nat_gateways_scan_failed",
                region=region,
                error=str(e)
            )

        return all_nat_gateways

    def _calculate_nat_optimization(
        self,
        nat,
        has_subnets: bool,
        subnet_count: int,
        has_public_ips: bool,
        public_ip_count: int,
        estimated_gb_outbound: float,
        nat_count_in_vnet: int,
        monthly_cost: float,
        gateway_cost: float
    ) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate NAT Gateway optimization opportunities.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: Pas de subnet attach (CRITICAL - 90 score)
        if not has_subnets or subnet_count == 0:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete NAT Gateway without subnets",
                "details": f"NAT Gateway has no subnets attached. Not routing any traffic. Delete to save ${monthly_cost}/month.",
                "alternatives": [
                    {"name": "Delete NAT Gateway", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "critical",
            })

        # Scenario 2: Pas d'IP publiques configures (HIGH - 75 score)
        elif not has_public_ips or public_ip_count == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete NAT Gateway without public IPs",
                "details": f"NAT Gateway has no public IPs configured. Cannot provide outbound connectivity. Delete to save ${monthly_cost}/month.",
                "alternatives": [
                    {"name": "Delete NAT Gateway", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "high",
            })

        # Scenario 3: Trs faible utilisation (<100GB/mois) (MEDIUM - 50 score)
        elif estimated_gb_outbound < 100:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Savings: gateway cost only (keep minimal data cost)
            potential_savings = gateway_cost
            recommendations.append({
                "action": "Consider using Public IP instead of NAT Gateway",
                "details": f"Very low outbound data ({estimated_gb_outbound}GB/month). Public IP on VM may be more cost-effective. Save ${potential_savings}/month.",
                "alternatives": [
                    {"name": "Use Public IP instead", "cost": round(estimated_gb_outbound * 0.005, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "medium",
            })

        # Scenario 4: Multiple NAT Gateways dans mme VNet (LOW - 30 score)
        elif nat_count_in_vnet > 1:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            # Savings: consolidate to 1 NAT Gateway (save 1 gateway cost)
            potential_savings = gateway_cost
            recommendations.append({
                "action": f"Consolidate NAT Gateways in VNet ({nat_count_in_vnet} gateways)",
                "details": f"VNet has {nat_count_in_vnet} NAT Gateways. Consider consolidating to reduce costs. Save ${potential_savings}/month per gateway.",
                "alternatives": [
                    {"name": "Consolidate to 1 NAT Gateway", "cost": round(monthly_cost - potential_savings, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_azure_sql_databases(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure SQL Databases for cost intelligence.

        Detection criteria:
        - Base de donnes paused/stopped (CRITICAL - 90 score)
        - Aucune connexion 30 derniers jours (HIGH - 75 score)
        - DTU trs faible (<5% utilization) (HIGH - 70 score)
        - Premium tier en dev/test (HIGH - 65 score)
        - Geo-replication non ncessaire (MEDIUM - 50 score)

        Args:
            region: Azure region to scan (e.g., 'eastus')

        Returns:
            List of all Azure SQL Databases with optimization recommendations
        """
        logger.info("inventory.scan_azure_sql_databases_start", region=region)
        all_sql_databases: list[AllCloudResourceData] = []

        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.sql import SqlManagementClient

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            sql_client = SqlManagementClient(credential, self.subscription_id)

            # Get all SQL servers first
            sql_servers = list(sql_client.servers.list())

            logger.info(
                "inventory.azure_sql_servers_fetched",
                region=region,
                total_sql_servers=len(sql_servers)
            )

            for server in sql_servers:
                # Filter by region
                if server.location != region:
                    continue

                # Filter by resource group scope
                if not self.provider._is_resource_in_scope(server.id):
                    continue

                # Get resource group from server ID
                resource_group = server.id.split("/")[4]

                # Get all databases in this server
                try:
                    databases = list(sql_client.databases.list_by_server(
                        resource_group_name=resource_group,
                        server_name=server.name
                    ))

                    for db in databases:
                        # Skip 'master' database
                        if db.name.lower() == "master":
                            continue

                        # Database properties
                        status = db.status if db.status else "Online"
                        is_paused = status.lower() in ["paused", "stopped"]

                        sku_name = db.sku.name if db.sku else "Basic"
                        sku_tier = db.sku.tier if db.sku else "Basic"

                        # Capacity (DTU or vCores)
                        capacity = db.sku.capacity if db.sku and db.sku.capacity else 5

                        # Geo-replication
                        has_geo_replication = False  # Would need to check replication links

                        # Estimate DTU utilization (requires Azure Monitor - hardcoded for now)
                        dtu_utilization_percent = 25.0  # Placeholder - would need Monitor API

                        # Days since last connection (requires diagnostic logs - hardcoded)
                        days_since_last_connection = 15  # Placeholder - would need Monitor API

                        # Pricing calculation (Azure SQL Database pricing - US East 2025)
                        # Prices vary widely by tier and size
                        pricing_map = {
                            "Basic": 5.00,           # Basic: $5/month
                            "Standard_S0": 15.00,    # Standard S0: $15/month
                            "Standard_S1": 30.00,    # Standard S1: $30/month
                            "Standard_S2": 75.00,    # Standard S2: $75/month
                            "Standard_S3": 150.00,   # Standard S3: $150/month
                            "Standard_S4": 300.00,   # Standard S4: $300/month
                            "Premium_P1": 465.00,    # Premium P1: $465/month
                            "Premium_P2": 930.00,    # Premium P2: $930/month
                            "Premium_P4": 1860.00,   # Premium P4: $1860/month
                            "Premium_P6": 3720.00,   # Premium P6: $3720/month
                            "Premium_P11": 7000.00,  # Premium P11: $7000/month
                            "Premium_P15": 14000.00, # Premium P15: $14000/month
                        }

                        # Construct SKU key
                        sku_key = f"{sku_tier}_{sku_name}" if sku_tier != "Basic" else sku_tier
                        monthly_cost = pricing_map.get(sku_key, pricing_map.get(sku_tier, 15.00))

                        # Calculate optimization
                        (is_optimizable, optimization_score, optimization_priority,
                         potential_savings, recommendations) = self._calculate_sqldb_optimization(
                            db, status, is_paused, sku_tier, dtu_utilization_percent,
                            days_since_last_connection, has_geo_replication, monthly_cost
                        )

                        # Build resource metadata
                        resource_metadata = {
                            "server_name": server.name,
                            "sku_name": sku_name,
                            "sku_tier": sku_tier,
                            "capacity": capacity,
                            "status": status,
                            "dtu_utilization_percent": dtu_utilization_percent,
                            "days_since_last_connection": days_since_last_connection,
                            "has_geo_replication": has_geo_replication,
                        }

                        resource = AllCloudResourceData(
                            resource_type="azure_sql_database",
                            resource_id=db.id,
                            resource_name=db.name,
                            region=region,
                            estimated_monthly_cost=round(monthly_cost, 2),
                            currency="USD",
                            is_optimizable=is_optimizable,
                            optimization_priority=optimization_priority,
                            optimization_score=optimization_score,
                            potential_monthly_savings=round(potential_savings, 2),
                            optimization_recommendations=recommendations,
                            resource_metadata=resource_metadata,
                            created_at_cloud=db.creation_date,
                            last_used_at=None,  # No last-used timestamp available
                            status="active",
                        )

                        all_sql_databases.append(resource)

                except Exception as db_error:
                    logger.error(
                        "inventory.azure_sql_databases_list_failed",
                        server=server.name,
                        error=str(db_error)
                    )

            logger.info(
                "inventory.azure_sql_databases_scanned",
                region=region,
                total_scanned=len(all_sql_databases),
                optimizable=sum(1 for r in all_sql_databases if r.is_optimizable)
            )

        except ImportError:
            logger.error(
                "inventory.azure_sdk_missing",
                region=region,
                sdk="azure-mgmt-sql",
                message="Install azure-mgmt-sql to scan Azure SQL Databases"
            )
        except Exception as e:
            logger.exception(
                "inventory.azure_sql_databases_scan_failed",
                region=region,
                error=str(e)
            )

        return all_sql_databases

    def _calculate_sqldb_optimization(
        self,
        db,
        status: str,
        is_paused: bool,
        sku_tier: str,
        dtu_utilization_percent: float,
        days_since_last_connection: int,
        has_geo_replication: bool,
        monthly_cost: float
    ) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate Azure SQL Database optimization opportunities.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: Base de donnes paused/stopped (CRITICAL - 90 score)
        if is_paused:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete or resume paused database",
                "details": f"Database is '{status}'. Delete if no longer needed, or resume if required. Save ${monthly_cost}/month.",
                "alternatives": [
                    {"name": "Delete database", "cost": 0, "savings": monthly_cost},
                    {"name": "Resume database", "cost": monthly_cost, "savings": 0},
                ],
                "priority": "critical",
            })

        # Scenario 2: Aucune connexion 30 derniers jours (HIGH - 75 score)
        elif days_since_last_connection > 30:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete unused database (no connections for 30+ days)",
                "details": f"Database has no connections for {days_since_last_connection} days. Delete if no longer needed. Save ${monthly_cost}/month.",
                "alternatives": [
                    {"name": "Delete database", "cost": 0, "savings": monthly_cost},
                    {"name": "Pause database", "cost": round(monthly_cost * 0.1, 2), "savings": round(monthly_cost * 0.9, 2)},
                ],
                "priority": "high",
            })

        # Scenario 3: DTU trs faible (<5% utilization) (HIGH - 70 score)
        elif dtu_utilization_percent < 5.0:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Savings: Downgrade to lower tier (estimate 50% reduction)
            potential_savings = monthly_cost * 0.5
            recommendations.append({
                "action": "Downgrade database tier (very low DTU utilization)",
                "details": f"DTU utilization is only {dtu_utilization_percent}%. Consider downgrading to lower tier. Save ~${potential_savings}/month.",
                "alternatives": [
                    {"name": "Downgrade to lower tier", "cost": round(monthly_cost * 0.5, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "high",
            })

        # Scenario 4: Premium tier en dev/test (HIGH - 65 score)
        elif sku_tier == "Premium" and monthly_cost > 1000:
            is_optimizable = True
            optimization_score = 65
            priority = "high"
            # Savings: Premium  Standard (70% reduction)
            potential_savings = monthly_cost * 0.7
            recommendations.append({
                "action": "Downgrade from Premium to Standard tier",
                "details": f"Using Premium tier (${monthly_cost}/mo). If dev/test environment, Standard tier is sufficient. Save ${potential_savings}/month.",
                "alternatives": [
                    {"name": "Downgrade to Standard tier", "cost": round(monthly_cost * 0.3, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "high",
            })

        # Scenario 5: Geo-replication non ncessaire (MEDIUM - 50 score)
        elif has_geo_replication:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Savings: Geo-replication adds ~100% cost (double the database)
            potential_savings = monthly_cost * 0.5
            recommendations.append({
                "action": "Remove geo-replication if not required",
                "details": f"Database has geo-replication enabled. If not required for DR, remove to save ~${potential_savings}/month (50% reduction).",
                "alternatives": [
                    {"name": "Remove geo-replication", "cost": round(monthly_cost * 0.5, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "medium",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_aks_clusters(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure AKS (Kubernetes) Clusters for cost intelligence.

        Detection criteria:
        - Cluster stopped/deallocated (CRITICAL - 90 score)
        - No node pools configured (HIGH - 75 score)
        - Node pools overprovisioned (>50% idle) (HIGH - 70 score)
        - Premium tier in dev/test (MEDIUM - 50 score)
        - Auto-scaler disabled on production (LOW - 30 score)

        Args:
            region: Azure region to scan (e.g., 'eastus')

        Returns:
            List of all AKS Clusters with optimization recommendations
        """
        logger.info("inventory.scan_azure_aks_start", region=region)
        all_aks_clusters: list[AllCloudResourceData] = []

        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.containerservice import ContainerServiceClient

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            aks_client = ContainerServiceClient(credential, self.subscription_id)
            clusters = list(aks_client.managed_clusters.list())

            logger.info(
                "inventory.azure_aks_fetched",
                region=region,
                total_clusters=len(clusters)
            )

            for cluster in clusters:
                # Filter by region
                if cluster.location != region:
                    continue

                # Filter by resource group scope
                if not self.provider._is_resource_in_scope(cluster.id):
                    continue

                # Cluster properties
                provisioning_state = cluster.provisioning_state if cluster.provisioning_state else "Unknown"
                power_state = cluster.power_state.code if cluster.power_state else "Running"
                is_stopped = power_state.lower() in ["stopped", "deallocated"]

                # Node pools analysis
                agent_pools = cluster.agent_pool_profiles if cluster.agent_pool_profiles else []
                has_node_pools = len(agent_pools) > 0
                total_nodes = sum(pool.count if pool.count else 0 for pool in agent_pools)

                # Auto-scaler analysis
                has_autoscaler = any(
                    pool.enable_auto_scaling for pool in agent_pools if pool.enable_auto_scaling
                )

                # Tier analysis
                sku_tier = cluster.sku.tier if cluster.sku else "Free"

                # Pricing calculation (Azure AKS pricing - US East 2025)
                # Cluster management fee (Standard/Premium tier)
                cluster_fee = 0.0
                if sku_tier == "Standard":
                    cluster_fee = 73.00  # $0.10/h * 730h
                elif sku_tier == "Premium":
                    cluster_fee = 511.00  # $0.70/h * 730h

                # Node pools cost (estimate based on Standard_DS2_v2: $0.096/h)
                node_cost_per_hour = 0.096  # Average for Standard_DS2_v2
                node_pool_cost = total_nodes * node_cost_per_hour * 730

                monthly_cost = cluster_fee + node_pool_cost

                # Calculate optimization
                (is_optimizable, optimization_score, optimization_priority,
                 potential_savings, recommendations) = self._calculate_aks_optimization(
                    cluster, is_stopped, has_node_pools, total_nodes,
                    has_autoscaler, sku_tier, monthly_cost, cluster_fee
                )

                # Build resource metadata
                resource_metadata = {
                    "sku_tier": sku_tier,
                    "provisioning_state": provisioning_state,
                    "power_state": power_state,
                    "node_pool_count": len(agent_pools),
                    "total_nodes": total_nodes,
                    "has_autoscaler": has_autoscaler,
                    "kubernetes_version": cluster.kubernetes_version,
                }

                resource = AllCloudResourceData(
                    resource_type="azure_aks_cluster",
                    resource_id=cluster.id,
                    resource_name=cluster.name,
                    region=region,
                    estimated_monthly_cost=round(monthly_cost, 2),
                    currency="USD",
                    is_optimizable=is_optimizable,
                    optimization_priority=optimization_priority,
                    optimization_score=optimization_score,
                    potential_monthly_savings=round(potential_savings, 2),
                    optimization_recommendations=recommendations,
                    resource_metadata=resource_metadata,
                    created_at_cloud=cluster.provisioning_state,  # Use provisioning state as proxy
                    last_used_at=None,  # No last-used timestamp available
                    status="active",
                )

                all_aks_clusters.append(resource)

            logger.info(
                "inventory.azure_aks_scanned",
                region=region,
                total_scanned=len(all_aks_clusters),
                optimizable=sum(1 for r in all_aks_clusters if r.is_optimizable)
            )

        except ImportError:
            logger.error(
                "inventory.azure_sdk_missing",
                region=region,
                sdk="azure-mgmt-containerservice",
                message="Install azure-mgmt-containerservice to scan AKS Clusters"
            )
        except Exception as e:
            logger.exception(
                "inventory.azure_aks_scan_failed",
                region=region,
                error=str(e)
            )

        return all_aks_clusters

    def _calculate_aks_optimization(
        self,
        cluster,
        is_stopped: bool,
        has_node_pools: bool,
        total_nodes: int,
        has_autoscaler: bool,
        sku_tier: str,
        monthly_cost: float,
        cluster_fee: float
    ) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate AKS Cluster optimization opportunities.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: Cluster stopped/deallocated (CRITICAL - 90 score)
        if is_stopped:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete stopped AKS cluster",
                "details": f"AKS cluster is stopped. You're still paying ${monthly_cost}/month for cluster fee + stopped nodes. Delete if no longer needed.",
                "alternatives": [
                    {"name": "Delete cluster", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "critical",
            })

        # Scenario 2: No node pools configured (HIGH - 75 score)
        elif not has_node_pools or total_nodes == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete AKS cluster without nodes",
                "details": f"AKS cluster has no node pools or nodes. Paying ${monthly_cost}/month for empty cluster. Delete if not needed.",
                "alternatives": [
                    {"name": "Delete cluster", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "high",
            })

        # Scenario 3: Node pools overprovisioned (>50% idle) (HIGH - 70 score)
        # Note: Would need Azure Monitor metrics to detect idle nodes
        elif total_nodes > 5:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Estimate 30% of nodes are idle
            potential_savings = monthly_cost * 0.3
            recommendations.append({
                "action": "Review node pool sizing (potential idle nodes)",
                "details": f"AKS cluster has {total_nodes} nodes. Review actual usage to identify idle nodes. Estimated savings: ${potential_savings}/month.",
                "alternatives": [
                    {"name": "Reduce node count by 30%", "cost": round(monthly_cost * 0.7, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "high",
            })

        # Scenario 4: Premium tier in dev/test (MEDIUM - 50 score)
        elif sku_tier == "Premium" and cluster_fee > 400:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Savings: Premium  Standard ($511  $73)
            potential_savings = cluster_fee - 73.00
            recommendations.append({
                "action": "Downgrade from Premium to Standard tier",
                "details": f"Using Premium tier (${cluster_fee}/mo cluster fee). If dev/test, Standard tier ($73/mo) is sufficient. Save ${potential_savings}/month.",
                "alternatives": [
                    {"name": "Downgrade to Standard tier", "cost": round(monthly_cost - potential_savings, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "medium",
            })

        # Scenario 5: Auto-scaler disabled on production (LOW - 30 score)
        elif not has_autoscaler and total_nodes > 2:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            # Estimate 15% savings with auto-scaler
            potential_savings = monthly_cost * 0.15
            recommendations.append({
                "action": "Enable cluster auto-scaler",
                "details": f"Auto-scaler is disabled. Enable auto-scaling to automatically adjust node count based on demand. Estimated savings: ${potential_savings}/month (15%).",
                "alternatives": [
                    {"name": "Enable auto-scaler", "cost": round(monthly_cost * 0.85, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_function_apps(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Function Apps for cost intelligence.

        Detection criteria:
        - Function app stopped (CRITICAL - 90 score)
        - Zero executions 30 derniers jours (HIGH - 75 score)
        - Premium plan underutilized (<1000 exec/day) (HIGH - 65 score)
        - Dedicated plan when Consumption sufficient (MEDIUM - 50 score)
        - Always On enabled on Consumption (LOW - 30 score)

        Args:
            region: Azure region to scan (e.g., 'eastus')

        Returns:
            List of all Function Apps with optimization recommendations
        """
        logger.info("inventory.scan_azure_functions_start", region=region)
        all_function_apps: list[AllCloudResourceData] = []

        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.web import WebSiteManagementClient

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            web_client = WebSiteManagementClient(credential, self.subscription_id)

            # List all web apps (includes Function Apps)
            all_sites = list(web_client.web_apps.list())

            # Filter to only Function Apps
            function_apps = [site for site in all_sites if site.kind and "functionapp" in site.kind.lower()]

            logger.info(
                "inventory.azure_functions_fetched",
                region=region,
                total_function_apps=len(function_apps)
            )

            for func_app in function_apps:
                # Filter by region
                if func_app.location != region:
                    continue

                # Filter by resource group scope
                if not self.provider._is_resource_in_scope(func_app.id):
                    continue

                # Function App properties
                state = func_app.state if func_app.state else "Unknown"
                is_stopped = state.lower() == "stopped"

                # Get hosting plan type
                # Extract resource group from func_app.id
                resource_group = func_app.id.split("/")[4]

                # Get app service plan
                plan_type = "Consumption"  # Default
                plan_sku = "Y1"  # Default for Consumption

                if func_app.server_farm_id:
                    try:
                        plan_id_parts = func_app.server_farm_id.split("/")
                        plan_name = plan_id_parts[-1]
                        plan_rg = plan_id_parts[4]

                        plan = web_client.app_service_plans.get(plan_rg, plan_name)
                        plan_sku = plan.sku.name if plan.sku else "Y1"

                        # Determine plan type
                        if plan_sku.startswith("Y"):
                            plan_type = "Consumption"
                        elif plan_sku.startswith("EP"):
                            plan_type = "Premium"
                        else:
                            plan_type = "Dedicated"
                    except Exception:
                        pass

                # Estimate executions per day (requires Azure Monitor - hardcoded)
                daily_executions = 500  # Placeholder

                # Always On setting (only valid for Premium/Dedicated)
                always_on = func_app.site_config.always_on if func_app.site_config else False

                # Pricing calculation (Azure Functions pricing - US East 2025)
                monthly_cost = 0.0

                if plan_type == "Consumption":
                    # Consumption: $0.20 per million executions
                    monthly_executions = daily_executions * 30
                    execution_cost = (monthly_executions / 1_000_000) * 0.20
                    # GB-seconds: assume 128MB avg, 100ms avg duration
                    gb_seconds = (monthly_executions * 0.1) * (128 / 1024)
                    memory_cost = gb_seconds * 0.000016
                    monthly_cost = execution_cost + memory_cost
                elif plan_type == "Premium":
                    # Premium plans
                    premium_pricing = {
                        "EP1": 169.00,  # 1 vCPU, 3.5GB
                        "EP2": 338.00,  # 2 vCPU, 7GB
                        "EP3": 676.00,  # 4 vCPU, 14GB
                    }
                    monthly_cost = premium_pricing.get(plan_sku, 169.00)
                else:
                    # Dedicated (App Service Plan)
                    # Pricing varies widely, estimate average
                    monthly_cost = 100.00  # Conservative estimate

                # Calculate optimization
                (is_optimizable, optimization_score, optimization_priority,
                 potential_savings, recommendations) = self._calculate_function_optimization(
                    func_app, is_stopped, plan_type, plan_sku, daily_executions,
                    always_on, monthly_cost
                )

                # Build resource metadata
                resource_metadata = {
                    "state": state,
                    "plan_type": plan_type,
                    "plan_sku": plan_sku,
                    "daily_executions": daily_executions,
                    "always_on": always_on,
                    "runtime": func_app.kind,
                }

                resource = AllCloudResourceData(
                    resource_type="azure_function_app",
                    resource_id=func_app.id,
                    resource_name=func_app.name,
                    region=region,
                    estimated_monthly_cost=round(monthly_cost, 2),
                    currency="USD",
                    is_optimizable=is_optimizable,
                    optimization_priority=optimization_priority,
                    optimization_score=optimization_score,
                    potential_monthly_savings=round(potential_savings, 2),
                    optimization_recommendations=recommendations,
                    resource_metadata=resource_metadata,
                    created_at_cloud=func_app.type,  # Use type as proxy
                    last_used_at=None,
                    status="active",
                )

                all_function_apps.append(resource)

            logger.info(
                "inventory.azure_functions_scanned",
                region=region,
                total_scanned=len(all_function_apps),
                optimizable=sum(1 for r in all_function_apps if r.is_optimizable)
            )

        except ImportError:
            logger.error(
                "inventory.azure_sdk_missing",
                region=region,
                sdk="azure-mgmt-web",
                message="Install azure-mgmt-web to scan Function Apps"
            )
        except Exception as e:
            logger.exception(
                "inventory.azure_functions_scan_failed",
                region=region,
                error=str(e)
            )

        return all_function_apps

    def _calculate_function_optimization(
        self,
        func_app,
        is_stopped: bool,
        plan_type: str,
        plan_sku: str,
        daily_executions: int,
        always_on: bool,
        monthly_cost: float
    ) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate Function App optimization opportunities.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: Function app stopped (CRITICAL - 90 score)
        if is_stopped:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete stopped Function App",
                "details": f"Function App is stopped. Delete if no longer needed. Save ${monthly_cost}/month.",
                "alternatives": [
                    {"name": "Delete Function App", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "critical",
            })

        # Scenario 2: Zero executions 30 days (HIGH - 75 score)
        elif daily_executions < 10:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete unused Function App (no executions)",
                "details": f"Function App has very few executions ({daily_executions}/day). Delete if not needed. Save ${monthly_cost}/month.",
                "alternatives": [
                    {"name": "Delete Function App", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "high",
            })

        # Scenario 3: Premium plan underutilized (<1000 exec/day) (HIGH - 65 score)
        elif plan_type == "Premium" and daily_executions < 1000:
            is_optimizable = True
            optimization_score = 65
            priority = "high"
            # Savings: Premium  Consumption (estimate $165/mo)
            consumption_cost = (daily_executions * 30 / 1_000_000) * 0.20
            potential_savings = monthly_cost - consumption_cost
            recommendations.append({
                "action": "Downgrade from Premium to Consumption plan",
                "details": f"Using Premium plan (${monthly_cost}/mo) but only {daily_executions} executions/day. Consumption plan would cost ${consumption_cost:.2f}/month. Save ${potential_savings:.2f}/month.",
                "alternatives": [
                    {"name": "Switch to Consumption plan", "cost": round(consumption_cost, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "high",
            })

        # Scenario 4: Dedicated plan when Consumption sufficient (MEDIUM - 50 score)
        elif plan_type == "Dedicated" and daily_executions < 5000:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Savings: Dedicated  Consumption
            consumption_cost = (daily_executions * 30 / 1_000_000) * 0.20
            potential_savings = monthly_cost - consumption_cost
            recommendations.append({
                "action": "Switch from Dedicated to Consumption plan",
                "details": f"Using Dedicated plan (${monthly_cost}/mo) with {daily_executions} executions/day. Consumption plan sufficient. Save ${potential_savings:.2f}/month.",
                "alternatives": [
                    {"name": "Switch to Consumption plan", "cost": round(consumption_cost, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "medium",
            })

        # Scenario 5: Always On enabled on Consumption (LOW - 30 score)
        elif plan_type == "Consumption" and always_on:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            # Always On not supported on Consumption, but if somehow enabled
            potential_savings = monthly_cost * 0.1
            recommendations.append({
                "action": "Disable Always On (not supported on Consumption)",
                "details": "Always On is enabled but not beneficial on Consumption plan. Disable to avoid cold starts being masked.",
                "alternatives": [
                    {"name": "Disable Always On", "cost": round(monthly_cost, 2), "savings": 0},
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_cosmos_dbs(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Cosmos DB accounts for cost intelligence.

        Detection criteria:
        - Database account paused/offline (CRITICAL - 90 score)
        - Zero requests 30 derniers jours (HIGH - 75 score)
        - Provisioned throughput >> actual usage (>50% idle) (HIGH - 70 score)
        - Multi-region replication without need (MEDIUM - 50 score)
        - Serverless would be cheaper based on usage (LOW - 30 score)
        """
        try:
            from azure.mgmt.cosmosdb import CosmosDBManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-cosmosdb not installed")
            return []

        resources = []
        self.logger.info(f"Scanning Cosmos DB accounts in region: {region}")

        try:
            cosmos_client = CosmosDBManagementClient(
                credential=self.credential,
                subscription_id=self.subscription_id
            )

            # List all Cosmos DB accounts
            async for account in cosmos_client.database_accounts.list():
                try:
                    # Filter by region if specified
                    if region.lower() != "all" and account.location.lower() != region.lower():
                        continue

                    # Get resource group from account ID
                    resource_group = account.id.split("/")[4]

                    # Calculate optimization
                    is_optimizable, score, priority, savings, recommendations = (
                        self._calculate_cosmos_optimization(account)
                    )

                    # Get database count
                    databases = []
                    try:
                        # Get SQL databases
                        sql_dbs = list(cosmos_client.sql_resources.list_sql_databases(
                            resource_group_name=resource_group,
                            account_name=account.name
                        ))
                        databases.extend(sql_dbs)
                    except:
                        pass

                    try:
                        # Get Table API databases
                        tables = list(cosmos_client.table_resources.list_tables(
                            resource_group_name=resource_group,
                            account_name=account.name
                        ))
                        databases.extend(tables)
                    except:
                        pass

                    # Pricing (Azure US East 2025)
                    # Serverless: ~$0.25/1M RUs + $0.25/GB storage
                    # Provisioned: $0.008/RU/hour ($5.84/100 RU/mo)
                    # Multi-region adds 2x-3x cost
                    pricing_info = {
                        "serverless_base": 25.0,  # Typical small workload
                        "provisioned_100ru": 5.84,
                        "provisioned_400ru": 23.36,
                        "provisioned_1000ru": 58.40,
                        "provisioned_10000ru": 584.00,
                        "multi_region_multiplier": 2.0,
                    }

                    # Get provisioning state and capabilities
                    provisioning_state = getattr(account, 'provisioning_state', 'Unknown')
                    capabilities = getattr(account, 'capabilities', [])
                    capability_names = [cap.name for cap in capabilities] if capabilities else []

                    # Detect account type
                    is_serverless = 'EnableServerless' in capability_names
                    is_multi_region = len(account.locations) > 1 if hasattr(account, 'locations') else False

                    # Detect API type via capabilities
                    is_gremlin = 'EnableGremlin' in capability_names
                    is_mongodb = 'EnableMongo' in capability_names
                    account_kind = getattr(account, 'kind', 'GlobalDocumentDB')

                    # Set account kind based on API
                    if account_kind == 'GlobalDocumentDB':
                        if is_gremlin:
                            account_kind = 'Gremlin'
                        elif is_mongodb:
                            account_kind = 'MongoDB'

                    # Estimate monthly cost based on configuration
                    if is_serverless:
                        base_cost = pricing_info["serverless_base"]
                    else:
                        # Assume 400 RU/s for small, 1000 for medium
                        base_cost = pricing_info["provisioned_400ru"]

                    if is_multi_region:
                        base_cost *= pricing_info["multi_region_multiplier"]

                    # Determine resource type based on API
                    if is_gremlin:
                        resource_type = "azure_cosmos_db_gremlin"
                    elif is_mongodb:
                        resource_type = "azure_cosmos_db_mongodb"
                    else:
                        resource_type = "azure_cosmos_db"

                    resources.append(AllCloudResourceData(
                        resource_id=account.id,
                        resource_type=resource_type,
                        resource_name=account.name or f"Unnamed Cosmos DB ({account_kind})",
                        region=account.location,
                        estimated_monthly_cost=base_cost,
                        currency="USD",
                        resource_metadata={
                            "account_id": account.id,
                            "resource_group": resource_group,
                            "provisioning_state": provisioning_state,
                            "kind": getattr(account, 'kind', 'GlobalDocumentDB'),
                            "database_account_offer_type": getattr(account, 'database_account_offer_type', 'Standard'),
                            "consistency_policy": str(getattr(account, 'default_consistency_level', 'Session')),
                            "is_serverless": is_serverless,
                            "is_multi_region": is_multi_region,
                            "locations": [loc.location_name for loc in account.locations] if hasattr(account, 'locations') else [],
                            "database_count": len(databases),
                            "capabilities": capability_names,
                            "tags": dict(account.tags) if account.tags else {},
                        },
                        is_optimizable=is_optimizable,
                        optimization_score=score,
                        optimization_priority=priority,
                        potential_monthly_savings=savings,
                        optimization_recommendations=recommendations,
                        last_used_at=None,
                        created_at_cloud=None,
                    ))

                except Exception as e:
                    self.logger.error(f"Error processing Cosmos DB account {getattr(account, 'name', 'unknown')}: {str(e)}")
                    continue

            self.logger.info(f"Found {len(resources)} Cosmos DB accounts in region {region}")
            return resources

        except Exception as e:
            self.logger.error(f"Error scanning Cosmos DB accounts: {str(e)}")
            return []

    def _calculate_cosmos_optimization(self, account) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate optimization potential for Cosmos DB account.

        Returns:
            (is_optimizable, score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Get account properties
        provisioning_state = getattr(account, 'provisioning_state', 'Unknown')
        capabilities = getattr(account, 'capabilities', [])
        capability_names = [cap.name for cap in capabilities] if capabilities else []
        is_serverless = 'EnableServerless' in capability_names
        is_multi_region = len(account.locations) > 1 if hasattr(account, 'locations') else False
        location_count = len(account.locations) if hasattr(account, 'locations') else 1

        # Scenario 1: Database account paused/offline (CRITICAL - 90)
        if provisioning_state.lower() in ['deleting', 'failed', 'canceled']:
            is_optimizable = True
            optimization_score = max(optimization_score, 90)
            priority = "critical"
            # Estimate savings: Full cost if stopped/failed
            estimated_cost = 100.0 if not is_serverless else 25.0
            if is_multi_region:
                estimated_cost *= 2.0
            potential_savings = max(potential_savings, estimated_cost)

            recommendations.append({
                "title": "Database Account Non Fonctionnel",
                "description": f"Ce compte Cosmos DB est dans l'tat '{provisioning_state}'. Il peut gnrer des cots inutiles s'il n'est pas utilis.",
                "estimated_savings": round(estimated_cost, 2),
                "actions": [
                    "Vrifier l'tat du compte et corriger les problmes",
                    "Supprimer le compte s'il n'est plus ncessaire",
                    "Restaurer depuis une sauvegarde si donnes importantes"
                ],
                "priority": "critical",
            })

        # Scenario 2: Zero requests last 30 days (HIGH - 75)
        # Note: We can't get actual metrics without Azure Monitor, so this is a placeholder
        # In production, you'd check actual request metrics

        # Scenario 3: Provisioned throughput >> actual usage (HIGH - 70)
        if not is_serverless:  # Only applicable to provisioned throughput
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            if priority not in ["critical"]:
                priority = "high"

            # Estimate 50% savings by right-sizing
            estimated_cost = 58.40  # Assume 1000 RU/s
            if is_multi_region:
                estimated_cost *= 2.0
            savings = estimated_cost * 0.5
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Dbit Provisionn Potentiellement Surdimensionn",
                "description": "Ce compte Cosmos DB utilise le mode provisionn. Vrifiez si le dbit configur correspond  l'utilisation relle.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Analyser les mtriques d'utilisation des RU/s dans Azure Monitor",
                    "Rduire le dbit provisionn si utilisation <50%",
                    "Activer l'auto-scaling pour adapter automatiquement",
                    "Considrer le mode Serverless si utilisation sporadique"
                ],
                "priority": "high",
            })

        # Scenario 4: Multi-region without need (MEDIUM - 50)
        if is_multi_region and location_count > 2:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            if priority not in ["critical", "high"]:
                priority = "medium"

            # Multi-region doubles cost, removing extra regions saves significant
            base_cost = 100.0 if not is_serverless else 25.0
            # Savings from removing extra regions (keep 1-2 regions max)
            savings = base_cost * (location_count - 2) * 0.8
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Rplication Multi-Rgion Excessive",
                "description": f"Ce compte Cosmos DB est rpliqu dans {location_count} rgions. Chaque rgion ajoute des cots significatifs.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "valuer la ncessit de chaque rgion de rplication",
                    "Garder uniquement les rgions essentielles (1-2 max)",
                    "Supprimer les rgions secondaires non utilises",
                    f"conomies potentielles: ~{int(savings)}$/mois par rgion supprime"
                ],
                "priority": "medium",
            })

        # Scenario 5: Serverless would be cheaper (LOW - 30)
        if not is_serverless:
            # Serverless is better for sporadic/low-volume workloads
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            if priority not in ["critical", "high", "medium"]:
                priority = "low"

            # Estimate savings if switching to serverless
            provisioned_cost = 58.40  # Assume 1000 RU/s
            serverless_cost = 25.0
            savings = max(0, provisioned_cost - serverless_cost)
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Mode Serverless Potentiellement Plus conomique",
                "description": "Ce compte utilise le dbit provisionn. Le mode Serverless peut tre plus rentable pour les workloads sporadiques.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Analyser le pattern d'utilisation (sporadique vs constant)",
                    "valuer le cot Serverless vs Provisionn pour votre charge",
                    "Crer un nouveau compte Serverless et migrer si pertinent",
                    "Note: Serverless idal pour <1M RU/s par jour"
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_container_apps(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Container Apps for cost intelligence.

        Detection criteria:
        - Container app stopped/deprovisioned (CRITICAL - 90 score)
        - Zero requests 30 derniers jours (HIGH - 75 score)
        - Replicas overprovisioned (>50% idle capacity) (HIGH - 70 score)
        - Consumption plan when Dedicated sufficient (MEDIUM - 50 score)
        - Auto-scaling disabled on production (LOW - 30 score)
        """
        try:
            from azure.mgmt.appcontainers import ContainerAppsAPIClient
        except ImportError:
            self.logger.error("azure-mgmt-appcontainers not installed")
            return []

        resources = []
        self.logger.info(f"Scanning Container Apps in region: {region}")

        try:
            container_client = ContainerAppsAPIClient(
                credential=self.credential,
                subscription_id=self.subscription_id
            )

            # List all Container Apps
            async for app in container_client.container_apps.list_by_subscription():
                try:
                    # Filter by region if specified
                    if region.lower() != "all" and app.location.lower() != region.lower():
                        continue

                    # Get resource group from app ID
                    resource_group = app.id.split("/")[4]

                    # Calculate optimization
                    is_optimizable, score, priority, savings, recommendations = (
                        self._calculate_container_app_optimization(app)
                    )

                    # Pricing (Azure US East 2025)
                    # Consumption: $0.000012/vCPU-second + $0.000003/GiB-second
                    # Dedicated: $72/month per vCPU + $18/month per GiB
                    # Typical small app: 0.25 vCPU, 0.5 GiB = ~$50/mo consumption
                    pricing_info = {
                        "consumption_vcpu_second": 0.000012,
                        "consumption_gb_second": 0.000003,
                        "dedicated_vcpu_month": 72.0,
                        "dedicated_gb_month": 18.0,
                    }

                    # Get configuration
                    provisioning_state = getattr(app, 'provisioning_state', 'Unknown')
                    configuration = getattr(app, 'configuration', None)
                    template = getattr(app, 'template', None)

                    # Get scale settings
                    min_replicas = 0
                    max_replicas = 1
                    if template and hasattr(template, 'scale'):
                        scale = template.scale
                        min_replicas = getattr(scale, 'min_replicas', 0)
                        max_replicas = getattr(scale, 'max_replicas', 1)

                    # Get container resources
                    containers = []
                    total_vcpu = 0.25  # Default
                    total_memory_gb = 0.5  # Default
                    if template and hasattr(template, 'containers'):
                        containers = template.containers
                        for container in containers:
                            resources_config = getattr(container, 'resources', None)
                            if resources_config:
                                cpu = getattr(resources_config, 'cpu', 0.25)
                                memory = getattr(resources_config, 'memory', '0.5Gi')
                                # Parse memory (e.g., "0.5Gi" -> 0.5)
                                try:
                                    mem_value = float(memory.replace('Gi', '').replace('G', ''))
                                except:
                                    mem_value = 0.5
                                total_vcpu += cpu
                                total_memory_gb += mem_value

                    # Get environment type (consumption vs dedicated)
                    managed_environment_id = getattr(app, 'managed_environment_id', '')
                    is_consumption = 'consumption' in managed_environment_id.lower() if managed_environment_id else True

                    # Estimate monthly cost (assume running 24/7)
                    if is_consumption:
                        # Consumption: per second pricing
                        seconds_per_month = 730 * 3600  # 730 hours
                        vcpu_cost = pricing_info["consumption_vcpu_second"] * total_vcpu * seconds_per_month
                        memory_cost = pricing_info["consumption_gb_second"] * total_memory_gb * seconds_per_month
                        base_cost = vcpu_cost + memory_cost
                    else:
                        # Dedicated: monthly pricing
                        base_cost = (pricing_info["dedicated_vcpu_month"] * total_vcpu +
                                   pricing_info["dedicated_gb_month"] * total_memory_gb)

                    # Multiply by average replicas (assume avg = (min + max) / 2)
                    avg_replicas = max(1, (min_replicas + max_replicas) / 2)
                    estimated_cost = base_cost * avg_replicas

                    resources.append(AllCloudResourceData(
                        resource_id=app.id,
                        resource_type="azure_container_app",
                        resource_name=app.name or "Unnamed Container App",
                        region=app.location,
                        estimated_monthly_cost=round(estimated_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "app_id": app.id,
                            "resource_group": resource_group,
                            "provisioning_state": provisioning_state,
                            "managed_environment_id": managed_environment_id,
                            "is_consumption": is_consumption,
                            "min_replicas": min_replicas,
                            "max_replicas": max_replicas,
                            "total_vcpu": total_vcpu,
                            "total_memory_gb": total_memory_gb,
                            "container_count": len(containers),
                            "ingress_enabled": configuration and hasattr(configuration, 'ingress') and configuration.ingress is not None,
                            "tags": dict(app.tags) if app.tags else {},
                        },
                        is_optimizable=is_optimizable,
                        optimization_score=score,
                        optimization_priority=priority,
                        potential_monthly_savings=savings,
                        optimization_recommendations=recommendations,
                        last_used_at=None,
                        created_at_cloud=None,
                    ))

                except Exception as e:
                    self.logger.error(f"Error processing Container App {getattr(app, 'name', 'unknown')}: {str(e)}")
                    continue

            self.logger.info(f"Found {len(resources)} Container Apps in region {region}")
            return resources

        except Exception as e:
            self.logger.error(f"Error scanning Container Apps: {str(e)}")
            return []

    def _calculate_container_app_optimization(self, app) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate optimization potential for Container App.

        Returns:
            (is_optimizable, score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Get app properties
        provisioning_state = getattr(app, 'provisioning_state', 'Unknown')
        template = getattr(app, 'template', None)
        managed_environment_id = getattr(app, 'managed_environment_id', '')
        is_consumption = 'consumption' in managed_environment_id.lower() if managed_environment_id else True

        # Get scale settings
        min_replicas = 0
        max_replicas = 1
        if template and hasattr(template, 'scale'):
            scale = template.scale
            min_replicas = getattr(scale, 'min_replicas', 0)
            max_replicas = getattr(scale, 'max_replicas', 1)

        # Get resources
        total_vcpu = 0.25
        total_memory_gb = 0.5
        if template and hasattr(template, 'containers'):
            containers = template.containers
            for container in containers:
                resources_config = getattr(container, 'resources', None)
                if resources_config:
                    cpu = getattr(resources_config, 'cpu', 0.25)
                    memory = getattr(resources_config, 'memory', '0.5Gi')
                    try:
                        mem_value = float(memory.replace('Gi', '').replace('G', ''))
                    except:
                        mem_value = 0.5
                    total_vcpu += cpu
                    total_memory_gb += mem_value

        # Estimate monthly cost
        if is_consumption:
            seconds_per_month = 730 * 3600
            base_cost = (0.000012 * total_vcpu * seconds_per_month +
                        0.000003 * total_memory_gb * seconds_per_month)
        else:
            base_cost = 72.0 * total_vcpu + 18.0 * total_memory_gb

        avg_replicas = max(1, (min_replicas + max_replicas) / 2)
        monthly_cost = base_cost * avg_replicas

        # Scenario 1: Container app stopped/deprovisioned (CRITICAL - 90)
        if provisioning_state.lower() in ['deprovisioning', 'failed', 'canceled', 'deleting']:
            is_optimizable = True
            optimization_score = max(optimization_score, 90)
            priority = "critical"
            potential_savings = max(potential_savings, monthly_cost)

            recommendations.append({
                "title": "Container App Non Fonctionnelle",
                "description": f"Cette Container App est dans l'tat '{provisioning_state}'. Elle gnre des cots inutiles.",
                "estimated_savings": round(monthly_cost, 2),
                "actions": [
                    "Vrifier l'tat de l'application et corriger les problmes",
                    "Supprimer l'application si elle n'est plus ncessaire",
                    "Redployer l'application si elle est encore utilise"
                ],
                "priority": "critical",
            })

        # Scenario 2: Zero requests last 30 days (HIGH - 75)
        # Note: We can't get actual metrics without Azure Monitor
        # In production, check ingress metrics

        # Scenario 3: Replicas overprovisioned (HIGH - 70)
        if max_replicas > 3 and min_replicas > 1:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            if priority not in ["critical"]:
                priority = "high"

            # Savings from reducing replicas by 50%
            savings = monthly_cost * 0.5
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Rplicas Potentiellement Surdimensionns",
                "description": f"Cette app est configure avec {min_replicas}-{max_replicas} rplicas. Vrifiez si c'est ncessaire.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Analyser les mtriques de charge CPU/mmoire dans Azure Monitor",
                    f"Rduire min_replicas de {min_replicas}  1 si charge faible",
                    f"Rduire max_replicas de {max_replicas}  3 si pic de charge modr",
                    "Activer l'auto-scaling pour adapter dynamiquement"
                ],
                "priority": "high",
            })

        # Scenario 4: Consumption when Dedicated sufficient (MEDIUM - 50)
        # This is the opposite of typical cloud optimization (dedicated is usually more expensive)
        # Only applicable if usage is very high and predictable
        # Skip this scenario as consumption is typically better

        # Scenario 5: Auto-scaling disabled (LOW - 30)
        if min_replicas == max_replicas and min_replicas > 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            if priority not in ["critical", "high", "medium"]:
                priority = "low"

            # Savings from enabling auto-scaling (assume 20% reduction)
            savings = monthly_cost * 0.2
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Auto-Scaling Dsactiv",
                "description": f"Cette app a un nombre fixe de rplicas ({min_replicas}). L'auto-scaling permettrait d'adapter la capacit.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Activer l'auto-scaling pour adapter automatiquement",
                    "Configurer min_replicas=1 et max_replicas=5 pour commencer",
                    "Dfinir des rgles de scaling bases sur CPU/HTTP requests",
                    "conomies potentielles: ~20% en adaptant aux heures creuses"
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_virtual_desktops(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Virtual Desktop (AVD) host pools for cost intelligence.

        Detection criteria:
        - Host pool stopped/deallocated (CRITICAL - 90 score)
        - Zero active sessions 30 derniers jours (HIGH - 75 score)
        - Session hosts overprovisioned (>50% idle capacity) (HIGH - 70 score)
        - Pooled when Personal sufficient (MEDIUM - 50 score)
        - No auto-scaling configured (LOW - 30 score)
        """
        try:
            from azure.mgmt.desktopvirtualization import DesktopVirtualizationMgmtClient
        except ImportError:
            self.logger.error("azure-mgmt-desktopvirtualization not installed")
            return []

        resources = []
        self.logger.info(f"Scanning Virtual Desktop host pools in region: {region}")

        try:
            vd_client = DesktopVirtualizationMgmtClient(
                credential=self.credential,
                subscription_id=self.subscription_id
            )

            # List all host pools
            async for host_pool in vd_client.host_pools.list():
                try:
                    # Filter by region if specified
                    if region.lower() != "all" and host_pool.location.lower() != region.lower():
                        continue

                    # Get resource group from host pool ID
                    resource_group = host_pool.id.split("/")[4]

                    # Get session hosts count
                    session_hosts = []
                    try:
                        session_hosts = list(vd_client.session_hosts.list(
                            resource_group_name=resource_group,
                            host_pool_name=host_pool.name
                        ))
                    except:
                        pass

                    # Calculate optimization
                    is_optimizable, score, priority, savings, recommendations = (
                        self._calculate_virtual_desktop_optimization(host_pool, session_hosts)
                    )

                    # Pricing (Azure US East 2025)
                    # Session host: D2s_v3 (2 vCPU, 8 GB) = $0.096/h = $70/mo
                    # Storage: Premium SSD $0.135/GB/mo
                    # Typical host pool: 2-10 session hosts = $140-$700/mo
                    pricing_info = {
                        "session_host_hourly": 0.096,  # D2s_v3
                        "session_host_monthly": 70.08,
                        "storage_gb_monthly": 0.135,
                    }

                    # Estimate monthly cost
                    session_host_count = len(session_hosts)
                    base_cost = pricing_info["session_host_monthly"] * max(1, session_host_count)

                    # Add storage estimate (assume 128 GB per session host)
                    storage_gb = 128 * max(1, session_host_count)
                    storage_cost = pricing_info["storage_gb_monthly"] * storage_gb

                    estimated_cost = base_cost + storage_cost

                    # Get host pool properties
                    load_balancer_type = getattr(host_pool, 'load_balancer_type', 'BreadthFirst')
                    max_session_limit = getattr(host_pool, 'max_session_limit', 10)
                    host_pool_type = getattr(host_pool, 'host_pool_type', 'Pooled')

                    resources.append(AllCloudResourceData(
                        resource_id=host_pool.id,
                        resource_type="azure_virtual_desktop",
                        resource_name=host_pool.name or "Unnamed Host Pool",
                        region=host_pool.location,
                        estimated_monthly_cost=round(estimated_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "host_pool_id": host_pool.id,
                            "resource_group": resource_group,
                            "host_pool_type": host_pool_type,
                            "load_balancer_type": load_balancer_type,
                            "max_session_limit": max_session_limit,
                            "session_host_count": session_host_count,
                            "storage_gb": storage_gb,
                            "tags": dict(host_pool.tags) if host_pool.tags else {},
                        },
                        is_optimizable=is_optimizable,
                        optimization_score=score,
                        optimization_priority=priority,
                        potential_monthly_savings=savings,
                        optimization_recommendations=recommendations,
                        last_used_at=None,
                        created_at_cloud=None,
                    ))

                except Exception as e:
                    self.logger.error(f"Error processing Virtual Desktop host pool {getattr(host_pool, 'name', 'unknown')}: {str(e)}")
                    continue

            self.logger.info(f"Found {len(resources)} Virtual Desktop host pools in region {region}")
            return resources

        except Exception as e:
            self.logger.error(f"Error scanning Virtual Desktop host pools: {str(e)}")
            return []

    def _calculate_virtual_desktop_optimization(self, host_pool, session_hosts: list) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate optimization potential for Virtual Desktop host pool.

        Returns:
            (is_optimizable, score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Get host pool properties
        host_pool_type = getattr(host_pool, 'host_pool_type', 'Pooled')
        max_session_limit = getattr(host_pool, 'max_session_limit', 10)
        session_host_count = len(session_hosts)

        # Estimate monthly cost
        session_host_monthly = 70.08  # D2s_v3
        storage_monthly = 0.135 * 128  # 128 GB per host
        monthly_cost = (session_host_monthly + storage_monthly) * max(1, session_host_count)

        # Count active vs stopped session hosts
        active_hosts = 0
        stopped_hosts = 0
        for host in session_hosts:
            status = getattr(host, 'status', 'Unknown')
            if status.lower() in ['available', 'running']:
                active_hosts += 1
            elif status.lower() in ['stopped', 'deallocated', 'unavailable']:
                stopped_hosts += 1

        # Scenario 1: Host pool stopped/deallocated (CRITICAL - 90)
        if session_host_count > 0 and stopped_hosts == session_host_count:
            is_optimizable = True
            optimization_score = max(optimization_score, 90)
            priority = "critical"
            potential_savings = max(potential_savings, monthly_cost)

            recommendations.append({
                "title": "Host Pool Entirement Arrt",
                "description": f"Tous les {session_host_count} session hosts sont arrts. Le host pool gnre des cots de stockage inutiles.",
                "estimated_savings": round(monthly_cost * 0.9, 2),  # Can save 90% (storage remains)
                "actions": [
                    "Dmarrer les session hosts si le host pool est encore utilis",
                    "Supprimer le host pool s'il n'est plus ncessaire",
                    "Configurer auto-start/stop pour optimiser les cots"
                ],
                "priority": "critical",
            })

        # Scenario 2: Zero active sessions 30 days (HIGH - 75)
        # Note: We can't get actual session metrics without Azure Monitor
        # In production, check active session count from monitoring

        # Scenario 3: Session hosts overprovisioned (HIGH - 70)
        if session_host_count > 5 and host_pool_type == 'Pooled':
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            if priority not in ["critical"]:
                priority = "high"

            # Assume 50% of hosts can be removed
            savings = monthly_cost * 0.5
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Session Hosts Potentiellement Surdimensionns",
                "description": f"Ce host pool a {session_host_count} session hosts. Vrifiez si tous sont ncessaires.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Analyser les mtriques d'utilisation dans Azure Monitor",
                    f"Rduire de {session_host_count}  {session_host_count // 2} hosts si charge faible",
                    "Activer l'auto-scaling pour adapter automatiquement",
                    "Considrer le scaling bas sur les heures de bureau"
                ],
                "priority": "high",
            })

        # Scenario 4: Pooled when Personal sufficient (MEDIUM - 50)
        if host_pool_type == 'Pooled' and session_host_count <= 2:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            if priority not in ["critical", "high"]:
                priority = "medium"

            # Personal can be cheaper for small user count
            savings = monthly_cost * 0.2
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Type de Host Pool Potentiellement Inadapt",
                "description": f"Host pool 'Pooled' avec seulement {session_host_count} hosts. Un host pool 'Personal' peut tre plus adapt.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "valuer le nombre d'utilisateurs et leur pattern d'utilisation",
                    "Considrer un host pool 'Personal' si <10 utilisateurs",
                    "Personal offre une exprience plus consistente pour petits groupes",
                    "conomies potentielles: ~20% avec Personal pour usage lger"
                ],
                "priority": "medium",
            })

        # Scenario 5: No auto-scaling configured (LOW - 30)
        # Note: Auto-scaling is configured separately, we can't detect it from host pool properties
        # In production, check if scaling plan exists
        if session_host_count > 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            if priority not in ["critical", "high", "medium"]:
                priority = "low"

            # Savings from auto-scaling (assume 30% reduction during off-hours)
            savings = monthly_cost * 0.3
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Auto-Scaling Non Configur",
                "description": "Ce host pool peut bnficier d'un scaling plan pour adapter la capacit automatiquement.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Crer un scaling plan pour ce host pool",
                    "Configurer scaling bas sur les heures de bureau (8h-18h)",
                    "Rduire automatiquement les hosts pendant les week-ends",
                    "conomies potentielles: ~30% avec auto-scaling optimis"
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_hdinsight_clusters(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure HDInsight Spark/Hadoop clusters for cost intelligence.

        Detection criteria:
        - Cluster stopped/failed (CRITICAL - 90 score)
        - Zero jobs 30 derniers jours (HIGH - 75 score)
        - Worker nodes underutilized (<30% CPU) (HIGH - 70 score)
        - Running 24/7 for batch workloads (MEDIUM - 50 score)
        - No auto-scaling enabled (LOW - 30 score)
        """
        try:
            from azure.mgmt.hdinsight import HDInsightManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-hdinsight not installed")
            return []

        resources = []
        self.logger.info(f"Scanning HDInsight clusters in region: {region}")

        try:
            hdi_client = HDInsightManagementClient(
                credential=self.credential,
                subscription_id=self.subscription_id
            )

            # List all clusters
            async for cluster in hdi_client.clusters.list():
                try:
                    # Filter by region if specified
                    if region.lower() != "all" and cluster.location.lower() != region.lower():
                        continue

                    # Get resource group from cluster ID
                    resource_group = cluster.id.split("/")[4]

                    # Calculate optimization
                    is_optimizable, score, priority, savings, recommendations = (
                        self._calculate_hdinsight_optimization(cluster)
                    )

                    # Pricing (Azure US East 2025)
                    # Head node: D3 v2 (4 vCPU, 14 GB) = $0.21/h = $153/mo x 2 = $307/mo
                    # Worker node: D3 v2 = $0.21/h = $153/mo per node
                    # Storage: Standard $0.05/GB/mo
                    # Typical cluster: 2 head + 4 workers = $922/mo
                    pricing_info = {
                        "head_node_hourly": 0.21,  # D3 v2
                        "head_node_monthly": 153.30,
                        "worker_node_hourly": 0.21,
                        "worker_node_monthly": 153.30,
                        "storage_gb_monthly": 0.05,
                    }

                    # Get cluster configuration
                    cluster_state = getattr(cluster.properties, 'cluster_state', 'Unknown')
                    cluster_version = getattr(cluster.properties, 'cluster_version', 'Unknown')
                    tier = getattr(cluster.properties, 'tier', 'Standard')

                    # Get node counts
                    head_node_count = 2  # Always 2 for HDInsight
                    worker_node_count = 0

                    compute_profile = getattr(cluster.properties, 'compute_profile', None)
                    if compute_profile and hasattr(compute_profile, 'roles'):
                        for role in compute_profile.roles:
                            if role.name == 'workernode':
                                target_instance_count = getattr(role, 'target_instance_count', 0)
                                worker_node_count = target_instance_count

                    # Estimate monthly cost
                    head_cost = pricing_info["head_node_monthly"] * head_node_count
                    worker_cost = pricing_info["worker_node_monthly"] * worker_node_count

                    # Add storage estimate (assume 256 GB per worker node)
                    storage_gb = 256 * max(1, worker_node_count)
                    storage_cost = pricing_info["storage_gb_monthly"] * storage_gb

                    estimated_cost = head_cost + worker_cost + storage_cost

                    # Get cluster type
                    cluster_definition = getattr(cluster.properties, 'cluster_definition', None)
                    kind = 'Hadoop'
                    if cluster_definition and hasattr(cluster_definition, 'kind'):
                        kind = cluster_definition.kind

                    # Determine resource type based on cluster kind
                    # Kafka clusters are specialized streaming platforms, treat them separately
                    is_kafka = kind.lower() == 'kafka'
                    resource_type = "azure_hdinsight_kafka" if is_kafka else "azure_hdinsight_cluster"

                    resources.append(AllCloudResourceData(
                        resource_id=cluster.id,
                        resource_type=resource_type,
                        resource_name=cluster.name or f"Unnamed HDInsight {kind} Cluster",
                        region=cluster.location,
                        estimated_monthly_cost=round(estimated_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "cluster_id": cluster.id,
                            "resource_group": resource_group,
                            "cluster_state": cluster_state,
                            "cluster_version": cluster_version,
                            "tier": tier,
                            "kind": kind,
                            "head_node_count": head_node_count,
                            "worker_node_count": worker_node_count,
                            "storage_gb": storage_gb,
                            "tags": dict(cluster.tags) if cluster.tags else {},
                        },
                        is_optimizable=is_optimizable,
                        optimization_score=score,
                        optimization_priority=priority,
                        potential_monthly_savings=savings,
                        optimization_recommendations=recommendations,
                        last_used_at=None,
                        created_at_cloud=None,
                    ))

                except Exception as e:
                    self.logger.error(f"Error processing HDInsight cluster {getattr(cluster, 'name', 'unknown')}: {str(e)}")
                    continue

            self.logger.info(f"Found {len(resources)} HDInsight clusters in region {region}")
            return resources

        except Exception as e:
            self.logger.error(f"Error scanning HDInsight clusters: {str(e)}")
            return []

    def _calculate_hdinsight_optimization(self, cluster) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate optimization potential for HDInsight cluster.

        Returns:
            (is_optimizable, score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Get cluster properties
        cluster_state = getattr(cluster.properties, 'cluster_state', 'Unknown')
        tier = getattr(cluster.properties, 'tier', 'Standard')

        # Get node counts
        head_node_count = 2
        worker_node_count = 0

        compute_profile = getattr(cluster.properties, 'compute_profile', None)
        if compute_profile and hasattr(compute_profile, 'roles'):
            for role in compute_profile.roles:
                if role.name == 'workernode':
                    worker_node_count = getattr(role, 'target_instance_count', 0)

        # Estimate monthly cost
        head_monthly = 153.30 * head_node_count
        worker_monthly = 153.30 * worker_node_count
        storage_monthly = 0.05 * 256 * max(1, worker_node_count)
        monthly_cost = head_monthly + worker_monthly + storage_monthly

        # Scenario 1: Cluster stopped/failed (CRITICAL - 90)
        if cluster_state.lower() in ['error', 'deleting', 'deleted']:
            is_optimizable = True
            optimization_score = max(optimization_score, 90)
            priority = "critical"
            potential_savings = max(potential_savings, monthly_cost)

            recommendations.append({
                "title": "Cluster en tat d'Erreur",
                "description": f"Ce cluster HDInsight est dans l'tat '{cluster_state}'. Il gnre des cots inutiles.",
                "estimated_savings": round(monthly_cost, 2),
                "actions": [
                    "Vrifier les logs pour identifier le problme",
                    "Supprimer le cluster s'il ne peut pas tre rpar",
                    "Restaurer depuis une configuration sauvegarde si ncessaire"
                ],
                "priority": "critical",
            })

        # Scenario 2: Zero jobs 30 days (HIGH - 75)
        # Note: We can't get actual job metrics without Azure Monitor
        # In production, check job submission history

        # Scenario 3: Worker nodes underutilized (HIGH - 70)
        if worker_node_count > 6:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            if priority not in ["critical"]:
                priority = "high"

            # Assume 50% of worker nodes can be removed
            savings = worker_monthly * 0.5
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Worker Nodes Potentiellement Surdimensionns",
                "description": f"Ce cluster a {worker_node_count} worker nodes. Vrifiez si tous sont ncessaires.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Analyser les mtriques CPU/mmoire dans Azure Monitor",
                    f"Rduire de {worker_node_count}  {worker_node_count // 2} workers si charge <30%",
                    "Activer l'auto-scaling pour adapter automatiquement",
                    "Considrer le scaling bas sur la charge de travail"
                ],
                "priority": "high",
            })

        # Scenario 4: Running 24/7 for batch workloads (MEDIUM - 50)
        if worker_node_count > 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            if priority not in ["critical", "high"]:
                priority = "medium"

            # Assume cluster runs 24/7 but only needed 8h/day
            savings = monthly_cost * 0.67  # Save 16h/day
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Cluster Running 24/7 pour Batch",
                "description": "Ce cluster tourne en permanence. Les workloads batch peuvent tre scheduls.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Identifier si les jobs sont batch ou streaming",
                    "Arrter/dmarrer le cluster selon le schedule des jobs",
                    "Utiliser Azure Data Factory pour orchestrer les pipelines",
                    "conomies potentielles: ~67% en arrtant 16h/jour"
                ],
                "priority": "medium",
            })

        # Scenario 5: No auto-scaling enabled (LOW - 30)
        if worker_node_count > 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            if priority not in ["critical", "high", "medium"]:
                priority = "low"

            # Savings from auto-scaling (assume 25% reduction)
            savings = worker_monthly * 0.25
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Auto-Scaling Non Activ",
                "description": "Ce cluster peut bnficier de l'auto-scaling pour adapter la capacit automatiquement.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Activer l'auto-scaling dans les paramtres du cluster",
                    "Configurer min/max workers selon la charge",
                    "Dfinir des mtriques de scaling (CPU, mmoire, pending tasks)",
                    "conomies potentielles: ~25% avec auto-scaling optimis"
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_ml_compute_instances(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure ML Compute Instances for cost intelligence.

        Detection criteria:
        - Instance stopped but billing (CRITICAL - 90 score)
        - Zero activity 30 derniers jours (HIGH - 75 score)
        - Running but no notebooks active (HIGH - 70 score)
        - GPU instance for CPU workload (MEDIUM - 50 score)
        - No auto-shutdown configured (LOW - 30 score)
        """
        try:
            from azure.mgmt.machinelearningservices import AzureMachineLearningWorkspaces
        except ImportError:
            self.logger.error("azure-mgmt-machinelearningservices not installed")
            return []

        resources = []
        self.logger.info(f"Scanning ML Compute Instances in region: {region}")

        try:
            ml_client = AzureMachineLearningWorkspaces(
                credential=self.credential,
                subscription_id=self.subscription_id
            )

            # List all workspaces first
            async for workspace in ml_client.workspaces.list_by_subscription():
                try:
                    # Filter by region if specified
                    if region.lower() != "all" and workspace.location.lower() != region.lower():
                        continue

                    # Get resource group from workspace ID
                    resource_group = workspace.id.split("/")[4]

                    # List compute instances in this workspace
                    try:
                        compute_instances = list(ml_client.compute.list(
                            resource_group_name=resource_group,
                            workspace_name=workspace.name
                        ))
                    except:
                        continue

                    for compute in compute_instances:
                        try:
                            # Only process ComputeInstance type (not AML clusters)
                            compute_type = getattr(compute.properties, 'compute_type', 'Unknown')
                            if compute_type != 'ComputeInstance':
                                continue

                            # Calculate optimization
                            is_optimizable, score, priority, savings, recommendations = (
                                self._calculate_ml_compute_optimization(compute)
                            )

                            # Pricing (Azure US East 2025)
                            # Standard_DS3_v2 (4 vCPU, 14 GB): $0.21/h = $153/mo
                            # Standard_NC6 (6 vCPU, 56 GB, 1 GPU): $0.90/h = $657/mo
                            # Standard_NC24 (24 vCPU, 224 GB, 4 GPU): $3.60/h = $2628/mo
                            pricing_map = {
                                "Standard_DS3_v2": 153.30,
                                "Standard_DS4_v2": 306.60,
                                "Standard_NC6": 657.00,
                                "Standard_NC12": 1314.00,
                                "Standard_NC24": 2628.00,
                            }

                            # Get VM size
                            vm_size = 'Standard_DS3_v2'
                            compute_properties = getattr(compute.properties, 'properties', None)
                            if compute_properties and hasattr(compute_properties, 'vm_size'):
                                vm_size = compute_properties.vm_size

                            # Estimate monthly cost
                            estimated_cost = pricing_map.get(vm_size, 153.30)

                            # Get instance state
                            provisioning_state = getattr(compute.properties, 'provisioning_state', 'Unknown')
                            state_dict = {}
                            if compute_properties:
                                state = getattr(compute_properties, 'state', 'Unknown')
                                state_dict['state'] = state

                            # Get auto-shutdown settings
                            idle_time_before_shutdown = None
                            if compute_properties and hasattr(compute_properties, 'idle_time_before_shutdown'):
                                idle_time_before_shutdown = compute_properties.idle_time_before_shutdown

                            # Detect if GPU instance
                            is_gpu = 'NC' in vm_size or 'ND' in vm_size or 'NV' in vm_size

                            resources.append(AllCloudResourceData(
                                resource_id=compute.id,
                                resource_type="azure_ml_compute",
                                resource_name=compute.name or "Unnamed ML Compute",
                                region=workspace.location,
                                estimated_monthly_cost=round(estimated_cost, 2),
                                currency="USD",
                                resource_metadata={
                                    "compute_id": compute.id,
                                    "resource_group": resource_group,
                                    "workspace_name": workspace.name,
                                    "provisioning_state": provisioning_state,
                                    "vm_size": vm_size,
                                    "is_gpu": is_gpu,
                                    "idle_time_before_shutdown": idle_time_before_shutdown,
                                    "state": state_dict.get('state', 'Unknown'),
                                    "tags": dict(compute.tags) if compute.tags else {},
                                },
                                is_optimizable=is_optimizable,
                                optimization_score=score,
                                optimization_priority=priority,
                                potential_monthly_savings=savings,
                                optimization_recommendations=recommendations,
                                last_used_at=None,
                                created_at_cloud=None,
                            ))

                        except Exception as e:
                            self.logger.error(f"Error processing ML compute {getattr(compute, 'name', 'unknown')}: {str(e)}")
                            continue

                except Exception as e:
                    self.logger.error(f"Error processing ML workspace {getattr(workspace, 'name', 'unknown')}: {str(e)}")
                    continue

            self.logger.info(f"Found {len(resources)} ML Compute Instances in region {region}")
            return resources

        except Exception as e:
            self.logger.error(f"Error scanning ML Compute Instances: {str(e)}")
            return []

    def _calculate_ml_compute_optimization(self, compute) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate optimization potential for ML Compute Instance.

        Returns:
            (is_optimizable, score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Get compute properties
        provisioning_state = getattr(compute.properties, 'provisioning_state', 'Unknown')
        compute_properties = getattr(compute.properties, 'properties', None)

        vm_size = 'Standard_DS3_v2'
        state = 'Unknown'
        idle_time_before_shutdown = None

        if compute_properties:
            vm_size = getattr(compute_properties, 'vm_size', 'Standard_DS3_v2')
            state = getattr(compute_properties, 'state', 'Unknown')
            idle_time_before_shutdown = getattr(compute_properties, 'idle_time_before_shutdown', None)

        # Pricing map
        pricing_map = {
            "Standard_DS3_v2": 153.30,
            "Standard_DS4_v2": 306.60,
            "Standard_NC6": 657.00,
            "Standard_NC12": 1314.00,
            "Standard_NC24": 2628.00,
        }

        monthly_cost = pricing_map.get(vm_size, 153.30)
        is_gpu = 'NC' in vm_size or 'ND' in vm_size or 'NV' in vm_size

        # Scenario 1: Instance stopped but billing (CRITICAL - 90)
        if provisioning_state.lower() in ['failed', 'deleting', 'deleted']:
            is_optimizable = True
            optimization_score = max(optimization_score, 90)
            priority = "critical"
            potential_savings = max(potential_savings, monthly_cost)

            recommendations.append({
                "title": "Instance en tat d'Erreur",
                "description": f"Cette instance ML est dans l'tat '{provisioning_state}'. Elle gnre des cots inutiles.",
                "estimated_savings": round(monthly_cost, 2),
                "actions": [
                    "Vrifier les logs pour identifier le problme",
                    "Supprimer l'instance si elle ne peut pas tre rpare",
                    "Recrer l'instance si elle est encore ncessaire"
                ],
                "priority": "critical",
            })

        # Scenario 2: Zero activity 30 days (HIGH - 75)
        # Note: We can't get actual usage metrics without Azure Monitor
        # In production, check notebook execution history

        # Scenario 3: Running but no notebooks active (HIGH - 70)
        if state.lower() == 'running':
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            if priority not in ["critical"]:
                priority = "high"

            # Assume instance runs but unused 50% of time
            savings = monthly_cost * 0.5
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Instance Running Sans Activit",
                "description": "Cette instance ML est en cours d'excution. Vrifiez si des notebooks sont actifs.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Vrifier les notebooks actifs dans le workspace",
                    "Arrter l'instance si aucune activit",
                    "Configurer auto-shutdown pour arrter automatiquement",
                    "Utiliser des compute clusters pour workloads batch"
                ],
                "priority": "high",
            })

        # Scenario 4: GPU instance for CPU workload (MEDIUM - 50)
        if is_gpu:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            if priority not in ["critical", "high"]:
                priority = "medium"

            # Savings from switching to CPU instance
            cpu_cost = 153.30  # Standard_DS3_v2
            savings = max(0, monthly_cost - cpu_cost)
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Instance GPU pour Workload CPU",
                "description": f"Instance GPU ({vm_size}) cote {int(monthly_cost)}$/mois. Vrifiez si GPU est ncessaire.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Vrifier si vos notebooks utilisent rellement le GPU",
                    "Passer  Standard_DS3_v2 (CPU) si GPU non utilis",
                    "conomies potentielles: ~{int(savings)}$/mois",
                    "Garder GPU uniquement pour deep learning/training"
                ],
                "priority": "medium",
            })

        # Scenario 5: No auto-shutdown configured (LOW - 30)
        if not idle_time_before_shutdown:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            if priority not in ["critical", "high", "medium"]:
                priority = "low"

            # Savings from auto-shutdown (assume 40% reduction)
            savings = monthly_cost * 0.4
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Auto-Shutdown Non Configur",
                "description": "Cette instance n'a pas d'auto-shutdown. Elle peut tourner inutilement.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Configurer auto-shutdown aprs 30 min d'inactivit",
                    "Paramtrer dans Compute > Settings > Auto-shutdown",
                    "conomies potentielles: ~40% avec auto-shutdown optimis",
                    "Instance redmarre rapidement quand ncessaire"
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_app_services(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure App Services (Web Apps) for cost intelligence.

        IMPORTANT: Excludes Function Apps (already scanned separately).

        Detection criteria:
        - App stopped (CRITICAL - 90 score)
        - Zero requests 30 derniers jours (HIGH - 75 score)
        - Premium/Isolated for dev/test (HIGH - 70 score)
        - Over-provisioned tier vs usage (MEDIUM - 50 score)
        - Always On when not needed (LOW - 30 score)
        """
        try:
            from azure.mgmt.web import WebSiteManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-web not installed")
            return []

        resources = []
        self.logger.info(f"Scanning App Services in region: {region}")

        try:
            web_client = WebSiteManagementClient(
                credential=self.credential,
                subscription_id=self.subscription_id
            )

            # List all web apps
            async for site in web_client.web_apps.list():
                try:
                    # IMPORTANT: Filter OUT Function Apps (already scanned in scan_function_apps)
                    if site.kind and "functionapp" in site.kind.lower():
                        continue

                    # Filter by region if specified
                    if region.lower() != "all" and site.location.lower() != region.lower():
                        continue

                    # Get resource group from site ID
                    resource_group = site.id.split("/")[4]

                    # Calculate optimization
                    is_optimizable, score, priority, savings, recommendations = (
                        self._calculate_app_service_optimization(site)
                    )

                    # Pricing (Azure US East 2025)
                    # Free: $0/mo
                    # Shared: $0/mo (dev/test only)
                    # Basic B1: $13/mo
                    # Standard S1: $73/mo
                    # Premium P1v2: $81/mo
                    # Premium P1v3: $124/mo
                    # Isolated I1: $214/mo
                    pricing_map = {
                        "Free": 0.0,
                        "Shared": 0.0,
                        "Basic": 13.14,
                        "Standard": 73.00,
                        "Premium": 81.03,
                        "PremiumV2": 81.03,
                        "PremiumV3": 124.10,
                        "Isolated": 214.00,
                    }

                    # Get App Service Plan
                    server_farm_id = getattr(site, 'server_farm_id', '')
                    plan_tier = 'Standard'
                    plan_name = 'S1'

                    if server_farm_id:
                        # Extract plan resource group and name from server_farm_id
                        parts = server_farm_id.split('/')
                        if len(parts) >= 9:
                            plan_resource_group = parts[4]
                            plan_name_from_id = parts[8]

                            try:
                                plan = web_client.app_service_plans.get(
                                    resource_group_name=plan_resource_group,
                                    name=plan_name_from_id
                                )
                                if plan and hasattr(plan, 'sku'):
                                    sku = plan.sku
                                    plan_tier = getattr(sku, 'tier', 'Standard')
                                    plan_name = getattr(sku, 'name', 'S1')
                            except:
                                pass

                    # Estimate monthly cost
                    estimated_cost = pricing_map.get(plan_tier, 73.00)

                    # Get app state
                    state = getattr(site, 'state', 'Unknown')
                    enabled = getattr(site, 'enabled', True)

                    # Get site config
                    always_on = False
                    try:
                        site_config = web_client.web_apps.get_configuration(
                            resource_group_name=resource_group,
                            name=site.name
                        )
                        always_on = getattr(site_config, 'always_on', False)
                    except:
                        pass

                    resources.append(AllCloudResourceData(
                        resource_id=site.id,
                        resource_type="azure_app_service",
                        resource_name=site.name or "Unnamed App Service",
                        region=site.location,
                        estimated_monthly_cost=round(estimated_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "app_id": site.id,
                            "resource_group": resource_group,
                            "state": state,
                            "enabled": enabled,
                            "kind": site.kind or "app",
                            "plan_tier": plan_tier,
                            "plan_name": plan_name,
                            "always_on": always_on,
                            "default_host_name": getattr(site, 'default_host_name', ''),
                            "tags": dict(site.tags) if site.tags else {},
                        },
                        is_optimizable=is_optimizable,
                        optimization_score=score,
                        optimization_priority=priority,
                        potential_monthly_savings=savings,
                        optimization_recommendations=recommendations,
                        last_used_at=None,
                        created_at_cloud=None,
                    ))

                except Exception as e:
                    self.logger.error(f"Error processing App Service {getattr(site, 'name', 'unknown')}: {str(e)}")
                    continue

            self.logger.info(f"Found {len(resources)} App Services in region {region}")
            return resources

        except Exception as e:
            self.logger.error(f"Error scanning App Services: {str(e)}")
            return []

    def _calculate_app_service_optimization(self, site) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate optimization potential for App Service.

        Returns:
            (is_optimizable, score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Get site properties
        state = getattr(site, 'state', 'Unknown')
        enabled = getattr(site, 'enabled', True)

        # Get plan info from server_farm_id
        server_farm_id = getattr(site, 'server_farm_id', '')
        plan_tier = 'Standard'

        # Pricing map
        pricing_map = {
            "Free": 0.0,
            "Shared": 0.0,
            "Basic": 13.14,
            "Standard": 73.00,
            "Premium": 81.03,
            "PremiumV2": 81.03,
            "PremiumV3": 124.10,
            "Isolated": 214.00,
        }

        # Try to extract tier from tags or default to Standard
        tags = dict(site.tags) if site.tags else {}
        if 'tier' in tags:
            plan_tier = tags['tier']

        monthly_cost = pricing_map.get(plan_tier, 73.00)

        # Scenario 1: App stopped (CRITICAL - 90)
        if state.lower() == 'stopped' or not enabled:
            is_optimizable = True
            optimization_score = max(optimization_score, 90)
            priority = "critical"
            potential_savings = max(potential_savings, monthly_cost)

            recommendations.append({
                "title": "App Service Arrte",
                "description": f"Cette app est dans l'tat '{state}'. Elle gnre des cots inutiles.",
                "estimated_savings": round(monthly_cost, 2),
                "actions": [
                    "Dmarrer l'app si elle est encore ncessaire",
                    "Supprimer l'app et le plan si plus utilis",
                    "Sauvegarder la configuration avant suppression"
                ],
                "priority": "critical",
            })

        # Scenario 2: Zero requests 30 days (HIGH - 75)
        # Note: We can't get actual request metrics without Azure Monitor
        # In production, check Application Insights metrics

        # Scenario 3: Premium/Isolated for dev/test (HIGH - 70)
        if plan_tier in ['Premium', 'PremiumV2', 'PremiumV3', 'Isolated']:
            # Check if dev/test based on name or tags
            name_lower = site.name.lower() if site.name else ''
            is_dev_test = any(keyword in name_lower for keyword in ['dev', 'test', 'staging', 'qa'])

            if is_dev_test or 'environment' in tags and tags['environment'].lower() in ['dev', 'test', 'staging']:
                is_optimizable = True
                optimization_score = max(optimization_score, 70)
                if priority not in ["critical"]:
                    priority = "high"

                # Savings from downgrading to Basic or Standard
                basic_cost = 13.14
                savings = max(0, monthly_cost - basic_cost)
                potential_savings = max(potential_savings, savings)

                recommendations.append({
                    "title": "Tier Premium pour Environnement Dev/Test",
                    "description": f"App {plan_tier} ({int(monthly_cost)}$/mo) pour dev/test. Basic suffit.",
                    "estimated_savings": round(savings, 2),
                    "actions": [
                        "Passer au tier Basic B1 ($13/mo) pour dev/test",
                        "Garder Premium uniquement pour production",
                        f"conomies potentielles: ~{int(savings)}$/mois",
                        "Performances largement suffisantes pour dev/test"
                    ],
                    "priority": "high",
                })

        # Scenario 4: Over-provisioned tier vs usage (MEDIUM - 50)
        if plan_tier in ['Standard', 'Premium', 'PremiumV2', 'PremiumV3', 'Isolated']:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            if priority not in ["critical", "high"]:
                priority = "medium"

            # Assume can downgrade one tier
            basic_cost = 13.14
            savings = (monthly_cost - basic_cost) * 0.5
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Tier Potentiellement Surdimensionn",
                "description": f"App sur {plan_tier}. Vrifiez si ce tier est ncessaire.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Analyser les mtriques CPU/mmoire dans Azure Monitor",
                    "Passer  un tier infrieur si utilisation <50%",
                    "Tester avec Basic ou Standard si charge faible",
                    "conomies potentielles: ~50% en descendant d'un tier"
                ],
                "priority": "medium",
            })

        # Scenario 5: Always On when not needed (LOW - 30)
        # Note: Always On adds ~10% to cost for keeping instance warm
        # We can't detect it without getting the site config, which we already tried above
        # Skip this scenario as it's low priority and complex to detect

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_redis_caches(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Cache for Redis instances for cost intelligence.

        Detection criteria:
        - Cache stopped/failed (CRITICAL - 90 score)
        - Zero connections 30 derniers jours (HIGH - 75 score)
        - Low cache hit rate <50% (HIGH - 70 score)
        - Premium tier for dev/test (MEDIUM - 50 score)
        - No persistence configured on Premium (LOW - 30 score)
        """
        try:
            from azure.mgmt.redis import RedisManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-redis not installed")
            return []

        resources = []
        self.logger.info(f"Scanning Redis caches in region: {region}")

        try:
            redis_client = RedisManagementClient(
                credential=self.credential,
                subscription_id=self.subscription_id
            )

            # List all Redis caches
            async for cache in redis_client.redis.list():
                try:
                    # Filter by region if specified
                    if region.lower() != "all" and cache.location.lower() != region.lower():
                        continue

                    # Get resource group from cache ID
                    resource_group = cache.id.split("/")[4]

                    # Calculate optimization
                    is_optimizable, score, priority, savings, recommendations = (
                        self._calculate_redis_optimization(cache)
                    )

                    # Pricing (Azure US East 2025)
                    # Basic C0 (250 MB): $16/mo
                    # Basic C1 (1 GB): $55/mo
                    # Standard C0 (250 MB): $32/mo (with replication)
                    # Standard C2 (2.5 GB): $123/mo
                    # Premium P1 (6 GB): $255/mo
                    # Premium P4 (26 GB): $1020/mo
                    pricing_map = {
                        "Basic_C0": 16.24,
                        "Basic_C1": 55.48,
                        "Basic_C2": 110.96,
                        "Standard_C0": 32.48,
                        "Standard_C1": 110.96,
                        "Standard_C2": 123.13,
                        "Standard_C3": 246.26,
                        "Standard_C4": 492.52,
                        "Premium_P1": 255.50,
                        "Premium_P2": 511.00,
                        "Premium_P3": 1022.00,
                        "Premium_P4": 1022.00,
                    }

                    # Get SKU info
                    sku = cache.sku
                    sku_name = getattr(sku, 'name', 'Standard')
                    sku_family = getattr(sku, 'family', 'C')
                    sku_capacity = getattr(sku, 'capacity', 0)

                    # Build pricing key
                    pricing_key = f"{sku_name}_{sku_family}{sku_capacity}"
                    estimated_cost = pricing_map.get(pricing_key, 110.96)  # Default to Standard C1

                    # Get cache properties
                    provisioning_state = getattr(cache, 'provisioning_state', 'Unknown')
                    redis_version = getattr(cache, 'redis_version', 'Unknown')
                    enable_non_ssl_port = getattr(cache, 'enable_non_ssl_port', False)

                    # Get persistence settings (Premium only)
                    redis_configuration = getattr(cache, 'redis_configuration', None)
                    persistence_enabled = False
                    if redis_configuration:
                        rdb_backup_enabled = getattr(redis_configuration, 'rdb_backup_enabled', None)
                        if rdb_backup_enabled:
                            persistence_enabled = True

                    # Get port and hostname
                    port = getattr(cache, 'port', 6379)
                    ssl_port = getattr(cache, 'ssl_port', 6380)
                    host_name = getattr(cache, 'host_name', '')

                    resources.append(AllCloudResourceData(
                        resource_id=cache.id,
                        resource_type="azure_redis_cache",
                        resource_name=cache.name or "Unnamed Redis Cache",
                        region=cache.location,
                        estimated_monthly_cost=round(estimated_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "cache_id": cache.id,
                            "resource_group": resource_group,
                            "provisioning_state": provisioning_state,
                            "sku_name": sku_name,
                            "sku_family": sku_family,
                            "sku_capacity": sku_capacity,
                            "redis_version": redis_version,
                            "enable_non_ssl_port": enable_non_ssl_port,
                            "persistence_enabled": persistence_enabled,
                            "port": port,
                            "ssl_port": ssl_port,
                            "host_name": host_name,
                            "tags": dict(cache.tags) if cache.tags else {},
                        },
                        is_optimizable=is_optimizable,
                        optimization_score=score,
                        optimization_priority=priority,
                        potential_monthly_savings=savings,
                        optimization_recommendations=recommendations,
                        last_used_at=None,
                        created_at_cloud=None,
                    ))

                except Exception as e:
                    self.logger.error(f"Error processing Redis cache {getattr(cache, 'name', 'unknown')}: {str(e)}")
                    continue

            self.logger.info(f"Found {len(resources)} Redis caches in region {region}")
            return resources

        except Exception as e:
            self.logger.error(f"Error scanning Redis caches: {str(e)}")
            return []

    def _calculate_redis_optimization(self, cache) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate optimization potential for Redis cache.

        Returns:
            (is_optimizable, score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Get cache properties
        provisioning_state = getattr(cache, 'provisioning_state', 'Unknown')
        sku = cache.sku
        sku_name = getattr(sku, 'name', 'Standard')
        sku_family = getattr(sku, 'family', 'C')
        sku_capacity = getattr(sku, 'capacity', 0)

        # Pricing map
        pricing_map = {
            "Basic_C0": 16.24,
            "Basic_C1": 55.48,
            "Standard_C0": 32.48,
            "Standard_C1": 110.96,
            "Standard_C2": 123.13,
            "Premium_P1": 255.50,
            "Premium_P2": 511.00,
            "Premium_P3": 1022.00,
            "Premium_P4": 1022.00,
        }

        pricing_key = f"{sku_name}_{sku_family}{sku_capacity}"
        monthly_cost = pricing_map.get(pricing_key, 110.96)

        # Get persistence settings
        redis_configuration = getattr(cache, 'redis_configuration', None)
        persistence_enabled = False
        if redis_configuration:
            rdb_backup_enabled = getattr(redis_configuration, 'rdb_backup_enabled', None)
            if rdb_backup_enabled:
                persistence_enabled = True

        # Scenario 1: Cache stopped/failed (CRITICAL - 90)
        if provisioning_state.lower() in ['failed', 'deleting', 'deleted', 'disabled']:
            is_optimizable = True
            optimization_score = max(optimization_score, 90)
            priority = "critical"
            potential_savings = max(potential_savings, monthly_cost)

            recommendations.append({
                "title": "Redis Cache en tat d'Erreur",
                "description": f"Ce cache Redis est dans l'tat '{provisioning_state}'. Il gnre des cots inutiles.",
                "estimated_savings": round(monthly_cost, 2),
                "actions": [
                    "Vrifier les logs pour identifier le problme",
                    "Supprimer le cache s'il ne peut pas tre rpar",
                    "Recrer le cache si encore ncessaire"
                ],
                "priority": "critical",
            })

        # Scenario 2: Zero connections 30 days (HIGH - 75)
        # Note: We can't get actual connection metrics without Azure Monitor
        # In production, check connection count from monitoring

        # Scenario 3: Low cache hit rate <50% (HIGH - 70)
        # Note: We can't get cache hit rate without Azure Monitor
        # In production, check cache hit/miss ratio

        # Scenario 4: Premium tier for dev/test (MEDIUM - 50)
        if sku_name == 'Premium':
            # Check if dev/test based on name or tags
            name_lower = cache.name.lower() if cache.name else ''
            tags = dict(cache.tags) if cache.tags else {}
            is_dev_test = any(keyword in name_lower for keyword in ['dev', 'test', 'staging', 'qa'])

            if is_dev_test or 'environment' in tags and tags['environment'].lower() in ['dev', 'test', 'staging']:
                is_optimizable = True
                optimization_score = max(optimization_score, 50)
                if priority not in ["critical", "high"]:
                    priority = "medium"

                # Savings from downgrading to Standard
                standard_cost = 110.96  # Standard C1
                savings = max(0, monthly_cost - standard_cost)
                potential_savings = max(potential_savings, savings)

                recommendations.append({
                    "title": "Tier Premium pour Environnement Dev/Test",
                    "description": f"Cache Premium ({int(monthly_cost)}$/mo) pour dev/test. Standard suffit.",
                    "estimated_savings": round(savings, 2),
                    "actions": [
                        "Passer au tier Standard pour dev/test",
                        "Garder Premium uniquement pour production",
                        f"conomies potentielles: ~{int(savings)}$/mois",
                        "Standard offre performances suffisantes pour dev/test"
                    ],
                    "priority": "medium",
                })

        # Scenario 5: No persistence on Premium (LOW - 30)
        if sku_name == 'Premium' and not persistence_enabled:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            if priority not in ["critical", "high", "medium"]:
                priority = "low"

            # Savings: none directly, but best practice recommendation
            savings = 0
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Persistence Non Configure sur Premium",
                "description": "Cache Premium sans persistence. Risque de perte de donnes en cas de redmarrage.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Activer RDB persistence dans les paramtres",
                    "Configurer backup automatique quotidien/hebdomadaire",
                    "Protger contre la perte de donnes",
                    "Cot de persistence: ~5% du prix du cache"
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_event_hubs(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Event Hubs namespaces for cost intelligence.

        Detection criteria:
        - Namespace inactive/failed (CRITICAL - 90 score)
        - Zero incoming messages 30 derniers jours (HIGH - 75 score)
        - Throughput units overprovisioned (HIGH - 70 score)
        - Premium for low-volume workload (MEDIUM - 50 score)
        - Auto-inflate disabled (LOW - 30 score)
        """
        try:
            from azure.mgmt.eventhub import EventHubManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-eventhub not installed")
            return []

        resources = []
        self.logger.info(f"Scanning Event Hubs namespaces in region: {region}")

        try:
            eh_client = EventHubManagementClient(
                credential=self.credential,
                subscription_id=self.subscription_id
            )

            # List all Event Hub namespaces
            async for namespace in eh_client.namespaces.list():
                try:
                    # Filter by region if specified
                    if region.lower() != "all" and namespace.location.lower() != region.lower():
                        continue

                    # Get resource group from namespace ID
                    resource_group = namespace.id.split("/")[4]

                    # Get Event Hubs count
                    event_hubs_count = 0
                    try:
                        event_hubs = list(eh_client.event_hubs.list_by_namespace(
                            resource_group_name=resource_group,
                            namespace_name=namespace.name
                        ))
                        event_hubs_count = len(event_hubs)
                    except:
                        pass

                    # Calculate optimization
                    is_optimizable, score, priority, savings, recommendations = (
                        self._calculate_event_hub_optimization(namespace, event_hubs_count)
                    )

                    # Pricing (Azure US East 2025)
                    # Basic: $11.36/mo (1 throughput unit, 1M events)
                    # Standard: $22.72/mo per throughput unit (1M events, 1 day retention)
                    # Premium: $673/mo per processing unit (100M events, 7 days retention)
                    # Dedicated: Custom pricing (1 CU = ~$8000/mo)
                    pricing_map = {
                        "Basic": 11.36,
                        "Standard": 22.72,  # per TU
                        "Premium": 673.00,  # per PU
                    }

                    # Get SKU info
                    sku = namespace.sku
                    sku_name = getattr(sku, 'name', 'Standard')
                    sku_tier = getattr(sku, 'tier', 'Standard')
                    sku_capacity = getattr(sku, 'capacity', 1)

                    # Estimate monthly cost
                    base_price = pricing_map.get(sku_name, 22.72)
                    estimated_cost = base_price * sku_capacity

                    # Get namespace properties
                    provisioning_state = getattr(namespace, 'provisioning_state', 'Unknown')
                    status = getattr(namespace, 'status', 'Unknown')
                    is_auto_inflate_enabled = getattr(namespace, 'is_auto_inflate_enabled', False)
                    maximum_throughput_units = getattr(namespace, 'maximum_throughput_units', 0)

                    # Get Kafka and zone redundancy settings
                    kafka_enabled = getattr(namespace, 'kafka_enabled', False)
                    zone_redundant = getattr(namespace, 'zone_redundant', False)

                    resources.append(AllCloudResourceData(
                        resource_id=namespace.id,
                        resource_type="azure_event_hub",
                        resource_name=namespace.name or "Unnamed Event Hub Namespace",
                        region=namespace.location,
                        estimated_monthly_cost=round(estimated_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "namespace_id": namespace.id,
                            "resource_group": resource_group,
                            "provisioning_state": provisioning_state,
                            "status": status,
                            "sku_name": sku_name,
                            "sku_tier": sku_tier,
                            "sku_capacity": sku_capacity,
                            "is_auto_inflate_enabled": is_auto_inflate_enabled,
                            "maximum_throughput_units": maximum_throughput_units,
                            "kafka_enabled": kafka_enabled,
                            "zone_redundant": zone_redundant,
                            "event_hubs_count": event_hubs_count,
                            "tags": dict(namespace.tags) if namespace.tags else {},
                        },
                        is_optimizable=is_optimizable,
                        optimization_score=score,
                        optimization_priority=priority,
                        potential_monthly_savings=savings,
                        optimization_recommendations=recommendations,
                        last_used_at=None,
                        created_at_cloud=None,
                    ))

                except Exception as e:
                    self.logger.error(f"Error processing Event Hub namespace {getattr(namespace, 'name', 'unknown')}: {str(e)}")
                    continue

            self.logger.info(f"Found {len(resources)} Event Hub namespaces in region {region}")
            return resources

        except Exception as e:
            self.logger.error(f"Error scanning Event Hub namespaces: {str(e)}")
            return []

    def _calculate_event_hub_optimization(self, namespace, event_hubs_count: int) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate optimization potential for Event Hub namespace.

        Returns:
            (is_optimizable, score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Get namespace properties
        provisioning_state = getattr(namespace, 'provisioning_state', 'Unknown')
        status = getattr(namespace, 'status', 'Unknown')
        sku = namespace.sku
        sku_name = getattr(sku, 'name', 'Standard')
        sku_capacity = getattr(sku, 'capacity', 1)
        is_auto_inflate_enabled = getattr(namespace, 'is_auto_inflate_enabled', False)

        # Pricing map
        pricing_map = {
            "Basic": 11.36,
            "Standard": 22.72,
            "Premium": 673.00,
        }

        base_price = pricing_map.get(sku_name, 22.72)
        monthly_cost = base_price * sku_capacity

        # Scenario 1: Namespace inactive/failed (CRITICAL - 90)
        if provisioning_state.lower() in ['failed', 'deleting', 'deleted'] or status.lower() in ['disabled', 'restoring', 'unknown']:
            is_optimizable = True
            optimization_score = max(optimization_score, 90)
            priority = "critical"
            potential_savings = max(potential_savings, monthly_cost)

            recommendations.append({
                "title": "Event Hub Namespace Non Fonctionnel",
                "description": f"Ce namespace est dans l'tat '{provisioning_state}/{status}'. Il gnre des cots inutiles.",
                "estimated_savings": round(monthly_cost, 2),
                "actions": [
                    "Vrifier les logs pour identifier le problme",
                    "Supprimer le namespace s'il ne peut pas tre rpar",
                    "Recrer le namespace si encore ncessaire"
                ],
                "priority": "critical",
            })

        # Scenario 2: Zero incoming messages 30 days (HIGH - 75)
        # Note: We can't get actual message metrics without Azure Monitor
        # In production, check incoming messages/bytes from monitoring

        # Scenario 3: Throughput units overprovisioned (HIGH - 70)
        if sku_name == 'Standard' and sku_capacity > 3:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            if priority not in ["critical"]:
                priority = "high"

            # Assume can reduce by 50%
            savings = monthly_cost * 0.5
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Throughput Units Potentiellement Surdimensionns",
                "description": f"Ce namespace a {sku_capacity} throughput units. Vrifiez si tous sont ncessaires.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Analyser les mtriques de throughput dans Azure Monitor",
                    f"Rduire de {sku_capacity}  {sku_capacity // 2} TUs si charge <50%",
                    "Activer auto-inflate pour adapter automatiquement",
                    "conomies potentielles: ~50% en rduisant les TUs"
                ],
                "priority": "high",
            })

        # Scenario 4: Premium for low-volume (MEDIUM - 50)
        if sku_name == 'Premium' and event_hubs_count <= 2:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            if priority not in ["critical", "high"]:
                priority = "medium"

            # Savings from downgrading to Standard
            standard_cost = 22.72 * 2  # 2 TUs
            savings = max(0, monthly_cost - standard_cost)
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Tier Premium pour Faible Volume",
                "description": f"Namespace Premium ({int(monthly_cost)}$/mo) avec seulement {event_hubs_count} Event Hubs.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Passer au tier Standard si volume <100M events/mois",
                    "Garder Premium uniquement pour haute performance",
                    f"conomies potentielles: ~{int(savings)}$/mois",
                    "Standard suffit pour la plupart des workloads"
                ],
                "priority": "medium",
            })

        # Scenario 5: Auto-inflate disabled (LOW - 30)
        if sku_name == 'Standard' and not is_auto_inflate_enabled and sku_capacity < 10:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            if priority not in ["critical", "high", "medium"]:
                priority = "low"

            # Savings from auto-inflate (reduce base capacity, scale on demand)
            savings = monthly_cost * 0.2
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Auto-Inflate Non Activ",
                "description": "L'auto-inflate permet d'adapter automatiquement la capacit selon la charge.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Activer auto-inflate dans les paramtres du namespace",
                    "Configurer max throughput units (ex: 10-20)",
                    "Rduire la capacit de base et laisser auto-scale",
                    "conomies potentielles: ~20% en adaptant  la charge"
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_netapp_files(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure NetApp Files capacity pools for cost intelligence.

        Detection criteria:
        - Pool/volume not mounted (CRITICAL - 90 score)
        - Zero IOPS 30 derniers jours (HIGH - 75 score)
        - Ultra tier for standard workload (HIGH - 70 score)
        - Overprovisioned capacity >50% unused (MEDIUM - 50 score)
        - No snapshots configured (LOW - 30 score)
        """
        try:
            from azure.mgmt.netappfiles import NetAppManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-netappfiles not installed")
            return []

        resources = []
        self.logger.info(f"Scanning NetApp Files capacity pools in region: {region}")

        try:
            netapp_client = NetAppManagementClient(
                credential=self.credential,
                subscription_id=self.subscription_id
            )

            # List all NetApp accounts
            async for account in netapp_client.accounts.list():
                try:
                    # Filter by region if specified
                    if region.lower() != "all" and account.location.lower() != region.lower():
                        continue

                    # Get resource group from account ID
                    resource_group = account.id.split("/")[4]

                    # List capacity pools in this account
                    try:
                        pools = list(netapp_client.pools.list(
                            resource_group_name=resource_group,
                            account_name=account.name
                        ))
                    except:
                        pools = []

                    for pool in pools:
                        try:
                            # Get volumes in this pool
                            volumes_count = 0
                            try:
                                volumes = list(netapp_client.volumes.list(
                                    resource_group_name=resource_group,
                                    account_name=account.name,
                                    pool_name=pool.name
                                ))
                                volumes_count = len(volumes)
                            except:
                                pass

                            # Calculate optimization
                            is_optimizable, score, priority, savings, recommendations = (
                                self._calculate_netapp_optimization(pool, volumes_count)
                            )

                            # Pricing (Azure US East 2025)
                            # Standard: $0.000202/GB/hour = $147.50/TB/mo
                            # Premium: $0.000403/GB/hour = $294.19/TB/mo
                            # Ultra: $0.000538/GB/hour = $392.54/TB/mo
                            pricing_map = {
                                "Standard": 0.000202,  # per GB/hour
                                "Premium": 0.000403,
                                "Ultra": 0.000538,
                            }

                            # Get pool properties
                            service_level = getattr(pool, 'service_level', 'Standard')
                            size_bytes = getattr(pool, 'size', 0)
                            size_gb = size_bytes / (1024 ** 3) if size_bytes else 0

                            # Calculate monthly cost (730 hours per month)
                            hourly_rate = pricing_map.get(service_level, 0.000202)
                            estimated_cost = size_gb * hourly_rate * 730

                            # Get pool state
                            provisioning_state = getattr(pool, 'provisioning_state', 'Unknown')

                            # Get QoS type
                            qos_type = getattr(pool, 'qos_type', 'Auto')

                            resources.append(AllCloudResourceData(
                                resource_id=pool.id,
                                resource_type="azure_netapp_files",
                                resource_name=f"{account.name}/{pool.name}",
                                region=account.location,
                                estimated_monthly_cost=round(estimated_cost, 2),
                                currency="USD",
                                resource_metadata={
                                    "pool_id": pool.id,
                                    "account_name": account.name,
                                    "pool_name": pool.name,
                                    "resource_group": resource_group,
                                    "provisioning_state": provisioning_state,
                                    "service_level": service_level,
                                    "size_gb": round(size_gb, 2),
                                    "size_tb": round(size_gb / 1024, 2),
                                    "qos_type": qos_type,
                                    "volumes_count": volumes_count,
                                    "tags": dict(pool.tags) if pool.tags else {},
                                },
                                is_optimizable=is_optimizable,
                                optimization_score=score,
                                optimization_priority=priority,
                                potential_monthly_savings=savings,
                                optimization_recommendations=recommendations,
                                last_used_at=None,
                                created_at_cloud=None,
                            ))

                        except Exception as e:
                            self.logger.error(f"Error processing NetApp pool {getattr(pool, 'name', 'unknown')}: {str(e)}")
                            continue

                except Exception as e:
                    self.logger.error(f"Error processing NetApp account {getattr(account, 'name', 'unknown')}: {str(e)}")
                    continue

            self.logger.info(f"Found {len(resources)} NetApp Files capacity pools in region {region}")
            return resources

        except Exception as e:
            self.logger.error(f"Error scanning NetApp Files: {str(e)}")
            return []

    def _calculate_netapp_optimization(self, pool, volumes_count: int) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate optimization potential for NetApp Files capacity pool.

        Returns:
            (is_optimizable, score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Get pool properties
        provisioning_state = getattr(pool, 'provisioning_state', 'Unknown')
        service_level = getattr(pool, 'service_level', 'Standard')
        size_bytes = getattr(pool, 'size', 0)
        size_gb = size_bytes / (1024 ** 3) if size_bytes else 0
        size_tb = size_gb / 1024

        # Pricing map (per GB/hour)
        pricing_map = {
            "Standard": 0.000202,
            "Premium": 0.000403,
            "Ultra": 0.000538,
        }

        hourly_rate = pricing_map.get(service_level, 0.000202)
        monthly_cost = size_gb * hourly_rate * 730

        # Scenario 1: Pool not mounted / no volumes (CRITICAL - 90)
        if volumes_count == 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 90)
            priority = "critical"
            potential_savings = max(potential_savings, monthly_cost)

            recommendations.append({
                "title": "Capacity Pool Sans Volumes",
                "description": f"Ce pool NetApp ({size_tb:.2f} TB) n'a aucun volume. Il gnre des cots inutiles.",
                "estimated_savings": round(monthly_cost, 2),
                "actions": [
                    "Crer des volumes si le pool est encore ncessaire",
                    "Supprimer le pool s'il n'est plus utilis",
                    f"conomies potentielles: {int(monthly_cost)}$/mois"
                ],
                "priority": "critical",
            })

        # Scenario 2: Zero IOPS 30 days (HIGH - 75)
        # Note: We can't get actual IOPS metrics without Azure Monitor
        # In production, check IOPS/throughput from monitoring

        # Scenario 3: Ultra tier for standard workload (HIGH - 70)
        if service_level == 'Ultra':
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            if priority not in ["critical"]:
                priority = "high"

            # Savings from downgrading to Premium or Standard
            premium_cost = size_gb * 0.000403 * 730
            savings = max(0, monthly_cost - premium_cost)
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Tier Ultra pour Workload Standard",
                "description": f"Pool Ultra ({int(monthly_cost)}$/mo pour {size_tb:.2f} TB). Vrifiez si Ultra est ncessaire.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Analyser les besoins IOPS/throughput rels",
                    "Passer  Premium si <64MB/s ou <4000 IOPS/TB",
                    "Passer  Standard si <16MB/s ou <1000 IOPS/TB",
                    f"conomies potentielles: ~{int(savings)}$/mois avec Premium"
                ],
                "priority": "high",
            })

        # Scenario 4: Overprovisioned capacity (MEDIUM - 50)
        if size_tb > 4:
            # Assume pool is overprovisioned if >4TB
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            if priority not in ["critical", "high"]:
                priority = "medium"

            # Assume 50% overprovisioned
            savings = monthly_cost * 0.5
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Capacit Potentiellement Surdimensionne",
                "description": f"Pool de {size_tb:.2f} TB. Vrifiez l'utilisation relle des volumes.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Analyser l'utilisation actuelle des volumes",
                    "Rduire la taille du pool si usage <50%",
                    "Taille minimum: 4 TB par pool",
                    "conomies potentielles: ~50% en rduisant la capacit"
                ],
                "priority": "medium",
            })

        # Scenario 5: No snapshots configured (LOW - 30)
        # Note: We can't easily detect snapshot policies without additional API calls
        # This is a best practice recommendation
        if volumes_count > 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            if priority not in ["critical", "high", "medium"]:
                priority = "low"

            # No direct savings, but best practice
            savings = 0
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Snapshots Non Configurs",
                "description": "Aucune politique de snapshot dtecte. Risque de perte de donnes.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Configurer une snapshot policy sur les volumes",
                    "Schedule recommand: hourly, daily, weekly",
                    "Snapshots consomment de la capacit du pool",
                    "Cot des snapshots: inclus dans la capacit du pool"
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_cognitive_search(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Cognitive Search services for cost intelligence.

        Detection criteria:
        - Service failed/stopped (CRITICAL - 90 score)
        - Zero queries 30 derniers jours (HIGH - 75 score)
        - Overprovisioned replicas (HIGH - 70 score)
        - Standard tier for low query volume (MEDIUM - 50 score)
        - No indexing activity 90 days (LOW - 30 score)

        Args:
            region: Azure region

        Returns:
            List of Azure Cognitive Search services with optimization analysis
        """
        resources = []

        try:
            from azure.mgmt.search import SearchManagementClient

            search_client = SearchManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing map per tier per month (approximate)
            pricing_map = {
                "free": 0.0,
                "basic": 75.0,  # Basic tier
                "standard": 250.0,  # Standard S1
                "standard2": 1000.0,  # Standard S2
                "standard3": 2400.0,  # Standard S3
                "storage_optimized_l1": 1500.0,  # Storage Optimized L1
                "storage_optimized_l2": 3000.0,  # Storage Optimized L2
            }

            # List all Search services
            search_services = list(search_client.services.list_by_subscription())
            logger.info(
                "inventory.azure.cognitive_search.found",
                region=region,
                count=len(search_services),
            )

            for service in search_services:
                try:
                    # Filter by region
                    if service.location != region:
                        continue

                    service_name = service.name or "Unknown"
                    resource_group = service.id.split("/")[4] if service.id else "Unknown"

                    # Get SKU tier
                    sku_name = service.sku.name.lower() if service.sku and service.sku.name else "unknown"

                    # Get replica and partition count
                    replica_count = getattr(service, "replica_count", 1)
                    partition_count = getattr(service, "partition_count", 1)

                    # Get provisioning state
                    provisioning_state = getattr(service, "provisioning_state", "Unknown")
                    status = getattr(service, "status", "Unknown")

                    # Get search units (replicas  partitions)
                    search_units = replica_count * partition_count

                    # Calculate monthly cost
                    base_cost = pricing_map.get(sku_name, 250.0)
                    monthly_cost = base_cost * search_units

                    # TODO: Get actual metrics from Azure Monitor (queries, indexing operations)
                    # For MVP, use placeholder metrics
                    query_count_30d = 0  # Placeholder
                    indexing_operations_90d = 0  # Placeholder

                    # Calculate optimization potential
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_cognitive_search_optimization(
                        provisioning_state=provisioning_state,
                        status=status,
                        sku_name=sku_name,
                        replica_count=replica_count,
                        partition_count=partition_count,
                        query_count_30d=query_count_30d,
                        indexing_operations_90d=indexing_operations_90d,
                        monthly_cost=monthly_cost,
                    )

                    # Build metadata
                    metadata = {
                        "service_name": service_name,
                        "resource_group": resource_group,
                        "sku": sku_name,
                        "replica_count": replica_count,
                        "partition_count": partition_count,
                        "search_units": search_units,
                        "provisioning_state": provisioning_state,
                        "status": status,
                        "public_network_access": getattr(service, "public_network_access", "Unknown"),
                        "hosting_mode": getattr(service, "hosting_mode", "default"),
                        "optimization_details": recommendations,
                    }

                    resource = AllCloudResourceData(
                        resource_type="azure_cognitive_search",
                        resource_id=service.id or f"search-{service_name}",
                        resource_name=service_name,
                        region=region,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata=metadata,
                        is_optimizable=is_optimizable,
                        optimization_priority=priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations.get("actions", []),
                    )

                    resources.append(resource)
                    logger.info(
                        "inventory.azure.cognitive_search.processed",
                        service_name=service_name,
                        sku=sku_name,
                        cost=monthly_cost,
                        optimizable=is_optimizable,
                    )

                except Exception as e:
                    logger.error(
                        "inventory.azure.cognitive_search.error",
                        service=service.name if hasattr(service, "name") else "Unknown",
                        error=str(e),
                    )
                    continue

        except Exception as e:
            logger.error("inventory.azure.cognitive_search.scan_error", region=region, error=str(e))

        return resources

    def _calculate_cognitive_search_optimization(
        self,
        provisioning_state: str,
        status: str,
        sku_name: str,
        replica_count: int,
        partition_count: int,
        query_count_30d: int,
        indexing_operations_90d: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """Calculate optimization potential for Cognitive Search service."""
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = {"actions": [], "estimated_savings": 0.0, "priority": "low"}

        # CRITICAL (90 score): Service failed or stopped
        if provisioning_state.lower() in ["failed", "deleting"] or status.lower() in [
            "degraded",
            "disabled",
        ]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"Service en tat '{provisioning_state}' - investigate et rparez ou supprimez",
                    "Service non oprationnel - cot 100% vitable",
                    "Action: Vrifier les logs Azure, rparer ou supprimer le service",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "critical",
            })

        # HIGH (75 score): Zero queries in 30 days
        elif query_count_30d == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    "Aucune requte dtecte depuis 30 jours",
                    "Service potentiellement inutilis - considrer la suppression",
                    "Action: Vrifier si le service est encore ncessaire",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # HIGH (70 score): Overprovisioned replicas (>1 replica, low query volume)
        elif replica_count > 1 and query_count_30d < 10000:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Savings = cost of extra replicas
            savings = monthly_cost * ((replica_count - 1) / (replica_count * partition_count))
            potential_savings = savings
            recommendations.update({
                "actions": [
                    f"Replicas surprovisionns: {replica_count} replicas pour faible volume",
                    f"Volume de requtes: {query_count_30d} sur 30 jours",
                    "Action: Rduire  1 replica pour conomiser",
                    f"conomies estimes: ${savings:.2f}/mois",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # MEDIUM (50 score): Standard tier for low query volume
        elif sku_name in ["standard", "standard2", "standard3"] and query_count_30d < 1000:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Savings = downgrade to Basic
            savings = monthly_cost - 75.0  # Basic tier cost
            potential_savings = max(savings, 0.0)
            recommendations.update({
                "actions": [
                    f"Tier Standard pour faible volume: {query_count_30d} queries/30j",
                    f"Cot actuel: ${monthly_cost:.2f}/mois ({sku_name})",
                    "Action: Downgrade vers Basic tier ($75/mois)",
                    f"conomies estimes: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "medium",
            })

        # LOW (30 score): No indexing activity in 90 days
        elif indexing_operations_90d == 0 and monthly_cost > 0:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            savings = monthly_cost * 0.2  # 20% potential savings
            potential_savings = savings
            recommendations.update({
                "actions": [
                    "Aucune opration d'indexation depuis 90 jours",
                    "Index potentiellement obsoltes ou statiques",
                    "Action: Vrifier si le service est encore utilis",
                    f"conomies potentielles: ${savings:.2f}/mois si supprim",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_api_management(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure API Management services for cost intelligence.

        Detection criteria:
        - Service failed/stopped (CRITICAL - 90 score)
        - Zero API calls 30 derniers jours (HIGH - 75 score)
        - Premium/Standard tier for low volume (HIGH - 70 score)
        - Multiple gateways underutilized (MEDIUM - 50 score)
        - Developer tier in production (LOW - 30 score)

        Args:
            region: Azure region

        Returns:
            List of Azure API Management services with optimization analysis
        """
        resources = []

        try:
            from azure.mgmt.apimanagement import ApiManagementClient

            apim_client = ApiManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing map per tier per month (approximate)
            pricing_map = {
                "consumption": 0.0,  # Pay per call: $0.04/10K calls
                "developer": 50.0,  # Developer tier (no SLA)
                "basic": 150.0,  # Basic tier
                "standard": 700.0,  # Standard tier
                "premium": 2795.0,  # Premium tier (per unit)
            }

            # List all API Management services
            apim_services = list(apim_client.api_management_service.list_by_subscription())
            logger.info(
                "inventory.azure.api_management.found",
                region=region,
                count=len(apim_services),
            )

            for service in apim_services:
                try:
                    # Filter by region
                    if service.location != region:
                        continue

                    service_name = service.name or "Unknown"
                    resource_group = service.id.split("/")[4] if service.id else "Unknown"

                    # Get SKU tier
                    sku_name = service.sku.name.lower() if service.sku and service.sku.name else "unknown"
                    sku_capacity = service.sku.capacity if service.sku and service.sku.capacity else 1

                    # Get provisioning state
                    provisioning_state = getattr(service, "provisioning_state", "Unknown")

                    # Get gateway count (self-hosted gateways)
                    gateway_count = 0  # TODO: Query self-hosted gateways via API

                    # Calculate monthly cost
                    base_cost = pricing_map.get(sku_name, 700.0)
                    if sku_name == "consumption":
                        # Consumption tier: $0.04 per 10K calls (estimate 100K calls/month)
                        monthly_cost = (100_000 / 10_000) * 0.04
                    else:
                        monthly_cost = base_cost * sku_capacity

                    # TODO: Get actual metrics from Azure Monitor (API calls, request count)
                    # For MVP, use placeholder metrics
                    api_calls_30d = 0  # Placeholder
                    request_count_30d = 0  # Placeholder

                    # Calculate optimization potential
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_api_management_optimization(
                        provisioning_state=provisioning_state,
                        sku_name=sku_name,
                        sku_capacity=sku_capacity,
                        api_calls_30d=api_calls_30d,
                        request_count_30d=request_count_30d,
                        gateway_count=gateway_count,
                        monthly_cost=monthly_cost,
                    )

                    # Build metadata
                    metadata = {
                        "service_name": service_name,
                        "resource_group": resource_group,
                        "sku": sku_name,
                        "sku_capacity": sku_capacity,
                        "provisioning_state": provisioning_state,
                        "publisher_email": getattr(service, "publisher_email", "Unknown"),
                        "publisher_name": getattr(service, "publisher_name", "Unknown"),
                        "gateway_url": getattr(service, "gateway_url", "Unknown"),
                        "portal_url": getattr(service, "portal_url", "Unknown"),
                        "optimization_details": recommendations,
                    }

                    resource = AllCloudResourceData(
                        resource_type="azure_api_management",
                        resource_id=service.id or f"apim-{service_name}",
                        resource_name=service_name,
                        region=region,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata=metadata,
                        is_optimizable=is_optimizable,
                        optimization_priority=priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations.get("actions", []),
                    )

                    resources.append(resource)
                    logger.info(
                        "inventory.azure.api_management.processed",
                        service_name=service_name,
                        sku=sku_name,
                        cost=monthly_cost,
                        optimizable=is_optimizable,
                    )

                except Exception as e:
                    logger.error(
                        "inventory.azure.api_management.error",
                        service=service.name if hasattr(service, "name") else "Unknown",
                        error=str(e),
                    )
                    continue

        except Exception as e:
            logger.error("inventory.azure.api_management.scan_error", region=region, error=str(e))

        return resources

    def _calculate_api_management_optimization(
        self,
        provisioning_state: str,
        sku_name: str,
        sku_capacity: int,
        api_calls_30d: int,
        request_count_30d: int,
        gateway_count: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """Calculate optimization potential for API Management service."""
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = {"actions": [], "estimated_savings": 0.0, "priority": "low"}

        # CRITICAL (90 score): Service failed or stopped
        if provisioning_state.lower() in ["failed", "deleting", "deleted"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"Service en tat '{provisioning_state}' - investigate et rparez ou supprimez",
                    "Service non oprationnel - cot 100% vitable",
                    "Action: Vrifier les logs Azure, rparer ou supprimer le service",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "critical",
            })

        # HIGH (75 score): Zero API calls in 30 days
        elif api_calls_30d == 0 and request_count_30d == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    "Aucun appel API dtect depuis 30 jours",
                    "Service potentiellement inutilis - considrer la suppression",
                    "Action: Vrifier si le service est encore ncessaire",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # HIGH (70 score): Premium/Standard tier for low volume (<10K calls/day)
        elif sku_name in ["premium", "standard"] and api_calls_30d < 300000:  # <10K/day
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Savings = downgrade to Basic or Consumption
            if sku_name == "premium":
                savings = monthly_cost - 150.0  # Downgrade to Basic
            else:  # standard
                savings = monthly_cost - 150.0  # Downgrade to Basic
            potential_savings = max(savings, 0.0)
            recommendations.update({
                "actions": [
                    f"Tier {sku_name.capitalize()} pour faible volume: {api_calls_30d} calls/30j",
                    f"Cot actuel: ${monthly_cost:.2f}/mois",
                    "Action: Downgrade vers Basic ($150/mois) ou Consumption (pay-per-call)",
                    f"conomies estimes: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # MEDIUM (50 score): Multiple gateways underutilized
        elif gateway_count > 1 and api_calls_30d < 100000:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            savings = monthly_cost * 0.3  # 30% potential savings
            potential_savings = savings
            recommendations.update({
                "actions": [
                    f"Multiples gateways ({gateway_count}) pour faible volume",
                    f"Volume: {api_calls_30d} calls/30 jours",
                    "Action: Consolider vers un seul gateway",
                    f"conomies estimes: ${savings:.2f}/mois",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "medium",
            })

        # LOW (30 score): Developer tier in production (no SLA)
        elif sku_name == "developer" and monthly_cost > 0:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            # No direct savings, but best practice
            savings = 0
            potential_savings = savings
            recommendations.update({
                "actions": [
                    "Developer tier utilis (pas de SLA)",
                    "Tier Developer destin au dveloppement, pas  la production",
                    "Action: Upgrade vers Basic ou Standard pour SLA",
                    "Note: Upgrade cote plus, mais garantit SLA et stabilit",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_cdn(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure CDN profiles and endpoints for cost intelligence.

        Detection criteria:
        - Endpoint stopped/disabled (CRITICAL - 90 score)
        - Zero bandwidth 30 derniers jours (HIGH - 75 score)
        - Zero requests 30 derniers jours (HIGH - 70 score)
        - Premium tier for low traffic (MEDIUM - 50 score)
        - No caching rules configured (LOW - 30 score)

        Args:
            region: Azure region

        Returns:
            List of Azure CDN profiles with optimization analysis
        """
        resources = []

        try:
            from azure.mgmt.cdn import CdnManagementClient

            cdn_client = CdnManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing per GB (approximate)
            pricing_per_gb = {
                "standard_microsoft": 0.087,  # Standard Microsoft
                "standard_akamai": 0.087,  # Standard Akamai
                "standard_verizon": 0.087,  # Standard Verizon
                "premium_verizon": 0.20,  # Premium Verizon
            }

            # List all CDN profiles
            cdn_profiles = list(cdn_client.profiles.list())
            logger.info(
                "inventory.azure.cdn.found",
                region=region,
                count=len(cdn_profiles),
            )

            for profile in cdn_profiles:
                try:
                    # Filter by region
                    if profile.location != region:
                        continue

                    profile_name = profile.name or "Unknown"
                    resource_group = profile.id.split("/")[4] if profile.id else "Unknown"

                    # Get SKU tier
                    sku_name = profile.sku.name.lower() if profile.sku and profile.sku.name else "unknown"

                    # Get provisioning state
                    provisioning_state = getattr(profile, "provisioning_state", "Unknown")

                    # Count endpoints in this profile
                    endpoint_count = 0
                    total_bandwidth_gb = 0
                    total_requests = 0
                    has_caching_rules = False

                    try:
                        endpoints = list(cdn_client.endpoints.list_by_profile(resource_group, profile_name))
                        endpoint_count = len(endpoints)

                        for endpoint in endpoints:
                            # Check if endpoint has caching rules
                            delivery_policy = getattr(endpoint, "delivery_policy", None)
                            if delivery_policy:
                                has_caching_rules = True

                            # TODO: Get actual metrics from Azure Monitor (bandwidth, requests)
                            # For MVP, use placeholder metrics
                            # total_bandwidth_gb += endpoint bandwidth (30 days)
                            # total_requests += endpoint requests (30 days)

                    except Exception as e:
                        logger.warning("inventory.azure.cdn.endpoints_error", profile=profile_name, error=str(e))

                    # Calculate monthly cost estimate
                    # CDN is pay-as-you-go: bandwidth + requests
                    # Estimate: $0.087/GB + $0.0075/10K requests
                    # For MVP, estimate 100 GB/month if endpoints exist
                    estimated_bandwidth_gb = 100 if endpoint_count > 0 else 0
                    bandwidth_cost = estimated_bandwidth_gb * pricing_per_gb.get(sku_name, 0.087)
                    requests_cost = (100_000 / 10_000) * 0.0075  # Estimate 100K requests
                    monthly_cost = bandwidth_cost + requests_cost

                    # Calculate optimization potential
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_cdn_optimization(
                        provisioning_state=provisioning_state,
                        sku_name=sku_name,
                        endpoint_count=endpoint_count,
                        bandwidth_gb_30d=total_bandwidth_gb,
                        requests_30d=total_requests,
                        has_caching_rules=has_caching_rules,
                        monthly_cost=monthly_cost,
                    )

                    # Build metadata
                    metadata = {
                        "profile_name": profile_name,
                        "resource_group": resource_group,
                        "sku": sku_name,
                        "endpoint_count": endpoint_count,
                        "provisioning_state": provisioning_state,
                        "resource_state": getattr(profile, "resource_state", "Unknown"),
                        "has_caching_rules": has_caching_rules,
                        "optimization_details": recommendations,
                    }

                    resource = AllCloudResourceData(
                        resource_type="azure_cdn",
                        resource_id=profile.id or f"cdn-{profile_name}",
                        resource_name=profile_name,
                        region=region,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata=metadata,
                        is_optimizable=is_optimizable,
                        optimization_priority=priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations.get("actions", []),
                    )

                    resources.append(resource)
                    logger.info(
                        "inventory.azure.cdn.processed",
                        profile_name=profile_name,
                        sku=sku_name,
                        endpoints=endpoint_count,
                        cost=monthly_cost,
                        optimizable=is_optimizable,
                    )

                except Exception as e:
                    logger.error(
                        "inventory.azure.cdn.error",
                        profile=profile.name if hasattr(profile, "name") else "Unknown",
                        error=str(e),
                    )
                    continue

        except Exception as e:
            logger.error("inventory.azure.cdn.scan_error", region=region, error=str(e))

        return resources

    def _calculate_cdn_optimization(
        self,
        provisioning_state: str,
        sku_name: str,
        endpoint_count: int,
        bandwidth_gb_30d: float,
        requests_30d: int,
        has_caching_rules: bool,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """Calculate optimization potential for CDN profile."""
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = {"actions": [], "estimated_savings": 0.0, "priority": "low"}

        # CRITICAL (90 score): Profile failed or deleting
        if provisioning_state.lower() in ["failed", "deleting", "deleted"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"Profile en tat '{provisioning_state}' - investigate et rparez ou supprimez",
                    "Profile non oprationnel - cot 100% vitable",
                    "Action: Vrifier les logs Azure, rparer ou supprimer le profile",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "critical",
            })

        # HIGH (75 score): Zero bandwidth in 30 days
        elif bandwidth_gb_30d == 0 and endpoint_count > 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    "Aucune bande passante utilise depuis 30 jours",
                    f"{endpoint_count} endpoint(s) configur(s) mais non utilis(s)",
                    "Action: Supprimer les endpoints inutiliss ou le profile",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # HIGH (70 score): Zero requests in 30 days
        elif requests_30d == 0 and endpoint_count > 0:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    "Aucune requte dtecte depuis 30 jours",
                    f"{endpoint_count} endpoint(s) configur(s) mais non sollicit(s)",
                    "Action: Vrifier si les endpoints sont encore ncessaires",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # MEDIUM (50 score): Premium tier for low traffic (<100GB/month)
        elif "premium" in sku_name and bandwidth_gb_30d < 100:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Savings = downgrade to Standard
            # Premium: $0.20/GB, Standard: $0.087/GB
            savings = bandwidth_gb_30d * (0.20 - 0.087)
            potential_savings = max(savings, 0.0)
            recommendations.update({
                "actions": [
                    f"Premium tier pour faible traffic: {bandwidth_gb_30d:.1f} GB/30j",
                    "Cot Premium: $0.20/GB vs Standard: $0.087/GB",
                    "Action: Downgrade vers Standard Microsoft ou Verizon",
                    f"conomies estimes: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "medium",
            })

        # LOW (30 score): No caching rules configured
        elif not has_caching_rules and endpoint_count > 0:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            # No direct savings, but best practice
            savings = 0
            potential_savings = savings
            recommendations.update({
                "actions": [
                    "Aucune rgle de caching configure",
                    "Rgles de caching optimisent performance et rduisent cots",
                    "Action: Configurer des caching rules (TTL, query string caching)",
                    "Note: conomies indirectes via rduction de bandwidth origin",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_container_instances(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Container Instances for cost intelligence.

        Detection criteria:
        - Container stopped/failed (CRITICAL - 90 score)
        - Zero CPU usage 30 derniers jours (HIGH - 75 score)
        - High cost per container (HIGH - 70 score)
        - Long-running containers (MEDIUM - 50 score)
        - No resource limits configured (LOW - 30 score)

        Args:
            region: Azure region

        Returns:
            List of Azure Container Instances with optimization analysis
        """
        resources = []

        try:
            from azure.mgmt.containerinstance import ContainerInstanceManagementClient

            aci_client = ContainerInstanceManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing per hour (approximate)
            vcpu_price_per_hour = 0.0000125  # ~$0.0000125/vCPU-second
            memory_gb_price_per_hour = 0.0000014  # ~$0.0000014/GB-second

            # List all Container Groups
            container_groups = list(aci_client.container_groups.list())
            logger.info(
                "inventory.azure.container_instances.found",
                region=region,
                count=len(container_groups),
            )

            for group in container_groups:
                try:
                    # Filter by region
                    if group.location != region:
                        continue

                    group_name = group.name or "Unknown"
                    resource_group = group.id.split("/")[4] if group.id else "Unknown"

                    # Get container group properties
                    provisioning_state = getattr(group, "provisioning_state", "Unknown")
                    instance_view_state = getattr(group, "instance_view", None)
                    state = "Unknown"
                    if instance_view_state and hasattr(instance_view_state, "state"):
                        state = instance_view_state.state

                    # Get resource requests (vCPU + memory)
                    total_vcpu = 0.0
                    total_memory_gb = 0.0
                    container_count = 0
                    has_resource_limits = False

                    if group.containers:
                        for container in group.containers:
                            container_count += 1
                            if hasattr(container, "resources") and container.resources:
                                requests = container.resources.requests
                                if requests:
                                    total_vcpu += getattr(requests, "cpu", 0.0)
                                    total_memory_gb += getattr(requests, "memory_in_gb", 0.0)

                                limits = getattr(container.resources, "limits", None)
                                if limits:
                                    has_resource_limits = True

                    # Calculate monthly cost (assuming 730 hours/month)
                    hours_per_month = 730
                    vcpu_cost = total_vcpu * vcpu_price_per_hour * hours_per_month * 3600  # Convert to seconds
                    memory_cost = total_memory_gb * memory_gb_price_per_hour * hours_per_month * 3600
                    monthly_cost = vcpu_cost + memory_cost

                    # Get uptime (creation time)
                    uptime_days = 0
                    if hasattr(group, "instance_view") and group.instance_view:
                        if hasattr(group.instance_view, "events") and group.instance_view.events:
                            # Calculate uptime from first event
                            uptime_days = 30  # Placeholder

                    # TODO: Get actual metrics from Azure Monitor (CPU usage, uptime)
                    # For MVP, use placeholder metrics
                    cpu_usage_percent = 0  # Placeholder

                    # Calculate optimization potential
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_container_instance_optimization(
                        provisioning_state=provisioning_state,
                        state=state,
                        total_vcpu=total_vcpu,
                        total_memory_gb=total_memory_gb,
                        cpu_usage_percent=cpu_usage_percent,
                        uptime_days=uptime_days,
                        has_resource_limits=has_resource_limits,
                        monthly_cost=monthly_cost,
                    )

                    # Build metadata
                    metadata = {
                        "container_group_name": group_name,
                        "resource_group": resource_group,
                        "provisioning_state": provisioning_state,
                        "state": state,
                        "container_count": container_count,
                        "total_vcpu": total_vcpu,
                        "total_memory_gb": total_memory_gb,
                        "os_type": getattr(group, "os_type", "Unknown"),
                        "restart_policy": getattr(group, "restart_policy", "Always"),
                        "has_resource_limits": has_resource_limits,
                        "optimization_details": recommendations,
                    }

                    resource = AllCloudResourceData(
                        resource_type="azure_container_instance",
                        resource_id=group.id or f"aci-{group_name}",
                        resource_name=group_name,
                        region=region,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata=metadata,
                        is_optimizable=is_optimizable,
                        optimization_priority=priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations.get("actions", []),
                    )

                    resources.append(resource)
                    logger.info(
                        "inventory.azure.container_instances.processed",
                        group_name=group_name,
                        vcpu=total_vcpu,
                        memory_gb=total_memory_gb,
                        cost=monthly_cost,
                        optimizable=is_optimizable,
                    )

                except Exception as e:
                    logger.error(
                        "inventory.azure.container_instances.error",
                        group=group.name if hasattr(group, "name") else "Unknown",
                        error=str(e),
                    )
                    continue

        except Exception as e:
            logger.error("inventory.azure.container_instances.scan_error", region=region, error=str(e))

        return resources

    def _calculate_container_instance_optimization(
        self,
        provisioning_state: str,
        state: str,
        total_vcpu: float,
        total_memory_gb: float,
        cpu_usage_percent: float,
        uptime_days: int,
        has_resource_limits: bool,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """Calculate optimization potential for Container Instance."""
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = {"actions": [], "estimated_savings": 0.0, "priority": "low"}

        # CRITICAL (90 score): Container stopped or failed
        if provisioning_state.lower() in ["failed", "deleting"] or state.lower() in ["stopped", "terminated"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"Container en tat '{state}' (provisioning: {provisioning_state})",
                    "Container non oprationnel - cot 100% vitable",
                    "Action: Vrifier les logs, rparer ou supprimer le container group",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "critical",
            })

        # HIGH (75 score): Zero CPU usage in 30 days
        elif cpu_usage_percent == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    "Aucune utilisation CPU dtecte depuis 30 jours",
                    "Container potentiellement inutilis ou en idle permanent",
                    "Action: Vrifier si le container est encore ncessaire",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # HIGH (70 score): High cost per container (>$100/mo)
        elif monthly_cost > 100:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            savings = monthly_cost * 0.5  # 50% potential savings via AKS migration
            potential_savings = savings
            recommendations.update({
                "actions": [
                    f"Cot lev pour Container Instance: ${monthly_cost:.2f}/mois",
                    f"Ressources: {total_vcpu} vCPUs, {total_memory_gb} GB RAM",
                    "Action: Migrer vers Azure Kubernetes Service (AKS) pour conomiser",
                    f"conomies estimes: ${savings:.2f}/mois (50% via AKS)",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # MEDIUM (50 score): Long-running containers (>30 days)
        elif uptime_days > 30:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            savings = monthly_cost * 0.3  # 30% potential savings
            potential_savings = savings
            recommendations.update({
                "actions": [
                    f"Container long-running: {uptime_days} jours d'uptime",
                    "Containers persistants cotent plus cher sur ACI",
                    "Action: Migrer vers AKS ou App Service pour workloads persistants",
                    f"conomies estimes: ${savings:.2f}/mois",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "medium",
            })

        # LOW (30 score): No resource limits configured
        elif not has_resource_limits:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            savings = 0
            potential_savings = savings
            recommendations.update({
                "actions": [
                    "Aucune limite de ressources configure",
                    "Resource limits vitent les dpassements de cots",
                    "Action: Configurer CPU/memory limits pour contrler les cots",
                    "Note: Meilleure pratique pour la gestion des cots",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_logic_apps(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Logic Apps workflows for cost intelligence.

        Detection criteria:
        - Workflow disabled/failed (CRITICAL - 90 score)
        - Zero workflow runs 30 derniers jours (HIGH - 75 score)
        - High execution failure rate (HIGH - 70 score)
        - Consumption plan for high volume (MEDIUM - 50 score)
        - No error handling configured (LOW - 30 score)

        Args:
            region: Azure region

        Returns:
            List of Azure Logic Apps workflows with optimization analysis
        """
        resources = []

        try:
            from azure.mgmt.logic import LogicManagementClient

            logic_client = LogicManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing (approximate)
            action_price_consumption = 0.000025  # After 4000 free actions/month
            connector_price_standard = 0.000125  # Standard connector per call
            vcpu_price_standard = 0.192  # Standard plan per vCPU-hour
            memory_price_standard = 0.0137  # Standard plan per GB-hour

            # List all workflows (we need to list by resource group)
            # For simplicity, we'll query all resource groups
            from azure.mgmt.resource import ResourceManagementClient

            resource_client = ResourceManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            all_workflows = []
            resource_groups = list(resource_client.resource_groups.list())

            for rg in resource_groups:
                try:
                    rg_name = rg.name
                    workflows = list(logic_client.workflows.list_by_resource_group(rg_name))
                    all_workflows.extend([(rg_name, wf) for wf in workflows])
                except Exception as e:
                    logger.warning("inventory.azure.logic_apps.rg_error", rg=rg.name, error=str(e))
                    continue

            logger.info(
                "inventory.azure.logic_apps.found",
                region=region,
                count=len(all_workflows),
            )

            for rg_name, workflow in all_workflows:
                try:
                    # Filter by region
                    if workflow.location != region:
                        continue

                    workflow_name = workflow.name or "Unknown"

                    # Get workflow properties
                    state = getattr(workflow, "state", "Unknown")  # Enabled/Disabled
                    provisioning_state = getattr(workflow, "provisioning_state", "Unknown")

                    # Get workflow definition (to check error handling)
                    has_error_handling = False
                    if hasattr(workflow, "definition") and workflow.definition:
                        definition_str = str(workflow.definition)
                        if "runAfter" in definition_str or "catch" in definition_str.lower():
                            has_error_handling = True

                    # Get integration account (Standard vs Consumption)
                    integration_account = getattr(workflow, "integration_account", None)
                    is_standard = integration_account is not None

                    # TODO: Get actual metrics from Azure Monitor (runs, failures, executions)
                    # For MVP, use placeholder metrics
                    total_runs_30d = 0  # Placeholder
                    failed_runs_30d = 0  # Placeholder
                    failure_rate = 0.0  # Placeholder
                    total_actions_30d = 0  # Placeholder (for Consumption pricing)

                    # Calculate monthly cost estimate
                    if is_standard:
                        # Standard plan: estimate vCPU + memory cost
                        # Assume 1 vCPU + 1.75 GB memory for small workflow
                        monthly_cost = (vcpu_price_standard * 730) + (memory_price_standard * 1.75 * 730)
                    else:
                        # Consumption plan: estimate based on actions
                        # Assume 10K actions/month if workflow exists
                        estimated_actions = 10_000
                        monthly_cost = max(0, (estimated_actions - 4000)) * action_price_consumption
                        monthly_cost += estimated_actions * connector_price_standard * 0.5  # 50% connector calls

                    # Calculate optimization potential
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_logic_app_optimization(
                        state=state,
                        provisioning_state=provisioning_state,
                        total_runs_30d=total_runs_30d,
                        failed_runs_30d=failed_runs_30d,
                        failure_rate=failure_rate,
                        total_actions_30d=total_actions_30d,
                        is_standard=is_standard,
                        has_error_handling=has_error_handling,
                        monthly_cost=monthly_cost,
                    )

                    # Build metadata
                    metadata = {
                        "workflow_name": workflow_name,
                        "resource_group": rg_name,
                        "state": state,
                        "provisioning_state": provisioning_state,
                        "plan_type": "Standard" if is_standard else "Consumption",
                        "has_error_handling": has_error_handling,
                        "sku": getattr(workflow.sku, "name", "Unknown") if hasattr(workflow, "sku") else "Unknown",
                        "optimization_details": recommendations,
                    }

                    resource = AllCloudResourceData(
                        resource_type="azure_logic_app",
                        resource_id=workflow.id or f"logic-{workflow_name}",
                        resource_name=workflow_name,
                        region=region,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata=metadata,
                        is_optimizable=is_optimizable,
                        optimization_priority=priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations.get("actions", []),
                    )

                    resources.append(resource)
                    logger.info(
                        "inventory.azure.logic_apps.processed",
                        workflow_name=workflow_name,
                        state=state,
                        plan_type="Standard" if is_standard else "Consumption",
                        cost=monthly_cost,
                        optimizable=is_optimizable,
                    )

                except Exception as e:
                    logger.error(
                        "inventory.azure.logic_apps.error",
                        workflow=workflow.name if hasattr(workflow, "name") else "Unknown",
                        error=str(e),
                    )
                    continue

        except Exception as e:
            logger.error("inventory.azure.logic_apps.scan_error", region=region, error=str(e))

        return resources

    def _calculate_logic_app_optimization(
        self,
        state: str,
        provisioning_state: str,
        total_runs_30d: int,
        failed_runs_30d: int,
        failure_rate: float,
        total_actions_30d: int,
        is_standard: bool,
        has_error_handling: bool,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """Calculate optimization potential for Logic App workflow."""
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = {"actions": [], "estimated_savings": 0.0, "priority": "low"}

        # CRITICAL (90 score): Workflow disabled or failed
        if state.lower() == "disabled" or provisioning_state.lower() in ["failed", "deleting"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"Workflow en tat '{state}' (provisioning: {provisioning_state})",
                    "Workflow non oprationnel - cot 100% vitable",
                    "Action: Vrifier les logs, rparer ou supprimer le workflow",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "critical",
            })

        # HIGH (75 score): Zero workflow runs in 30 days
        elif total_runs_30d == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    "Aucune excution dtecte depuis 30 jours",
                    "Workflow potentiellement inutilis",
                    "Action: Vrifier si le workflow est encore ncessaire",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # HIGH (70 score): High execution failure rate (>50%)
        elif failure_rate > 0.5:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            savings = monthly_cost * 0.5  # 50% potential savings by fixing errors
            potential_savings = savings
            recommendations.update({
                "actions": [
                    f"Taux d'chec lev: {failure_rate*100:.1f}% des excutions",
                    f"checs: {failed_runs_30d} sur {total_runs_30d} excutions",
                    "Action: Investiguer et corriger les erreurs du workflow",
                    f"conomies estimes: ${savings:.2f}/mois en vitant les re-runs",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # MEDIUM (50 score): Consumption plan for high volume (>1M actions/mo)
        elif not is_standard and total_actions_30d > 1_000_000:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Standard plan would be cheaper for high volume
            standard_cost = (0.192 * 730) + (0.0137 * 1.75 * 730)  # 1 vCPU + 1.75GB
            savings = monthly_cost - standard_cost
            potential_savings = max(savings, 0.0)
            recommendations.update({
                "actions": [
                    f"Consumption plan pour haut volume: {total_actions_30d:,} actions/30j",
                    f"Cot actuel: ${monthly_cost:.2f}/mois",
                    "Action: Migrer vers Standard plan pour conomiser",
                    f"conomies estimes: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "medium",
            })

        # LOW (30 score): No error handling configured
        elif not has_error_handling:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            savings = 0
            potential_savings = savings
            recommendations.update({
                "actions": [
                    "Aucun error handling configur dans le workflow",
                    "Error handling vite les checs coteux et les re-runs",
                    "Action: Ajouter try-catch ou runAfter avec conditions",
                    "Note: Meilleure pratique pour la fiabilit",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_log_analytics(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Log Analytics Workspaces for cost intelligence.

        Detection criteria:
        - Workspace not used/failed (CRITICAL - 90 score)
        - Zero data ingestion 30 derniers jours (HIGH - 75 score)
        - High retention cost (HIGH - 70 score)
        - Pay-as-you-go for high volume (MEDIUM - 50 score)
        - No data retention policy configured (LOW - 30 score)

        Args:
            region: Azure region

        Returns:
            List of Azure Log Analytics Workspaces with optimization analysis
        """
        resources = []

        try:
            from azure.mgmt.loganalytics import LogAnalyticsManagementClient

            log_client = LogAnalyticsManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing per GB (approximate)
            ingestion_price_payg = 2.30  # Pay-as-you-go per GB
            retention_price_per_gb = 0.10  # Per GB per month after 31 days
            commitment_100gb_price = 230.0  # Commitment tier 100GB/day (30% discount)

            # List all workspaces
            workspaces = list(log_client.workspaces.list())
            logger.info(
                "inventory.azure.log_analytics.found",
                region=region,
                count=len(workspaces),
            )

            for workspace in workspaces:
                try:
                    # Filter by region
                    if workspace.location != region:
                        continue

                    workspace_name = workspace.name or "Unknown"
                    resource_group = workspace.id.split("/")[4] if workspace.id else "Unknown"

                    # Get workspace properties
                    provisioning_state = getattr(workspace, "provisioning_state", "Unknown")

                    # Get SKU (pricing tier)
                    sku_name = "Unknown"
                    if hasattr(workspace, "sku") and workspace.sku:
                        sku_name = workspace.sku.name if hasattr(workspace.sku, "name") else "Unknown"

                    # Get retention days
                    retention_days = getattr(workspace, "retention_in_days", 30)
                    has_retention_policy = retention_days > 0

                    # Get daily quota (commitment tier indicator)
                    daily_quota_gb = getattr(workspace, "daily_quota_gb", -1)
                    is_commitment_tier = daily_quota_gb > 0

                    # TODO: Get actual metrics from Azure Monitor (data ingestion, queries)
                    # For MVP, use placeholder metrics
                    total_ingestion_gb_30d = 0  # Placeholder
                    daily_ingestion_gb = 0  # Placeholder
                    query_count_30d = 0  # Placeholder

                    # Calculate monthly cost estimate
                    if is_commitment_tier and daily_quota_gb >= 100:
                        # Commitment tier: 100GB/day = $230/day
                        monthly_cost = commitment_100gb_price * 30
                    else:
                        # Pay-as-you-go: estimate 10GB/day ingestion
                        estimated_daily_ingestion = 10
                        ingestion_cost = estimated_daily_ingestion * 30 * ingestion_price_payg

                        # Retention cost (beyond 31 days free)
                        if retention_days > 31:
                            retention_gb = estimated_daily_ingestion * retention_days
                            retention_cost = retention_gb * retention_price_per_gb
                        else:
                            retention_cost = 0

                        monthly_cost = ingestion_cost + retention_cost

                    # Calculate optimization potential
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_log_analytics_optimization(
                        provisioning_state=provisioning_state,
                        sku_name=sku_name,
                        retention_days=retention_days,
                        has_retention_policy=has_retention_policy,
                        total_ingestion_gb_30d=total_ingestion_gb_30d,
                        daily_ingestion_gb=daily_ingestion_gb,
                        query_count_30d=query_count_30d,
                        is_commitment_tier=is_commitment_tier,
                        monthly_cost=monthly_cost,
                    )

                    # Build metadata
                    metadata = {
                        "workspace_name": workspace_name,
                        "resource_group": resource_group,
                        "provisioning_state": provisioning_state,
                        "sku": sku_name,
                        "retention_days": retention_days,
                        "has_retention_policy": has_retention_policy,
                        "daily_quota_gb": daily_quota_gb,
                        "is_commitment_tier": is_commitment_tier,
                        "public_network_access": getattr(workspace, "public_network_access_for_ingestion", "Unknown"),
                        "optimization_details": recommendations,
                    }

                    resource = AllCloudResourceData(
                        resource_type="azure_log_analytics",
                        resource_id=workspace.id or f"log-{workspace_name}",
                        resource_name=workspace_name,
                        region=region,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata=metadata,
                        is_optimizable=is_optimizable,
                        optimization_priority=priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations.get("actions", []),
                    )

                    resources.append(resource)
                    logger.info(
                        "inventory.azure.log_analytics.processed",
                        workspace_name=workspace_name,
                        sku=sku_name,
                        retention_days=retention_days,
                        cost=monthly_cost,
                        optimizable=is_optimizable,
                    )

                except Exception as e:
                    logger.error(
                        "inventory.azure.log_analytics.error",
                        workspace=workspace.name if hasattr(workspace, "name") else "Unknown",
                        error=str(e),
                    )
                    continue

        except Exception as e:
            logger.error("inventory.azure.log_analytics.scan_error", region=region, error=str(e))

        return resources

    def _calculate_log_analytics_optimization(
        self,
        provisioning_state: str,
        sku_name: str,
        retention_days: int,
        has_retention_policy: bool,
        total_ingestion_gb_30d: float,
        daily_ingestion_gb: float,
        query_count_30d: int,
        is_commitment_tier: bool,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """Calculate optimization potential for Log Analytics Workspace."""
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = {"actions": [], "estimated_savings": 0.0, "priority": "low"}

        # CRITICAL (90 score): Workspace not used or failed
        if provisioning_state.lower() in ["failed", "deleting", "deleted"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"Workspace en tat '{provisioning_state}'",
                    "Workspace non oprationnel - cot 100% vitable",
                    "Action: Vrifier les logs, rparer ou supprimer le workspace",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "critical",
            })

        # HIGH (75 score): Zero data ingestion in 30 days
        elif total_ingestion_gb_30d == 0 and query_count_30d == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    "Aucune ingestion de donnes depuis 30 jours",
                    "Workspace potentiellement inutilis",
                    "Action: Vrifier si le workspace est encore ncessaire",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # HIGH (70 score): High retention cost (>90 days for non-critical data)
        elif retention_days > 90:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Savings = reduce retention from 90+ to 31 days
            savings_per_gb = (retention_days - 31) * 0.10
            estimated_gb = daily_ingestion_gb * retention_days if daily_ingestion_gb > 0 else 300  # 10GB/day * 30 days
            savings = estimated_gb * (savings_per_gb / retention_days)  # Proportional savings
            potential_savings = max(savings, monthly_cost * 0.3)  # At least 30% savings
            recommendations.update({
                "actions": [
                    f"Rtention leve: {retention_days} jours",
                    "Rtention longue cote cher pour donnes non-critiques",
                    "Action: Rduire la rtention  31 jours (gratuit) ou archiver vers blob storage",
                    f"conomies estimes: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # MEDIUM (50 score): Pay-as-you-go for high volume (>100GB/day)
        elif not is_commitment_tier and daily_ingestion_gb > 100:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Commitment tier saves 30%
            current_monthly = daily_ingestion_gb * 30 * 2.30
            commitment_monthly = 230 * 30  # 100GB/day commitment
            savings = current_monthly - commitment_monthly
            potential_savings = max(savings, 0.0)
            recommendations.update({
                "actions": [
                    f"Pay-as-you-go pour haut volume: {daily_ingestion_gb:.1f} GB/jour",
                    f"Cot actuel: ${current_monthly:.2f}/mois",
                    "Action: Migrer vers Commitment Tier (100GB/day) pour conomiser 30%",
                    f"conomies estimes: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "medium",
            })

        # LOW (30 score): No data retention policy configured
        elif not has_retention_policy or retention_days == 0:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            savings = 0
            potential_savings = savings
            recommendations.update({
                "actions": [
                    "Aucune politique de rtention configure",
                    "Rtention par dfaut peut entraner cots non contrls",
                    "Action: Configurer une retention policy adapte aux besoins",
                    "Note: Meilleure pratique pour gestion des cots",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_backup_vaults(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure Backup Vaults for cost intelligence."""
        resources = []

        try:
            from azure.mgmt.recoveryservices import RecoveryServicesClient

            client = RecoveryServicesClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing per protected instance (varies by redundancy and tier)
            pricing_map = {
                "LRS_Standard": 5.0,  # Locally redundant standard
                "LRS_Archive": 2.5,  # Archive tier
                "ZRS_Standard": 6.25,  # Zone redundant
                "GRS_Standard": 10.0,  # Geo redundant standard
                "GRS_Archive": 5.0,  # Geo redundant archive
            }

            # List all Recovery Services Vaults
            vaults = client.vaults.list_by_subscription_id()

            for vault in vaults:
                try:
                    # Extract basic info
                    vault_id = vault.id or "unknown"
                    vault_name = vault.name or "unknown"
                    vault_location = vault.location or region
                    provisioning_state = (
                        vault.properties.provisioning_state
                        if vault.properties
                        else "Unknown"
                    )

                    # Get redundancy settings
                    redundancy = "LRS"
                    if vault.sku and vault.sku.name:
                        redundancy = vault.sku.name  # Standard, RS0 (GRS)

                    # Estimate protected items (requires backup client)
                    protected_items_count = 0
                    backup_jobs_30d = 0
                    backup_policies_count = 0
                    last_backup_time = None

                    try:
                        from azure.mgmt.recoveryservicesbackup import (
                            RecoveryServicesBackupClient,
                        )

                        backup_client = RecoveryServicesBackupClient(
                            credential=self.credential,
                            subscription_id=self.subscription_id,
                        )

                        # Get resource group from vault ID
                        resource_group = vault_id.split("/")[4] if "/" in vault_id else ""

                        # Count protected items
                        try:
                            protected_items = (
                                backup_client.backup_protected_items.list(
                                    vault_name=vault_name, resource_group_name=resource_group
                                )
                            )
                            protected_items_count = sum(1 for _ in protected_items)
                        except Exception:
                            pass

                        # Count backup policies
                        try:
                            policies = backup_client.backup_policies.list(
                                vault_name=vault_name, resource_group_name=resource_group
                            )
                            backup_policies_count = sum(1 for _ in policies)
                        except Exception:
                            pass

                        # Count recent backup jobs
                        try:
                            from datetime import datetime, timedelta

                            start_time = datetime.utcnow() - timedelta(days=30)
                            jobs = backup_client.backup_jobs.list(
                                vault_name=vault_name,
                                resource_group_name=resource_group,
                                filter=f"startTime eq '{start_time.isoformat()}Z'",
                            )
                            backup_jobs_30d = sum(1 for _ in jobs)
                        except Exception:
                            pass

                    except Exception as e:
                        logger.debug(
                            "backup_vault_metrics_error",
                            vault_name=vault_name,
                            error=str(e),
                        )

                    # Calculate monthly cost
                    redundancy_key = f"{redundancy}_Standard"
                    price_per_instance = pricing_map.get(redundancy_key, 5.0)
                    monthly_cost = protected_items_count * price_per_instance

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_backup_vault_optimization(
                        provisioning_state=provisioning_state,
                        protected_items_count=protected_items_count,
                        backup_jobs_30d=backup_jobs_30d,
                        backup_policies_count=backup_policies_count,
                        redundancy=redundancy,
                        monthly_cost=monthly_cost,
                    )

                    # Build metadata
                    metadata = {
                        "vault_id": vault_id,
                        "vault_name": vault_name,
                        "location": vault_location,
                        "provisioning_state": provisioning_state,
                        "redundancy": redundancy,
                        "protected_items": protected_items_count,
                        "backup_jobs_30d": backup_jobs_30d,
                        "backup_policies": backup_policies_count,
                        "price_per_instance": round(price_per_instance, 2),
                        "tags": dict(vault.tags) if vault.tags else {},
                    }

                    # Create resource data
                    resource_data = AllCloudResourceData(
                        resource_id=vault_id,
                        resource_name=vault_name,
                        resource_type="azure_backup_vault",
                        region=vault_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata=metadata,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_priority=priority,
                        potential_monthly_savings=round(potential_savings, 2),
                        optimization_recommendations=recommendations,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        "backup_vault_scan_error",
                        vault_name=vault.name if vault.name else "unknown",
                        error=str(e),
                    )
                    continue

        except Exception as e:
            logger.error("scan_backup_vaults_error", region=region, error=str(e))

        logger.info(
            "scan_backup_vaults_complete",
            region=region,
            total_vaults=len(resources),
        )
        return resources

    def _calculate_backup_vault_optimization(
        self,
        provisioning_state: str,
        protected_items_count: int,
        backup_jobs_30d: int,
        backup_policies_count: int,
        redundancy: str,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """Calculate optimization potential for Backup Vault."""
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = {"actions": [], "estimated_savings": 0.0, "priority": "low"}

        # CRITICAL (90 score): Vault in failed state
        if provisioning_state.lower() in ["failed", "deleting", "deleted"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"Vault en tat '{provisioning_state}'",
                    "Vault non oprationnel - cot 100% vitable",
                    "Action: Vrifier les logs, rparer ou supprimer le vault",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "critical",
            })

        # HIGH (75 score): Zero protected items
        elif protected_items_count == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    "Aucun lment protg dans le vault",
                    "Vault vide - cot 100% vitable",
                    "Action: Supprimer le vault ou commencer  l'utiliser",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # HIGH (70 score): No backup jobs in 30 days
        elif backup_jobs_30d == 0 and protected_items_count > 0:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"{protected_items_count} lments protgs mais aucun backup en 30 jours",
                    "Sauvegardes potentiellement non fonctionnelles",
                    "Action: Vrifier la configuration des policies ou supprimer les items",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # MEDIUM (50 score): GRS redundancy for non-critical data
        elif redundancy in ["RS0", "GeoRedundant"] and protected_items_count > 0:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Savings = switch from GRS ($10) to LRS ($5) per instance
            savings_per_instance = 5.0
            potential_savings = protected_items_count * savings_per_instance
            recommendations.update({
                "actions": [
                    f"Redondance gographique (GRS) pour {protected_items_count} items",
                    f"GRS cote 2x plus cher que LRS (${10.0} vs ${5.0}/instance)",
                    "Action: Migrer vers LRS si la redondance gographique n'est pas critique",
                    f"conomies estimes: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "medium",
            })

        # LOW (30 score): No backup policies configured
        elif backup_policies_count == 0 and protected_items_count > 0:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            savings = 0
            potential_savings = savings
            recommendations.update({
                "actions": [
                    f"{protected_items_count} items protgs sans backup policy",
                    "Absence de policies peut indiquer une configuration incomplte",
                    "Action: Configurer des backup policies appropries",
                    "Note: Meilleure pratique pour gestion des sauvegardes",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_data_factory_pipelines(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure Data Factory instances for cost intelligence."""
        resources = []

        try:
            from azure.mgmt.datafactory import DataFactoryManagementClient

            client = DataFactoryManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing: $0.005 per activity run (orchestration), $1 per vCore-hour (data flow)
            price_per_activity_run = 0.005
            price_per_vcore_hour = 1.0

            # List all Data Factories
            factories = client.factories.list()

            for factory in factories:
                try:
                    # Extract basic info
                    factory_id = factory.id or "unknown"
                    factory_name = factory.name or "unknown"
                    factory_location = factory.location or region
                    provisioning_state = (
                        factory.provisioning_state if factory.provisioning_state else "Unknown"
                    )

                    # Get resource group from factory ID
                    resource_group = factory_id.split("/")[4] if "/" in factory_id else ""

                    # Count pipelines, triggers, and runs
                    pipeline_count = 0
                    trigger_count = 0
                    pipeline_runs_30d = 0
                    failed_runs_30d = 0
                    total_activity_runs = 0

                    try:
                        # Count pipelines
                        pipelines = client.pipelines.list_by_factory(
                            resource_group_name=resource_group, factory_name=factory_name
                        )
                        pipeline_count = sum(1 for _ in pipelines)

                        # Count triggers
                        triggers = client.triggers.list_by_factory(
                            resource_group_name=resource_group, factory_name=factory_name
                        )
                        trigger_count = sum(1 for _ in triggers)

                        # Get pipeline runs from last 30 days
                        try:
                            from datetime import datetime, timedelta

                            end_time = datetime.utcnow()
                            start_time = end_time - timedelta(days=30)

                            # Query pipeline runs
                            filter_params = {
                                "lastUpdatedAfter": start_time,
                                "lastUpdatedBefore": end_time,
                            }

                            pipeline_runs = client.pipeline_runs.query_by_factory(
                                resource_group_name=resource_group,
                                factory_name=factory_name,
                                filter_parameters=filter_params,
                            )

                            for run in pipeline_runs.value if pipeline_runs.value else []:
                                pipeline_runs_30d += 1
                                if run.status in ["Failed", "Cancelled"]:
                                    failed_runs_30d += 1

                            # Estimate activity runs (average 5 activities per pipeline run)
                            total_activity_runs = pipeline_runs_30d * 5

                        except Exception:
                            pass

                    except Exception as e:
                        logger.debug(
                            "data_factory_metrics_error",
                            factory_name=factory_name,
                            error=str(e),
                        )

                    # Calculate monthly cost
                    # Base on activity runs only (data flows require separate analysis)
                    monthly_activity_cost = total_activity_runs * price_per_activity_run
                    monthly_cost = monthly_activity_cost

                    # Calculate optimization
                    failure_rate = (
                        (failed_runs_30d / pipeline_runs_30d * 100)
                        if pipeline_runs_30d > 0
                        else 0
                    )

                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_data_factory_optimization(
                        provisioning_state=provisioning_state,
                        pipeline_count=pipeline_count,
                        trigger_count=trigger_count,
                        pipeline_runs_30d=pipeline_runs_30d,
                        failed_runs_30d=failed_runs_30d,
                        failure_rate=failure_rate,
                        monthly_cost=monthly_cost,
                    )

                    # Build metadata
                    metadata = {
                        "factory_id": factory_id,
                        "factory_name": factory_name,
                        "location": factory_location,
                        "provisioning_state": provisioning_state,
                        "pipeline_count": pipeline_count,
                        "trigger_count": trigger_count,
                        "pipeline_runs_30d": pipeline_runs_30d,
                        "failed_runs_30d": failed_runs_30d,
                        "failure_rate_pct": round(failure_rate, 2),
                        "total_activity_runs": total_activity_runs,
                        "price_per_activity": price_per_activity_run,
                        "tags": dict(factory.tags) if factory.tags else {},
                    }

                    # Create resource data
                    resource_data = AllCloudResourceData(
                        resource_id=factory_id,
                        resource_name=factory_name,
                        resource_type="azure_data_factory_pipeline",
                        region=factory_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata=metadata,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_priority=priority,
                        potential_monthly_savings=round(potential_savings, 2),
                        optimization_recommendations=recommendations,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        "data_factory_scan_error",
                        factory_name=factory.name if factory.name else "unknown",
                        error=str(e),
                    )
                    continue

        except Exception as e:
            logger.error("scan_data_factory_pipelines_error", region=region, error=str(e))

        logger.info(
            "scan_data_factory_pipelines_complete",
            region=region,
            total_factories=len(resources),
        )
        return resources

    def _calculate_data_factory_optimization(
        self,
        provisioning_state: str,
        pipeline_count: int,
        trigger_count: int,
        pipeline_runs_30d: int,
        failed_runs_30d: int,
        failure_rate: float,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """Calculate optimization potential for Data Factory."""
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = {"actions": [], "estimated_savings": 0.0, "priority": "low"}

        # CRITICAL (90 score): Data Factory in failed state
        if provisioning_state.lower() in ["failed", "deleting", "deleted"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"Data Factory en tat '{provisioning_state}'",
                    "Data Factory non oprationnel - cot 100% vitable",
                    "Action: Vrifier les logs, rparer ou supprimer la Data Factory",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "critical",
            })

        # HIGH (75 score): Zero pipeline runs in 30 days
        elif pipeline_runs_30d == 0 and pipeline_count > 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"{pipeline_count} pipelines mais aucun run en 30 jours",
                    "Data Factory potentiellement inutilise",
                    "Action: Supprimer la Data Factory ou activer les pipelines",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # HIGH (70 score): High failure rate >50%
        elif failure_rate > 50 and pipeline_runs_30d > 0:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Assume failures waste 50% of costs
            potential_savings = monthly_cost * 0.5
            recommendations.update({
                "actions": [
                    f"Taux d'chec lev: {failure_rate:.1f}% ({failed_runs_30d}/{pipeline_runs_30d} runs)",
                    "checs frquents gaspillent des ressources",
                    "Action: Dbugger les pipelines, amliorer la gestion d'erreurs",
                    f"conomies estimes: ${potential_savings:.2f}/mois (50% de rduction d'checs)",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # MEDIUM (50 score): Many pipelines with no recent runs
        elif pipeline_count >= 5 and pipeline_runs_30d == 0:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Assume we can delete half the unused pipelines
            potential_savings = monthly_cost * 0.5 if monthly_cost > 0 else 0
            recommendations.update({
                "actions": [
                    f"{pipeline_count} pipelines inactifs",
                    "Pipelines inutiliss crent de la complexit et du risque",
                    "Action: Nettoyer les pipelines obsoltes",
                    f"conomies estimes: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "medium",
            })

        # LOW (30 score): No triggers configured
        elif trigger_count == 0 and pipeline_count > 0:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            savings = 0
            potential_savings = savings
            recommendations.update({
                "actions": [
                    f"{pipeline_count} pipelines sans triggers",
                    "Absence de triggers peut indiquer configuration manuelle",
                    "Action: Configurer des triggers pour automation",
                    "Note: Meilleure pratique pour orchestration automatise",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_synapse_serverless_sql(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure Synapse Analytics workspaces for serverless SQL pool cost intelligence."""
        resources = []

        try:
            from azure.mgmt.synapse import SynapseManagementClient

            client = SynapseManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing: $5 per TB of data processed
            price_per_tb = 5.0

            # List all Synapse workspaces
            workspaces = client.workspaces.list()

            for workspace in workspaces:
                try:
                    # Extract basic info
                    workspace_id = workspace.id or "unknown"
                    workspace_name = workspace.name or "unknown"
                    workspace_location = workspace.location or region
                    provisioning_state = workspace.provisioning_state if workspace.provisioning_state else "Unknown"

                    # Serverless SQL pool is built-in to every workspace
                    # Endpoint format: {workspace_name}-ondemand.sql.azuresynapse.net
                    serverless_endpoint = f"{workspace_name}-ondemand.sql.azuresynapse.net" if workspace.connectivity_endpoints else "unknown"

                    # Check if workspace has serverless SQL endpoint active
                    has_serverless_sql = False
                    if workspace.connectivity_endpoints:
                        if "sqlOnDemand" in workspace.connectivity_endpoints:
                            has_serverless_sql = True
                            serverless_endpoint = workspace.connectivity_endpoints.get("sqlOnDemand", serverless_endpoint)

                    # Estimate monthly data processed (would require actual query metrics)
                    # For now, we'll use placeholder values and flag for investigation
                    estimated_tb_per_month = 0.0  # Would need actual metrics from Azure Monitor
                    monthly_cost = estimated_tb_per_month * price_per_tb

                    # Get resource group from workspace ID
                    resource_group = workspace_id.split("/")[4] if "/" in workspace_id else ""

                    # Try to get SQL pools count
                    sql_pools_count = 0
                    try:
                        sql_pools = client.sql_pools.list_by_workspace(
                            resource_group_name=resource_group,
                            workspace_name=workspace_name
                        )
                        sql_pools_count = sum(1 for _ in sql_pools)
                    except Exception:
                        pass

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_synapse_serverless_optimization(
                        provisioning_state=provisioning_state,
                        has_serverless_sql=has_serverless_sql,
                        sql_pools_count=sql_pools_count,
                        estimated_tb_per_month=estimated_tb_per_month,
                        monthly_cost=monthly_cost,
                    )

                    # Build metadata
                    metadata = {
                        "workspace_id": workspace_id,
                        "workspace_name": workspace_name,
                        "location": workspace_location,
                        "provisioning_state": provisioning_state,
                        "has_serverless_sql": has_serverless_sql,
                        "serverless_endpoint": serverless_endpoint,
                        "sql_pools_count": sql_pools_count,
                        "estimated_tb_per_month": round(estimated_tb_per_month, 2),
                        "price_per_tb": price_per_tb,
                        "tags": dict(workspace.tags) if workspace.tags else {},
                    }

                    # Create resource data
                    resource_data = AllCloudResourceData(
                        resource_id=workspace_id,
                        resource_name=workspace_name,
                        resource_type="azure_synapse_serverless_sql",
                        region=workspace_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata=metadata,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_priority=priority,
                        potential_monthly_savings=round(potential_savings, 2),
                        optimization_recommendations=recommendations,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        "synapse_workspace_scan_error",
                        workspace_name=workspace.name if workspace.name else "unknown",
                        error=str(e),
                    )
                    continue

        except Exception as e:
            logger.error("scan_synapse_serverless_sql_error", region=region, error=str(e))

        logger.info(
            "scan_synapse_serverless_sql_complete",
            region=region,
            total_workspaces=len(resources),
        )
        return resources

    def _calculate_synapse_serverless_optimization(
        self,
        provisioning_state: str,
        has_serverless_sql: bool,
        sql_pools_count: int,
        estimated_tb_per_month: float,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """Calculate optimization potential for Synapse Serverless SQL."""
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = {"actions": [], "estimated_savings": 0.0, "priority": "low"}

        # CRITICAL (90 score): Workspace in failed state
        if provisioning_state.lower() in ["failed", "deleting", "deleted"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"Workspace Synapse en tat '{provisioning_state}'",
                    "Workspace non oprationnel - cot potentiellement vitable",
                    "Action: Vrifier les logs, rparer ou supprimer le workspace",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "critical",
            })

        # HIGH (75 score): Workspace actif sans serverless SQL pool configur
        elif not has_serverless_sql and sql_pools_count == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = 0.0  # No cost if not using it, but workspace overhead
            recommendations.update({
                "actions": [
                    "Workspace Synapse sans serverless SQL ni dedicated pools",
                    "Workspace potentiellement inutilis",
                    "Action: Supprimer le workspace ou commencer  l'utiliser",
                    "Note: conomies indirectes (complexit, maintenance)",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # HIGH (70 score): High data processing cost (>$500/month)
        elif monthly_cost > 500:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Suggest optimization strategies (partitioning, caching, etc.)
            potential_savings = monthly_cost * 0.3  # 30% savings through optimization
            recommendations.update({
                "actions": [
                    f"Cot lev de data processing: ${monthly_cost:.2f}/mois ({estimated_tb_per_month:.2f} TB)",
                    "Usage intensif de serverless SQL peut tre optimis",
                    "Action: Analyser les queries, implmenter caching, partitioning, ou considrer dedicated SQL pool",
                    f"conomies estimes: ${potential_savings:.2f}/mois (30% rduction)",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # MEDIUM (50 score): Workspace inactif mais serverless SQL enabled
        elif has_serverless_sql and estimated_tb_per_month == 0.0:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            potential_savings = 0.0  # Serverless has no idle cost, but investigate
            recommendations.update({
                "actions": [
                    "Serverless SQL endpoint actif mais aucune donne processed en 30 jours",
                    "Workspace potentiellement inutilis ou mtriques non disponibles",
                    "Action: Vrifier l'usage rel via Azure Monitor ou supprimer si inutilis",
                    "Note: Serverless SQL n'a pas de cot idle (pay-per-query uniquement)",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "medium",
            })

        # LOW (30 score): Pas de quotas/budgets configurs
        elif has_serverless_sql and monthly_cost == 0.0:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            savings = 0
            potential_savings = savings
            recommendations.update({
                "actions": [
                    "Serverless SQL actif sans quotas/budgets configurs",
                    "Absence de limites peut entraner cots non contrls",
                    "Action: Configurer des cost controls via Azure Cost Management",
                    "Note: Meilleure pratique pour gestion des cots",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_storage_sftp(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure Storage Accounts with SFTP enabled for cost intelligence."""
        resources = []

        try:
            from azure.mgmt.storage import StorageManagementClient

            client = StorageManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing: $0.30/hour ($220/month) + storage costs
            sftp_hourly_cost = 0.30
            sftp_monthly_cost = sftp_hourly_cost * 24 * 30  # ~$216/month

            # List all storage accounts
            storage_accounts = client.storage_accounts.list()

            for account in storage_accounts:
                try:
                    # Check if SFTP is enabled
                    if not account.is_sftp_enabled:
                        continue  # Skip accounts without SFTP

                    # Extract basic info
                    account_id = account.id or "unknown"
                    account_name = account.name or "unknown"
                    account_location = account.location or region
                    provisioning_state = safe_get_value(account.provisioning_state, "Unknown")

                    # Get resource group from account ID
                    resource_group = account_id.split("/")[4] if "/" in account_id else ""

                    # Get account properties
                    sku_name = safe_get_value(account.sku.name if account.sku else None, "Unknown")
                    account_kind = safe_get_value(account.kind, "Unknown")

                    # Estimate storage usage (would need actual metrics)
                    storage_used_gb = 0.0  # Would need Azure Monitor metrics
                    storage_cost = 0.0  # Depends on tier, redundancy, etc.

                    # Total monthly cost: SFTP fee + storage
                    monthly_cost = sftp_monthly_cost + storage_cost

                    # Try to estimate transaction count (would need metrics)
                    transaction_count_30d = 0  # Would need actual metrics

                    # Calculate days since SFTP enabled (if creation time available)
                    days_sftp_enabled = 30  # Placeholder
                    if account.creation_time:
                        from datetime import datetime
                        delta = datetime.utcnow() - account.creation_time
                        days_sftp_enabled = delta.days

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_storage_sftp_optimization(
                        provisioning_state=provisioning_state,
                        days_sftp_enabled=days_sftp_enabled,
                        transaction_count_30d=transaction_count_30d,
                        sftp_monthly_cost=sftp_monthly_cost,
                        monthly_cost=monthly_cost,
                    )

                    # Build metadata
                    metadata = {
                        "account_id": account_id,
                        "account_name": account_name,
                        "location": account_location,
                        "provisioning_state": provisioning_state,
                        "is_sftp_enabled": True,
                        "sku": sku_name,
                        "kind": account_kind,
                        "days_sftp_enabled": days_sftp_enabled,
                        "transaction_count_30d": transaction_count_30d,
                        "storage_used_gb": round(storage_used_gb, 2),
                        "sftp_hourly_cost": sftp_hourly_cost,
                        "sftp_monthly_cost": round(sftp_monthly_cost, 2),
                        "tags": dict(account.tags) if account.tags else {},
                    }

                    # Create resource data
                    resource_data = AllCloudResourceData(
                        resource_id=account_id,
                        resource_name=account_name,
                        resource_type="azure_storage_sftp",
                        region=account_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata=metadata,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_priority=priority,
                        potential_monthly_savings=round(potential_savings, 2),
                        optimization_recommendations=recommendations,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        "storage_sftp_scan_error",
                        account_name=account.name if account.name else "unknown",
                        error=str(e),
                    )
                    continue

        except Exception as e:
            logger.error("scan_storage_sftp_error", region=region, error=str(e))

        logger.info(
            "scan_storage_sftp_complete",
            region=region,
            total_sftp_accounts=len(resources),
        )
        return resources

    def _calculate_storage_sftp_optimization(
        self,
        provisioning_state: str,
        days_sftp_enabled: int,
        transaction_count_30d: int,
        sftp_monthly_cost: float,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """Calculate optimization potential for Storage Account with SFTP."""
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = {"actions": [], "estimated_savings": 0.0, "priority": "low"}

        # CRITICAL (90 score): Storage account failed but SFTP still enabled
        if provisioning_state.lower() in ["failed", "deleting", "deleted"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = sftp_monthly_cost
            recommendations.update({
                "actions": [
                    f"Storage Account en tat '{provisioning_state}' mais SFTP enabled",
                    f"SFTP cote ${sftp_monthly_cost:.2f}/mois mme si account failed",
                    "Action: Dsactiver SFTP ou supprimer le storage account",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "critical",
            })

        # HIGH (75 score): SFTP enabled >30 days sans transactions
        elif days_sftp_enabled >= 30 and transaction_count_30d == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = sftp_monthly_cost
            recommendations.update({
                "actions": [
                    f"SFTP enabled depuis {days_sftp_enabled} jours sans aucune transaction",
                    f"SFTP inutilis gaspille ${sftp_monthly_cost:.2f}/mois",
                    "Action: Dsactiver SFTP (peut ractiver au besoin sans perte de config)",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # HIGH (70 score): SFTP enabled avec trs peu de transactions
        elif transaction_count_30d > 0 and transaction_count_30d < 100:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = sftp_monthly_cost
            recommendations.update({
                "actions": [
                    f"SFTP enabled avec seulement {transaction_count_30d} transactions en 30 jours",
                    f"Usage trs faible pour ${sftp_monthly_cost:.2f}/mois",
                    "Action: Dsactiver SFTP et enable on-demand, ou utiliser alternative (Azure Files)",
                    f"conomies estimes: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # MEDIUM (50 score): SFTP enabled en permanence pour usage occasionnel
        elif transaction_count_30d >= 100 and transaction_count_30d < 1000:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Suggest enable/disable strategy instead of always-on
            potential_savings = sftp_monthly_cost * 0.5  # 50% savings with on-demand
            recommendations.update({
                "actions": [
                    f"SFTP enabled 24/7 avec {transaction_count_30d} transactions/mois",
                    f"Usage modr pour ${sftp_monthly_cost:.2f}/mois always-on",
                    "Action: Enable SFTP uniquement quand ncessaire (disable aprs usage)",
                    f"conomies estimes: ${potential_savings:.2f}/mois (50% rduction)",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "medium",
            })

        # LOW (30 score): Pas de monitoring des connexions SFTP
        elif transaction_count_30d == 0:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            savings = 0
            potential_savings = savings
            recommendations.update({
                "actions": [
                    "SFTP enabled sans monitoring des connexions configur",
                    "Mtriques non disponibles pour analyser l'usage rel",
                    "Action: Configurer Azure Monitor pour tracking SFTP usage",
                    "Note: Meilleure pratique pour gestion des cots",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_ad_domain_services(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure AD Domain Services (Microsoft Entra Domain Services) for cost intelligence."""
        resources = []

        try:
            from azure.mgmt.resource import ResourceManagementClient

            resource_client = ResourceManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing per SKU per month
            pricing_map = {
                "Standard": 109,  # Estimated (not found in search results)
                "Enterprise": 292,
                "Premium": 1168,
            }

            # Query for Microsoft.AAD/domainServices resources
            filter_query = "resourceType eq 'Microsoft.AAD/domainServices'"
            domain_services = resource_client.resources.list(filter=filter_query)

            for domain_service in domain_services:
                try:
                    # Extract basic info
                    domain_id = domain_service.id or "unknown"
                    domain_name = domain_service.name or "unknown"
                    domain_location = domain_service.location or region

                    # Get resource properties
                    properties = domain_service.properties if domain_service.properties else {}

                    # Extract SKU
                    sku = properties.get("sku", "Unknown")
                    if isinstance(sku, dict):
                        sku = sku.get("name", "Unknown")

                    # Get provisioning state
                    provisioning_state = properties.get("provisioningState", "Unknown")

                    # Get health status
                    health_monitors = properties.get("healthMonitors", [])
                    health_alerts = properties.get("healthAlerts", [])
                    has_health_alerts = len(health_alerts) > 0 if health_alerts else False

                    # Get sync status
                    sync_scope = properties.get("syncScope", "Unknown")
                    sync_owner = properties.get("syncOwner", "Unknown")

                    # Get deployment configuration
                    replica_sets = properties.get("replicaSets", [])
                    replica_count = len(replica_sets) if replica_sets else 0

                    # Calculate monthly cost based on SKU
                    monthly_cost = pricing_map.get(sku, 109)  # Default to Standard if unknown

                    # Estimate VMs joined to domain (would need actual metrics)
                    vms_joined = 0  # Would need Azure Monitor or LDAP queries
                    auth_requests_per_hour = 0  # Would need metrics

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_ad_domain_services_optimization(
                        provisioning_state=provisioning_state,
                        sku=sku,
                        has_health_alerts=has_health_alerts,
                        vms_joined=vms_joined,
                        auth_requests_per_hour=auth_requests_per_hour,
                        monthly_cost=monthly_cost,
                    )

                    # Build metadata
                    metadata = {
                        "domain_id": domain_id,
                        "domain_name": domain_name,
                        "location": domain_location,
                        "provisioning_state": provisioning_state,
                        "sku": sku,
                        "sync_scope": sync_scope,
                        "sync_owner": sync_owner,
                        "replica_count": replica_count,
                        "has_health_alerts": has_health_alerts,
                        "health_alerts_count": len(health_alerts) if health_alerts else 0,
                        "vms_joined": vms_joined,
                        "auth_requests_per_hour": auth_requests_per_hour,
                        "tags": dict(domain_service.tags) if domain_service.tags else {},
                    }

                    # Create resource data
                    resource_data = AllCloudResourceData(
                        resource_id=domain_id,
                        resource_name=domain_name,
                        resource_type="azure_ad_domain_services",
                        region=domain_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata=metadata,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_priority=priority,
                        potential_monthly_savings=round(potential_savings, 2),
                        optimization_recommendations=recommendations,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        "ad_domain_services_scan_error",
                        domain_name=domain_service.name if domain_service.name else "unknown",
                        error=str(e),
                    )
                    continue

        except Exception as e:
            logger.error("scan_ad_domain_services_error", region=region, error=str(e))

        logger.info(
            "scan_ad_domain_services_complete",
            region=region,
            total_domain_services=len(resources),
        )
        return resources

    def _calculate_ad_domain_services_optimization(
        self,
        provisioning_state: str,
        sku: str,
        has_health_alerts: bool,
        vms_joined: int,
        auth_requests_per_hour: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """Calculate optimization potential for Azure AD Domain Services."""
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = {"actions": [], "estimated_savings": 0.0, "priority": "low"}

        # CRITICAL (90 score): Domain Services not running or failed
        if provisioning_state.lower() in ["failed", "deleting", "deleted", "notrunning"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"Azure AD Domain Services en tat '{provisioning_state}'",
                    f"Service non oprationnel - cot ${monthly_cost:.2f}/mois vitable",
                    "Action: Vrifier les logs, rparer ou supprimer le domain service",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "critical",
            })

        # HIGH (75 score): Premium SKU ($1168/mois) pour <10K auth/hour
        elif sku == "Premium" and auth_requests_per_hour < 10000:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            # Downgrade from Premium ($1168) to Enterprise ($292)
            potential_savings = 1168 - 292
            recommendations.update({
                "actions": [
                    f"SKU Premium (${1168}/mois) surdimensionn pour {auth_requests_per_hour} auth/hour",
                    "Premium supporte 10K-70K auth/hour, usage actuel plus faible",
                    "Action: Downgrade vers Enterprise SKU pour conomiser 75%",
                    f"conomies estimes: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # HIGH (70 score): Domain Services non utilis (0 VMs joines)
        elif vms_joined == 0:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"Azure AD Domain Services actif mais aucune VM joine au domaine",
                    f"Service inutilis - gaspille ${monthly_cost:.2f}/mois",
                    "Action: Supprimer le domain service ou commencer  l'utiliser",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # MEDIUM (50 score): Enterprise SKU pour <3K auth/hour
        elif sku == "Enterprise" and auth_requests_per_hour < 3000:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Downgrade from Enterprise ($292) to Standard ($109)
            potential_savings = 292 - 109
            recommendations.update({
                "actions": [
                    f"SKU Enterprise (${292}/mois) pour {auth_requests_per_hour} auth/hour",
                    "Enterprise supporte 3K-10K auth/hour, Standard suffit pour <3K",
                    "Action: Downgrade vers Standard SKU pour conomiser 63%",
                    f"conomies estimes: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "medium",
            })

        # LOW (30 score): Health alerts non rsolus
        elif has_health_alerts:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            savings = 0
            potential_savings = savings
            recommendations.update({
                "actions": [
                    "Azure AD Domain Services a des health alerts non rsolus",
                    "Alerts peuvent indiquer problmes de performance ou scurit",
                    "Action: Rsoudre les health alerts via Azure Portal",
                    "Note: Meilleure pratique pour fiabilit du service",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_service_bus_premium(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure Service Bus Premium namespaces for cost intelligence."""
        resources = []

        try:
            from azure.mgmt.servicebus import ServiceBusManagementClient

            client = ServiceBusManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing: ~$670/month per messaging unit (flat rate)
            price_per_messaging_unit_monthly = 670.0

            # List all Service Bus namespaces
            namespaces = client.namespaces.list()

            for namespace in namespaces:
                try:
                    # Filter only Premium tier namespaces
                    if not namespace.sku or namespace.sku.name.lower() != "premium":
                        continue  # Skip non-Premium namespaces

                    # Extract basic info
                    namespace_id = namespace.id or "unknown"
                    namespace_name = namespace.name or "unknown"
                    namespace_location = namespace.location or region
                    provisioning_state = namespace.provisioning_state if namespace.provisioning_state else "Unknown"

                    # Get messaging units (capacity)
                    messaging_units = namespace.sku.capacity if namespace.sku else 1

                    # Calculate monthly cost
                    monthly_cost = messaging_units * price_per_messaging_unit_monthly

                    # Get resource group from namespace ID
                    resource_group = namespace_id.split("/")[4] if "/" in namespace_id else ""

                    # Count queues and topics
                    queues_count = 0
                    topics_count = 0
                    try:
                        queues = client.queues.list_by_namespace(
                            resource_group_name=resource_group,
                            namespace_name=namespace_name
                        )
                        queues_count = sum(1 for _ in queues)
                    except Exception:
                        pass

                    try:
                        topics = client.topics.list_by_namespace(
                            resource_group_name=resource_group,
                            namespace_name=namespace_name
                        )
                        topics_count = sum(1 for _ in topics)
                    except Exception:
                        pass

                    # Check if geo-disaster recovery is configured
                    has_geo_dr = False
                    try:
                        disaster_recovery_configs = client.disaster_recovery_configs.list(
                            resource_group_name=resource_group,
                            namespace_name=namespace_name
                        )
                        has_geo_dr = sum(1 for _ in disaster_recovery_configs) > 0
                    except Exception:
                        pass

                    # Estimate throughput usage (would need metrics)
                    estimated_throughput_percent = 0  # Would need Azure Monitor metrics

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_service_bus_premium_optimization(
                        provisioning_state=provisioning_state,
                        messaging_units=messaging_units,
                        queues_count=queues_count,
                        topics_count=topics_count,
                        has_geo_dr=has_geo_dr,
                        estimated_throughput_percent=estimated_throughput_percent,
                        monthly_cost=monthly_cost,
                    )

                    # Build metadata
                    metadata = {
                        "namespace_id": namespace_id,
                        "namespace_name": namespace_name,
                        "location": namespace_location,
                        "provisioning_state": provisioning_state,
                        "sku": "Premium",
                        "messaging_units": messaging_units,
                        "queues_count": queues_count,
                        "topics_count": topics_count,
                        "has_geo_dr": has_geo_dr,
                        "estimated_throughput_percent": estimated_throughput_percent,
                        "price_per_unit_monthly": price_per_messaging_unit_monthly,
                        "tags": dict(namespace.tags) if namespace.tags else {},
                    }

                    # Create resource data
                    resource_data = AllCloudResourceData(
                        resource_id=namespace_id,
                        resource_name=namespace_name,
                        resource_type="azure_service_bus_premium",
                        region=namespace_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata=metadata,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_priority=priority,
                        potential_monthly_savings=round(potential_savings, 2),
                        optimization_recommendations=recommendations,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        "service_bus_premium_scan_error",
                        namespace_name=namespace.name if namespace.name else "unknown",
                        error=str(e),
                    )
                    continue

        except Exception as e:
            logger.error("scan_service_bus_premium_error", region=region, error=str(e))

        logger.info(
            "scan_service_bus_premium_complete",
            region=region,
            total_premium_namespaces=len(resources),
        )
        return resources

    def _calculate_service_bus_premium_optimization(
        self,
        provisioning_state: str,
        messaging_units: int,
        queues_count: int,
        topics_count: int,
        has_geo_dr: bool,
        estimated_throughput_percent: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """Calculate optimization potential for Service Bus Premium."""
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = {"actions": [], "estimated_savings": 0.0, "priority": "low"}

        # CRITICAL (90 score): Namespace in failed state
        if provisioning_state.lower() in ["failed", "deleting", "deleted"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"Service Bus Premium namespace en tat '{provisioning_state}'",
                    f"Namespace non oprationnel - cot ${monthly_cost:.2f}/mois vitable",
                    "Action: Vrifier les logs, rparer ou supprimer le namespace",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "critical",
            })

        # HIGH (75 score): Premium tier avec 0 queues/topics
        elif queues_count == 0 and topics_count == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"Premium namespace vide (0 queues, 0 topics) - cot ${monthly_cost:.2f}/mois",
                    f"{messaging_units} messaging unit(s) inutilises",
                    "Action: Supprimer le namespace ou crer des queues/topics",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # HIGH (70 score): Premium tier avec trs faible dbit
        elif estimated_throughput_percent > 0 and estimated_throughput_percent < 10:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = monthly_cost * 0.5  # Suggest downgrade or Standard tier
            recommendations.update({
                "actions": [
                    f"Faible utilisation: {estimated_throughput_percent}% du dbit Premium",
                    f"Premium tier cote ${monthly_cost:.2f}/mois pour usage minimal",
                    "Action: Considrer Standard tier ou rduire messaging units",
                    f"conomies estimes: ${potential_savings:.2f}/mois (50% rduction)",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # MEDIUM (50 score): Premium tier surdimensionn
        elif messaging_units >= 4:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Suggest reducing from 4+ units to 2 units
            units_to_reduce = messaging_units - 2
            potential_savings = units_to_reduce * 670
            recommendations.update({
                "actions": [
                    f"Surdimensionnement potentiel: {messaging_units} messaging units",
                    f"Cot actuel: ${monthly_cost:.2f}/mois",
                    "Action: Analyser le dbit rel et rduire messaging units si possible",
                    f"conomies estimes: ${potential_savings:.2f}/mois (rduction  2 units)",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "medium",
            })

        # LOW (30 score): Premium sans geo-disaster recovery
        elif not has_geo_dr and messaging_units >= 2:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            savings = 0
            potential_savings = savings
            recommendations.update({
                "actions": [
                    "Premium tier sans geo-disaster recovery configur",
                    "Premium offre geo-DR mais non utilis",
                    "Action: Configurer geo-DR ou considrer Standard tier",
                    "Note: Meilleure pratique pour haute disponibilit",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_iot_hub(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure IoT Hubs for cost intelligence."""
        resources = []

        try:
            from azure.mgmt.iothub import IotHubClient

            client = IotHubClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing map (monthly)
            pricing_map = {
                "F1": 0.0,  # Free tier (8K messages/day)
                "B1": 10.0,  # Basic tier
                "B2": 50.0,
                "B3": 500.0,
                "S1": 25.0,  # Standard tier (400K messages/day)
                "S2": 250.0,  # 6M messages/day
                "S3": 2500.0,  # 300M messages/day
            }

            # List all IoT Hubs
            iot_hubs = client.iot_hub_resource.list_by_subscription()

            for hub in iot_hubs:
                try:
                    # Extract basic info
                    hub_id = hub.id or "unknown"
                    hub_name = hub.name or "unknown"
                    hub_location = hub.location or region
                    provisioning_state = hub.properties.provisioning_state if hub.properties else "Unknown"

                    # Get SKU info
                    sku_name = hub.sku.name if hub.sku else "Unknown"
                    sku_tier = hub.sku.tier if hub.sku else "Unknown"
                    sku_capacity = hub.sku.capacity if hub.sku else 1  # Number of units

                    # Calculate monthly cost
                    price_per_unit = pricing_map.get(sku_name, 25.0)
                    monthly_cost = price_per_unit * sku_capacity

                    # Get resource group from hub ID
                    resource_group = hub_id.split("/")[4] if "/" in hub_id else ""

                    # Get device statistics
                    device_count = 0
                    try:
                        stats = client.iot_hub_resource.get_stats(
                            resource_group_name=resource_group,
                            resource_name=hub_name
                        )
                        device_count = stats.total_device_count if stats.total_device_count else 0
                    except Exception:
                        pass

                    # Estimate message quota usage (would need metrics)
                    daily_message_quota = 0
                    if sku_name == "F1":
                        daily_message_quota = 8000
                    elif sku_name in ["B1", "S1"]:
                        daily_message_quota = 400000 * sku_capacity
                    elif sku_name == "S2":
                        daily_message_quota = 6000000 * sku_capacity
                    elif sku_name == "S3":
                        daily_message_quota = 300000000 * sku_capacity

                    # Estimate usage (would need actual metrics)
                    estimated_daily_messages = 0  # Would need Azure Monitor metrics
                    usage_percent = 0
                    if daily_message_quota > 0 and estimated_daily_messages > 0:
                        usage_percent = (estimated_daily_messages / daily_message_quota) * 100

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_iot_hub_optimization(
                        provisioning_state=provisioning_state,
                        sku_name=sku_name,
                        sku_tier=sku_tier,
                        sku_capacity=sku_capacity,
                        device_count=device_count,
                        usage_percent=usage_percent,
                        monthly_cost=monthly_cost,
                    )

                    # Build metadata
                    metadata = {
                        "hub_id": hub_id,
                        "hub_name": hub_name,
                        "location": hub_location,
                        "provisioning_state": provisioning_state,
                        "sku_name": sku_name,
                        "sku_tier": sku_tier,
                        "sku_capacity": sku_capacity,
                        "device_count": device_count,
                        "daily_message_quota": daily_message_quota,
                        "usage_percent": round(usage_percent, 2),
                        "price_per_unit": price_per_unit,
                        "tags": dict(hub.tags) if hub.tags else {},
                    }

                    # Create resource data
                    resource_data = AllCloudResourceData(
                        resource_id=hub_id,
                        resource_name=hub_name,
                        resource_type="azure_iot_hub",
                        region=hub_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata=metadata,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_priority=priority,
                        potential_monthly_savings=round(potential_savings, 2),
                        optimization_recommendations=recommendations,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        "iot_hub_scan_error",
                        hub_name=hub.name if hub.name else "unknown",
                        error=str(e),
                    )
                    continue

        except Exception as e:
            logger.error("scan_iot_hub_error", region=region, error=str(e))

        logger.info(
            "scan_iot_hub_complete",
            region=region,
            total_iot_hubs=len(resources),
        )
        return resources

    def _calculate_iot_hub_optimization(
        self,
        provisioning_state: str,
        sku_name: str,
        sku_tier: str,
        sku_capacity: int,
        device_count: int,
        usage_percent: float,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """Calculate optimization potential for IoT Hub."""
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = {"actions": [], "estimated_savings": 0.0, "priority": "low"}

        # CRITICAL (90 score): IoT Hub in failed state
        if provisioning_state.lower() in ["failed", "deleting", "deleted"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"IoT Hub en tat '{provisioning_state}'",
                    f"Hub non oprationnel - cot ${monthly_cost:.2f}/mois vitable",
                    "Action: Vrifier les logs, rparer ou supprimer l'IoT Hub",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "critical",
            })

        # HIGH (75 score): Standard tier avec 0 devices
        elif sku_tier.lower() == "standard" and device_count == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"IoT Hub Standard ({sku_name}) avec 0 devices enregistrs",
                    f"Hub inutilis - cot ${monthly_cost:.2f}/mois",
                    "Action: Supprimer le hub ou commencer  enregistrer des devices",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # HIGH (70 score): Standard tier avec usage <10%
        elif sku_tier.lower() == "standard" and usage_percent > 0 and usage_percent < 10:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Suggest downgrade to Basic or Free tier
            potential_savings = monthly_cost * 0.6  # 60% savings with Basic
            recommendations.update({
                "actions": [
                    f"Faible utilisation: {usage_percent:.1f}% du quota messages",
                    f"Standard tier ({sku_name}) cote ${monthly_cost:.2f}/mois pour usage minimal",
                    "Action: Downgrade vers Basic tier ou rduire capacity",
                    f"conomies estimes: ${potential_savings:.2f}/mois (60% rduction)",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # MEDIUM (50 score): S2/S3 tier surdimensionn
        elif sku_name in ["S2", "S3"] and device_count < 1000:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Suggest downgrade from S2/S3 to S1
            current_cost = monthly_cost
            s1_cost = 25.0 * sku_capacity
            potential_savings = current_cost - s1_cost
            recommendations.update({
                "actions": [
                    f"Tier {sku_name} surdimensionn pour {device_count} devices",
                    f"Cot actuel: ${current_cost:.2f}/mois",
                    "Action: Downgrade vers S1 tier pour conomiser",
                    f"conomies estimes: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "medium",
            })

        # LOW (30 score): Pas de monitoring configur
        elif usage_percent == 0 and device_count > 0:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            savings = 0
            potential_savings = savings
            recommendations.update({
                "actions": [
                    f"IoT Hub avec {device_count} devices mais metrics non disponibles",
                    "Monitoring non configur pour analyser l'usage rel",
                    "Action: Configurer Azure Monitor pour tracking messages",
                    "Note: Meilleure pratique pour gestion des cots",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_stream_analytics(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure Stream Analytics jobs for cost intelligence."""
        resources = []

        try:
            from azure.mgmt.streamanalytics import StreamAnalyticsManagementClient

            client = StreamAnalyticsManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing: $0.11 per streaming unit-hour (V2 pricing)
            # Monthly cost = $0.11 * 24h * 30 days * streaming_units = $79.20 per SU
            price_per_su_monthly = 0.11 * 24 * 30

            # List all Stream Analytics jobs
            jobs = client.streaming_jobs.list()

            for job in jobs:
                try:
                    # Extract metadata
                    job_name = job.name
                    job_id = job.id
                    resource_group = job_id.split("/")[4] if len(job_id.split("/")) > 4 else "unknown"
                    job_location = job.location or region
                    job_state = job.job_state or "Unknown"

                    # Get transformation (streaming units)
                    streaming_units = 0
                    if job.transformation and hasattr(job.transformation, "streaming_units"):
                        streaming_units = job.transformation.streaming_units or 0

                    # Count inputs and outputs
                    inputs_count = 0
                    outputs_count = 0
                    try:
                        inputs_list = list(client.inputs.list_by_streaming_job(
                            resource_group_name=resource_group,
                            job_name=job_name
                        ))
                        inputs_count = len(inputs_list)
                    except Exception:
                        pass

                    try:
                        outputs_list = list(client.outputs.list_by_streaming_job(
                            resource_group_name=resource_group,
                            job_name=job_name
                        ))
                        outputs_count = len(outputs_list)
                    except Exception:
                        pass

                    # Get diagnostic settings (monitoring)
                    has_diagnostics = False
                    try:
                        from azure.mgmt.monitor import MonitorManagementClient
                        monitor_client = MonitorManagementClient(
                            credential=self.credential, subscription_id=self.subscription_id
                        )
                        diag_settings = list(monitor_client.diagnostic_settings.list(resource_uri=job_id))
                        has_diagnostics = len(diag_settings) > 0
                    except Exception:
                        pass

                    # Calculate monthly cost
                    monthly_cost = price_per_su_monthly * streaming_units

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations_data,
                    ) = self._calculate_stream_analytics_optimization(
                        job_state=job_state,
                        streaming_units=streaming_units,
                        inputs_count=inputs_count,
                        outputs_count=outputs_count,
                        has_diagnostics=has_diagnostics,
                        monthly_cost=monthly_cost,
                    )

                    # Build AllCloudResourceData
                    resource_data = AllCloudResourceData(
                        resource_id=job_id,
                        resource_name=job_name,
                        resource_type="azure_stream_analytics",
                        region=job_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "job_state": job_state,
                            "streaming_units": streaming_units,
                            "inputs_count": inputs_count,
                            "outputs_count": outputs_count,
                            "has_diagnostics": has_diagnostics,
                            "resource_group": resource_group,
                            "sku": job.sku.name if job.sku else "Unknown",
                        },
                        last_used_at=None,
                        created_at_cloud=job.created_date if hasattr(job, "created_date") and job.created_date else None,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_recommendations=recommendations_data,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        f"Error scanning Stream Analytics job {job.name}: {str(e)}"
                    )
                    continue

        except Exception as e:
            logger.error(f"Error listing Stream Analytics jobs: {str(e)}")

        return resources

    def _calculate_stream_analytics_optimization(
        self,
        job_state: str,
        streaming_units: int,
        inputs_count: int,
        outputs_count: int,
        has_diagnostics: bool,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """
        Calculate Stream Analytics optimization based on 5 scenarios.

        Scenarios:
        1. CRITICAL (90): Job failed
        2. HIGH (75): Job stopped >30 days (inferred from state)
        3. HIGH (70): Job running but 0 inputs/outputs
        4. MEDIUM (50): Oversized streaming units (>6 SU)
        5. LOW (30): No diagnostic monitoring configured
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = {"scenarios": []}

        # Scenario 1: CRITICAL - Job failed
        if job_state.lower() in ["failed", "degraded"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            savings = monthly_cost
            recommendations["scenarios"].append({
                "scenario": "Job failed or degraded",
                "description": (
                    f"Le job Stream Analytics est en tat '{job_state}'. "
                    f"Cot gaspill: ${monthly_cost:.2f}/mois. "
                    f"Actions recommandes: (1) Vrifier les logs d'erreur, "
                    f"(2) Corriger la configuration des inputs/outputs, "
                    f"(3) Arrter le job si non corrigeable pour viter les cots."
                ),
                "actions": [
                    f"Vrifier les logs d'erreur du job '{job_state}'",
                    "Corriger la configuration des inputs/outputs",
                    "Arrter le job si non rparable",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "critical",
            })

        # Scenario 2: HIGH - Job stopped >30 days (inferred from stopped state)
        elif job_state.lower() == "stopped":
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority != "critical" else priority
            savings = monthly_cost
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Job arrt depuis longtemps",
                "description": (
                    f"Le job Stream Analytics est arrt. "
                    f"Si non utilis depuis >30 jours, supprimer pour conomiser ${monthly_cost:.2f}/mois. "
                    f"Actions recommandes: (1) Vrifier la dernire utilisation, "
                    f"(2) Supprimer si obsolte, "
                    f"(3) Redmarrer si ncessaire."
                ),
                "actions": [
                    "Vrifier la dernire date d'utilisation du job",
                    "Supprimer le job si obsolte (>30 jours)",
                    "Redmarrer si encore ncessaire",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 3: HIGH - Job running but 0 inputs/outputs
        elif job_state.lower() == "running" and (inputs_count == 0 or outputs_count == 0):
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority not in ["critical"] else priority
            savings = monthly_cost
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Job actif sans inputs/outputs configurs",
                "description": (
                    f"Le job Stream Analytics tourne avec {inputs_count} inputs et {outputs_count} outputs. "
                    f"Job inutilisable sans inputs/outputs. Cot: ${monthly_cost:.2f}/mois. "
                    f"Actions recommandes: (1) Configurer les inputs/outputs, "
                    f"(2) Arrter le job si configuration impossible."
                ),
                "actions": [
                    f"Configurer les inputs ({inputs_count}) et outputs ({outputs_count})",
                    "Tester le job avec des donnes relles",
                    "Arrter le job si non utilis",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 4: MEDIUM - Oversized streaming units (>6 SU)
        elif streaming_units > 6:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority not in ["critical", "high"] else priority
            # Estimate 50% reduction possible
            target_su = max(3, streaming_units // 2)
            savings = monthly_cost * 0.5
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Streaming Units surdimensionns",
                "description": (
                    f"Le job utilise {streaming_units} SU (Streaming Units). "
                    f"Cot actuel: ${monthly_cost:.2f}/mois. "
                    f"Analyser le throughput rel pour rduire  ~{target_su} SU. "
                    f"Actions recommandes: (1) Analyser les mtriques de throughput, "
                    f"(2) Rduire les SU progressivement, "
                    f"(3) conomiser jusqu' ${savings:.2f}/mois."
                ),
                "actions": [
                    f"Analyser les mtriques de throughput actuelles ({streaming_units} SU)",
                    f"Rduire progressivement  ~{target_su} SU",
                    "Monitorer les performances aprs rduction",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "medium",
            })

        # Scenario 5: LOW - No diagnostic monitoring configured
        elif not has_diagnostics:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            savings = monthly_cost * 0.1  # Visibility helps optimize
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Pas de monitoring configur",
                "description": (
                    f"Le job Stream Analytics n'a pas de diagnostic monitoring activ. "
                    f"Sans mtriques, impossible d'optimiser le throughput et les cots. "
                    f"Actions recommandes: (1) Activer Diagnostic Settings, "
                    f"(2) Envoyer logs vers Log Analytics, "
                    f"(3) Crer des alertes sur les mtriques cls."
                ),
                "actions": [
                    "Activer Diagnostic Settings pour le job",
                    "Envoyer logs vers Log Analytics Workspace",
                    "Crer des alertes sur mtriques cls (errors, throughput)",
                    f"Note: Meilleure visibilit pour optimiser (conomie ~${savings:.2f}/mois)"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_ai_document_intelligence(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure Document Intelligence (Form Recognizer) endpoints for cost intelligence."""
        resources = []

        try:
            from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient

            client = CognitiveServicesManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing map (monthly estimates based on usage)
            # F0: Free tier (500 pages/month)
            # S0: Standard tier $1.50 per 1K pages
            pricing_map = {
                "F0": 0.0,  # Free tier
                "S0": 150.0,  # Estimate: 100K pages/month = $150
            }

            # List all Cognitive Services accounts
            accounts = client.accounts.list()

            for account in accounts:
                try:
                    # Filter only FormRecognizer/Document Intelligence accounts
                    if not account.kind or account.kind.lower() != "formrecognizer":
                        continue

                    # Extract metadata
                    account_name = account.name
                    account_id = account.id
                    resource_group = account_id.split("/")[4] if len(account_id.split("/")) > 4 else "unknown"
                    account_location = account.location or region
                    sku_name = account.sku.name if account.sku else "Unknown"
                    provisioning_state = account.properties.provisioning_state if hasattr(account, "properties") and hasattr(account.properties, "provisioning_state") else "Unknown"

                    # Check if endpoint is accessible
                    endpoint_accessible = False
                    if account.properties and hasattr(account.properties, "endpoint") and account.properties.endpoint:
                        endpoint_accessible = True

                    # Get diagnostic settings (monitoring)
                    has_diagnostics = False
                    try:
                        from azure.mgmt.monitor import MonitorManagementClient
                        monitor_client = MonitorManagementClient(
                            credential=self.credential, subscription_id=self.subscription_id
                        )
                        diag_settings = list(monitor_client.diagnostic_settings.list(resource_uri=account_id))
                        has_diagnostics = len(diag_settings) > 0
                    except Exception:
                        pass

                    # Check if private endpoint is configured
                    has_private_endpoint = False
                    if account.properties and hasattr(account.properties, "private_endpoint_connections"):
                        connections = account.properties.private_endpoint_connections or []
                        has_private_endpoint = len(connections) > 0

                    # Get estimated monthly cost
                    monthly_cost = pricing_map.get(sku_name, 150.0)

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations_data,
                    ) = self._calculate_document_intelligence_optimization(
                        sku_name=sku_name,
                        provisioning_state=provisioning_state,
                        endpoint_accessible=endpoint_accessible,
                        has_diagnostics=has_diagnostics,
                        has_private_endpoint=has_private_endpoint,
                        monthly_cost=monthly_cost,
                    )

                    # Build AllCloudResourceData
                    resource_data = AllCloudResourceData(
                        resource_id=account_id,
                        resource_name=account_name,
                        resource_type="azure_document_intelligence",
                        region=account_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "sku": sku_name,
                            "provisioning_state": provisioning_state,
                            "endpoint_accessible": endpoint_accessible,
                            "has_diagnostics": has_diagnostics,
                            "has_private_endpoint": has_private_endpoint,
                            "resource_group": resource_group,
                            "kind": account.kind,
                        },
                        last_used_at=None,
                        created_at_cloud=None,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_recommendations=recommendations_data,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        f"Error scanning Document Intelligence account {account.name}: {str(e)}"
                    )
                    continue

        except Exception as e:
            logger.error(f"Error listing Document Intelligence accounts: {str(e)}")

        return resources

    def _calculate_document_intelligence_optimization(
        self,
        sku_name: str,
        provisioning_state: str,
        endpoint_accessible: bool,
        has_diagnostics: bool,
        has_private_endpoint: bool,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """
        Calculate Document Intelligence optimization based on 5 scenarios.

        Scenarios:
        1. CRITICAL (90): Account failed provisioning
        2. HIGH (75): S0 tier with inaccessible endpoint
        3. HIGH (70): S0 tier without any usage metrics
        4. MEDIUM (50): S0 tier without private endpoint (security)
        5. LOW (30): No diagnostic monitoring configured
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = {"scenarios": []}

        # Scenario 1: CRITICAL - Account failed provisioning
        if provisioning_state.lower() in ["failed", "deleting"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            savings = monthly_cost
            recommendations["scenarios"].append({
                "scenario": "Account en tat d'chec",
                "description": (
                    f"Le compte Document Intelligence est en tat '{provisioning_state}'. "
                    f"Cot gaspill: ${monthly_cost:.2f}/mois. "
                    f"Actions recommandes: (1) Vrifier les logs d'erreur, "
                    f"(2) Recrer le compte si ncessaire, "
                    f"(3) Supprimer si non utilis."
                ),
                "actions": [
                    f"Vrifier les logs d'erreur pour '{provisioning_state}'",
                    "Recrer le compte si ncessaire",
                    "Supprimer le compte si non utilis",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "critical",
            })

        # Scenario 2: HIGH - S0 tier with inaccessible endpoint
        elif sku_name == "S0" and not endpoint_accessible:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority != "critical" else priority
            savings = monthly_cost
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Endpoint inaccessible en tier payant",
                "description": (
                    f"Le compte Document Intelligence S0 a un endpoint inaccessible. "
                    f"Cot: ${monthly_cost:.2f}/mois sans utilisation possible. "
                    f"Actions recommandes: (1) Vrifier la configuration rseau, "
                    f"(2) Corriger les rgles de pare-feu, "
                    f"(3) Downgrade vers F0 ou supprimer si non utilis."
                ),
                "actions": [
                    "Vrifier la configuration rseau et endpoint",
                    "Corriger les rgles de pare-feu/NSG",
                    "Downgrade vers F0 (gratuit) si non utilis",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 3: HIGH - S0 tier without usage monitoring
        elif sku_name == "S0" and not has_diagnostics:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority not in ["critical"] else priority
            # Assume 50% of cost is waste without monitoring
            savings = monthly_cost * 0.5
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Tier payant sans monitoring d'usage",
                "description": (
                    f"Le compte Document Intelligence S0 n'a pas de monitoring configur. "
                    f"Cot actuel: ${monthly_cost:.2f}/mois. "
                    f"Impossible de vrifier si le tier S0 est justifi sans mtriques. "
                    f"Actions recommandes: (1) Activer diagnostic settings, "
                    f"(2) Analyser l'usage rel, "
                    f"(3) Downgrade vers F0 si usage <500 pages/mois."
                ),
                "actions": [
                    "Activer Diagnostic Settings pour tracking usage",
                    "Analyser le volume de pages traites par mois",
                    "Downgrade vers F0 si usage <500 pages/mois",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 4: MEDIUM - S0 tier without private endpoint (security)
        elif sku_name == "S0" and not has_private_endpoint:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority not in ["critical", "high"] else priority
            # Estimate 10% cost for security improvement
            savings = monthly_cost * 0.1
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Pas de Private Endpoint configur",
                "description": (
                    f"Le compte Document Intelligence S0 expose un endpoint public. "
                    f"Cot actuel: ${monthly_cost:.2f}/mois. "
                    f"Risque de scurit pour donnes sensibles (documents). "
                    f"Actions recommandes: (1) Configurer Private Endpoint, "
                    f"(2) Restreindre l'accs rseau, "
                    f"(3) Activer firewall rules."
                ),
                "actions": [
                    "Configurer Azure Private Endpoint",
                    "Restreindre accs rseau (VNet only)",
                    "Activer firewall rules et IP filtering",
                    f"Note: Meilleure scurit pour donnes sensibles"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "medium",
            })

        # Scenario 5: LOW - No diagnostic monitoring configured (F0 tier)
        elif sku_name == "F0" and not has_diagnostics:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            savings = 0  # Free tier, but monitoring helps prevent overages
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Pas de monitoring configur (Free tier)",
                "description": (
                    f"Le compte Document Intelligence F0 (gratuit) n'a pas de monitoring. "
                    f"Sans mtriques, risque de dpasser la limite gratuite (500 pages/mois). "
                    f"Actions recommandes: (1) Activer Diagnostic Settings, "
                    f"(2) Crer des alertes sur quota usage, "
                    f"(3) Monitorer pour viter charges inattendues."
                ),
                "actions": [
                    "Activer Diagnostic Settings",
                    "Crer alertes sur quota usage (500 pages/mois)",
                    "Monitorer pour viter dpassement et charges",
                    "Note: Prvention de cots inattendus"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_computer_vision(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure Computer Vision accounts for cost intelligence."""
        resources = []

        try:
            from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient

            client = CognitiveServicesManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing map (monthly estimates based on usage)
            # F0: Free tier (5K transactions/month)
            # S1: Standard tier ~$1 per 1K transactions
            pricing_map = {
                "F0": 0.0,  # Free tier
                "S1": 150.0,  # Estimate: 150K transactions/month = $150
                "S0": 150.0,  # Legacy tier, same as S1
            }

            # List all Cognitive Services accounts
            accounts = client.accounts.list()

            for account in accounts:
                try:
                    # Filter only ComputerVision accounts
                    if not account.kind or account.kind.lower() != "computervision":
                        continue

                    # Extract metadata
                    account_name = account.name
                    account_id = account.id
                    resource_group = account_id.split("/")[4] if len(account_id.split("/")) > 4 else "unknown"
                    account_location = account.location or region
                    sku_name = account.sku.name if account.sku else "Unknown"
                    provisioning_state = account.properties.provisioning_state if hasattr(account, "properties") and hasattr(account.properties, "provisioning_state") else "Unknown"

                    # Check if endpoint is accessible
                    endpoint_accessible = False
                    if account.properties and hasattr(account.properties, "endpoint") and account.properties.endpoint:
                        endpoint_accessible = True

                    # Get diagnostic settings (monitoring)
                    has_diagnostics = False
                    try:
                        from azure.mgmt.monitor import MonitorManagementClient
                        monitor_client = MonitorManagementClient(
                            credential=self.credential, subscription_id=self.subscription_id
                        )
                        diag_settings = list(monitor_client.diagnostic_settings.list(resource_uri=account_id))
                        has_diagnostics = len(diag_settings) > 0
                    except Exception:
                        pass

                    # Check if private endpoint is configured
                    has_private_endpoint = False
                    if account.properties and hasattr(account.properties, "private_endpoint_connections"):
                        connections = account.properties.private_endpoint_connections or []
                        has_private_endpoint = len(connections) > 0

                    # Get estimated monthly cost
                    monthly_cost = pricing_map.get(sku_name, 150.0)

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations_data,
                    ) = self._calculate_computer_vision_optimization(
                        sku_name=sku_name,
                        provisioning_state=provisioning_state,
                        endpoint_accessible=endpoint_accessible,
                        has_diagnostics=has_diagnostics,
                        has_private_endpoint=has_private_endpoint,
                        monthly_cost=monthly_cost,
                    )

                    # Build AllCloudResourceData
                    resource_data = AllCloudResourceData(
                        resource_id=account_id,
                        resource_name=account_name,
                        resource_type="azure_computer_vision",
                        region=account_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "sku": sku_name,
                            "provisioning_state": provisioning_state,
                            "endpoint_accessible": endpoint_accessible,
                            "has_diagnostics": has_diagnostics,
                            "has_private_endpoint": has_private_endpoint,
                            "resource_group": resource_group,
                            "kind": account.kind,
                        },
                        last_used_at=None,
                        created_at_cloud=None,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_recommendations=recommendations_data,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        f"Error scanning Computer Vision account {account.name}: {str(e)}"
                    )
                    continue

        except Exception as e:
            logger.error(f"Error listing Computer Vision accounts: {str(e)}")

        return resources

    def _calculate_computer_vision_optimization(
        self,
        sku_name: str,
        provisioning_state: str,
        endpoint_accessible: bool,
        has_diagnostics: bool,
        has_private_endpoint: bool,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """
        Calculate Computer Vision optimization based on 5 scenarios.

        Scenarios:
        1. CRITICAL (90): Account failed provisioning
        2. HIGH (75): S1 tier avec endpoint inaccessible
        3. HIGH (70): S1 tier sans usage metrics/monitoring
        4. MEDIUM (50): S1 tier sans private endpoint (scurit images)
        5. LOW (30): F0 tier sans monitoring (risque dpassement quota)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = {"scenarios": []}

        # Scenario 1: CRITICAL - Account failed provisioning
        if provisioning_state.lower() in ["failed", "deleting"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            savings = monthly_cost
            recommendations["scenarios"].append({
                "scenario": "Account en tat d'chec",
                "description": (
                    f"Le compte Computer Vision est en tat '{provisioning_state}'. "
                    f"Cot gaspill: ${monthly_cost:.2f}/mois. "
                    f"Actions recommandes: (1) Vrifier les logs d'erreur, "
                    f"(2) Recrer le compte si ncessaire, "
                    f"(3) Supprimer si non utilis."
                ),
                "actions": [
                    f"Vrifier les logs d'erreur pour '{provisioning_state}'",
                    "Recrer le compte si ncessaire",
                    "Supprimer le compte si non utilis",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "critical",
            })

        # Scenario 2: HIGH - S1 tier avec endpoint inaccessible
        elif sku_name in ["S1", "S0"] and not endpoint_accessible:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority != "critical" else priority
            savings = monthly_cost
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Endpoint inaccessible en tier payant",
                "description": (
                    f"Le compte Computer Vision {sku_name} a un endpoint inaccessible. "
                    f"Cot: ${monthly_cost:.2f}/mois sans utilisation possible. "
                    f"Actions recommandes: (1) Vrifier la configuration rseau, "
                    f"(2) Corriger les rgles de pare-feu, "
                    f"(3) Downgrade vers F0 ou supprimer si non utilis."
                ),
                "actions": [
                    "Vrifier la configuration rseau et endpoint",
                    "Corriger les rgles de pare-feu/NSG",
                    "Downgrade vers F0 (gratuit) si non utilis",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 3: HIGH - S1 tier sans usage metrics/monitoring
        elif sku_name in ["S1", "S0"] and not has_diagnostics:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority not in ["critical"] else priority
            # Assume 50% of cost is waste without monitoring
            savings = monthly_cost * 0.5
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Tier payant sans monitoring d'usage",
                "description": (
                    f"Le compte Computer Vision {sku_name} n'a pas de monitoring configur. "
                    f"Cot actuel: ${monthly_cost:.2f}/mois. "
                    f"Impossible de vrifier si le tier {sku_name} est justifi sans mtriques. "
                    f"Actions recommandes: (1) Activer diagnostic settings, "
                    f"(2) Analyser l'usage rel (transactions/mois), "
                    f"(3) Downgrade vers F0 si usage <5K transactions/mois."
                ),
                "actions": [
                    "Activer Diagnostic Settings pour tracking usage",
                    "Analyser le volume de transactions par mois",
                    "Downgrade vers F0 si usage <5K transactions/mois",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 4: MEDIUM - S1 tier sans private endpoint (scurit images)
        elif sku_name in ["S1", "S0"] and not has_private_endpoint:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority not in ["critical", "high"] else priority
            # Estimate 10% cost for security improvement
            savings = monthly_cost * 0.1
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Pas de Private Endpoint configur",
                "description": (
                    f"Le compte Computer Vision {sku_name} expose un endpoint public. "
                    f"Cot actuel: ${monthly_cost:.2f}/mois. "
                    f"Risque de scurit pour images sensibles (OCR, analyse visuelle). "
                    f"Actions recommandes: (1) Configurer Private Endpoint, "
                    f"(2) Restreindre l'accs rseau, "
                    f"(3) Activer firewall rules."
                ),
                "actions": [
                    "Configurer Azure Private Endpoint",
                    "Restreindre accs rseau (VNet only)",
                    "Activer firewall rules et IP filtering",
                    "Note: Meilleure scurit pour images sensibles"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "medium",
            })

        # Scenario 5: LOW - F0 tier sans monitoring (risque dpassement quota)
        elif sku_name == "F0" and not has_diagnostics:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            savings = 0  # Free tier, but monitoring helps prevent overages
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Pas de monitoring configur (Free tier)",
                "description": (
                    f"Le compte Computer Vision F0 (gratuit) n'a pas de monitoring. "
                    f"Sans mtriques, risque de dpasser la limite gratuite (5K transactions/mois). "
                    f"Actions recommandes: (1) Activer Diagnostic Settings, "
                    f"(2) Crer des alertes sur quota usage, "
                    f"(3) Monitorer pour viter charges inattendues."
                ),
                "actions": [
                    "Activer Diagnostic Settings",
                    "Crer alertes sur quota usage (5K transactions/mois)",
                    "Monitorer pour viter dpassement et charges",
                    "Note: Prvention de cots inattendus"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_face_api(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure Face API accounts for cost intelligence."""
        resources = []

        try:
            from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient

            client = CognitiveServicesManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing map (monthly estimates based on usage)
            # F0: Free tier (30K transactions/month)
            # S0: Standard tier $0.40-$1 per 1K transactions
            pricing_map = {
                "F0": 0.0,  # Free tier
                "S0": 150.0,  # Estimate: 150K transactions/month = $150
            }

            # List all Cognitive Services accounts
            accounts = client.accounts.list()

            for account in accounts:
                try:
                    # Filter only Face accounts
                    if not account.kind or account.kind.lower() != "face":
                        continue

                    # Extract metadata
                    account_name = account.name
                    account_id = account.id
                    resource_group = account_id.split("/")[4] if len(account_id.split("/")) > 4 else "unknown"
                    account_location = account.location or region
                    sku_name = account.sku.name if account.sku else "Unknown"
                    provisioning_state = account.properties.provisioning_state if hasattr(account, "properties") and hasattr(account.properties, "provisioning_state") else "Unknown"

                    # Check if endpoint is accessible
                    endpoint_accessible = False
                    if account.properties and hasattr(account.properties, "endpoint") and account.properties.endpoint:
                        endpoint_accessible = True

                    # Get diagnostic settings (monitoring)
                    has_diagnostics = False
                    try:
                        from azure.mgmt.monitor import MonitorManagementClient
                        monitor_client = MonitorManagementClient(
                            credential=self.credential, subscription_id=self.subscription_id
                        )
                        diag_settings = list(monitor_client.diagnostic_settings.list(resource_uri=account_id))
                        has_diagnostics = len(diag_settings) > 0
                    except Exception:
                        pass

                    # Check if private endpoint is configured
                    has_private_endpoint = False
                    if account.properties and hasattr(account.properties, "private_endpoint_connections"):
                        connections = account.properties.private_endpoint_connections or []
                        has_private_endpoint = len(connections) > 0

                    # Get estimated monthly cost
                    monthly_cost = pricing_map.get(sku_name, 150.0)

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations_data,
                    ) = self._calculate_face_api_optimization(
                        sku_name=sku_name,
                        provisioning_state=provisioning_state,
                        endpoint_accessible=endpoint_accessible,
                        has_diagnostics=has_diagnostics,
                        has_private_endpoint=has_private_endpoint,
                        monthly_cost=monthly_cost,
                    )

                    # Build AllCloudResourceData
                    resource_data = AllCloudResourceData(
                        resource_id=account_id,
                        resource_name=account_name,
                        resource_type="azure_face_api",
                        region=account_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "sku": sku_name,
                            "provisioning_state": provisioning_state,
                            "endpoint_accessible": endpoint_accessible,
                            "has_diagnostics": has_diagnostics,
                            "has_private_endpoint": has_private_endpoint,
                            "resource_group": resource_group,
                            "kind": account.kind,
                        },
                        last_used_at=None,
                        created_at_cloud=None,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_recommendations=recommendations_data,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        f"Error scanning Face API account {account.name}: {str(e)}"
                    )
                    continue

        except Exception as e:
            logger.error(f"Error listing Face API accounts: {str(e)}")

        return resources

    def _calculate_face_api_optimization(
        self,
        sku_name: str,
        provisioning_state: str,
        endpoint_accessible: bool,
        has_diagnostics: bool,
        has_private_endpoint: bool,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """
        Calculate Face API optimization based on 5 scenarios.

        Scenarios:
        1. CRITICAL (90): Account failed provisioning
        2. HIGH (75): S0 tier avec endpoint inaccessible
        3. HIGH (70): S0 tier sans usage metrics
        4. MEDIUM (50): S0 tier sans private endpoint (donnes biomtriques sensibles)
        5. LOW (30): F0 tier sans monitoring quota
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = {"scenarios": []}

        # Scenario 1: CRITICAL - Account failed provisioning
        if provisioning_state.lower() in ["failed", "deleting"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            savings = monthly_cost
            recommendations["scenarios"].append({
                "scenario": "Account en tat d'chec",
                "description": (
                    f"Le compte Face API est en tat '{provisioning_state}'. "
                    f"Cot gaspill: ${monthly_cost:.2f}/mois. "
                    f"Actions recommandes: (1) Vrifier les logs d'erreur, "
                    f"(2) Recrer le compte si ncessaire, "
                    f"(3) Supprimer si non utilis."
                ),
                "actions": [
                    f"Vrifier les logs d'erreur pour '{provisioning_state}'",
                    "Recrer le compte si ncessaire",
                    "Supprimer le compte si non utilis",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "critical",
            })

        # Scenario 2: HIGH - S0 tier avec endpoint inaccessible
        elif sku_name == "S0" and not endpoint_accessible:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority != "critical" else priority
            savings = monthly_cost
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Endpoint inaccessible en tier payant",
                "description": (
                    f"Le compte Face API S0 a un endpoint inaccessible. "
                    f"Cot: ${monthly_cost:.2f}/mois sans utilisation possible. "
                    f"Actions recommandes: (1) Vrifier la configuration rseau, "
                    f"(2) Corriger les rgles de pare-feu, "
                    f"(3) Downgrade vers F0 ou supprimer si non utilis."
                ),
                "actions": [
                    "Vrifier la configuration rseau et endpoint",
                    "Corriger les rgles de pare-feu/NSG",
                    "Downgrade vers F0 (gratuit) si non utilis",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 3: HIGH - S0 tier sans usage metrics
        elif sku_name == "S0" and not has_diagnostics:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority not in ["critical"] else priority
            # Assume 50% of cost is waste without monitoring
            savings = monthly_cost * 0.5
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Tier payant sans monitoring d'usage",
                "description": (
                    f"Le compte Face API S0 n'a pas de monitoring configur. "
                    f"Cot actuel: ${monthly_cost:.2f}/mois. "
                    f"Impossible de vrifier si le tier S0 est justifi sans mtriques. "
                    f"Actions recommandes: (1) Activer diagnostic settings, "
                    f"(2) Analyser l'usage rel (face detection/verification), "
                    f"(3) Downgrade vers F0 si usage <30K transactions/mois."
                ),
                "actions": [
                    "Activer Diagnostic Settings pour tracking usage",
                    "Analyser le volume de dtections faciales par mois",
                    "Downgrade vers F0 si usage <30K transactions/mois",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 4: MEDIUM - S0 tier sans private endpoint (donnes biomtriques)
        elif sku_name == "S0" and not has_private_endpoint:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority not in ["critical", "high"] else priority
            # Estimate 15% cost for security improvement (biometric data is critical)
            savings = monthly_cost * 0.15
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Pas de Private Endpoint pour donnes biomtriques",
                "description": (
                    f"Le compte Face API S0 expose un endpoint public. "
                    f"Cot actuel: ${monthly_cost:.2f}/mois. "
                    f"RISQUE CRITIQUE: Donnes biomtriques sensibles (RGPD/GDPR). "
                    f"Actions recommandes: (1) Configurer Private Endpoint URGENT, "
                    f"(2) Restreindre l'accs rseau, "
                    f"(3) Activer firewall rules."
                ),
                "actions": [
                    "URGENT: Configurer Azure Private Endpoint",
                    "Restreindre accs rseau (VNet only)",
                    "Activer firewall rules et IP filtering",
                    "Note: Conformit RGPD pour donnes biomtriques"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "medium",
            })

        # Scenario 5: LOW - F0 tier sans monitoring quota
        elif sku_name == "F0" and not has_diagnostics:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            savings = 0  # Free tier, but monitoring helps prevent overages
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Pas de monitoring configur (Free tier)",
                "description": (
                    f"Le compte Face API F0 (gratuit) n'a pas de monitoring. "
                    f"Sans mtriques, risque de dpasser la limite gratuite (30K transactions/mois). "
                    f"Actions recommandes: (1) Activer Diagnostic Settings, "
                    f"(2) Crer des alertes sur quota usage, "
                    f"(3) Monitorer pour viter charges inattendues."
                ),
                "actions": [
                    "Activer Diagnostic Settings",
                    "Crer alertes sur quota usage (30K transactions/mois)",
                    "Monitorer pour viter dpassement et charges",
                    "Note: Prvention de cots inattendus"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_text_analytics(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure Text Analytics (Language Service) accounts for cost intelligence."""
        resources = []

        try:
            from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient

            client = CognitiveServicesManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing map (monthly estimates based on usage)
            # F0: Free tier (5K text records/month)
            # S: Standard tier ~$2 per 1K text records
            pricing_map = {
                "F0": 0.0,  # Free tier
                "S": 200.0,  # Estimate: 100K text records/month = $200
                "S0": 200.0,  # Legacy tier, same as S
            }

            # List all Cognitive Services accounts
            accounts = client.accounts.list()

            for account in accounts:
                try:
                    # Filter only TextAnalytics accounts
                    if not account.kind or account.kind.lower() != "textanalytics":
                        continue

                    # Extract metadata
                    account_name = account.name
                    account_id = account.id
                    resource_group = account_id.split("/")[4] if len(account_id.split("/")) > 4 else "unknown"
                    account_location = account.location or region
                    sku_name = account.sku.name if account.sku else "Unknown"
                    provisioning_state = account.properties.provisioning_state if hasattr(account, "properties") and hasattr(account.properties, "provisioning_state") else "Unknown"

                    # Check if endpoint is accessible
                    endpoint_accessible = False
                    if account.properties and hasattr(account.properties, "endpoint") and account.properties.endpoint:
                        endpoint_accessible = True

                    # Get diagnostic settings (monitoring)
                    has_diagnostics = False
                    try:
                        from azure.mgmt.monitor import MonitorManagementClient
                        monitor_client = MonitorManagementClient(
                            credential=self.credential, subscription_id=self.subscription_id
                        )
                        diag_settings = list(monitor_client.diagnostic_settings.list(resource_uri=account_id))
                        has_diagnostics = len(diag_settings) > 0
                    except Exception:
                        pass

                    # Check if private endpoint is configured
                    has_private_endpoint = False
                    if account.properties and hasattr(account.properties, "private_endpoint_connections"):
                        connections = account.properties.private_endpoint_connections or []
                        has_private_endpoint = len(connections) > 0

                    # Get estimated monthly cost
                    monthly_cost = pricing_map.get(sku_name, 200.0)

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations_data,
                    ) = self._calculate_text_analytics_optimization(
                        sku_name=sku_name,
                        provisioning_state=provisioning_state,
                        endpoint_accessible=endpoint_accessible,
                        has_diagnostics=has_diagnostics,
                        has_private_endpoint=has_private_endpoint,
                        monthly_cost=monthly_cost,
                    )

                    # Build AllCloudResourceData
                    resource_data = AllCloudResourceData(
                        resource_id=account_id,
                        resource_name=account_name,
                        resource_type="azure_text_analytics",
                        region=account_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "sku": sku_name,
                            "provisioning_state": provisioning_state,
                            "endpoint_accessible": endpoint_accessible,
                            "has_diagnostics": has_diagnostics,
                            "has_private_endpoint": has_private_endpoint,
                            "resource_group": resource_group,
                            "kind": account.kind,
                        },
                        last_used_at=None,
                        created_at_cloud=None,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_recommendations=recommendations_data,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        f"Error scanning Text Analytics account {account.name}: {str(e)}"
                    )
                    continue

        except Exception as e:
            logger.error(f"Error listing Text Analytics accounts: {str(e)}")

        return resources

    def _calculate_text_analytics_optimization(
        self,
        sku_name: str,
        provisioning_state: str,
        endpoint_accessible: bool,
        has_diagnostics: bool,
        has_private_endpoint: bool,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """
        Calculate Text Analytics optimization based on 5 scenarios.

        Scenarios:
        1. CRITICAL (90): Account failed provisioning
        2. HIGH (75): S tier avec endpoint inaccessible
        3. HIGH (70): S tier sans usage metrics
        4. MEDIUM (50): S tier sans private endpoint
        5. LOW (30): F0 tier sans monitoring
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = {"scenarios": []}

        # Scenario 1: CRITICAL - Account failed provisioning
        if provisioning_state.lower() in ["failed", "deleting"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            savings = monthly_cost
            recommendations["scenarios"].append({
                "scenario": "Account en tat d'chec",
                "description": (
                    f"Le compte Text Analytics est en tat '{provisioning_state}'. "
                    f"Cot gaspill: ${monthly_cost:.2f}/mois. "
                    f"Actions recommandes: (1) Vrifier les logs d'erreur, "
                    f"(2) Recrer le compte si ncessaire, "
                    f"(3) Supprimer si non utilis."
                ),
                "actions": [
                    f"Vrifier les logs d'erreur pour '{provisioning_state}'",
                    "Recrer le compte si ncessaire",
                    "Supprimer le compte si non utilis",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "critical",
            })

        # Scenario 2: HIGH - S tier avec endpoint inaccessible
        elif sku_name in ["S", "S0"] and not endpoint_accessible:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority != "critical" else priority
            savings = monthly_cost
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Endpoint inaccessible en tier payant",
                "description": (
                    f"Le compte Text Analytics {sku_name} a un endpoint inaccessible. "
                    f"Cot: ${monthly_cost:.2f}/mois sans utilisation possible. "
                    f"Actions recommandes: (1) Vrifier la configuration rseau, "
                    f"(2) Corriger les rgles de pare-feu, "
                    f"(3) Downgrade vers F0 ou supprimer si non utilis."
                ),
                "actions": [
                    "Vrifier la configuration rseau et endpoint",
                    "Corriger les rgles de pare-feu/NSG",
                    "Downgrade vers F0 (gratuit) si non utilis",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 3: HIGH - S tier sans usage metrics
        elif sku_name in ["S", "S0"] and not has_diagnostics:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority not in ["critical"] else priority
            # Assume 50% of cost is waste without monitoring
            savings = monthly_cost * 0.5
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Tier payant sans monitoring d'usage",
                "description": (
                    f"Le compte Text Analytics {sku_name} n'a pas de monitoring configur. "
                    f"Cot actuel: ${monthly_cost:.2f}/mois. "
                    f"Impossible de vrifier si le tier {sku_name} est justifi sans mtriques. "
                    f"Actions recommandes: (1) Activer diagnostic settings, "
                    f"(2) Analyser l'usage rel (text records/mois), "
                    f"(3) Downgrade vers F0 si usage <5K text records/mois."
                ),
                "actions": [
                    "Activer Diagnostic Settings pour tracking usage",
                    "Analyser le volume de text records par mois",
                    "Downgrade vers F0 si usage <5K text records/mois",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 4: MEDIUM - S tier sans private endpoint
        elif sku_name in ["S", "S0"] and not has_private_endpoint:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority not in ["critical", "high"] else priority
            # Estimate 10% cost for security improvement
            savings = monthly_cost * 0.1
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Pas de Private Endpoint configur",
                "description": (
                    f"Le compte Text Analytics {sku_name} expose un endpoint public. "
                    f"Cot actuel: ${monthly_cost:.2f}/mois. "
                    f"Risque de scurit pour donnes textuelles sensibles (NER, PII). "
                    f"Actions recommandes: (1) Configurer Private Endpoint, "
                    f"(2) Restreindre l'accs rseau, "
                    f"(3) Activer firewall rules."
                ),
                "actions": [
                    "Configurer Azure Private Endpoint",
                    "Restreindre accs rseau (VNet only)",
                    "Activer firewall rules et IP filtering",
                    "Note: Meilleure scurit pour donnes textuelles sensibles"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "medium",
            })

        # Scenario 5: LOW - F0 tier sans monitoring
        elif sku_name == "F0" and not has_diagnostics:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            savings = 0  # Free tier, but monitoring helps prevent overages
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Pas de monitoring configur (Free tier)",
                "description": (
                    f"Le compte Text Analytics F0 (gratuit) n'a pas de monitoring. "
                    f"Sans mtriques, risque de dpasser la limite gratuite (5K text records/mois). "
                    f"Actions recommandes: (1) Activer Diagnostic Settings, "
                    f"(2) Crer des alertes sur quota usage, "
                    f"(3) Monitorer pour viter charges inattendues."
                ),
                "actions": [
                    "Activer Diagnostic Settings",
                    "Crer alertes sur quota usage (5K text records/mois)",
                    "Monitorer pour viter dpassement et charges",
                    "Note: Prvention de cots inattendus"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_speech_services(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure Speech Services accounts for cost intelligence."""
        resources = []

        try:
            from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient

            client = CognitiveServicesManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing map (monthly estimates based on usage)
            # F0: Free tier (5 audio hours/month STT + 0.5M chars/month TTS)
            # S0: Standard tier $1/hour STT + $16 per million chars TTS Neural
            pricing_map = {
                "F0": 0.0,  # Free tier
                "S0": 200.0,  # Estimate: 100 hours STT + 100K chars TTS = $200
            }

            # List all Cognitive Services accounts
            accounts = client.accounts.list()

            for account in accounts:
                try:
                    # Filter only SpeechServices accounts
                    if not account.kind or account.kind.lower() != "speechservices":
                        continue

                    # Extract metadata
                    account_name = account.name
                    account_id = account.id
                    resource_group = account_id.split("/")[4] if len(account_id.split("/")) > 4 else "unknown"
                    account_location = account.location or region
                    sku_name = account.sku.name if account.sku else "Unknown"
                    provisioning_state = account.properties.provisioning_state if hasattr(account, "properties") and hasattr(account.properties, "provisioning_state") else "Unknown"

                    # Check if endpoint is accessible
                    endpoint_accessible = False
                    if account.properties and hasattr(account.properties, "endpoint") and account.properties.endpoint:
                        endpoint_accessible = True

                    # Get diagnostic settings (monitoring)
                    has_diagnostics = False
                    try:
                        from azure.mgmt.monitor import MonitorManagementClient
                        monitor_client = MonitorManagementClient(
                            credential=self.credential, subscription_id=self.subscription_id
                        )
                        diag_settings = list(monitor_client.diagnostic_settings.list(resource_uri=account_id))
                        has_diagnostics = len(diag_settings) > 0
                    except Exception:
                        pass

                    # Check if private endpoint is configured
                    has_private_endpoint = False
                    if account.properties and hasattr(account.properties, "private_endpoint_connections"):
                        connections = account.properties.private_endpoint_connections or []
                        has_private_endpoint = len(connections) > 0

                    # Get estimated monthly cost
                    monthly_cost = pricing_map.get(sku_name, 200.0)

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations_data,
                    ) = self._calculate_speech_services_optimization(
                        sku_name=sku_name,
                        provisioning_state=provisioning_state,
                        endpoint_accessible=endpoint_accessible,
                        has_diagnostics=has_diagnostics,
                        has_private_endpoint=has_private_endpoint,
                        monthly_cost=monthly_cost,
                    )

                    # Build AllCloudResourceData
                    resource_data = AllCloudResourceData(
                        resource_id=account_id,
                        resource_name=account_name,
                        resource_type="azure_speech_services",
                        region=account_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "sku": sku_name,
                            "provisioning_state": provisioning_state,
                            "endpoint_accessible": endpoint_accessible,
                            "has_diagnostics": has_diagnostics,
                            "has_private_endpoint": has_private_endpoint,
                            "resource_group": resource_group,
                            "kind": account.kind,
                        },
                        last_used_at=None,
                        created_at_cloud=None,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_recommendations=recommendations_data,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        f"Error scanning Speech Services account {account.name}: {str(e)}"
                    )
                    continue

        except Exception as e:
            logger.error(f"Error listing Speech Services accounts: {str(e)}")

        return resources

    def _calculate_speech_services_optimization(
        self,
        sku_name: str,
        provisioning_state: str,
        endpoint_accessible: bool,
        has_diagnostics: bool,
        has_private_endpoint: bool,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """
        Calculate Speech Services optimization based on 5 scenarios.

        Scenarios:
        1. CRITICAL (90): Account failed provisioning
        2. HIGH (75): S0 tier avec endpoint inaccessible
        3. HIGH (70): S0 tier sans usage metrics
        4. MEDIUM (50): S0 tier sans private endpoint (audio sensible)
        5. LOW (30): F0 tier sans monitoring quota
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = {"scenarios": []}

        # Scenario 1: CRITICAL - Account failed provisioning
        if provisioning_state.lower() in ["failed", "deleting"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            savings = monthly_cost
            recommendations["scenarios"].append({
                "scenario": "Account en tat d'chec",
                "description": (
                    f"Le compte Speech Services est en tat '{provisioning_state}'. "
                    f"Cot gaspill: ${monthly_cost:.2f}/mois. "
                    f"Actions recommandes: (1) Vrifier les logs d'erreur, "
                    f"(2) Recrer le compte si ncessaire, "
                    f"(3) Supprimer si non utilis."
                ),
                "actions": [
                    f"Vrifier les logs d'erreur pour '{provisioning_state}'",
                    "Recrer le compte si ncessaire",
                    "Supprimer le compte si non utilis",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "critical",
            })

        # Scenario 2: HIGH - S0 tier avec endpoint inaccessible
        elif sku_name == "S0" and not endpoint_accessible:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority != "critical" else priority
            savings = monthly_cost
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Endpoint inaccessible en tier payant",
                "description": (
                    f"Le compte Speech Services S0 a un endpoint inaccessible. "
                    f"Cot: ${monthly_cost:.2f}/mois sans utilisation possible. "
                    f"Actions recommandes: (1) Vrifier la configuration rseau, "
                    f"(2) Corriger les rgles de pare-feu, "
                    f"(3) Downgrade vers F0 ou supprimer si non utilis."
                ),
                "actions": [
                    "Vrifier la configuration rseau et endpoint",
                    "Corriger les rgles de pare-feu/NSG",
                    "Downgrade vers F0 (gratuit) si non utilis",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 3: HIGH - S0 tier sans usage metrics
        elif sku_name == "S0" and not has_diagnostics:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority not in ["critical"] else priority
            # Assume 50% of cost is waste without monitoring
            savings = monthly_cost * 0.5
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Tier payant sans monitoring d'usage",
                "description": (
                    f"Le compte Speech Services S0 n'a pas de monitoring configur. "
                    f"Cot actuel: ${monthly_cost:.2f}/mois. "
                    f"Impossible de vrifier si le tier S0 est justifi sans mtriques. "
                    f"Actions recommandes: (1) Activer diagnostic settings, "
                    f"(2) Analyser l'usage rel (STT hours + TTS chars), "
                    f"(3) Downgrade vers F0 si usage <5 hours STT + <0.5M chars TTS/mois."
                ),
                "actions": [
                    "Activer Diagnostic Settings pour tracking usage",
                    "Analyser le volume STT (hours) et TTS (chars) par mois",
                    "Downgrade vers F0 si faible usage",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 4: MEDIUM - S0 tier sans private endpoint (audio sensible)
        elif sku_name == "S0" and not has_private_endpoint:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority not in ["critical", "high"] else priority
            # Estimate 10% cost for security improvement
            savings = monthly_cost * 0.1
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Pas de Private Endpoint pour audio sensible",
                "description": (
                    f"Le compte Speech Services S0 expose un endpoint public. "
                    f"Cot actuel: ${monthly_cost:.2f}/mois. "
                    f"Risque de scurit pour donnes audio sensibles (voix, conversations). "
                    f"Actions recommandes: (1) Configurer Private Endpoint, "
                    f"(2) Restreindre l'accs rseau, "
                    f"(3) Activer firewall rules."
                ),
                "actions": [
                    "Configurer Azure Private Endpoint",
                    "Restreindre accs rseau (VNet only)",
                    "Activer firewall rules et IP filtering",
                    "Note: Meilleure scurit pour donnes audio sensibles"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "medium",
            })

        # Scenario 5: LOW - F0 tier sans monitoring quota
        elif sku_name == "F0" and not has_diagnostics:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            savings = 0  # Free tier, but monitoring helps prevent overages
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Pas de monitoring configur (Free tier)",
                "description": (
                    f"Le compte Speech Services F0 (gratuit) n'a pas de monitoring. "
                    f"Sans mtriques, risque de dpasser limites gratuites (5 hours STT + 0.5M chars TTS/mois). "
                    f"Actions recommandes: (1) Activer Diagnostic Settings, "
                    f"(2) Crer des alertes sur quota usage, "
                    f"(3) Monitorer pour viter charges inattendues."
                ),
                "actions": [
                    "Activer Diagnostic Settings",
                    "Crer alertes sur quota usage (STT + TTS)",
                    "Monitorer pour viter dpassement et charges",
                    "Note: Prvention de cots inattendus"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_bot_service(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure Bot Service resources for cost intelligence."""
        resources = []

        try:
            from azure.mgmt.botservice import AzureBotService

            client = AzureBotService(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing: Standard Channels FREE, Premium Channels $0.50 per 1K messages after 10K free
            # Estimate: $50/month for 110K Premium messages

            # List all bot resources
            bots = client.bots.list()

            for bot in bots:
                try:
                    # Extract metadata
                    bot_name = bot.name
                    bot_id = bot.id
                    resource_group = bot_id.split("/")[4] if len(bot_id.split("/")) > 4 else "unknown"
                    bot_location = bot.location or region
                    bot_kind = bot.kind if hasattr(bot, "kind") and bot.kind else "Unknown"

                    # Get bot properties
                    provisioning_state = bot.properties.provisioning_state if hasattr(bot.properties, "provisioning_state") else "Unknown"
                    endpoint = bot.properties.endpoint if hasattr(bot.properties, "endpoint") and bot.properties.endpoint else None

                    # Check if bot has Application Insights configured
                    has_app_insights = False
                    if hasattr(bot.properties, "developer_app_insights_key") and bot.properties.developer_app_insights_key:
                        has_app_insights = True

                    # Count configured channels
                    channels_count = 0
                    try:
                        channels_list = list(client.channels.list_by_resource_group(
                            resource_group_name=resource_group,
                            resource_name=bot_name
                        ))
                        channels_count = len(channels_list)
                    except Exception:
                        pass

                    # Estimate monthly cost based on Premium channels
                    # Assumption: if bot exists, estimate ~$50/month for Premium usage
                    monthly_cost = 50.0 if channels_count > 0 else 0.0

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations_data,
                    ) = self._calculate_bot_service_optimization(
                        bot_kind=bot_kind,
                        provisioning_state=provisioning_state,
                        channels_count=channels_count,
                        has_app_insights=has_app_insights,
                        endpoint=endpoint,
                        monthly_cost=monthly_cost,
                    )

                    # Build AllCloudResourceData
                    resource_data = AllCloudResourceData(
                        resource_id=bot_id,
                        resource_name=bot_name,
                        resource_type="azure_bot_service",
                        region=bot_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "kind": bot_kind,
                            "provisioning_state": provisioning_state,
                            "channels_count": channels_count,
                            "has_app_insights": has_app_insights,
                            "endpoint": endpoint or "Not configured",
                            "resource_group": resource_group,
                        },
                        last_used_at=None,
                        created_at_cloud=None,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_recommendations=recommendations_data,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        f"Error scanning Bot Service {bot.name}: {str(e)}"
                    )
                    continue

        except Exception as e:
            logger.error(f"Error listing Bot Service resources: {str(e)}")

        return resources

    def _calculate_bot_service_optimization(
        self,
        bot_kind: str,
        provisioning_state: str,
        channels_count: int,
        has_app_insights: bool,
        endpoint: str | None,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """
        Calculate Bot Service optimization based on 5 scenarios.

        Scenarios:
        1. CRITICAL (90): Bot resource failed/deleting
        2. HIGH (75): Bot avec channels configurs mais aucun endpoint
        3. HIGH (70): Bot sans channels configurs
        4. MEDIUM (50): Bot sans monitoring/Application Insights
        5. LOW (30): Bot sans backup ou disaster recovery
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = {"scenarios": []}

        # Scenario 1: CRITICAL - Bot failed/deleting
        if provisioning_state.lower() in ["failed", "deleting"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            savings = monthly_cost
            recommendations["scenarios"].append({
                "scenario": "Bot en tat d'chec",
                "description": (
                    f"Le Bot Service est en tat '{provisioning_state}'. "
                    f"Cot potentiel gaspill: ${monthly_cost:.2f}/mois. "
                    f"Actions recommandes: (1) Vrifier les logs d'erreur, "
                    f"(2) Recrer le bot si ncessaire, "
                    f"(3) Supprimer si non utilis."
                ),
                "actions": [
                    f"Vrifier les logs d'erreur pour '{provisioning_state}'",
                    "Recrer le bot si ncessaire",
                    "Supprimer le bot si obsolte",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "critical",
            })

        # Scenario 2: HIGH - Bot avec channels mais sans endpoint
        elif channels_count > 0 and not endpoint:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority != "critical" else priority
            savings = monthly_cost
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Bot avec channels mais sans endpoint",
                "description": (
                    f"Le Bot a {channels_count} channel(s) configur(s) mais aucun endpoint. "
                    f"Cot: ${monthly_cost:.2f}/mois sans possibilit de rpondre aux messages. "
                    f"Actions recommandes: (1) Configurer l'endpoint du bot, "
                    f"(2) Dployer le code du bot, "
                    f"(3) Supprimer les channels si bot non utilis."
                ),
                "actions": [
                    "Configurer l'endpoint du bot (messaging endpoint)",
                    "Dployer le code du bot sur Azure App Service/Functions",
                    f"Supprimer les {channels_count} channel(s) si non utilis",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 3: HIGH - Bot sans channels configurs
        elif channels_count == 0 and monthly_cost == 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority not in ["critical"] else priority
            # No cost but waste of resource
            savings = 0
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Bot sans channels configurs",
                "description": (
                    f"Le Bot Service n'a aucun channel configur. "
                    f"Bot inutilisable sans channels (Teams, Slack, Web Chat, etc.). "
                    f"Actions recommandes: (1) Configurer au moins un channel, "
                    f"(2) Tester le bot, "
                    f"(3) Supprimer si projet abandonn."
                ),
                "actions": [
                    "Configurer au moins un channel (Teams, Web Chat, Slack...)",
                    "Tester le bot avec le channel configur",
                    "Supprimer le bot si projet abandonn",
                    "Note: Aucun cot actuel mais ressource gaspille"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 4: MEDIUM - Bot sans monitoring/Application Insights
        elif not has_app_insights and channels_count > 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority not in ["critical", "high"] else priority
            # Estimate 10% improvement with monitoring
            savings = monthly_cost * 0.1
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Pas de monitoring Application Insights",
                "description": (
                    f"Le Bot Service n'a pas Application Insights configur. "
                    f"Cot actuel: ${monthly_cost:.2f}/mois. "
                    f"Sans tlmtrie, impossible d'analyser conversations et optimiser. "
                    f"Actions recommandes: (1) Configurer Application Insights, "
                    f"(2) Analyser mtriques (messages, errors, latency), "
                    f"(3) Optimiser le bot bas sur usage rel."
                ),
                "actions": [
                    "Configurer Application Insights pour le bot",
                    "Analyser mtriques (messages count, errors, latency)",
                    "Optimiser le bot bas sur usage rel",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "medium",
            })

        # Scenario 5: LOW - Pas de backup/disaster recovery
        elif channels_count > 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            savings = 0  # Best practice, no direct savings
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Pas de stratgie de backup configure",
                "description": (
                    f"Le Bot Service n'a pas de stratgie de backup/disaster recovery. "
                    f"En cas de panne, risque de perte de service et impact business. "
                    f"Actions recommandes: (1) Configurer multi-region deployment, "
                    f"(2) Sauvegarder configuration et code du bot, "
                    f"(3) Tester disaster recovery plan."
                ),
                "actions": [
                    "Configurer multi-region deployment pour HA",
                    "Sauvegarder configuration bot (ARM template/Terraform)",
                    "Tester disaster recovery plan",
                    "Note: Meilleure pratique pour continuit de service"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_application_insights(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Application Insights resources for cost intelligence.

        Application Insights = Monitoring/observability service for applications (telemetry, logs, metrics).

        Pricing: Pay-as-you-go $2.30-$2.76/GB ingested (5 GB free/month) OR Commitment Tiers (15-36% discount).
        Typical cost: $50-500/month depending on data volume (10-200 GB/month).
        """
        resources = []

        try:
            from azure.mgmt.applicationinsights import ApplicationInsightsManagementClient

            client = ApplicationInsightsManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing map (monthly estimates based on typical data ingestion)
            # Pay-as-you-go: $2.50/GB average
            # Estimate assumes workspace-based pricing
            pricing_map = {
                "pay_as_you_go_10gb": 25.0,  # 10 GB/month * $2.50 (5 GB free included)
                "pay_as_you_go_50gb": 125.0,  # 50 GB/month * $2.50
                "pay_as_you_go_100gb": 250.0,  # 100 GB/month * $2.50
                "pay_as_you_go_200gb": 500.0,  # 200 GB/month * $2.50
                "commitment_tier_100gb": 200.0,  # 100 GB/day commitment (~20% discount)
            }

            # List all Application Insights components
            components = client.components.list()

            for component in components:
                try:
                    # Extract metadata
                    component_name = component.name
                    resource_group = component.id.split("/")[4] if "/" in component.id else "unknown"
                    location = component.location if hasattr(component, "location") else region
                    provisioning_state = component.provisioning_state if hasattr(component, "provisioning_state") else "Unknown"
                    application_type = component.application_type if hasattr(component, "application_type") else "other"

                    # Check if workspace-based or classic
                    is_workspace_based = False
                    workspace_resource_id = None
                    if hasattr(component, "workspace_resource_id") and component.workspace_resource_id:
                        is_workspace_based = True
                        workspace_resource_id = component.workspace_resource_id

                    # Get ingestion settings
                    daily_cap_gb = None
                    retention_days = 90  # Default
                    if hasattr(component, "ingestion_mode"):
                        ingestion_mode = component.ingestion_mode
                    else:
                        ingestion_mode = "LogAnalytics" if is_workspace_based else "ApplicationInsights"

                    # Try to get daily cap (if available)
                    try:
                        billing = client.component_current_billing_features.get(
                            resource_group_name=resource_group,
                            resource_name=component_name
                        )
                        if hasattr(billing, "current_billing_features"):
                            # Daily cap in GB
                            if "data_volume_cap" in billing.current_billing_features:
                                daily_cap_data = billing.current_billing_features.get("data_volume_cap")
                                if daily_cap_data and hasattr(daily_cap_data, "cap"):
                                    daily_cap_gb = daily_cap_data.cap
                    except Exception:
                        pass  # Daily cap not available

                    # Try to get retention period
                    try:
                        if hasattr(component, "retention_in_days"):
                            retention_days = component.retention_in_days
                    except Exception:
                        pass

                    # Estimate monthly cost (we don't have actual ingestion data via SDK easily)
                    # Default: assume 50 GB/month for typical app
                    estimated_monthly_cost = pricing_map["pay_as_you_go_50gb"]

                    # Calculate optimization opportunities
                    (
                        is_optimizable,
                        optimization_score,
                        optimization_priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_app_insights_optimization(
                        provisioning_state=provisioning_state,
                        is_workspace_based=is_workspace_based,
                        daily_cap_gb=daily_cap_gb,
                        retention_days=retention_days,
                        estimated_monthly_cost=estimated_monthly_cost,
                    )

                    # Build resource metadata
                    resource_metadata = {
                        "component_name": component_name,
                        "resource_group": resource_group,
                        "location": location,
                        "provisioning_state": provisioning_state,
                        "application_type": application_type,
                        "is_workspace_based": is_workspace_based,
                        "workspace_resource_id": workspace_resource_id,
                        "ingestion_mode": ingestion_mode,
                        "daily_cap_gb": daily_cap_gb,
                        "retention_days": retention_days,
                        "instrumentation_key": component.instrumentation_key if hasattr(component, "instrumentation_key") else None,
                        "connection_string": component.connection_string if hasattr(component, "connection_string") else None,
                    }

                    # Extract tags
                    tags = component.tags if hasattr(component, "tags") and component.tags else {}

                    # Create resource data
                    resource_data = AllCloudResourceData(
                        resource_type="azure_application_insights",
                        resource_id=component.id,
                        resource_name=component_name,
                        region=location,
                        estimated_monthly_cost=estimated_monthly_cost,
                        currency="USD",
                        utilization_status="unknown",  # Would need Azure Monitor metrics
                        is_optimizable=is_optimizable,
                        optimization_priority=optimization_priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations,
                        resource_metadata=resource_metadata,
                        tags=tags,
                        resource_status=provisioning_state,
                        created_at_cloud=None,  # Not available in SDK
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        "azure.application_insights.scan_component_failed",
                        component_id=getattr(component, "id", "unknown"),
                        error=str(e),
                    )
                    continue

            logger.info(
                "azure.application_insights.scan_complete",
                region=region,
                total_components=len(resources),
                optimizable=sum(1 for r in resources if r.is_optimizable),
            )

        except Exception as e:
            logger.error("azure.application_insights.scan_failed", region=region, error=str(e))

        return resources

    def _calculate_app_insights_optimization(
        self,
        provisioning_state: str,
        is_workspace_based: bool,
        daily_cap_gb: float | None,
        retention_days: int,
        estimated_monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate optimization opportunities for Application Insights.

        5 scenarios:
        1. CRITICAL (90): Failed/Deleted state
        2. HIGH (75): No data ingestion 30+ days (unused service)
        3. HIGH (70): Excessive ingestion >100 GB/month without Commitment Tier
        4. MEDIUM (50): Retention >90 days without business need
        5. LOW (30): No Daily Cap configured (budget risk)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Failed or Deleted state
        if provisioning_state.lower() in ["failed", "deleted", "deleting"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            savings = estimated_monthly_cost  # Full cost if removed
            potential_savings += savings

            recommendations.append({
                "scenario": "Application Insights en tat Failed/Deleted",
                "details": (
                    f"Cet Application Insights est en tat '{provisioning_state}' et ne fonctionne plus. "
                    f"Cot mensuel actuel: ${estimated_monthly_cost:.2f}. "
                    f"Actions recommandes: (1) Recrer le composant si ncessaire, "
                    f"(2) Supprimer compltement si obsolte, "
                    f"(3) Vrifier pourquoi le provisioning a chou."
                ),
                "actions": [
                    "Vrifier les logs de provisioning pour cause d'chec",
                    "Recrer Application Insights avec configuration correcte",
                    "OU supprimer dfinitivement si obsolte",
                    "Vrifier quotas et limites subscription Azure"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "critical",
            })

        # Scenario 2: HIGH - No data ingestion for 30+ days (unused service)
        # Note: We can't easily check ingestion volume via SDK, so this is commented for future
        # elif last_ingestion_days >= 30:
        #     is_optimizable = True
        #     optimization_score = 75
        #     priority = "high"
        #     savings = estimated_monthly_cost * 0.9  # 90% savings if deleted

        # Scenario 3: HIGH - Excessive ingestion >100 GB/month without Commitment Tier
        elif estimated_monthly_cost > 250 and not is_workspace_based:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            savings = estimated_monthly_cost * 0.20  # 20% savings with Commitment Tier
            potential_savings += savings

            recommendations.append({
                "scenario": "Ingestion volumineuse sans Commitment Tier",
                "details": (
                    f"Application Insights ingre >100 GB/mois (cot: ${estimated_monthly_cost:.2f}/mois). "
                    f"Utiliser un Commitment Tier peut rduire les cots de 15-36%. "
                    f"conomie potentielle: ${savings:.2f}/mois (20% estim). "
                    f"Actions recommandes: (1) Migrer vers workspace-based avec Commitment Tier, "
                    f"(2) Analyser volume ingestion rel, (3) Optimiser sampling rate si ncessaire."
                ),
                "actions": [
                    "Analyser volume ingestion mensuel exact (Azure Portal)",
                    "Migrer vers workspace-based Application Insights",
                    "Configurer Commitment Tier adapt (100 GB/day, 200 GB/day...)",
                    "Ajuster sampling rate pour rduire volume si pertinent"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 4: MEDIUM - Retention >90 days without business need
        elif retention_days > 90:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Retention cost ~$0.10/GB/month beyond 31 days free
            # Assume 50 GB/month ingestion  50 GB stored
            # Extra 60 days retention (90-30 free)  ~$3/month savings if reduced to 30 days
            savings = 5.0  # Conservative estimate
            potential_savings += savings

            recommendations.append({
                "scenario": f"Rtention excessive ({retention_days} jours)",
                "details": (
                    f"Application Insights retient les donnes pendant {retention_days} jours. "
                    f"Au-del de 31 jours gratuits, la rtention cote ~$0.10/GB/mois. "
                    f"Si pas de besoin mtier, rduire  30-60 jours. "
                    f"conomie potentielle: ${savings:.2f}/mois. "
                    f"Actions recommandes: (1) Vrifier exigences rglementaires/mtier, "
                    f"(2) Rduire rtention  30-60 jours si possible, "
                    f"(3) Exporter donnes anciennes vers stockage froid si archivage ncessaire."
                ),
                "actions": [
                    f"Vrifier si rtention {retention_days} jours est requise (compliance/mtier)",
                    "Rduire rtention  30-60 jours si pas de contrainte",
                    "Configurer export continu vers Azure Storage (archivage long-terme)",
                    "Note: 31 premiers jours gratuits, puis ~$0.10/GB/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "medium",
            })

        # Scenario 5: LOW - No Daily Cap configured (budget risk)
        elif daily_cap_gb is None and estimated_monthly_cost > 50:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            savings = 0.0  # No direct savings, but prevents overage

            recommendations.append({
                "scenario": "Pas de Daily Cap configur (risque dpassement budget)",
                "details": (
                    f"Application Insights n'a pas de Daily Cap configur. "
                    f"Risque: ingestion excessive imprvue  facture inattendue. "
                    f"Cot mensuel actuel: ${estimated_monthly_cost:.2f}. "
                    f"Actions recommandes: (1) Configurer Daily Cap adapt au budget, "
                    f"(2) Configurer alertes dpassement quota, "
                    f"(3) Surveiller ingestion quotidienne."
                ),
                "actions": [
                    "Configurer Daily Cap (ex: 5 GB/day pour app moyenne)",
                    "Activer alertes dpassement quota (Azure Monitor)",
                    "Surveiller ingestion quotidienne (Azure Portal  Usage and estimated costs)",
                    "Note: Prvient factures imprvues mais ne rduit pas cot directement"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_managed_devops_pools(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Managed DevOps Pools for cost intelligence.

        Managed DevOps Pools = Managed infrastructure for Azure DevOps pipeline agents.

        Pricing: 1st parallel job FREE, then $15/month per additional parallel job.
        Typical cost: $15-150/month depending on number of agents.
        """
        resources = []

        try:
            from azure.mgmt.devopsinfrastructure import DevOpsInfrastructureMgmtClient

            client = DevOpsInfrastructureMgmtClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing: $15 per parallel job (first job free)
            price_per_agent = 15.0

            # List all Managed DevOps Pools
            pools = client.pools.list_by_subscription()

            for pool in pools:
                try:
                    # Extract metadata
                    pool_name = pool.name
                    resource_group = pool.id.split("/")[4] if "/" in pool.id else "unknown"
                    location = pool.location if hasattr(pool, "location") else region
                    provisioning_state = pool.properties.provisioning_state if hasattr(pool.properties, "provisioning_state") else "Unknown"

                    # Get pool properties
                    max_agents = 0
                    agent_profile = None
                    organization_profile = None

                    if hasattr(pool.properties, "maximum_concurrency"):
                        max_agents = pool.properties.maximum_concurrency

                    if hasattr(pool.properties, "agent_profile"):
                        agent_profile = pool.properties.agent_profile
                        # Agent profile contains info about VM SKU, images, etc.

                    if hasattr(pool.properties, "dev_ops_organization_profile"):
                        organization_profile = pool.properties.dev_ops_organization_profile
                        # Contains Azure DevOps organization info

                    # Estimate monthly cost
                    # First agent free, then $15 per additional agent
                    if max_agents <= 1:
                        estimated_monthly_cost = 0.0  # First agent free
                    else:
                        estimated_monthly_cost = (max_agents - 1) * price_per_agent

                    # Calculate optimization opportunities
                    (
                        is_optimizable,
                        optimization_score,
                        optimization_priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_devops_pools_optimization(
                        provisioning_state=provisioning_state,
                        max_agents=max_agents,
                        agent_profile=agent_profile,
                        estimated_monthly_cost=estimated_monthly_cost,
                    )

                    # Build resource metadata
                    resource_metadata = {
                        "pool_name": pool_name,
                        "resource_group": resource_group,
                        "location": location,
                        "provisioning_state": provisioning_state,
                        "maximum_concurrency": max_agents,
                        "agent_profile": str(agent_profile) if agent_profile else None,
                        "organization_profile": str(organization_profile) if organization_profile else None,
                        "fabric_profile": str(pool.properties.fabric_profile) if hasattr(pool.properties, "fabric_profile") else None,
                    }

                    # Extract tags
                    tags = pool.tags if hasattr(pool, "tags") and pool.tags else {}

                    # Create resource data
                    resource_data = AllCloudResourceData(
                        resource_type="azure_managed_devops_pools",
                        resource_id=pool.id,
                        resource_name=pool_name,
                        region=location,
                        estimated_monthly_cost=estimated_monthly_cost,
                        currency="USD",
                        utilization_status="unknown",  # Would need pipeline run metrics
                        is_optimizable=is_optimizable,
                        optimization_priority=optimization_priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations,
                        resource_metadata=resource_metadata,
                        tags=tags,
                        resource_status=provisioning_state,
                        created_at_cloud=None,  # Not available in SDK
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        "azure.managed_devops_pools.scan_pool_failed",
                        pool_id=getattr(pool, "id", "unknown"),
                        error=str(e),
                    )
                    continue

            logger.info(
                "azure.managed_devops_pools.scan_complete",
                region=region,
                total_pools=len(resources),
                optimizable=sum(1 for r in resources if r.is_optimizable),
            )

        except Exception as e:
            logger.error("azure.managed_devops_pools.scan_failed", region=region, error=str(e))

        return resources

    def _calculate_devops_pools_optimization(
        self,
        provisioning_state: str,
        max_agents: int,
        agent_profile: any,
        estimated_monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate optimization opportunities for Managed DevOps Pools.

        5 scenarios:
        1. CRITICAL (90): Failed/Deleted state
        2. HIGH (75): Pool sans agents depuis 30+ jours (unused)
        3. HIGH (70): Agents idle >80% du temps (over-provisioned)
        4. MEDIUM (50): Pool Dev/Test avec agents premium (Standard suffisant)
        5. LOW (30): Multiple pools consolidables (economy of scale)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Failed or Deleted state
        if provisioning_state.lower() in ["failed", "deleted", "deleting"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            savings = estimated_monthly_cost  # Full cost if removed
            potential_savings += savings

            recommendations.append({
                "scenario": "Managed DevOps Pool en tat Failed/Deleted",
                "details": (
                    f"Ce pool DevOps est en tat '{provisioning_state}' et ne fonctionne plus. "
                    f"Cot mensuel actuel: ${estimated_monthly_cost:.2f}. "
                    f"Actions recommandes: (1) Recrer le pool si ncessaire, "
                    f"(2) Supprimer compltement si obsolte, "
                    f"(3) Vrifier pourquoi le provisioning a chou."
                ),
                "actions": [
                    "Vrifier les logs de provisioning pour cause d'chec",
                    "Recrer Managed DevOps Pool avec configuration correcte",
                    "OU supprimer dfinitivement si obsolte",
                    "Vrifier quotas et limites subscription Azure"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "critical",
            })

        # Scenario 2: HIGH - Pool sans agents (never used or abandoned)
        elif max_agents == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            savings = 15.0  # At least one agent's worth
            potential_savings += savings

            recommendations.append({
                "scenario": "Pool DevOps sans agents configurs",
                "details": (
                    f"Le pool '{provisioning_state}' n'a aucun agent configur (max_agents=0). "
                    f"Ce pool ne peut excuter aucun pipeline et gnre des frais fixes. "
                    f"conomie potentielle: ${savings:.2f}/mois si supprim. "
                    f"Actions recommandes: (1) Supprimer le pool si inutilis, "
                    f"(2) OU configurer des agents si besoin futur, "
                    f"(3) Vrifier pipelines Azure DevOps associs."
                ),
                "actions": [
                    "Vrifier si le pool est rfrenc dans des pipelines Azure DevOps",
                    "Supprimer le pool si jamais utilis",
                    "OU configurer agents si besoin pipeline identifi",
                    "Nettoyer pools obsoltes (organisation)"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 3: HIGH - Agents idle >80% (over-provisioned)
        # Note: We can't check actual usage via SDK easily, so this scenario uses agent count heuristic
        elif max_agents > 5:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Assume 30% of agents could be removed
            reducible_agents = int(max_agents * 0.3)
            savings = reducible_agents * 15.0
            potential_savings += savings

            recommendations.append({
                "scenario": f"Pool sur-dimensionn ({max_agents} agents)",
                "details": (
                    f"Le pool a {max_agents} agents configurs. "
                    f"Pour la plupart des organisations, 3-5 agents suffisent. "
                    f"Si agents idle >50% du temps, rduire la capacit. "
                    f"conomie potentielle: ${savings:.2f}/mois en rduisant de ~30%. "
                    f"Actions recommandes: (1) Analyser utilisation relle des agents, "
                    f"(2) Rduire maximum_concurrency si idle, "
                    f"(3) Utiliser autoscaling si pics de charge ponctuels."
                ),
                "actions": [
                    "Analyser utilisation agents via Azure DevOps Analytics",
                    "Identifier taux d'idle (cible: <50% idle)",
                    f"Rduire maximum_concurrency de {max_agents}  {max_agents - reducible_agents}",
                    "Configurer autoscaling pour pics de charge"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 4: MEDIUM - Dev/Test pool avec agents premium (Standard suffisant)
        # Check if agent_profile suggests premium SKU (heuristic: if profile mentions "Standard_D" or higher)
        elif agent_profile and "Standard_D" in str(agent_profile) and max_agents >= 2:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Assume 20% savings by downgrading to Basic SKU
            savings = estimated_monthly_cost * 0.20
            potential_savings += savings

            recommendations.append({
                "scenario": "Pool Dev/Test avec VM SKU premium",
                "details": (
                    f"Le pool utilise des VM SKU premium (ex: Standard_D series) pour {max_agents} agents. "
                    f"Pour environnements Dev/Test, des SKU Standard ou Basic suffisent souvent. "
                    f"conomie potentielle: ${savings:.2f}/mois (20% estim). "
                    f"Actions recommandes: (1) valuer besoins rels CPU/RAM, "
                    f"(2) Downgrader vers SKU moins cher si pertinent, "
                    f"(3) Rserver SKU premium pour production uniquement."
                ),
                "actions": [
                    "Analyser utilisation CPU/RAM des agents (Azure Monitor)",
                    "Downgrader vers VM SKU Basic/Standard si <50% utilisation",
                    "Rserver SKU premium pour pipelines production critiques",
                    "Estimer conomies: ~20-30% avec SKU infrieur"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "medium",
            })

        # Scenario 5: LOW - Multiple pools consolidables (note: this requires global view, so heuristic)
        elif max_agents == 1 and estimated_monthly_cost == 0:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            savings = 0.0  # No direct savings, but organizational cleanup

            recommendations.append({
                "scenario": "Pool avec 1 agent (gratuit) - consolidation possible",
                "details": (
                    f"Ce pool utilise 1 agent (gratuit). "
                    f"Si l'organisation a plusieurs pools similaires, la consolidation peut simplifier gestion. "
                    f"conomie potentielle: ${savings:.2f}/mois (mais gains organisationnels). "
                    f"Actions recommandes: (1) Auditer tous les pools de l'organisation, "
                    f"(2) Consolider pools similaires, "
                    f"(3) Standardiser configuration agents."
                ),
                "actions": [
                    "Auditer tous Managed DevOps Pools de l'organisation",
                    "Identifier pools redondants (mme projet/quipe)",
                    "Consolider en pools partags (conomie chelle)",
                    "Note: Gains organisationnels > conomies directes"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_private_endpoints(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Private Endpoints for cost intelligence.

        Private Endpoints enable private connectivity to Azure services.
        Pricing: ~$7.30/month per Private Endpoint (Standard) + data processing charges
        Typical cost: $7-15/month depending on data transfer volume

        Detection criteria:
        - Private Endpoint failed/deleting (CRITICAL - 90 score)
        - Not connected to any resource (orphan) (HIGH - 75 score)
        - Connected to deallocated/stopped resource (HIGH - 70 score)
        - Redundant endpoints for same resource (MEDIUM - 50 score)
        - Using Premium without justification (LOW - 30 score)
        """
        try:
            from azure.mgmt.network import NetworkManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-network not installed")
            return []

        resources = []
        self.logger.info(f"Scanning Private Endpoints in region: {region}")

        try:
            network_client = NetworkManagementClient(
                credential=self.credential,
                subscription_id=self.subscription_id
            )

            # List all Private Endpoints
            async for endpoint in network_client.private_endpoints.list_all():
                try:
                    # Filter by region if specified
                    if region.lower() != "all" and endpoint.location.lower() != region.lower():
                        continue

                    # Get resource group from endpoint ID
                    resource_group = endpoint.id.split("/")[4]

                    # Get connection state
                    connection_state = "unknown"
                    connected_resource_id = None
                    connected_resource_count = 0

                    if hasattr(endpoint, 'private_link_service_connections'):
                        connections = endpoint.private_link_service_connections or []
                        connected_resource_count = len(connections)
                        if connections:
                            first_conn = connections[0]
                            if hasattr(first_conn, 'private_link_service_connection_state'):
                                conn_state = first_conn.private_link_service_connection_state
                                if conn_state:
                                    connection_state = getattr(conn_state, 'status', 'unknown')
                            if hasattr(first_conn, 'private_link_service_id'):
                                connected_resource_id = first_conn.private_link_service_id

                    # Get provisioning state
                    provisioning_state = getattr(endpoint, 'provisioning_state', 'Unknown')

                    # Calculate optimization
                    is_optimizable, score, priority, savings, recommendations = (
                        self._calculate_private_endpoint_optimization(
                            provisioning_state,
                            connection_state,
                            connected_resource_count,
                            connected_resource_id
                        )
                    )

                    # Pricing (Azure US East 2025)
                    # Private Endpoint: $7.30/month (flat rate)
                    # Data processing: $0.01/GB inbound + $0.01/GB outbound
                    # Typical: $7-15/month depending on traffic
                    base_monthly_cost = 7.30

                    # Estimate data processing cost (assume 100 GB/month average)
                    estimated_data_gb = 100
                    data_processing_cost = estimated_data_gb * 0.02  # $0.01 in + $0.01 out

                    estimated_cost = base_monthly_cost + data_processing_cost

                    resources.append(AllCloudResourceData(
                        resource_id=endpoint.id,
                        resource_type="azure_private_endpoint",
                        resource_name=endpoint.name or "Unnamed Private Endpoint",
                        region=endpoint.location,
                        estimated_monthly_cost=round(estimated_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "endpoint_id": endpoint.id,
                            "resource_group": resource_group,
                            "provisioning_state": provisioning_state,
                            "connection_state": connection_state,
                            "connected_resource_id": connected_resource_id,
                            "connected_resource_count": connected_resource_count,
                            "subnet_id": endpoint.subnet.id if endpoint.subnet else None,
                            "tags": dict(endpoint.tags) if endpoint.tags else {},
                        },
                        is_optimizable=is_optimizable,
                        optimization_score=score,
                        optimization_priority=priority,
                        potential_monthly_savings=savings,
                        optimization_recommendations=recommendations,
                        last_used_at=None,
                        created_at_cloud=None,
                    ))

                except Exception as e:
                    self.logger.error(f"Error processing Private Endpoint {getattr(endpoint, 'name', 'unknown')}: {str(e)}")
                    continue

            self.logger.info(f"Found {len(resources)} Private Endpoints in region {region}")
            return resources

        except Exception as e:
            self.logger.error(f"Error scanning Private Endpoints: {str(e)}")
            return []

    def _calculate_private_endpoint_optimization(
        self,
        provisioning_state: str,
        connection_state: str,
        connected_resource_count: int,
        connected_resource_id: str | None,
    ) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate optimization potential for Private Endpoint.

        Returns:
            (is_optimizable, score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        base_cost = 9.30  # $7.30 base + ~$2 data processing

        # Scenario 1: Private Endpoint failed/deleting (CRITICAL - 90)
        if provisioning_state.lower() in ['failed', 'deleting', 'deleted']:
            is_optimizable = True
            optimization_score = max(optimization_score, 90)
            priority = "critical"
            potential_savings = max(potential_savings, base_cost)

            recommendations.append({
                "title": "Private Endpoint Non Fonctionnel",
                "description": f"Ce Private Endpoint est dans l'tat '{provisioning_state}'. Il gnre des cots inutiles.",
                "estimated_savings": round(base_cost, 2),
                "actions": [
                    "Vrifier les logs pour identifier le problme",
                    "Supprimer le Private Endpoint s'il ne peut pas tre rpar",
                    "Recrer le Private Endpoint si encore ncessaire"
                ],
                "priority": "critical",
            })

        # Scenario 2: Not connected to any resource (orphan) (HIGH - 75)
        if connected_resource_count == 0 or connection_state.lower() in ['rejected', 'disconnected']:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            if priority not in ["critical"]:
                priority = "high"
            potential_savings = max(potential_savings, base_cost)

            recommendations.append({
                "title": "Private Endpoint Non Connect (Orphelin)",
                "description": "Ce Private Endpoint n'est connect  aucune ressource. Il gnre des cots inutiles de $7-9/mois.",
                "estimated_savings": round(base_cost, 2),
                "actions": [
                    "Vrifier si le Private Endpoint est encore ncessaire",
                    "Supprimer si non utilis",
                    "Connecter  une ressource si oubli de configuration",
                    f"conomie: ${base_cost}/mois"
                ],
                "priority": "high",
            })

        # Scenario 3: Connected to deallocated/stopped resource (HIGH - 70)
        # Note: We can't easily check if connected resource is stopped without querying each resource type
        # This would require additional API calls per endpoint
        # In production, you'd check the connected resource status

        # Scenario 4: Redundant endpoints for same resource (MEDIUM - 50)
        if connected_resource_count > 1:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            if priority not in ["critical", "high"]:
                priority = "medium"

            # Assume can eliminate half of redundant connections
            savings = base_cost * 0.5
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Endpoints Redondants Potentiels",
                "description": f"Ce Private Endpoint a {connected_resource_count} connexions. Vrifiez si toutes sont ncessaires.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    f"Analyser les {connected_resource_count} connexions Private Link",
                    "Identifier si plusieurs endpoints pointent vers mme ressource",
                    "Consolider en un seul endpoint si possible",
                    "Note: Chaque endpoint cote $7-9/mois"
                ],
                "priority": "medium",
            })

        # Scenario 5: Using Premium without justification (LOW - 30)
        # Note: Private Endpoints don't have SKUs (Standard/Premium)
        # This scenario doesn't apply to Private Endpoints
        # Leaving as placeholder for consistency

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_ml_endpoints(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure ML Online Endpoints for cost intelligence.

        ML Endpoints are deployed models for real-time inference.
        Pricing: $0.50-$10/hour selon instance SKU ($360-$7200/mois) + inference costs
        Typical cost: $500-3000/month for production endpoints

        Detection criteria:
        - Endpoint failed/unhealthy (CRITICAL - 90 score)
        - Zero inference requests 30+ days (HIGH - 75 score)
        - Overprovisioned compute (traffic < 30% capacity) (HIGH - 70 score)
        - Premium SKU for low-traffic endpoint (MEDIUM - 50 score)
        - No auto-scaling configured (LOW - 30 score)
        """
        try:
            from azure.ai.ml import MLClient
        except ImportError:
            self.logger.error("azure-ai-ml not installed")
            return []

        resources = []
        self.logger.info(f"Scanning ML Endpoints in region: {region}")

        try:
            ml_client = MLClient(
                credential=self.credential,
                subscription_id=self.subscription_id
            )

            # List all ML workspaces first
            from azure.mgmt.machinelearningservices import AzureMachineLearningWorkspaces
            ml_mgmt_client = AzureMachineLearningWorkspaces(
                credential=self.credential,
                subscription_id=self.subscription_id
            )

            for workspace in ml_mgmt_client.workspaces.list():
                try:
                    # Filter by region
                    if region.lower() != "all" and workspace.location.lower() != region.lower():
                        continue

                    # Get resource group
                    resource_group = workspace.id.split("/")[4]

                    # Create MLClient for this specific workspace
                    workspace_ml_client = MLClient(
                        credential=self.credential,
                        subscription_id=self.subscription_id,
                        resource_group_name=resource_group,
                        workspace_name=workspace.name
                    )

                    # List all online endpoints in this workspace
                    endpoints = workspace_ml_client.online_endpoints.list()

                    for endpoint in endpoints:
                        try:
                            # Get endpoint details
                            endpoint_name = endpoint.name
                            provisioning_state = getattr(endpoint, 'provisioning_state', 'Unknown')

                            # Get deployments for this endpoint
                            deployments = list(workspace_ml_client.online_deployments.list(endpoint_name=endpoint_name))
                            deployment_count = len(deployments)

                            # Calculate total instance count and estimate cost
                            total_instances = 0
                            estimated_hourly_cost = 0.0

                            for deployment in deployments:
                                instance_count = getattr(deployment, 'instance_count', 0)
                                total_instances += instance_count

                                # Estimate cost based on instance type
                                # Simplified pricing: Standard_DS2_v2 = $0.50/hour, Premium = $10/hour
                                instance_type = getattr(deployment, 'instance_type', 'Standard_DS2_v2')
                                if 'Premium' in instance_type or 'GPU' in instance_type:
                                    estimated_hourly_cost += instance_count * 10.0
                                elif 'Standard_DS3' in instance_type or 'Standard_F' in instance_type:
                                    estimated_hourly_cost += instance_count * 2.0
                                else:
                                    estimated_hourly_cost += instance_count * 0.50

                            estimated_monthly_cost = estimated_hourly_cost * 730  # hours/month

                            # Calculate optimization
                            is_optimizable, score, priority, savings, recommendations = (
                                self._calculate_ml_endpoint_optimization(
                                    provisioning_state,
                                    deployment_count,
                                    total_instances,
                                    estimated_monthly_cost
                                )
                            )

                            resources.append(AllCloudResourceData(
                                resource_id=f"{workspace.id}/onlineEndpoints/{endpoint_name}",
                                resource_type="azure_ml_endpoint",
                                resource_name=endpoint_name or "Unnamed ML Endpoint",
                                region=workspace.location,
                                estimated_monthly_cost=round(estimated_monthly_cost, 2),
                                currency="USD",
                                resource_metadata={
                                    "endpoint_name": endpoint_name,
                                    "workspace_name": workspace.name,
                                    "resource_group": resource_group,
                                    "provisioning_state": provisioning_state,
                                    "deployment_count": deployment_count,
                                    "total_instances": total_instances,
                                    "estimated_hourly_cost": round(estimated_hourly_cost, 2),
                                },
                                is_optimizable=is_optimizable,
                                optimization_score=score,
                                optimization_priority=priority,
                                potential_monthly_savings=savings,
                                optimization_recommendations=recommendations,
                                last_used_at=None,
                                created_at_cloud=None,
                            ))

                        except Exception as e:
                            self.logger.error(f"Error processing ML endpoint {getattr(endpoint, 'name', 'unknown')}: {str(e)}")
                            continue

                except Exception as e:
                    self.logger.error(f"Error processing workspace {getattr(workspace, 'name', 'unknown')}: {str(e)}")
                    continue

            self.logger.info(f"Found {len(resources)} ML Endpoints in region {region}")
            return resources

        except Exception as e:
            self.logger.error(f"Error scanning ML Endpoints: {str(e)}")
            return []

    def _calculate_ml_endpoint_optimization(
        self,
        provisioning_state: str,
        deployment_count: int,
        total_instances: int,
        estimated_monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate optimization potential for ML Endpoint.

        Returns:
            (is_optimizable, score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: Endpoint failed/unhealthy (CRITICAL - 90)
        if provisioning_state.lower() in ['failed', 'deleting', 'deleted']:
            is_optimizable = True
            optimization_score = max(optimization_score, 90)
            priority = "critical"
            potential_savings = max(potential_savings, estimated_monthly_cost)

            recommendations.append({
                "title": "ML Endpoint Non Fonctionnel",
                "description": f"Cet endpoint est dans l'tat '{provisioning_state}'. Il gnre des cots inutiles.",
                "estimated_savings": round(estimated_monthly_cost, 2),
                "actions": [
                    "Vrifier les logs de dploiement",
                    "Supprimer l'endpoint s'il ne peut pas tre rpar",
                    "Redployer le modle si encore ncessaire"
                ],
                "priority": "critical",
            })

        # Scenario 2: Zero inference requests 30+ days (HIGH - 75)
        # Note: We can't get actual request metrics without Azure Monitor
        # In production, check actual inference request count

        # Scenario 3: Overprovisioned compute (HIGH - 70)
        if total_instances > 3:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            if priority not in ["critical"]:
                priority = "high"

            # Assume can reduce by 50%
            savings = estimated_monthly_cost * 0.5
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Instances Potentiellement Surdimensionnes",
                "description": f"Cet endpoint a {total_instances} instances. Vrifiez si toutes sont ncessaires.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Analyser les mtriques de trafic dans Azure Monitor",
                    f"Rduire de {total_instances}  {total_instances // 2} instances si charge <30%",
                    "Activer auto-scaling pour adapter automatiquement",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "priority": "high",
            })

        # Scenario 4: Premium SKU for low-traffic endpoint (MEDIUM - 50)
        if estimated_monthly_cost > 2000 and total_instances <= 2:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            if priority not in ["critical", "high"]:
                priority = "medium"

            # Assume can switch to cheaper SKU (60% savings)
            savings = estimated_monthly_cost * 0.6
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "SKU Premium pour Trafic Faible",
                "description": f"Cot ${estimated_monthly_cost:.2f}/mois avec peu d'instances suggre Premium SKU inutile.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Analyser le trafic rel d'infrence (requtes/jour)",
                    "Passer  Standard_DS2_v2 ou Standard_F2s_v2 si <1000 req/jour",
                    "Utiliser batch inference pour workloads non-temps-rel",
                    f"conomie: ~60% (${savings:.2f}/mois)"
                ],
                "priority": "medium",
            })

        # Scenario 5: No auto-scaling configured (LOW - 30)
        if deployment_count > 0 and total_instances > 1:
            # We can't easily check if auto-scaling is enabled without deployment details
            # This is a placeholder for best practice recommendation
            pass

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_synapse_sql_pools(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Synapse Dedicated SQL Pools for cost intelligence.

        Synapse SQL Pools are massively parallel processing (MPP) data warehouses.
        Pricing: $1.20-$360/hour depending on DWU level (DW100c to DW30000c)
        Typical cost: $900-259,000/month for production data warehouses

        Detection criteria:
        - SQL pool paused >7 days (CRITICAL - 90 score) - Still incurs storage costs
        - Zero queries 30+ days (HIGH - 75 score)
        - Overprovisioned DWUs (query load <30% capacity) (HIGH - 70 score)
        - No auto-pause configured (MEDIUM - 50 score)
        - Gen1 DWU (upgrade to Gen2) (LOW - 30 score)

        Returns:
            List of all Synapse SQL Pools with optimization recommendations
        """
        try:
            from azure.mgmt.synapse import SynapseManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-synapse not installed")
            return []

        resources = []
        self.logger.info(f"Scanning Synapse SQL Pools in region: {region}")

        try:
            # Create Synapse client
            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )
            synapse_client = SynapseManagementClient(credential, self.subscription_id)

            # Iterate through workspaces
            workspaces = synapse_client.workspaces.list()
            workspace_count = 0

            for workspace in workspaces:
                workspace_count += 1
                workspace_name = workspace.name
                workspace_location = workspace.location

                # Filter by region if specified
                if self.regions and workspace_location not in self.regions:
                    continue

                # Filter by resource group if specified
                resource_group = workspace.id.split('/')[4]
                if self.resource_groups and resource_group not in self.resource_groups:
                    continue

                # Get SQL pools in this workspace
                try:
                    sql_pools = synapse_client.sql_pools.list_by_workspace(
                        resource_group_name=resource_group,
                        workspace_name=workspace_name
                    )

                    for pool in sql_pools:
                        pool_name = pool.name
                        pool_status = getattr(pool, 'status', 'Unknown')
                        sku = getattr(pool, 'sku', None)

                        # Get DWU level (e.g., "DW1000c", "DW100c")
                        dw_name = sku.name if sku else "Unknown"
                        tier = sku.tier if sku and hasattr(sku, 'tier') else "Unknown"

                        # Calculate cost based on DWU level
                        monthly_cost = self._estimate_synapse_sql_pool_cost(dw_name, pool_status)

                        # Check optimization opportunities
                        is_optimizable, score, priority, savings, recommendations = \
                            await self._calculate_synapse_sql_pool_optimization(
                                pool, pool_status, dw_name, tier, monthly_cost
                            )

                        # Build metadata
                        metadata = {
                            "workspace_name": workspace_name,
                            "pool_name": pool_name,
                            "status": pool_status,
                            "dw_level": dw_name,
                            "tier": tier,
                            "resource_group": resource_group,
                            "collation": getattr(pool, 'collation', None),
                            "creation_date": getattr(pool, 'creation_date', None),
                            "max_size_bytes": getattr(pool, 'max_size_bytes', None),
                        }

                        # Determine if orphan (paused >90 days = likely abandoned)
                        is_orphan = pool_status.lower() == 'paused' and score >= 90

                        # Create resource record
                        resource = AllCloudResourceData(
                            resource_id=pool.id,
                            resource_name=pool_name,
                            resource_type="azure_synapse_sql_pool",
                            region=workspace_location,
                            estimated_monthly_cost=monthly_cost,
                            currency="USD",
                            resource_metadata=metadata,
                            is_orphan=is_orphan,
                            is_optimizable=is_optimizable and not is_orphan,
                            optimization_score=score if not is_orphan else 0,
                            optimization_priority=priority if not is_orphan else "none",
                            potential_monthly_savings=savings if not is_orphan else 0.0,
                            optimization_recommendations=recommendations if not is_orphan else []
                        )

                        resources.append(resource)
                        self.logger.info(
                            f"Found Synapse SQL Pool: {pool_name} "
                            f"(Status: {pool_status}, DWU: {dw_name}, "
                            f"Cost: ${monthly_cost:.2f}/mo, Optimizable: {is_optimizable})"
                        )

                except Exception as e:
                    self.logger.error(
                        f"Error scanning SQL pools in workspace {workspace_name}: {str(e)}"
                    )
                    continue

            self.logger.info(
                f"Synapse SQL Pool scan complete: {len(resources)} pools found "
                f"across {workspace_count} workspaces"
            )

        except Exception as e:
            self.logger.error(f"Error scanning Synapse SQL Pools: {str(e)}")

        return resources

    def _estimate_synapse_sql_pool_cost(self, dw_name: str, status: str) -> float:
        """
        Estimate monthly cost for Synapse SQL Pool based on DWU level.

        Pricing (Gen2 cDWU - compute optimized):
        - DW100c: $1.20/hour = $876/month
        - DW200c: $2.40/hour = $1,752/month
        - DW500c: $6.00/hour = $4,380/month
        - DW1000c: $12.00/hour = $8,760/month
        - DW2000c: $24.00/hour = $17,520/month
        - DW5000c: $60.00/hour = $43,800/month
        - DW10000c: $120.00/hour = $87,600/month
        - DW15000c: $180.00/hour = $131,400/month
        - DW30000c: $360.00/hour = $262,800/month

        Note: When paused, only storage costs apply (~$120/TB/month)
        """
        hours_per_month = 730  # Average

        # Pricing map for Gen2 cDWU levels
        pricing_map = {
            "DW100c": 1.20,
            "DW200c": 2.40,
            "DW300c": 3.60,
            "DW400c": 4.80,
            "DW500c": 6.00,
            "DW1000c": 12.00,
            "DW1500c": 18.00,
            "DW2000c": 24.00,
            "DW2500c": 30.00,
            "DW3000c": 36.00,
            "DW5000c": 60.00,
            "DW6000c": 72.00,
            "DW7500c": 90.00,
            "DW10000c": 120.00,
            "DW15000c": 180.00,
            "DW30000c": 360.00,
        }

        # Check if paused (storage-only costs)
        if status.lower() == 'paused':
            # Assume 1TB average storage = $120/month
            return 120.0

        # Get hourly rate
        hourly_rate = pricing_map.get(dw_name, 12.00)  # Default to DW1000c

        return hourly_rate * hours_per_month

    async def _calculate_synapse_sql_pool_optimization(
        self,
        pool: Any,
        status: str,
        dw_name: str,
        tier: str,
        monthly_cost: float
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Synapse SQL Pool.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # SCENARIO 1: CRITICAL - SQL pool paused >7 days
        # This is likely abandoned or forgotten - still incurs storage costs
        if status.lower() == 'paused':
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            # Savings = eliminate storage costs by deleting backup and pool
            potential_savings = 120.0  # Storage costs per month
            recommendations.append(
                "CRITICAL: SQL pool has been paused for extended period. "
                "If no longer needed, delete to eliminate storage costs ($120/TB/month)."
            )

        # SCENARIO 2: HIGH - Zero queries 30+ days (placeholder - needs query metrics)
        # In real implementation, would check Synapse analytics/monitoring
        elif status.lower() == 'online':
            # This is a placeholder - would require querying Synapse monitoring/analytics
            # For now, we can't determine query activity without additional API calls
            pass

        # SCENARIO 3: HIGH - Overprovisioned DWUs (placeholder - needs query metrics)
        # In real implementation, would analyze DWU utilization vs query load
        # If DWU usage <30%, could downgrade to smaller tier
        if status.lower() == 'online' and dw_name.startswith('DW'):
            # Extract DWU number (e.g., "DW5000c" -> 5000)
            try:
                dw_number = int(dw_name.replace('DW', '').replace('c', ''))

                # If using very large DWU (>5000c), recommend reviewing necessity
                if dw_number >= 5000:
                    is_optimizable = True
                    optimization_score = max(optimization_score, 70)
                    priority = "high" if priority == "none" else priority

                    # Potential savings: Downgrade from DW5000c to DW2000c
                    current_hourly = monthly_cost / 730
                    potential_hourly = 24.00  # DW2000c
                    if current_hourly > potential_hourly:
                        potential_savings = (current_hourly - potential_hourly) * 730
                        recommendations.append(
                            f"HIGH: Large DWU tier ({dw_name}). Review query patterns - "
                            f"if average load <30%, consider downgrading to save ${potential_savings:.2f}/month."
                        )
            except ValueError:
                pass

        # SCENARIO 4: MEDIUM - No auto-pause configured
        # Synapse SQL Pools support auto-pause to save costs when idle
        # This is a placeholder - would require checking pool settings via API
        if status.lower() == 'online' and not is_optimizable:
            # This is a best practice recommendation
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority == "none" else priority
            recommendations.append(
                "MEDIUM: Consider enabling auto-pause for idle periods. "
                "SQL pool can auto-pause after inactivity to reduce costs."
            )

        # SCENARIO 5: LOW - Gen1 DWU (upgrade to Gen2)
        # Gen1 = "DW100", "DW200", etc. (no 'c' suffix)
        # Gen2 = "DW100c", "DW200c", etc. ('c' suffix = compute optimized)
        if tier == "DW" or (dw_name.startswith('DW') and not dw_name.endswith('c')):
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            recommendations.append(
                "LOW: Using Gen1 DWU tier. Migrate to Gen2 (compute optimized) "
                "for 5x better performance at same cost."
            )

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_vpn_gateways(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure VPN Gateways for cost intelligence.

        VPN Gateways provide site-to-site, point-to-site, and VNet-to-VNet connectivity.
        Pricing: $27-$650/month depending on SKU (Basic, VpnGw1-5, VpnGw1AZ-5AZ)
        Typical cost: $150-400/month for production gateways

        Detection criteria:
        - No active connections 30+ days (CRITICAL - 90 score)
        - Very low data transfer <1GB/month (HIGH - 75 score)
        - Overprovisioned SKU (traffic <30% capacity) (HIGH - 70 score)
        - Point-to-Site only (use Azure Bastion instead) (MEDIUM - 50 score)
        - Legacy Basic SKU (LOW - 30 score)

        Returns:
            List of all VPN Gateways with optimization recommendations
        """
        try:
            from azure.mgmt.network import NetworkManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-network not installed")
            return []

        resources = []
        self.logger.info(f"Scanning VPN Gateways in region: {region}")

        try:
            # Create network client
            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )
            network_client = NetworkManagementClient(credential, self.subscription_id)

            # Get all resource groups
            resource_groups = await self._get_resource_groups()

            for rg in resource_groups:
                rg_name = rg.name

                # Filter by resource group if specified
                if self.resource_groups and rg_name not in self.resource_groups:
                    continue

                try:
                    # List VPN gateways in this resource group
                    vpn_gateways = network_client.virtual_network_gateways.list(rg_name)

                    for gateway in vpn_gateways:
                        gateway_name = gateway.name
                        gateway_location = gateway.location

                        # Filter by region if specified
                        if self.regions and gateway_location not in self.regions:
                            continue

                        # Only process VPN gateways (not ExpressRoute)
                        gateway_type = getattr(gateway, 'gateway_type', 'Unknown')
                        if gateway_type.lower() != 'vpn':
                            continue

                        # Get SKU and configuration
                        sku = getattr(gateway, 'sku', None)
                        sku_name = sku.name if sku else "Unknown"
                        sku_tier = sku.tier if sku and hasattr(sku, 'tier') else "Unknown"

                        # Get VPN type and configuration
                        vpn_type = getattr(gateway, 'vpn_type', 'Unknown')
                        vpn_client_config = getattr(gateway, 'vpn_client_configuration', None)
                        has_p2s = vpn_client_config is not None
                        bgp_settings = getattr(gateway, 'bgp_settings', None)
                        has_bgp = bgp_settings is not None

                        # Get active connections count (placeholder - needs additional API call)
                        # In real implementation, would query network_client.virtual_network_gateway_connections.list()
                        active_connections = 0  # Placeholder

                        # Calculate monthly cost based on SKU
                        monthly_cost = self._estimate_vpn_gateway_cost(sku_name)

                        # Check optimization opportunities
                        is_optimizable, score, priority, savings, recommendations = \
                            await self._calculate_vpn_gateway_optimization(
                                gateway, sku_name, has_p2s, active_connections, monthly_cost
                            )

                        # Build metadata
                        metadata = {
                            "gateway_name": gateway_name,
                            "sku": sku_name,
                            "tier": sku_tier,
                            "vpn_type": vpn_type,
                            "has_point_to_site": has_p2s,
                            "has_bgp": has_bgp,
                            "active_connections": active_connections,
                            "resource_group": rg_name,
                            "provisioning_state": getattr(gateway, 'provisioning_state', 'Unknown'),
                        }

                        # Determine if orphan (no connections for 90+ days = likely abandoned)
                        is_orphan = active_connections == 0 and score >= 90

                        # Create resource record
                        resource = AllCloudResourceData(
                            resource_id=gateway.id,
                            resource_name=gateway_name,
                            resource_type="azure_vpn_gateway",
                            region=gateway_location,
                            estimated_monthly_cost=monthly_cost,
                            currency="USD",
                            resource_metadata=metadata,
                            is_orphan=is_orphan,
                            is_optimizable=is_optimizable and not is_orphan,
                            optimization_score=score if not is_orphan else 0,
                            optimization_priority=priority if not is_orphan else "none",
                            potential_monthly_savings=savings if not is_orphan else 0.0,
                            optimization_recommendations=recommendations if not is_orphan else []
                        )

                        resources.append(resource)
                        self.logger.info(
                            f"Found VPN Gateway: {gateway_name} "
                            f"(SKU: {sku_name}, P2S: {has_p2s}, "
                            f"Cost: ${monthly_cost:.2f}/mo, Optimizable: {is_optimizable})"
                        )

                except Exception as e:
                    self.logger.error(
                        f"Error scanning VPN gateways in resource group {rg_name}: {str(e)}"
                    )
                    continue

            self.logger.info(
                f"VPN Gateway scan complete: {len(resources)} gateways found"
            )

        except Exception as e:
            self.logger.error(f"Error scanning VPN Gateways: {str(e)}")

        return resources

    def _estimate_vpn_gateway_cost(self, sku_name: str) -> float:
        """
        Estimate monthly cost for VPN Gateway based on SKU.

        Pricing (monthly, includes 730 hours):
        - Basic: $27/month (legacy, site-to-site only, no BGP, max 10 tunnels)
        - VpnGw1: $150/month (30 tunnels, 650 Mbps, BGP)
        - VpnGw2: $380/month (30 tunnels, 1 Gbps, BGP)
        - VpnGw3: $410/month (30 tunnels, 1.25 Gbps, BGP)
        - VpnGw4: $580/month (100 tunnels, 5 Gbps, BGP)
        - VpnGw5: $650/month (100 tunnels, 10 Gbps, BGP)
        - VpnGw1AZ-5AZ: Zone-redundant versions (+10% cost)

        Note: Plus data transfer costs ($0.087/GB outbound)
        """
        pricing_map = {
            "Basic": 27.0,
            "VpnGw1": 150.0,
            "VpnGw2": 380.0,
            "VpnGw3": 410.0,
            "VpnGw4": 580.0,
            "VpnGw5": 650.0,
            "VpnGw1AZ": 165.0,  # +10% for zone redundancy
            "VpnGw2AZ": 418.0,
            "VpnGw3AZ": 451.0,
            "VpnGw4AZ": 638.0,
            "VpnGw5AZ": 715.0,
        }

        return pricing_map.get(sku_name, 150.0)  # Default to VpnGw1

    async def _calculate_vpn_gateway_optimization(
        self,
        gateway: Any,
        sku_name: str,
        has_p2s: bool,
        active_connections: int,
        monthly_cost: float
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for VPN Gateway.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # SCENARIO 1: CRITICAL - No active connections 30+ days
        # This is a placeholder - would require querying connection history/metrics
        if active_connections == 0:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost  # Full cost savings by deletion
            recommendations.append(
                f"CRITICAL: VPN Gateway has no active connections. "
                f"If no longer needed, delete to save ${monthly_cost:.2f}/month."
            )

        # SCENARIO 2: HIGH - Very low data transfer <1GB/month (placeholder)
        # In real implementation, would check Azure Monitor metrics for GatewayBandwidth
        # For now, this is a placeholder for future implementation
        elif False:  # Placeholder condition
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority == "none" else priority
            recommendations.append(
                "HIGH: Very low data transfer detected (<1GB/month). "
                "Verify VPN is actively used or consider deletion."
            )

        # SCENARIO 3: HIGH - Overprovisioned SKU (traffic <30% capacity)
        # Placeholder - would require analyzing bandwidth metrics vs SKU capacity
        if sku_name in ["VpnGw4", "VpnGw5", "VpnGw4AZ", "VpnGw5AZ"]:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority == "none" else priority

            # Calculate savings by downgrading to VpnGw1
            potential_savings = monthly_cost - 150.0
            if potential_savings > 0:
                recommendations.append(
                    f"HIGH: High-tier SKU ({sku_name}). Review bandwidth usage - "
                    f"if <30% capacity, downgrade to VpnGw1 to save ${potential_savings:.2f}/month."
                )

        # SCENARIO 4: MEDIUM - Point-to-Site only (use Azure Bastion instead)
        # Azure Bastion provides secure RDP/SSH access without VPN (~$140/month)
        if has_p2s and active_connections == 0 and not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority == "none" else priority
            # Bastion is ~$140/month, VPN Gateway is ~$150/month, so minimal savings
            # But Bastion is simpler and more secure for admin access only
            recommendations.append(
                "MEDIUM: VPN Gateway configured only for Point-to-Site. "
                "Consider Azure Bastion instead for secure admin access (simpler, more secure)."
            )

        # SCENARIO 5: LOW - Legacy Basic SKU
        # Basic SKU lacks BGP, IKEv2, zone redundancy, and modern features
        if sku_name == "Basic" and not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            recommendations.append(
                "LOW: Using legacy Basic SKU. Upgrade to VpnGw1 for BGP support, "
                "IKEv2, better performance, and modern features (+$123/month)."
            )

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_vnet_peerings(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure VNet Peerings for cost intelligence.

        VNet Peering connects two Azure Virtual Networks for private communication.
        Pricing: $0.01/GB intra-region, $0.035-0.05/GB inter-region/global
        Typical cost: $10-100/month depending on traffic volume

        Detection criteria:
        - Peering in Failed/Disconnected state (CRITICAL - 90 score)
        - Peering with 0 traffic 30+ days (HIGH - 75 score)
        - Global peering with very low traffic <1GB/month (HIGH - 70 score)
        - Unidirectional peering (should be bidirectional) (MEDIUM - 50 score)
        - Redundant peerings between same VNets (LOW - 30 score)

        Returns:
            List of all VNet Peerings with optimization recommendations
        """
        try:
            from azure.mgmt.network import NetworkManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-network not installed")
            return []

        resources = []
        self.logger.info(f"Scanning VNet Peerings in region: {region}")

        try:
            # Create network client
            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )
            network_client = NetworkManagementClient(credential, self.subscription_id)

            # Get all resource groups
            resource_groups = await self._get_resource_groups()

            for rg in resource_groups:
                rg_name = rg.name

                # Filter by resource group if specified
                if self.resource_groups and rg_name not in self.resource_groups:
                    continue

                try:
                    # List virtual networks in this resource group
                    vnets = network_client.virtual_networks.list(rg_name)

                    for vnet in vnets:
                        vnet_name = vnet.name
                        vnet_location = vnet.location

                        # Filter by region if specified
                        if self.regions and vnet_location not in self.regions:
                            continue

                        # Get peerings for this VNet
                        peerings = getattr(vnet, 'virtual_network_peerings', [])

                        for peering in peerings:
                            peering_name = peering.name
                            peering_state = getattr(peering, 'peering_state', 'Unknown')
                            provisioning_state = getattr(peering, 'provisioning_state', 'Unknown')

                            # Get remote VNet info
                            remote_vnet = getattr(peering, 'remote_virtual_network', None)
                            remote_vnet_id = remote_vnet.id if remote_vnet else "Unknown"

                            # Determine if global peering (different regions)
                            # Parse remote VNet region from ID or assume same region
                            is_global = False  # Placeholder - would need to query remote VNet

                            # Estimate traffic (placeholder - would need Azure Monitor metrics)
                            monthly_gb_transfer = 0.0  # Placeholder

                            # Calculate cost
                            monthly_cost = self._estimate_vnet_peering_cost(is_global, monthly_gb_transfer)

                            # Check optimization opportunities
                            is_optimizable, score, priority, savings, recommendations = \
                                await self._calculate_vnet_peering_optimization(
                                    peering, peering_state, is_global, monthly_gb_transfer, monthly_cost
                                )

                            # Build metadata
                            metadata = {
                                "vnet_name": vnet_name,
                                "peering_name": peering_name,
                                "peering_state": peering_state,
                                "provisioning_state": provisioning_state,
                                "remote_vnet_id": remote_vnet_id,
                                "is_global": is_global,
                                "allow_virtual_network_access": getattr(peering, 'allow_virtual_network_access', False),
                                "allow_forwarded_traffic": getattr(peering, 'allow_forwarded_traffic', False),
                                "allow_gateway_transit": getattr(peering, 'allow_gateway_transit', False),
                                "use_remote_gateways": getattr(peering, 'use_remote_gateways', False),
                                "resource_group": rg_name,
                            }

                            # Determine if orphan (failed/disconnected state = waste)
                            is_orphan = peering_state in ['Failed', 'Disconnected'] and score >= 90

                            # Create resource record
                            resource = AllCloudResourceData(
                                resource_id=f"{vnet.id}/virtualNetworkPeerings/{peering_name}",
                                resource_name=f"{vnet_name}  {peering_name}",
                                resource_type="azure_vnet_peering",
                                region=vnet_location,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=metadata,
                                is_orphan=is_orphan,
                                is_optimizable=is_optimizable and not is_orphan,
                                optimization_score=score if not is_orphan else 0,
                                optimization_priority=priority if not is_orphan else "none",
                                potential_monthly_savings=savings if not is_orphan else 0.0,
                                optimization_recommendations=recommendations if not is_orphan else []
                            )

                            resources.append(resource)
                            self.logger.info(
                                f"Found VNet Peering: {vnet_name}  {peering_name} "
                                f"(State: {peering_state}, Global: {is_global}, "
                                f"Cost: ${monthly_cost:.2f}/mo, Optimizable: {is_optimizable})"
                            )

                except Exception as e:
                    self.logger.error(
                        f"Error scanning VNet peerings in resource group {rg_name}: {str(e)}"
                    )
                    continue

            self.logger.info(
                f"VNet Peering scan complete: {len(resources)} peerings found"
            )

        except Exception as e:
            self.logger.error(f"Error scanning VNet Peerings: {str(e)}")

        return resources

    def _estimate_vnet_peering_cost(self, is_global: bool, monthly_gb: float) -> float:
        """
        Estimate monthly cost for VNet Peering based on traffic.

        Pricing:
        - Intra-region: $0.01/GB ingress + $0.01/GB egress = $0.02/GB total
        - Inter-region (same geography): $0.035/GB
        - Global peering (cross-geography): $0.05/GB

        Note: Most peerings have low traffic, so base cost is often minimal
        """
        if monthly_gb == 0:
            # Peering itself is free, only data transfer is charged
            # Assume minimum $5/month for minimal traffic
            return 5.0

        if is_global:
            # Global peering
            cost_per_gb = 0.05
        else:
            # Assume intra-region (most common)
            cost_per_gb = 0.02

        return monthly_gb * cost_per_gb

    async def _calculate_vnet_peering_optimization(
        self,
        peering: Any,
        peering_state: str,
        is_global: bool,
        monthly_gb: float,
        monthly_cost: float
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for VNet Peering.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # SCENARIO 1: CRITICAL - Peering in Failed/Disconnected state
        if peering_state in ['Failed', 'Disconnected']:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append(
                f"CRITICAL: VNet Peering is in {peering_state} state. "
                f"Delete if no longer needed to save ${monthly_cost:.2f}/month."
            )

        # SCENARIO 2: HIGH - Peering with 0 traffic 30+ days (placeholder)
        # In real implementation, would check Azure Monitor metrics for BytesTransferred
        elif monthly_gb == 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority == "none" else priority
            potential_savings = monthly_cost
            recommendations.append(
                "HIGH: VNet Peering has no traffic for 30+ days. "
                f"Verify if still needed or delete to save ${monthly_cost:.2f}/month."
            )

        # SCENARIO 3: HIGH - Global peering with very low traffic <1GB/month
        # Global peering is expensive ($0.05/GB), recommend migrating data or using alternative
        if is_global and monthly_gb < 1.0 and monthly_gb > 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority == "none" else priority
            # Savings: Use ExpressRoute or VPN Gateway for low-volume traffic
            potential_savings = monthly_cost * 0.5  # 50% savings estimate
            recommendations.append(
                f"HIGH: Global VNet Peering with very low traffic ({monthly_gb:.2f} GB/month). "
                f"Consider ExpressRoute or VPN Gateway for cost optimization (~50% savings)."
            )

        # SCENARIO 4: MEDIUM - Unidirectional peering (should be bidirectional)
        # This is a best practice check - bidirectional peering ensures full connectivity
        # Placeholder - would require checking if remote VNet has reciprocal peering
        allow_virtual_network_access = getattr(peering, 'allow_virtual_network_access', False)
        if not allow_virtual_network_access and not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority == "none" else priority
            recommendations.append(
                "MEDIUM: VNet Peering may not have reciprocal peering configured. "
                "Ensure bidirectional peering for full connectivity."
            )

        # SCENARIO 5: LOW - Redundant peerings (placeholder)
        # In real implementation, would check if multiple peerings exist between same VNets
        # This is a placeholder for future enhancement
        if False:  # Placeholder condition
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            recommendations.append(
                "LOW: Multiple VNet Peerings detected between same VNets. "
                "Consolidate to single peering to simplify management."
            )

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_front_doors(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Front Door profiles for cost intelligence.

        Front Door is a global CDN + WAF + load balancer service.
        Pricing: Standard $35/mois, Premium $330/mois + data transfer
        Typical cost: $50-500/month

        Detection criteria:
        - Front Door with 0 requests 30+ days (CRITICAL - 90 score)
        - Premium tier with WAF disabled (overpaying) (HIGH - 75 score)
        - Very low traffic <1GB/month (HIGH - 70 score)
        - Endpoints not used or redundant (MEDIUM - 50 score)
        - Classic tier (migration to Standard/Premium) (LOW - 30 score)

        Returns:
            List of all Front Door profiles with optimization recommendations
        """
        try:
            from azure.mgmt.frontdoor import FrontDoorManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-frontdoor not installed")
            return []

        resources = []
        self.logger.info(f"Scanning Front Door profiles (global service)")

        try:
            # Create Front Door client
            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )
            frontdoor_client = FrontDoorManagementClient(credential, self.subscription_id)

            # Get all resource groups
            resource_groups = await self._get_resource_groups()

            for rg in resource_groups:
                rg_name = rg.name

                # Filter by resource group if specified
                if self.resource_groups and rg_name not in self.resource_groups:
                    continue

                try:
                    # List Front Door profiles (new Standard/Premium)
                    # Note: This API may differ - azure-mgmt-frontdoor has multiple versions
                    # Using legacy Front Door list for compatibility
                    front_doors = frontdoor_client.front_doors.list_by_resource_group(rg_name)

                    for fd in front_doors:
                        fd_name = fd.name
                        fd_location = getattr(fd, 'location', 'Global')  # Front Door is global

                        # Get SKU/tier
                        sku = getattr(fd, 'sku', None)
                        sku_name = sku.name if sku and hasattr(sku, 'name') else "Classic"

                        # Get provisioning state
                        provisioning_state = getattr(fd, 'provisioning_state', 'Unknown')
                        resource_state = getattr(fd, 'resource_state', 'Unknown')

                        # Get endpoints
                        frontend_endpoints = getattr(fd, 'frontend_endpoints', [])
                        backend_pools = getattr(fd, 'backend_pools', [])
                        routing_rules = getattr(fd, 'routing_rules', [])

                        # Get WAF policy (if Premium)
                        web_application_firewall_policy_link = getattr(fd, 'web_application_firewall_policy_link', None)
                        has_waf = web_application_firewall_policy_link is not None

                        # Estimate monthly traffic (placeholder - would need Azure Monitor)
                        monthly_requests = 0  # Placeholder
                        monthly_gb_transfer = 0.0  # Placeholder

                        # Calculate cost
                        monthly_cost = self._estimate_front_door_cost(sku_name, monthly_requests)

                        # Check optimization opportunities
                        is_optimizable, score, priority, savings, recommendations = \
                            await self._calculate_front_door_optimization(
                                fd, sku_name, has_waf, monthly_requests, monthly_gb_transfer, monthly_cost
                            )

                        # Build metadata
                        metadata = {
                            "front_door_name": fd_name,
                            "sku": sku_name,
                            "provisioning_state": provisioning_state,
                            "resource_state": resource_state,
                            "has_waf": has_waf,
                            "frontend_endpoints_count": len(frontend_endpoints),
                            "backend_pools_count": len(backend_pools),
                            "routing_rules_count": len(routing_rules),
                            "resource_group": rg_name,
                        }

                        # Determine if orphan (0 requests for 90+ days = waste)
                        is_orphan = monthly_requests == 0 and score >= 90

                        # Create resource record
                        resource = AllCloudResourceData(
                            resource_id=fd.id,
                            resource_name=fd_name,
                            resource_type="azure_front_door",
                            region=fd_location,
                            estimated_monthly_cost=monthly_cost,
                            currency="USD",
                            resource_metadata=metadata,
                            is_orphan=is_orphan,
                            is_optimizable=is_optimizable and not is_orphan,
                            optimization_score=score if not is_orphan else 0,
                            optimization_priority=priority if not is_orphan else "none",
                            potential_monthly_savings=savings if not is_orphan else 0.0,
                            optimization_recommendations=recommendations if not is_orphan else []
                        )

                        resources.append(resource)
                        self.logger.info(
                            f"Found Front Door: {fd_name} "
                            f"(SKU: {sku_name}, WAF: {has_waf}, "
                            f"Endpoints: {len(frontend_endpoints)}, "
                            f"Cost: ${monthly_cost:.2f}/mo, Optimizable: {is_optimizable})"
                        )

                except Exception as e:
                    self.logger.error(
                        f"Error scanning Front Doors in resource group {rg_name}: {str(e)}"
                    )
                    continue

            self.logger.info(
                f"Front Door scan complete: {len(resources)} profiles found"
            )

        except Exception as e:
            self.logger.error(f"Error scanning Front Doors: {str(e)}")

        return resources

    def _estimate_front_door_cost(self, sku: str, monthly_requests: int) -> float:
        """
        Estimate monthly cost for Front Door based on SKU and usage.

        Pricing:
        - Classic: $35/month base + $0.03/GB data transfer + $0.01/10K requests
        - Standard: $35/month base + $0.03/GB data transfer + $0.01/10K requests
        - Premium: $330/month base + $0.04/GB data transfer + WAF ($10/policy + $1/rule)

        For simplicity, using base pricing + minimal traffic assumption
        """
        if sku in ["Premium", "Premium_AzureFrontDoor"]:
            base_cost = 330.0
        elif sku in ["Standard", "Standard_AzureFrontDoor"]:
            base_cost = 35.0
        else:
            # Classic or unknown
            base_cost = 35.0

        # Add estimated data transfer costs (assume 10GB/month average)
        if sku in ["Premium", "Premium_AzureFrontDoor"]:
            data_transfer_cost = 10 * 0.04  # $0.04/GB
        else:
            data_transfer_cost = 10 * 0.03  # $0.03/GB

        return base_cost + data_transfer_cost

    async def _calculate_front_door_optimization(
        self,
        front_door: Any,
        sku: str,
        has_waf: bool,
        monthly_requests: int,
        monthly_gb: float,
        monthly_cost: float
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Front Door.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # SCENARIO 1: CRITICAL - Front Door with 0 requests 30+ days
        if monthly_requests == 0:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append(
                f"CRITICAL: Front Door has no requests for 30+ days. "
                f"Delete if no longer needed to save ${monthly_cost:.2f}/month."
            )

        # SCENARIO 2: HIGH - Premium tier with WAF disabled (overpaying)
        # Premium is $330/month vs Standard $35/month - WAF is the main differentiator
        elif sku in ["Premium", "Premium_AzureFrontDoor"] and not has_waf:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority == "none" else priority
            # Savings: Downgrade to Standard
            potential_savings = 330.0 - 35.0  # $295/month
            recommendations.append(
                f"HIGH: Premium tier without WAF enabled. "
                f"Downgrade to Standard tier to save ${potential_savings:.2f}/month."
            )

        # SCENARIO 3: HIGH - Very low traffic <1GB/month
        # Front Door has high base cost - not cost-effective for very low traffic
        elif monthly_gb < 1.0 and monthly_gb > 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority == "none" else priority
            # Savings: Use Azure CDN Standard instead (~$10/month)
            potential_savings = monthly_cost - 10.0
            if potential_savings > 0:
                recommendations.append(
                    f"HIGH: Very low traffic ({monthly_gb:.2f} GB/month). "
                    f"Consider Azure CDN Standard instead to save ${potential_savings:.2f}/month."
                )

        # SCENARIO 4: MEDIUM - Unused endpoints or redundant rules
        # Check if endpoints are configured but not used
        frontend_endpoints = getattr(front_door, 'frontend_endpoints', [])
        routing_rules = getattr(front_door, 'routing_rules', [])

        if len(frontend_endpoints) > 5 and not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority == "none" else priority
            recommendations.append(
                f"MEDIUM: {len(frontend_endpoints)} frontend endpoints configured. "
                f"Review and remove unused endpoints to simplify configuration."
            )

        # SCENARIO 5: LOW - Classic tier (migration recommended)
        # Classic tier is being deprecated, recommend migration
        if sku == "Classic" and not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            recommendations.append(
                "LOW: Using Classic tier. Migrate to Standard or Premium tier "
                "for better performance, features, and support."
            )

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_container_registries(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Container Registries for cost intelligence.

        Container Registry is a private Docker registry for storing container images.
        Pricing: Basic $5/mois, Standard $20/mois, Premium $50/mois + storage + bandwidth
        Typical cost: $20-200/month

        Detection criteria:
        - Registry inutilis (0 pulls 90+ days) (CRITICAL - 90 score)
        - Premium tier sans geo-replication (HIGH - 75 score)
        - Images obsoltes non nettoyes (>50 untagged) (HIGH - 70 score)
        - Pas de retention policy (MEDIUM - 50 score)
        - Basic tier pour production (LOW - 30 score)

        Returns:
            List of all Container Registries with optimization recommendations
        """
        try:
            from azure.mgmt.containerregistry import ContainerRegistryManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-containerregistry not installed")
            return []

        resources = []
        self.logger.info(f"Scanning Container Registries in region: {region}")

        try:
            # Create Container Registry client
            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )
            acr_client = ContainerRegistryManagementClient(credential, self.subscription_id)

            # Get all resource groups
            resource_groups = await self._get_resource_groups()

            for rg in resource_groups:
                rg_name = rg.name

                # Filter by resource group if specified
                if self.resource_groups and rg_name not in self.resource_groups:
                    continue

                try:
                    # List registries in this resource group
                    registries = acr_client.registries.list_by_resource_group(rg_name)

                    for registry in registries:
                        registry_name = registry.name
                        registry_location = registry.location

                        # Filter by region if specified
                        if self.regions and registry_location not in self.regions:
                            continue

                        # Get SKU
                        sku = getattr(registry, 'sku', None)
                        sku_name = sku.name if sku else "Basic"

                        # Get provisioning state
                        provisioning_state = getattr(registry, 'provisioning_state', 'Unknown')

                        # Get geo-replication status (Premium only)
                        replications = []
                        has_geo_replication = False
                        if sku_name == "Premium":
                            try:
                                replications_list = acr_client.replications.list(rg_name, registry_name)
                                replications = list(replications_list)
                                has_geo_replication = len(replications) > 1  # More than 1 = geo-replicated
                            except Exception:
                                pass

                        # Get storage usage (placeholder - would need Azure Monitor)
                        storage_gb = 10.0  # Placeholder

                        # Calculate cost
                        monthly_cost = self._estimate_container_registry_cost(sku_name, storage_gb, has_geo_replication)

                        # Estimate usage metrics (placeholder - would need Azure Monitor)
                        successful_pulls_30d = 0  # Placeholder
                        successful_pushes_30d = 0  # Placeholder
                        untagged_images_count = 0  # Placeholder

                        # Check optimization opportunities
                        is_optimizable, score, priority, savings, recommendations = \
                            await self._calculate_container_registry_optimization(
                                registry, sku_name, has_geo_replication, successful_pulls_30d,
                                untagged_images_count, monthly_cost
                            )

                        # Build metadata
                        metadata = {
                            "registry_name": registry_name,
                            "sku": sku_name,
                            "provisioning_state": provisioning_state,
                            "admin_user_enabled": getattr(registry, 'admin_user_enabled', False),
                            "has_geo_replication": has_geo_replication,
                            "replication_count": len(replications),
                            "storage_gb": storage_gb,
                            "successful_pulls_30d": successful_pulls_30d,
                            "successful_pushes_30d": successful_pushes_30d,
                            "untagged_images_count": untagged_images_count,
                            "resource_group": rg_name,
                        }

                        # Determine if orphan (0 pulls for 90+ days = waste)
                        is_orphan = successful_pulls_30d == 0 and score >= 90

                        # Create resource record
                        resource = AllCloudResourceData(
                            resource_id=registry.id,
                            resource_name=registry_name,
                            resource_type="azure_container_registry",
                            region=registry_location,
                            estimated_monthly_cost=monthly_cost,
                            currency="USD",
                            resource_metadata=metadata,
                            is_orphan=is_orphan,
                            is_optimizable=is_optimizable and not is_orphan,
                            optimization_score=score if not is_orphan else 0,
                            optimization_priority=priority if not is_orphan else "none",
                            potential_monthly_savings=savings if not is_orphan else 0.0,
                            optimization_recommendations=recommendations if not is_orphan else []
                        )

                        resources.append(resource)
                        self.logger.info(
                            f"Found Container Registry: {registry_name} "
                            f"(SKU: {sku_name}, Geo-replication: {has_geo_replication}, "
                            f"Cost: ${monthly_cost:.2f}/mo, Optimizable: {is_optimizable})"
                        )

                except Exception as e:
                    self.logger.error(
                        f"Error scanning container registries in resource group {rg_name}: {str(e)}"
                    )
                    continue

            self.logger.info(
                f"Container Registry scan complete: {len(resources)} registries found"
            )

        except Exception as e:
            self.logger.error(f"Error scanning Container Registries: {str(e)}")

        return resources

    def _estimate_container_registry_cost(self, sku: str, storage_gb: float, geo_replication: bool) -> float:
        """
        Estimate monthly cost for Container Registry based on SKU and usage.

        Pricing:
        - Basic: $5/month + $0.10/GB storage
        - Standard: $20/month + $0.10/GB storage + webhooks
        - Premium: $50/month + $0.10/GB storage + geo-replication + content trust

        Geo-replication: ~$50/month per additional region (Premium only)
        """
        # Base costs
        base_costs = {
            "Basic": 5.0,
            "Standard": 20.0,
            "Premium": 50.0,
        }

        base_cost = base_costs.get(sku, 20.0)

        # Storage costs ($0.10/GB)
        storage_cost = storage_gb * 0.10

        # Geo-replication cost (Premium only, ~$50 per additional region)
        geo_cost = 0.0
        if sku == "Premium" and geo_replication:
            geo_cost = 50.0  # Assume 1 additional region

        return base_cost + storage_cost + geo_cost

    async def _calculate_container_registry_optimization(
        self,
        registry: Any,
        sku: str,
        has_geo_replication: bool,
        successful_pulls_30d: int,
        untagged_images_count: int,
        monthly_cost: float
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Container Registry.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # SCENARIO 1: CRITICAL - Registry inutilis (0 pulls pendant 90+ jours)
        if successful_pulls_30d == 0:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append(
                f"CRITICAL: Container Registry has no pulls for 90+ days. "
                f"Delete if no longer needed to save ${monthly_cost:.2f}/month."
            )

        # SCENARIO 2: HIGH - Premium tier sans geo-replication (overpaying)
        elif sku == "Premium" and not has_geo_replication:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority == "none" else priority
            # Savings: Downgrade to Standard
            potential_savings = 50.0 - 20.0  # $30/month
            recommendations.append(
                f"HIGH: Premium tier without geo-replication enabled. "
                f"Downgrade to Standard tier to save ${potential_savings:.2f}/month."
            )

        # SCENARIO 3: HIGH - Images obsoltes non nettoyes (>50 untagged images)
        # Untagged images consume storage and indicate poor CI/CD hygiene
        elif untagged_images_count > 50:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority == "none" else priority
            # Savings: Reduce storage costs by cleaning up
            storage_savings = (untagged_images_count / 50) * 5.0  # Estimate $5 per 50 images
            potential_savings = min(storage_savings, monthly_cost * 0.3)  # Max 30% of cost
            recommendations.append(
                f"HIGH: {untagged_images_count} untagged images detected. "
                f"Enable retention policy to auto-delete unused images and save ~${potential_savings:.2f}/month."
            )

        # SCENARIO 4: MEDIUM - Pas de retention policy configure
        # Placeholder - would require checking registry policies via API
        # For now, this is a best practice recommendation
        if not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority == "none" else priority
            recommendations.append(
                "MEDIUM: No retention policy configured. "
                "Enable automatic cleanup of old/untagged images to reduce storage costs."
            )

        # SCENARIO 5: LOW - Basic tier pour production (upgrade recommended)
        # Basic tier lacks webhooks, geo-replication, and advanced security features
        if sku == "Basic" and not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            recommendations.append(
                "LOW: Using Basic tier. Upgrade to Standard for webhooks, "
                "better performance, and production-grade features (+$15/month)."
            )

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_service_bus_topics(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Service Bus Topics for cost intelligence.

        Service Bus Topic is a pub/sub messaging service with multiple subscriptions.
        Pricing: Standard $10/mois + $0.05/million ops, Premium $677/mois (dedicated capacity)
        Typical cost: $15-100/month

        Detection criteria:
        - Topic sans abonnements actifs 30+ days (CRITICAL - 90 score)
        - Premium tier avec faible volume <1M messages/mois (HIGH - 75 score)
        - Messages morts non traits >1000 (HIGH - 70 score)
        - Pas de TTL configur (MEDIUM - 50 score)
        - Auto-delete non configur (LOW - 30 score)

        Returns:
            List of all Service Bus Topics with optimization recommendations
        """
        try:
            from azure.mgmt.servicebus import ServiceBusManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-servicebus not installed")
            return []

        resources = []
        self.logger.info(f"Scanning Service Bus Topics in region: {region}")

        try:
            # Create Service Bus client
            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )
            sb_client = ServiceBusManagementClient(credential, self.subscription_id)

            # Get all resource groups
            resource_groups = await self._get_resource_groups()

            for rg in resource_groups:
                rg_name = rg.name

                # Filter by resource group if specified
                if self.resource_groups and rg_name not in self.resource_groups:
                    continue

                try:
                    # List Service Bus namespaces in this resource group
                    namespaces = sb_client.namespaces.list_by_resource_group(rg_name)

                    for namespace in namespaces:
                        namespace_name = namespace.name
                        namespace_location = namespace.location

                        # Filter by region if specified
                        if self.regions and namespace_location not in self.regions:
                            continue

                        # Get SKU/tier
                        sku = getattr(namespace, 'sku', None)
                        tier = sku.name if sku else "Standard"

                        # List topics in this namespace
                        topics = sb_client.topics.list_by_namespace(rg_name, namespace_name)

                        for topic in topics:
                            topic_name = topic.name
                            status = getattr(topic, 'status', 'Unknown')

                            # Get topic properties
                            max_size_in_mb = getattr(topic, 'max_size_in_megabytes', 0)
                            enable_partitioning = getattr(topic, 'enable_partitioning', False)
                            enable_batched_operations = getattr(topic, 'enable_batched_operations', False)
                            default_message_time_to_live = getattr(topic, 'default_message_time_to_live', None)
                            auto_delete_on_idle = getattr(topic, 'auto_delete_on_idle', None)

                            # Get subscriptions count
                            subscriptions = sb_client.subscriptions.list_by_topic(rg_name, namespace_name, topic_name)
                            subscriptions_list = list(subscriptions)
                            subscription_count = len(subscriptions_list)

                            # Estimate usage metrics (placeholder - would need Azure Monitor)
                            monthly_operations = 0  # Placeholder
                            dead_letter_messages_count = 0  # Placeholder

                            # Calculate cost (shared with namespace, estimate per topic)
                            monthly_cost = self._estimate_service_bus_cost(tier, monthly_operations)

                            # Check optimization opportunities
                            is_optimizable, score, priority, savings, recommendations = \
                                await self._calculate_service_bus_topic_optimization(
                                    topic, tier, subscription_count, monthly_operations,
                                    dead_letter_messages_count, default_message_time_to_live,
                                    auto_delete_on_idle, monthly_cost
                                )

                            # Build metadata
                            metadata = {
                                "namespace_name": namespace_name,
                                "topic_name": topic_name,
                                "tier": tier,
                                "status": status,
                                "subscription_count": subscription_count,
                                "max_size_mb": max_size_in_mb,
                                "enable_partitioning": enable_partitioning,
                                "enable_batched_operations": enable_batched_operations,
                                "has_ttl": default_message_time_to_live is not None,
                                "has_auto_delete": auto_delete_on_idle is not None,
                                "dead_letter_messages_count": dead_letter_messages_count,
                                "resource_group": rg_name,
                            }

                            # Determine if orphan (no subscriptions for 30+ days = waste)
                            is_orphan = subscription_count == 0 and score >= 90

                            # Create resource record
                            resource = AllCloudResourceData(
                                resource_id=topic.id,
                                resource_name=f"{namespace_name}/{topic_name}",
                                resource_type="azure_service_bus_topic",
                                region=namespace_location,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=metadata,
                                is_orphan=is_orphan,
                                is_optimizable=is_optimizable and not is_orphan,
                                optimization_score=score if not is_orphan else 0,
                                optimization_priority=priority if not is_orphan else "none",
                                potential_monthly_savings=savings if not is_orphan else 0.0,
                                optimization_recommendations=recommendations if not is_orphan else []
                            )

                            resources.append(resource)
                            self.logger.info(
                                f"Found Service Bus Topic: {namespace_name}/{topic_name} "
                                f"(Tier: {tier}, Subscriptions: {subscription_count}, "
                                f"Cost: ${monthly_cost:.2f}/mo, Optimizable: {is_optimizable})"
                            )

                except Exception as e:
                    self.logger.error(
                        f"Error scanning Service Bus topics in resource group {rg_name}: {str(e)}"
                    )
                    continue

            self.logger.info(
                f"Service Bus Topic scan complete: {len(resources)} topics found"
            )

        except Exception as e:
            self.logger.error(f"Error scanning Service Bus Topics: {str(e)}")

        return resources

    async def scan_service_bus_queues(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Service Bus Queues for cost intelligence.

        Service Bus Queue is a FIFO messaging service with delivery guarantees.
        Pricing: Standard $10/mois + $0.05/million ops, Premium $677/mois
        Typical cost: $15-100/month

        Detection criteria:
        - Queue inutilise (0 messages 90+ days) (CRITICAL - 90 score)
        - Premium tier avec faible volume <1M messages/mois (HIGH - 75 score)
        - Messages morts (Dead Letter) non traits >1000 (HIGH - 70 score)
        - Duplicate detection non active (MEDIUM - 50 score)
        - Lock duration excessive >5min (LOW - 30 score)

        Returns:
            List of all Service Bus Queues with optimization recommendations
        """
        try:
            from azure.mgmt.servicebus import ServiceBusManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-servicebus not installed")
            return []

        resources = []
        self.logger.info(f"Scanning Service Bus Queues in region: {region}")

        try:
            # Create Service Bus client
            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )
            sb_client = ServiceBusManagementClient(credential, self.subscription_id)

            # Get all resource groups
            resource_groups = await self._get_resource_groups()

            for rg in resource_groups:
                rg_name = rg.name

                # Filter by resource group if specified
                if self.resource_groups and rg_name not in self.resource_groups:
                    continue

                try:
                    # List Service Bus namespaces
                    namespaces = sb_client.namespaces.list_by_resource_group(rg_name)

                    for namespace in namespaces:
                        namespace_name = namespace.name
                        namespace_location = namespace.location

                        # Filter by region if specified
                        if self.regions and namespace_location not in self.regions:
                            continue

                        # Get SKU/tier
                        sku = getattr(namespace, 'sku', None)
                        tier = sku.name if sku else "Standard"

                        # List queues in this namespace
                        queues = sb_client.queues.list_by_namespace(rg_name, namespace_name)

                        for queue in queues:
                            queue_name = queue.name
                            status = getattr(queue, 'status', 'Unknown')

                            # Get queue properties
                            max_size_in_mb = getattr(queue, 'max_size_in_megabytes', 0)
                            enable_partitioning = getattr(queue, 'enable_partitioning', False)
                            requires_duplicate_detection = getattr(queue, 'requires_duplicate_detection', False)
                            lock_duration = getattr(queue, 'lock_duration', None)
                            default_message_time_to_live = getattr(queue, 'default_message_time_to_live', None)
                            auto_delete_on_idle = getattr(queue, 'auto_delete_on_idle', None)

                            # Estimate usage metrics (placeholder - would need Azure Monitor)
                            monthly_operations = 0  # Placeholder
                            active_messages_count = 0  # Placeholder
                            dead_letter_messages_count = 0  # Placeholder

                            # Calculate cost
                            monthly_cost = self._estimate_service_bus_cost(tier, monthly_operations)

                            # Check optimization opportunities
                            is_optimizable, score, priority, savings, recommendations = \
                                await self._calculate_service_bus_queue_optimization(
                                    queue, tier, active_messages_count, monthly_operations,
                                    dead_letter_messages_count, requires_duplicate_detection,
                                    lock_duration, monthly_cost
                                )

                            # Build metadata
                            metadata = {
                                "namespace_name": namespace_name,
                                "queue_name": queue_name,
                                "tier": tier,
                                "status": status,
                                "max_size_mb": max_size_in_mb,
                                "enable_partitioning": enable_partitioning,
                                "requires_duplicate_detection": requires_duplicate_detection,
                                "lock_duration": str(lock_duration) if lock_duration else None,
                                "has_ttl": default_message_time_to_live is not None,
                                "has_auto_delete": auto_delete_on_idle is not None,
                                "active_messages_count": active_messages_count,
                                "dead_letter_messages_count": dead_letter_messages_count,
                                "resource_group": rg_name,
                            }

                            # Determine if orphan (0 active messages for 90+ days = waste)
                            is_orphan = active_messages_count == 0 and score >= 90

                            # Create resource record
                            resource = AllCloudResourceData(
                                resource_id=queue.id,
                                resource_name=f"{namespace_name}/{queue_name}",
                                resource_type="azure_service_bus_queue",
                                region=namespace_location,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=metadata,
                                is_orphan=is_orphan,
                                is_optimizable=is_optimizable and not is_orphan,
                                optimization_score=score if not is_orphan else 0,
                                optimization_priority=priority if not is_orphan else "none",
                                potential_monthly_savings=savings if not is_orphan else 0.0,
                                optimization_recommendations=recommendations if not is_orphan else []
                            )

                            resources.append(resource)
                            self.logger.info(
                                f"Found Service Bus Queue: {namespace_name}/{queue_name} "
                                f"(Tier: {tier}, Active messages: {active_messages_count}, "
                                f"Cost: ${monthly_cost:.2f}/mo, Optimizable: {is_optimizable})"
                            )

                except Exception as e:
                    self.logger.error(
                        f"Error scanning Service Bus queues in resource group {rg_name}: {str(e)}"
                    )
                    continue

            self.logger.info(
                f"Service Bus Queue scan complete: {len(resources)} queues found"
            )

        except Exception as e:
            self.logger.error(f"Error scanning Service Bus Queues: {str(e)}")

        return resources

    def _estimate_service_bus_cost(self, tier: str, monthly_operations: int) -> float:
        """
        Estimate monthly cost for Service Bus based on tier and operations.

        Pricing:
        - Basic: $0.05/million operations (queues only, no topics)
        - Standard: $10/month base + $0.05/million operations
        - Premium: $677/month (1 messaging unit) - dedicated capacity

        For simplicity, using base pricing + minimal operations assumption
        """
        if tier == "Premium":
            return 677.0
        elif tier == "Standard":
            base_cost = 10.0
            # Add operation costs ($0.05 per million)
            operation_cost = (monthly_operations / 1000000) * 0.05
            return base_cost + operation_cost
        else:
            # Basic
            operation_cost = (monthly_operations / 1000000) * 0.05
            return operation_cost

    async def _calculate_service_bus_topic_optimization(
        self,
        topic: Any,
        tier: str,
        subscription_count: int,
        monthly_operations: int,
        dead_letter_messages_count: int,
        default_message_time_to_live: Any,
        auto_delete_on_idle: Any,
        monthly_cost: float
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Service Bus Topic.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # SCENARIO 1: CRITICAL - Topic sans abonnements actifs 30+ jours
        if subscription_count == 0:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append(
                f"CRITICAL: Service Bus Topic has no active subscriptions. "
                f"Delete if no longer needed to save ${monthly_cost:.2f}/month."
            )

        # SCENARIO 2: HIGH - Premium tier avec faible volume <1M messages/mois
        elif tier == "Premium" and monthly_operations < 1000000:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority == "none" else priority
            # Savings: Downgrade to Standard
            potential_savings = 677.0 - 10.0  # $667/month
            recommendations.append(
                f"HIGH: Premium tier with low message volume ({monthly_operations/1000:.0f}K ops/month). "
                f"Downgrade to Standard tier to save ${potential_savings:.2f}/month."
            )

        # SCENARIO 3: HIGH - Messages morts non traits >1000
        elif dead_letter_messages_count > 1000:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority == "none" else priority
            recommendations.append(
                f"HIGH: {dead_letter_messages_count} dead letter messages detected. "
                f"Review and fix application errors causing message failures."
            )

        # SCENARIO 4: MEDIUM - Pas de TTL configur
        if default_message_time_to_live is None and not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority == "none" else priority
            recommendations.append(
                "MEDIUM: No message TTL (Time-To-Live) configured. "
                "Messages may accumulate indefinitely. Set TTL to prevent storage bloat."
            )

        # SCENARIO 5: LOW - Auto-delete non configur
        if auto_delete_on_idle is None and not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            recommendations.append(
                "LOW: Auto-delete on idle not configured. "
                "Enable to automatically clean up unused topics."
            )

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def _calculate_service_bus_queue_optimization(
        self,
        queue: Any,
        tier: str,
        active_messages_count: int,
        monthly_operations: int,
        dead_letter_messages_count: int,
        requires_duplicate_detection: bool,
        lock_duration: Any,
        monthly_cost: float
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Service Bus Queue.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # SCENARIO 1: CRITICAL - Queue inutilise (0 messages 90+ jours)
        if active_messages_count == 0:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append(
                f"CRITICAL: Service Bus Queue has no active messages for 90+ days. "
                f"Delete if no longer needed to save ${monthly_cost:.2f}/month."
            )

        # SCENARIO 2: HIGH - Premium tier avec faible volume
        elif tier == "Premium" and monthly_operations < 1000000:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority == "none" else priority
            potential_savings = 677.0 - 10.0
            recommendations.append(
                f"HIGH: Premium tier with low message volume ({monthly_operations/1000:.0f}K ops/month). "
                f"Downgrade to Standard tier to save ${potential_savings:.2f}/month."
            )

        # SCENARIO 3: HIGH - Messages morts non traits >1000
        elif dead_letter_messages_count > 1000:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority == "none" else priority
            recommendations.append(
                f"HIGH: {dead_letter_messages_count} dead letter messages detected. "
                f"Review and fix application errors causing message failures."
            )

        # SCENARIO 4: MEDIUM - Duplicate detection non active
        if not requires_duplicate_detection and not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority == "none" else priority
            recommendations.append(
                "MEDIUM: Duplicate detection not enabled. "
                "Enable to prevent processing duplicate messages (best practice)."
            )

        # SCENARIO 5: LOW - Lock duration excessive >5 minutes
        # Lock duration controls how long a message is locked for processing
        # Excessive lock duration can cause delays if consumer crashes
        if lock_duration and not is_optimizable:
            # lock_duration is a timedelta - extract minutes
            try:
                lock_minutes = lock_duration.total_seconds() / 60
                if lock_minutes > 5:
                    is_optimizable = True
                    optimization_score = max(optimization_score, 30)
                    priority = "low" if priority == "none" else priority
                    recommendations.append(
                        f"LOW: Lock duration is {lock_minutes:.0f} minutes (>5 min threshold). "
                        f"Reduce to improve message processing throughput."
                    )
            except:
                pass

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_event_grid_subscriptions(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Event Grid Subscriptions for cost intelligence.

        Event Grid Subscription routes events from sources to handlers (webhooks, functions, etc.).
        Pricing: $0.60 per million operations (premier million gratuit/mois)
        Typical cost: $1-20/month

        Detection criteria:
        - Subscription vers endpoint mort/inaccessible (CRITICAL - 90 score)
        - Subscription inactive (0 vnements 90+ days) (HIGH - 75 score)
        - Dead letter destination non configure (HIGH - 70 score)
        - Filtres trop larges (MEDIUM - 50 score)
        - Pas d'Advanced Filtering (LOW - 30 score)

        Returns:
            List of all Event Grid Subscriptions with optimization recommendations
        """
        try:
            from azure.mgmt.eventgrid import EventGridManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-eventgrid not installed")
            return []

        resources = []
        self.logger.info(f"Scanning Event Grid Subscriptions in region: {region}")

        try:
            # Create Event Grid client
            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )
            eventgrid_client = EventGridManagementClient(credential, self.subscription_id)

            # Get all resource groups
            resource_groups = await self._get_resource_groups()

            for rg in resource_groups:
                rg_name = rg.name

                # Filter by resource group if specified
                if self.resource_groups and rg_name not in self.resource_groups:
                    continue

                try:
                    # List Event Grid topics in this resource group
                    topics = eventgrid_client.topics.list_by_resource_group(rg_name)

                    for topic in topics:
                        topic_name = topic.name
                        topic_location = topic.location

                        # Filter by region if specified
                        if self.regions and topic_location not in self.regions:
                            continue

                        # Get subscriptions for this topic
                        try:
                            subscriptions = eventgrid_client.event_subscriptions.list_by_resource(
                                topic.id
                            )

                            for subscription in subscriptions:
                                subscription_name = subscription.name
                                provisioning_state = getattr(subscription, 'provisioning_state', 'Unknown')

                                # Get destination
                                destination = getattr(subscription, 'destination', None)
                                destination_type = type(destination).__name__ if destination else "Unknown"

                                # Get dead letter config
                                dead_letter_destination = getattr(subscription, 'dead_letter_destination', None)
                                has_dead_letter = dead_letter_destination is not None

                                # Get filter
                                filter_obj = getattr(subscription, 'filter', None)
                                has_subject_filter = False
                                has_advanced_filter = False
                                if filter_obj:
                                    has_subject_filter = getattr(filter_obj, 'subject_begins_with', None) is not None
                                    advanced_filters = getattr(filter_obj, 'advanced_filters', [])
                                    has_advanced_filter = len(advanced_filters) > 0

                                # Estimate usage (placeholder - would need Azure Monitor)
                                monthly_operations = 0  # Placeholder
                                delivery_success_rate = 100.0  # Placeholder (0-100%)

                                # Calculate cost
                                monthly_cost = self._estimate_event_grid_cost(monthly_operations)

                                # Check optimization opportunities
                                is_optimizable, score, priority, savings, recommendations = \
                                    await self._calculate_event_grid_subscription_optimization(
                                        subscription, delivery_success_rate, monthly_operations,
                                        has_dead_letter, has_subject_filter, has_advanced_filter, monthly_cost
                                    )

                                # Build metadata
                                metadata = {
                                    "topic_name": topic_name,
                                    "subscription_name": subscription_name,
                                    "destination_type": destination_type,
                                    "provisioning_state": provisioning_state,
                                    "has_dead_letter": has_dead_letter,
                                    "has_subject_filter": has_subject_filter,
                                    "has_advanced_filter": has_advanced_filter,
                                    "delivery_success_rate": delivery_success_rate,
                                    "monthly_operations": monthly_operations,
                                    "resource_group": rg_name,
                                }

                                # Determine if orphan (endpoint mort = waste)
                                is_orphan = delivery_success_rate < 10.0 and score >= 90

                                # Create resource record
                                resource = AllCloudResourceData(
                                    resource_id=subscription.id,
                                    resource_name=f"{topic_name}/{subscription_name}",
                                    resource_type="azure_event_grid_subscription",
                                    region=topic_location,
                                    estimated_monthly_cost=monthly_cost,
                                    currency="USD",
                                    resource_metadata=metadata,
                                    is_orphan=is_orphan,
                                    is_optimizable=is_optimizable and not is_orphan,
                                    optimization_score=score if not is_orphan else 0,
                                    optimization_priority=priority if not is_orphan else "none",
                                    potential_monthly_savings=savings if not is_orphan else 0.0,
                                    optimization_recommendations=recommendations if not is_orphan else []
                                )

                                resources.append(resource)
                                self.logger.info(
                                    f"Found Event Grid Subscription: {topic_name}/{subscription_name} "
                                    f"(Success rate: {delivery_success_rate:.1f}%, Dead letter: {has_dead_letter}, "
                                    f"Cost: ${monthly_cost:.2f}/mo, Optimizable: {is_optimizable})"
                                )

                        except Exception as e:
                            self.logger.error(
                                f"Error scanning subscriptions for topic {topic_name}: {str(e)}"
                            )
                            continue

                except Exception as e:
                    self.logger.error(
                        f"Error scanning Event Grid topics in resource group {rg_name}: {str(e)}"
                    )
                    continue

            self.logger.info(
                f"Event Grid Subscription scan complete: {len(resources)} subscriptions found"
            )

        except Exception as e:
            self.logger.error(f"Error scanning Event Grid Subscriptions: {str(e)}")

        return resources

    def _estimate_event_grid_cost(self, monthly_operations: int) -> float:
        """
        Estimate monthly cost for Event Grid based on operations.

        Pricing:
        - $0.60 per million operations
        - Premier million gratuit chaque mois

        Note: Trs bas cot - gnralement <$10/mois
        """
        # Premier million gratuit
        if monthly_operations <= 1000000:
            return 0.0

        # Au-del du million gratuit
        billable_operations = monthly_operations - 1000000
        cost_per_million = 0.60
        return (billable_operations / 1000000) * cost_per_million

    async def _calculate_event_grid_subscription_optimization(
        self,
        subscription: Any,
        delivery_success_rate: float,
        monthly_operations: int,
        has_dead_letter: bool,
        has_subject_filter: bool,
        has_advanced_filter: bool,
        monthly_cost: float
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Event Grid Subscription.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # SCENARIO 1: CRITICAL - Subscription vers endpoint mort/inaccessible
        # Delivery success rate <10% = endpoint probablement mort
        if delivery_success_rate < 10.0:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append(
                f"CRITICAL: Event Grid Subscription has very low delivery success rate ({delivery_success_rate:.1f}%). "
                f"Check endpoint health or delete if no longer needed to save ${monthly_cost:.2f}/month."
            )

        # SCENARIO 2: HIGH - Subscription inactive (0 vnements depuis 90+ jours)
        elif monthly_operations == 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority == "none" else priority
            potential_savings = monthly_cost
            recommendations.append(
                f"HIGH: Event Grid Subscription has no events for 90+ days. "
                f"Delete if no longer needed to save ${monthly_cost:.2f}/month."
            )

        # SCENARIO 3: HIGH - Dead letter destination non configure
        # Sans dead letter, vnements en chec sont perdus
        elif not has_dead_letter:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority == "none" else priority
            recommendations.append(
                "HIGH: No dead letter destination configured. "
                "Events that fail delivery are lost. Configure dead lettering for reliability."
            )

        # SCENARIO 4: MEDIUM - Filtres trop larges (traite tous vnements)
        # Pas de filtres = traite tous vnements = gaspillage potentiel
        if not has_subject_filter and not has_advanced_filter and not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority == "none" else priority
            recommendations.append(
                "MEDIUM: No event filters configured. "
                "Subscription processes all events. Add filters to reduce unnecessary processing."
            )

        # SCENARIO 5: LOW - Pas d'Advanced Filtering (best practice)
        # Advanced filters permettent filtrage plus granulaire
        if not has_advanced_filter and not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            recommendations.append(
                "LOW: No advanced filters configured. "
                "Use advanced filtering for better event routing optimization."
            )

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_key_vault_secrets(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Key Vault Secrets for cost intelligence.

        Key Vault stores secrets, keys, and certificates securely.
        Pricing: $0.03 per 10K operations + minimal storage
        Typical cost: $5-50/month

        Detection criteria:
        - Secrets expirs ou non accessibles 90+ days (CRITICAL - 90 score)
        - Pas de date d'expiration (HIGH - 75 score)
        - Secrets non rotationns 365+ days (HIGH - 70 score)
        - Soft-delete non activ (MEDIUM - 50 score)
        - Pas de monitoring/alerting (LOW - 30 score)

        Returns:
            List of all Key Vault Secrets with optimization recommendations
        """
        try:
            from azure.mgmt.keyvault import KeyVaultManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-keyvault not installed")
            return []

        resources = []
        self.logger.info(f"Scanning Key Vault Secrets in region: {region}")

        try:
            # Create Key Vault client
            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )
            kv_client = KeyVaultManagementClient(credential, self.subscription_id)

            # Get all resource groups
            resource_groups = await self._get_resource_groups()

            for rg in resource_groups:
                rg_name = rg.name

                # Filter by resource group if specified
                if self.resource_groups and rg_name not in self.resource_groups:
                    continue

                try:
                    # List Key Vaults in this resource group
                    vaults = kv_client.vaults.list_by_resource_group(rg_name)

                    for vault in vaults:
                        vault_name = vault.name
                        vault_location = vault.location

                        # Filter by region if specified
                        if self.regions and vault_location not in self.regions:
                            continue

                        # Get vault properties
                        sku = getattr(vault.properties, 'sku', None) if hasattr(vault, 'properties') else None
                        sku_name = sku.name if sku else "Standard"
                        soft_delete_enabled = getattr(vault.properties, 'enable_soft_delete', False) if hasattr(vault, 'properties') else False

                        # Estimate metrics (placeholder - would need Azure Monitor / Key Vault SDK)
                        monthly_operations = 1000  # Placeholder
                        secrets_count = 5  # Placeholder
                        secrets_without_expiration = 0  # Placeholder
                        secrets_last_access_90d_ago = 0  # Placeholder
                        secrets_not_rotated_365d = 0  # Placeholder

                        # Calculate cost
                        monthly_cost = self._estimate_key_vault_cost(monthly_operations, secrets_count)

                        # Check optimization opportunities
                        is_optimizable, score, priority, savings, recommendations = \
                            await self._calculate_key_vault_secret_optimization(
                                vault, soft_delete_enabled, secrets_without_expiration,
                                secrets_last_access_90d_ago, secrets_not_rotated_365d, monthly_cost
                            )

                        # Build metadata
                        metadata = {
                            "vault_name": vault_name,
                            "sku": sku_name,
                            "soft_delete_enabled": soft_delete_enabled,
                            "secrets_count": secrets_count,
                            "secrets_without_expiration": secrets_without_expiration,
                            "secrets_last_access_90d_ago": secrets_last_access_90d_ago,
                            "secrets_not_rotated_365d": secrets_not_rotated_365d,
                            "monthly_operations": monthly_operations,
                            "resource_group": rg_name,
                        }

                        # Determine if orphan (secrets non accds = waste)
                        is_orphan = secrets_last_access_90d_ago > 0 and score >= 90

                        # Create resource record
                        resource = AllCloudResourceData(
                            resource_id=vault.id,
                            resource_name=vault_name,
                            resource_type="azure_key_vault_secret",
                            region=vault_location,
                            estimated_monthly_cost=monthly_cost,
                            currency="USD",
                            resource_metadata=metadata,
                            is_orphan=is_orphan,
                            is_optimizable=is_optimizable and not is_orphan,
                            optimization_score=score if not is_orphan else 0,
                            optimization_priority=priority if not is_orphan else "none",
                            potential_monthly_savings=savings if not is_orphan else 0.0,
                            optimization_recommendations=recommendations if not is_orphan else []
                        )

                        resources.append(resource)
                        self.logger.info(
                            f"Found Key Vault: {vault_name} "
                            f"(SKU: {sku_name}, Soft-delete: {soft_delete_enabled}, "
                            f"Secrets: {secrets_count}, Cost: ${monthly_cost:.2f}/mo, Optimizable: {is_optimizable})"
                        )

                except Exception as e:
                    self.logger.error(
                        f"Error scanning Key Vaults in resource group {rg_name}: {str(e)}"
                    )
                    continue

            self.logger.info(
                f"Key Vault Secret scan complete: {len(resources)} vaults found"
            )

        except Exception as e:
            self.logger.error(f"Error scanning Key Vault Secrets: {str(e)}")

        return resources

    def _estimate_key_vault_cost(self, monthly_operations: int, secrets_count: int) -> float:
        """
        Estimate monthly cost for Key Vault based on operations.

        Pricing:
        - $0.03 per 10,000 operations
        - Secrets: Free storage, paid per access
        - HSM-backed secrets (Premium): $1/secret/month

        For simplicity, using operation-based pricing
        """
        cost_per_10k_ops = 0.03
        return (monthly_operations / 10000) * cost_per_10k_ops

    async def _calculate_key_vault_secret_optimization(
        self,
        vault: Any,
        soft_delete_enabled: bool,
        secrets_without_expiration: int,
        secrets_last_access_90d_ago: int,
        secrets_not_rotated_365d: int,
        monthly_cost: float
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Key Vault Secret.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # SCENARIO 1: CRITICAL - Secrets expirs ou non accessibles 90+ days
        if secrets_last_access_90d_ago > 0:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost * 0.5  # Assume 50% cost reduction
            recommendations.append(
                f"CRITICAL: {secrets_last_access_90d_ago} secrets not accessed for 90+ days. "
                f"Review and delete unused secrets to save ~${potential_savings:.2f}/month."
            )

        # SCENARIO 2: HIGH - Pas de date d'expiration configure (risque scurit)
        elif secrets_without_expiration > 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority == "none" else priority
            recommendations.append(
                f"HIGH: {secrets_without_expiration} secrets without expiration date. "
                f"Set expiration dates for automatic rotation and improved security."
            )

        # SCENARIO 3: HIGH - Secrets non rotationns depuis 365+ jours
        elif secrets_not_rotated_365d > 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority == "none" else priority
            recommendations.append(
                f"HIGH: {secrets_not_rotated_365d} secrets not rotated for 365+ days. "
                f"Rotate secrets regularly (recommended: every 90 days) for security."
            )

        # SCENARIO 4: MEDIUM - Soft-delete non activ (risque de perte)
        if not soft_delete_enabled and not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority == "none" else priority
            recommendations.append(
                "MEDIUM: Soft-delete not enabled. "
                "Enable soft-delete to protect against accidental secret deletion."
            )

        # SCENARIO 5: LOW - Pas de monitoring/alerting configur
        # Placeholder - would require checking diagnostic settings
        if not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            recommendations.append(
                "LOW: Configure monitoring and alerting for Key Vault access. "
                "Enable diagnostic logs to track secret access and detect anomalies."
            )

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_app_configurations(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure App Configuration stores for cost intelligence.

        Pricing (Azure App Configuration):
        - Free tier: $0/mois (1 store, 1000 requests/jour max)
        - Standard tier: $1.20/jour (~$36/mois) + data transfer

        Typical cost: $0-50/month

        Returns:
            List of AllCloudResourceData (is_orphan=False, is_optimizable=True)
        """
        try:
            from azure.mgmt.appconfiguration import AppConfigurationManagementClient
        except ImportError:
            logger.warning("azure-mgmt-appconfiguration not installed, skipping App Configuration scan")
            return []

        resources = []

        try:
            credential = self._get_azure_credential()
            app_config_client = AppConfigurationManagementClient(credential, self.subscription_id)

            # Iterate over resource groups
            for rg_name in self.resource_groups:
                try:
                    # List App Configuration stores in resource group
                    stores = app_config_client.configuration_stores.list_by_resource_group(rg_name)

                    for store in stores:
                        try:
                            # Extract location from store.location
                            store_location = getattr(store, "location", "unknown").lower()

                            # Filter by region if not global
                            if region.lower() != "global" and store_location != region.lower():
                                continue

                            # Extract metadata
                            store_id = store.id
                            store_name = store.name
                            sku_name = getattr(store.sku, "name", "Free").lower()  # "free" or "standard"
                            creation_date = getattr(store, "creation_date", None)
                            public_network_access = getattr(store, "public_network_access", "Enabled")

                            # Point-in-Time Recovery (only available in Standard tier)
                            soft_delete_retention_days = getattr(store, "soft_delete_retention_in_days", 0)
                            has_soft_delete = soft_delete_retention_days > 0

                            # Tags
                            tags = getattr(store, "tags", {}) or {}

                            # Calculate age
                            age_days = 0
                            if creation_date:
                                age_days = (datetime.now(timezone.utc) - creation_date).days

                            # Estimate daily requests (we don't have actual metrics, estimate based on tier)
                            estimated_daily_requests = 0
                            if sku_name == "standard":
                                # Assume Standard tier = high usage (otherwise why pay?)
                                estimated_daily_requests = 50000  # Estimate
                            else:
                                estimated_daily_requests = 500  # Free tier usage

                            # Count configuration keys (requires Azure Resource Graph or direct API call)
                            # For simplicity, we'll estimate based on tier
                            estimated_config_keys = 10 if sku_name == "free" else 50

                            # Estimate monthly cost
                            monthly_cost = self._estimate_app_configuration_cost(
                                sku_name=sku_name,
                                estimated_daily_requests=estimated_daily_requests
                            )

                            # Check optimization opportunities
                            is_optimizable, optimization_score, priority, potential_savings, recommendations = (
                                self._calculate_app_configuration_optimization(
                                    sku_name=sku_name,
                                    age_days=age_days,
                                    estimated_daily_requests=estimated_daily_requests,
                                    has_soft_delete=has_soft_delete,
                                    estimated_config_keys=estimated_config_keys,
                                    monthly_cost=monthly_cost,
                                )
                            )

                            # Build metadata
                            metadata = {
                                "store_id": store_id,
                                "store_name": store_name,
                                "sku": sku_name,
                                "location": store_location,
                                "resource_group": rg_name,
                                "age_days": age_days,
                                "estimated_daily_requests": estimated_daily_requests,
                                "estimated_config_keys": estimated_config_keys,
                                "has_soft_delete": has_soft_delete,
                                "soft_delete_retention_days": soft_delete_retention_days,
                                "public_network_access": public_network_access,
                                "tags": tags,
                            }

                            resource = AllCloudResourceData(
                                resource_id=store_id,
                                resource_type="azure_app_configuration",
                                resource_name=store_name,
                                region=store_location,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=metadata,
                                created_at_cloud=creation_date,
                                is_orphan=False,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                            )

                            resources.append(resource)
                            logger.info(
                                f"Found App Configuration store: {store_name} in {store_location} "
                                f"(SKU: {sku_name}, Optimizable: {is_optimizable}, Score: {optimization_score})"
                            )

                        except Exception as e:
                            logger.error(f"Error processing App Configuration store {getattr(store, 'name', 'unknown')}: {e}")
                            continue

                except Exception as e:
                    logger.error(f"Error listing App Configuration stores in resource group {rg_name}: {e}")
                    continue

        except Exception as e:
            logger.error(f"Error scanning Azure App Configuration stores: {e}")

        logger.info(f"Completed Azure App Configuration scan in region {region}: {len(resources)} stores found")
        return resources

    def _estimate_app_configuration_cost(
        self,
        sku_name: str,
        estimated_daily_requests: int,
    ) -> float:
        """
        Estimate monthly cost for Azure App Configuration.

        Pricing:
        - Free tier: $0 (1 store, 1000 requests/day max)
        - Standard tier: $1.20/day (~$36/month) + data transfer
        """
        if sku_name == "free":
            return 0.0

        # Standard tier
        base_cost_per_day = 1.20
        monthly_cost = base_cost_per_day * 30  # ~$36/month

        # Add data transfer cost (estimate $0.01/GB)
        # Assume 1KB per request, so 1M requests = 1GB
        monthly_requests = estimated_daily_requests * 30
        data_transfer_gb = monthly_requests / 1000000
        data_transfer_cost = data_transfer_gb * 0.01

        return round(monthly_cost + data_transfer_cost, 2)

    def _calculate_app_configuration_optimization(
        self,
        sku_name: str,
        age_days: int,
        estimated_daily_requests: int,
        has_soft_delete: bool,
        estimated_config_keys: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Azure App Configuration.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Standard tier with 0 requests for 30+ days
        if sku_name == "standard" and estimated_daily_requests == 0 and age_days >= 30:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost  # Full cost savings by downgrading
            recommendations.append(
                "Standard tier App Configuration store has 0 requests for 30+ days. "
                "Downgrade to Free tier or delete if unused. "
                f"Potential savings: ${potential_savings:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Standard tier with very low usage (<1K requests/day)
        if sku_name == "standard" and estimated_daily_requests < 1000:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost * 0.9  # 90% savings by downgrading
            recommendations.append(
                f"Standard tier App Configuration store has very low usage ({estimated_daily_requests} requests/day). "
                "Free tier supports 1000 requests/day. "
                f"Downgrade to Free tier to save ${potential_savings:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Point-in-Time Recovery not used (Standard feature waste)
        if sku_name == "standard" and not has_soft_delete:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = monthly_cost * 0.3  # Partial waste
            recommendations.append(
                "Standard tier App Configuration store does not have Point-in-Time Recovery enabled. "
                "Enable soft delete to leverage Standard tier features, or downgrade to Free tier. "
                f"Potential savings: ${potential_savings:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Configuration keys not used for 90+ days
        if age_days >= 90 and estimated_config_keys > 0 and estimated_daily_requests < 100:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            potential_savings = monthly_cost * 0.5 if sku_name == "standard" else 0.0
            recommendations.append(
                f"App Configuration store has {estimated_config_keys} configuration keys but very low usage "
                f"({estimated_daily_requests} requests/day for {age_days} days). "
                "Review and remove unused configuration keys, or delete the store if not needed."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - No feature flags used (underutilizing capabilities)
        if sku_name == "standard" and estimated_config_keys > 0:
            # This is a best practice recommendation, not a cost issue
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = 0.0
            recommendations.append(
                "App Configuration store is not leveraging feature flags. "
                "Consider using feature management capabilities to control feature rollouts dynamically."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_api_managements(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure API Management services for cost intelligence.

        Pricing (Azure API Management):
        - Consumption: $0.035 per 10K calls + $3.50 per GB
        - Developer: $49.79/mois (1M calls inclus)
        - Basic: $147.24/mois (pas de SLA)
        - Standard: $735.48/mois + scale units
        - Premium: $2943.04/mois + scale units + multi-region

        Typical cost: $50-500/month

        Returns:
            List of AllCloudResourceData (is_orphan=False, is_optimizable=True)
        """
        try:
            from azure.mgmt.apimanagement import ApiManagementClient
        except ImportError:
            logger.warning("azure-mgmt-apimanagement not installed, skipping API Management scan")
            return []

        resources = []

        try:
            credential = self._get_azure_credential()
            apim_client = ApiManagementClient(credential, self.subscription_id)

            # Iterate over resource groups
            for rg_name in self.resource_groups:
                try:
                    # List API Management services in resource group
                    services = apim_client.api_management_service.list_by_resource_group(rg_name)

                    for service in services:
                        try:
                            # Extract location
                            service_location = getattr(service, "location", "unknown").lower()

                            # Filter by region if not global
                            if region.lower() != "global" and service_location != region.lower():
                                continue

                            # Extract metadata
                            service_id = service.id
                            service_name = service.name
                            sku_name = getattr(service.sku, "name", "Unknown").lower()  # consumption, developer, basic, standard, premium
                            sku_capacity = getattr(service.sku, "capacity", 1)
                            creation_time = getattr(service, "created_at_utc", None)
                            provisioning_state = getattr(service, "provisioning_state", "Unknown")

                            # Gateway URL
                            gateway_url = getattr(service, "gateway_url", None)

                            # Public IP addresses (for Premium multi-region)
                            public_ip_addresses = getattr(service, "public_ip_addresses", [])

                            # Tags
                            tags = getattr(service, "tags", {}) or {}

                            # Calculate age
                            age_days = 0
                            if creation_time:
                                age_days = (datetime.now(timezone.utc) - creation_time).days

                            # Count APIs and revisions (requires API call)
                            # For simplicity, we'll estimate based on tier
                            estimated_apis = 0
                            estimated_revisions = 0
                            try:
                                apis = apim_client.api.list_by_service(rg_name, service_name)
                                api_list = list(apis)
                                estimated_apis = len(api_list)

                                # Count total revisions across all APIs
                                for api in api_list:
                                    try:
                                        revisions = apim_client.api_revision.list_by_service(
                                            rg_name, service_name, api.name
                                        )
                                        estimated_revisions += len(list(revisions))
                                    except Exception:
                                        pass
                            except Exception as e:
                                logger.debug(f"Error counting APIs for {service_name}: {e}")
                                # Fallback estimate
                                estimated_apis = 5 if sku_name in ["standard", "premium"] else 2

                            # Estimate daily requests (no direct metrics available, estimate based on tier)
                            estimated_daily_requests = 0
                            if sku_name == "consumption":
                                estimated_daily_requests = 10000  # 10K
                            elif sku_name == "developer":
                                estimated_daily_requests = 50000  # 50K
                            elif sku_name == "basic":
                                estimated_daily_requests = 100000  # 100K
                            elif sku_name == "standard":
                                estimated_daily_requests = 500000  # 500K
                            elif sku_name == "premium":
                                estimated_daily_requests = 2000000  # 2M

                            # Estimate monthly cost
                            monthly_cost = self._estimate_api_management_cost(
                                sku_name=sku_name,
                                sku_capacity=sku_capacity,
                                estimated_daily_requests=estimated_daily_requests
                            )

                            # Check optimization opportunities
                            is_optimizable, optimization_score, priority, potential_savings, recommendations = (
                                self._calculate_api_management_optimization(
                                    sku_name=sku_name,
                                    sku_capacity=sku_capacity,
                                    age_days=age_days,
                                    estimated_daily_requests=estimated_daily_requests,
                                    estimated_apis=estimated_apis,
                                    estimated_revisions=estimated_revisions,
                                    monthly_cost=monthly_cost,
                                )
                            )

                            # Build metadata
                            metadata = {
                                "service_id": service_id,
                                "service_name": service_name,
                                "sku": sku_name,
                                "sku_capacity": sku_capacity,
                                "location": service_location,
                                "resource_group": rg_name,
                                "age_days": age_days,
                                "provisioning_state": provisioning_state,
                                "gateway_url": gateway_url,
                                "estimated_daily_requests": estimated_daily_requests,
                                "estimated_apis": estimated_apis,
                                "estimated_revisions": estimated_revisions,
                                "public_ip_addresses": public_ip_addresses,
                                "tags": tags,
                            }

                            resource = AllCloudResourceData(
                                resource_id=service_id,
                                resource_type="azure_api_management",
                                resource_name=service_name,
                                region=service_location,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=metadata,
                                created_at_cloud=creation_time,
                                is_orphan=False,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                            )

                            resources.append(resource)
                            logger.info(
                                f"Found API Management service: {service_name} in {service_location} "
                                f"(SKU: {sku_name}, Optimizable: {is_optimizable}, Score: {optimization_score})"
                            )

                        except Exception as e:
                            logger.error(f"Error processing API Management service {getattr(service, 'name', 'unknown')}: {e}")
                            continue

                except Exception as e:
                    logger.error(f"Error listing API Management services in resource group {rg_name}: {e}")
                    continue

        except Exception as e:
            logger.error(f"Error scanning Azure API Management services: {e}")

        logger.info(f"Completed Azure API Management scan in region {region}: {len(resources)} services found")
        return resources

    def _estimate_api_management_cost(
        self,
        sku_name: str,
        sku_capacity: int,
        estimated_daily_requests: int,
    ) -> float:
        """
        Estimate monthly cost for Azure API Management.

        Pricing:
        - Consumption: $0.035 per 10K calls + $3.50 per GB
        - Developer: $49.79/mois (1M calls inclus)
        - Basic: $147.24/mois (pas de SLA)
        - Standard: $735.48/mois + scale units
        - Premium: $2943.04/mois + scale units + multi-region
        """
        if sku_name == "consumption":
            # Consumption pricing: $0.035 per 10K calls
            monthly_calls = estimated_daily_requests * 30
            cost = (monthly_calls / 10000) * 0.035
            # Add data transfer estimate ($3.50/GB, assume 1KB per call)
            data_gb = (monthly_calls / 1000000)  # 1KB per call
            cost += data_gb * 3.50
            return round(cost, 2)

        elif sku_name == "developer":
            return 49.79  # Fixed cost

        elif sku_name == "basic":
            return 147.24 * sku_capacity  # Per unit

        elif sku_name == "standard":
            return 735.48 * sku_capacity  # Per unit

        elif sku_name == "premium":
            return 2943.04 * sku_capacity  # Per unit

        else:
            return 50.0  # Fallback estimate

    def _calculate_api_management_optimization(
        self,
        sku_name: str,
        sku_capacity: int,
        age_days: int,
        estimated_daily_requests: int,
        estimated_apis: int,
        estimated_revisions: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Azure API Management.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Service with 0 requests for 90+ days
        if estimated_daily_requests == 0 and age_days >= 90:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost  # Full cost
            recommendations.append(
                "API Management service has 0 requests for 90+ days. "
                "Delete the service or downgrade to Consumption tier if keeping for testing. "
                f"Potential savings: ${potential_savings:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Premium tier with low usage (<100K requests/day)
        if sku_name == "premium" and estimated_daily_requests < 100000:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            # Premium costs $2943/month, Standard costs $735/month
            potential_savings = monthly_cost - (735.48 * sku_capacity)
            recommendations.append(
                f"Premium tier API Management service has low usage ({estimated_daily_requests} requests/day). "
                f"Downgrade to Standard tier to save ${potential_savings:.2f}/month. "
                "Premium features (multi-region, VNet) may not be needed."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Too many API revisions (>10 revisions per API)
        if estimated_apis > 0:
            avg_revisions_per_api = estimated_revisions / estimated_apis
            if avg_revisions_per_api > 10:
                is_optimizable = True
                optimization_score = 70
                priority = "high"
                potential_savings = 0.0  # No direct cost savings, but maintenance waste
                recommendations.append(
                    f"API Management service has {estimated_revisions} revisions for {estimated_apis} APIs "
                    f"(avg {avg_revisions_per_api:.1f} revisions/API). "
                    "Clean up old/unused revisions to improve performance and reduce complexity."
                )
                return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Standard/Premium tier with very low usage (<10K requests/day)
        if sku_name in ["standard", "premium"] and estimated_daily_requests < 10000:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Downgrade to Consumption tier
            consumption_cost = (estimated_daily_requests * 30 / 10000) * 0.035
            potential_savings = monthly_cost - consumption_cost
            recommendations.append(
                f"{sku_name.title()} tier API Management service has very low usage ({estimated_daily_requests} requests/day). "
                f"Downgrade to Consumption tier to save ${potential_savings:.2f}/month. "
                "Consumption tier is ideal for dev/test and low-volume APIs."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - No monitoring/alerting configured (best practice)
        # Placeholder - would require checking diagnostic settings
        if not is_optimizable:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = 0.0
            recommendations.append(
                "LOW: Configure monitoring and alerting for API Management service. "
                "Enable Application Insights integration and set up alerts for high latency, errors, and throttling."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_logic_apps(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Logic Apps for cost intelligence.

        Pricing (Azure Logic Apps):
        - Consumption: $0.000025 per action + $0.000125 per trigger
        - Standard: Starts at ~$200/mois (App Service Plan)

        Typical cost: $5-100/month

        Returns:
            List of AllCloudResourceData (is_orphan=False, is_optimizable=True)
        """
        try:
            from azure.mgmt.logic import LogicManagementClient
        except ImportError:
            logger.warning("azure-mgmt-logic not installed, skipping Logic Apps scan")
            return []

        resources = []

        try:
            credential = self._get_azure_credential()
            logic_client = LogicManagementClient(credential, self.subscription_id)

            # Iterate over resource groups
            for rg_name in self.resource_groups:
                try:
                    # List Logic Apps in resource group
                    workflows = logic_client.workflows.list_by_resource_group(rg_name)

                    for workflow in workflows:
                        try:
                            # Extract location
                            workflow_location = getattr(workflow, "location", "unknown").lower()

                            # Filter by region if not global
                            if region.lower() != "global" and workflow_location != region.lower():
                                continue

                            # Extract metadata
                            workflow_id = workflow.id
                            workflow_name = workflow.name
                            state = getattr(workflow, "state", "Unknown")  # Enabled, Disabled, etc.
                            sku_name = getattr(getattr(workflow, "sku", None), "name", "NotSpecified")  # NotSpecified (Consumption), Standard, etc.
                            created_time = getattr(workflow, "created_time", None)
                            changed_time = getattr(workflow, "changed_time", None)

                            # Workflow definition
                            definition = getattr(workflow, "definition", {})

                            # Tags
                            tags = getattr(workflow, "tags", {}) or {}

                            # Calculate age
                            age_days = 0
                            if created_time:
                                age_days = (datetime.now(timezone.utc) - created_time).days

                            # Get workflow run history (to detect activity)
                            runs_count_30d = 0
                            failed_runs_count = 0
                            try:
                                # Get runs from last 30 days
                                thirty_days_ago = datetime.now(timezone.utc) - timedelta(days=30)
                                runs = logic_client.workflow_runs.list(rg_name, workflow_name)

                                for run in runs:
                                    run_start_time = getattr(run, "start_time", None)
                                    run_status = getattr(run, "status", "Unknown")

                                    if run_start_time and run_start_time >= thirty_days_ago:
                                        runs_count_30d += 1
                                        if run_status in ["Failed", "Cancelled", "TimedOut"]:
                                            failed_runs_count += 1

                                    # Limit iteration for performance
                                    if runs_count_30d >= 1000:
                                        break
                            except Exception as e:
                                logger.debug(f"Error fetching runs for workflow {workflow_name}: {e}")
                                # Fallback estimate
                                if state == "Enabled":
                                    runs_count_30d = 100  # Estimate
                                else:
                                    runs_count_30d = 0

                            # Count actions in workflow definition
                            actions_count = 0
                            if isinstance(definition, dict):
                                actions = definition.get("actions", {})
                                actions_count = len(actions) if isinstance(actions, dict) else 0

                            # Estimate monthly executions
                            estimated_monthly_executions = int(runs_count_30d)

                            # Estimate monthly cost
                            monthly_cost = self._estimate_logic_app_cost(
                                sku_name=sku_name,
                                estimated_monthly_executions=estimated_monthly_executions,
                                actions_count=actions_count
                            )

                            # Check optimization opportunities
                            is_optimizable, optimization_score, priority, potential_savings, recommendations = (
                                self._calculate_logic_app_optimization(
                                    state=state,
                                    sku_name=sku_name,
                                    age_days=age_days,
                                    runs_count_30d=runs_count_30d,
                                    failed_runs_count=failed_runs_count,
                                    actions_count=actions_count,
                                    monthly_cost=monthly_cost,
                                )
                            )

                            # Build metadata
                            metadata = {
                                "workflow_id": workflow_id,
                                "workflow_name": workflow_name,
                                "state": state,
                                "sku": sku_name,
                                "location": workflow_location,
                                "resource_group": rg_name,
                                "age_days": age_days,
                                "runs_count_30d": runs_count_30d,
                                "failed_runs_count": failed_runs_count,
                                "actions_count": actions_count,
                                "estimated_monthly_executions": estimated_monthly_executions,
                                "created_time": created_time.isoformat() if created_time else None,
                                "changed_time": changed_time.isoformat() if changed_time else None,
                                "tags": tags,
                            }

                            resource = AllCloudResourceData(
                                resource_id=workflow_id,
                                resource_type="azure_logic_app",
                                resource_name=workflow_name,
                                region=workflow_location,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=metadata,
                                created_at_cloud=created_time,
                                is_orphan=False,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                            )

                            resources.append(resource)
                            logger.info(
                                f"Found Logic App: {workflow_name} in {workflow_location} "
                                f"(State: {state}, Optimizable: {is_optimizable}, Score: {optimization_score})"
                            )

                        except Exception as e:
                            logger.error(f"Error processing Logic App {getattr(workflow, 'name', 'unknown')}: {e}")
                            continue

                except Exception as e:
                    logger.error(f"Error listing Logic Apps in resource group {rg_name}: {e}")
                    continue

        except Exception as e:
            logger.error(f"Error scanning Azure Logic Apps: {e}")

        logger.info(f"Completed Azure Logic Apps scan in region {region}: {len(resources)} workflows found")
        return resources

    def _estimate_logic_app_cost(
        self,
        sku_name: str,
        estimated_monthly_executions: int,
        actions_count: int,
    ) -> float:
        """
        Estimate monthly cost for Azure Logic Apps.

        Pricing:
        - Consumption: $0.000025 per action execution
        - Standard: ~$200/month (App Service Plan based)
        """
        if sku_name.lower() in ["notspecified", "consumption"]:
            # Consumption pricing
            # Assume average of 5 actions per execution
            total_actions = estimated_monthly_executions * max(actions_count, 5)
            cost = total_actions * 0.000025
            return round(cost, 2)
        elif sku_name.lower() == "standard":
            # Standard tier (App Service Plan)
            return 200.0  # Base estimate
        else:
            # Fallback
            return 10.0

    def _calculate_logic_app_optimization(
        self,
        state: str,
        sku_name: str,
        age_days: int,
        runs_count_30d: int,
        failed_runs_count: int,
        actions_count: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Azure Logic Apps.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Disabled workflow for 90+ days
        if state.lower() == "disabled" and age_days >= 90:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost  # Full savings if deleted
            recommendations.append(
                f"Logic App workflow is disabled for {age_days} days. "
                "Delete the workflow if no longer needed to eliminate any residual costs. "
                f"Potential savings: ${potential_savings:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Standard tier with low executions (<100 runs/month)
        if sku_name.lower() == "standard" and runs_count_30d < 100:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            # Consumption would cost: 100 runs * 5 actions * $0.000025 = $0.0125
            consumption_cost = (runs_count_30d * max(actions_count, 5)) * 0.000025
            potential_savings = monthly_cost - consumption_cost
            recommendations.append(
                f"Standard tier Logic App has very low usage ({runs_count_30d} runs/month). "
                f"Downgrade to Consumption tier to save ${potential_savings:.2f}/month. "
                "Standard tier is only cost-effective for high-volume workflows."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - High failure rate (>50% failed runs)
        if runs_count_30d > 0:
            failure_rate = (failed_runs_count / runs_count_30d) * 100
            if failure_rate > 50:
                is_optimizable = True
                optimization_score = 70
                priority = "high"
                potential_savings = monthly_cost * 0.5  # Waste due to failed runs
                recommendations.append(
                    f"Logic App has high failure rate ({failure_rate:.1f}% of {runs_count_30d} runs failed). "
                    "Review and fix workflow errors to reduce wasted executions. "
                    f"Potential savings: ${potential_savings:.2f}/month."
                )
                return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - No executions in 30 days (but enabled)
        if state.lower() == "enabled" and runs_count_30d == 0 and age_days >= 30:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            potential_savings = monthly_cost * 0.8  # Partial savings
            recommendations.append(
                "Logic App workflow is enabled but has 0 executions in 30 days. "
                "Disable or delete if not needed, or verify trigger configuration. "
                f"Potential savings: ${potential_savings:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - No error handling configured
        if actions_count > 0:
            # Best practice recommendation (no direct cost savings)
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = 0.0
            recommendations.append(
                f"Logic App has {actions_count} actions but no explicit error handling. "
                "Add retry policies and error scopes to improve reliability and reduce failed runs."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_data_factories(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Data Factory instances for cost intelligence.

        Pricing (Azure Data Factory):
        - Pipeline orchestration: $0.001 per activity run
        - Data movement (Copy): $0.25 per DIU-hour
        - Integration Runtime (IR): $0.274/hour (Azure IR), $0.10-0.84/hour (Self-hosted IR)

        Typical cost: $10-200/month

        Returns:
            List of AllCloudResourceData (is_orphan=False, is_optimizable=True)
        """
        try:
            from azure.mgmt.datafactory import DataFactoryManagementClient
        except ImportError:
            logger.warning("azure-mgmt-datafactory not installed, skipping Data Factory scan")
            return []

        resources = []

        try:
            credential = self._get_azure_credential()
            df_client = DataFactoryManagementClient(credential, self.subscription_id)

            # Iterate over resource groups
            for rg_name in self.resource_groups:
                try:
                    # List Data Factories in resource group
                    factories = df_client.factories.list_by_resource_group(rg_name)

                    for factory in factories:
                        try:
                            # Extract location
                            factory_location = getattr(factory, "location", "unknown").lower()

                            # Filter by region if not global
                            if region.lower() != "global" and factory_location != region.lower():
                                continue

                            # Extract metadata
                            factory_id = getattr(factory, "id", "")
                            factory_name = getattr(factory, "name", "unknown")
                            provisioning_state = getattr(factory, "provisioning_state", "Unknown")
                            version = getattr(factory, "version", "V2")
                            created_time = getattr(factory, "create_time", None)

                            # Tags
                            tags = getattr(factory, "tags", {}) or {}

                            # Calculate age
                            age_days = 0
                            if created_time:
                                age_days = (datetime.now(timezone.utc) - created_time).days

                            # Count pipelines
                            pipelines_count = 0
                            active_pipelines_count = 0
                            try:
                                pipelines = df_client.pipelines.list_by_factory(rg_name, factory_name)
                                pipelines_list = list(pipelines)
                                pipelines_count = len(pipelines_list)

                                # Check which pipelines are actively used (heuristic)
                                for pipeline in pipelines_list[:20]:  # Limit to first 20 for performance
                                    try:
                                        # Try to get recent runs
                                        runs = df_client.pipeline_runs.query_by_factory(
                                            rg_name,
                                            factory_name,
                                            {
                                                "lastUpdatedAfter": (datetime.now(timezone.utc) - timedelta(days=30)).isoformat(),
                                                "lastUpdatedBefore": datetime.now(timezone.utc).isoformat(),
                                                "filters": [
                                                    {
                                                        "operand": "PipelineName",
                                                        "operator": "Equals",
                                                        "values": [pipeline.name]
                                                    }
                                                ]
                                            }
                                        )
                                        if runs.value and len(runs.value) > 0:
                                            active_pipelines_count += 1
                                    except Exception:
                                        pass
                            except Exception as e:
                                logger.debug(f"Error counting pipelines for {factory_name}: {e}")
                                # Fallback estimate
                                pipelines_count = 10

                            # Count Integration Runtimes
                            ir_count = 0
                            self_hosted_ir_count = 0
                            try:
                                irs = df_client.integration_runtimes.list_by_factory(rg_name, factory_name)
                                for ir in irs:
                                    ir_count += 1
                                    ir_type = getattr(getattr(ir, "properties", None), "type", "Unknown")
                                    if ir_type == "SelfHosted":
                                        self_hosted_ir_count += 1
                            except Exception:
                                ir_count = 1  # Fallback

                            # Estimate activity runs per month (no direct metric)
                            estimated_monthly_runs = active_pipelines_count * 30  # Estimate 1 run/day per active pipeline

                            # Estimate monthly cost
                            monthly_cost = self._estimate_data_factory_cost(
                                estimated_monthly_runs=estimated_monthly_runs,
                                ir_count=ir_count,
                                self_hosted_ir_count=self_hosted_ir_count
                            )

                            # Check optimization opportunities
                            is_optimizable, optimization_score, priority, potential_savings, recommendations = (
                                self._calculate_data_factory_optimization(
                                    age_days=age_days,
                                    pipelines_count=pipelines_count,
                                    active_pipelines_count=active_pipelines_count,
                                    ir_count=ir_count,
                                    self_hosted_ir_count=self_hosted_ir_count,
                                    estimated_monthly_runs=estimated_monthly_runs,
                                    monthly_cost=monthly_cost,
                                )
                            )

                            # Build metadata
                            metadata = {
                                "factory_id": factory_id,
                                "factory_name": factory_name,
                                "version": version,
                                "provisioning_state": provisioning_state,
                                "location": factory_location,
                                "resource_group": rg_name,
                                "age_days": age_days,
                                "pipelines_count": pipelines_count,
                                "active_pipelines_count": active_pipelines_count,
                                "ir_count": ir_count,
                                "self_hosted_ir_count": self_hosted_ir_count,
                                "estimated_monthly_runs": estimated_monthly_runs,
                                "tags": tags,
                            }

                            resource = AllCloudResourceData(
                                resource_id=factory_id,
                                resource_type="azure_data_factory",
                                resource_name=factory_name,
                                region=factory_location,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=metadata,
                                created_at_cloud=created_time,
                                is_orphan=False,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                            )

                            resources.append(resource)
                            logger.info(
                                f"Found Data Factory: {factory_name} in {factory_location} "
                                f"(Pipelines: {pipelines_count}, Optimizable: {is_optimizable}, Score: {optimization_score})"
                            )

                        except Exception as e:
                            logger.error(f"Error processing Data Factory {getattr(factory, 'name', 'unknown')}: {e}")
                            continue

                except Exception as e:
                    logger.error(f"Error listing Data Factories in resource group {rg_name}: {e}")
                    continue

        except Exception as e:
            logger.error(f"Error scanning Azure Data Factories: {e}")

        logger.info(f"Completed Azure Data Factory scan in region {region}: {len(resources)} factories found")
        return resources

    def _estimate_data_factory_cost(
        self,
        estimated_monthly_runs: int,
        ir_count: int,
        self_hosted_ir_count: int,
    ) -> float:
        """
        Estimate monthly cost for Azure Data Factory.

        Pricing:
        - Pipeline orchestration: $0.001 per activity run
        - Integration Runtime: $0.274/hour (Azure IR)
        """
        # Activity runs cost
        activity_runs_cost = estimated_monthly_runs * 0.001

        # IR cost (assume IR runs 8 hours/day for active factories)
        ir_hours_per_month = 8 * 30 * ir_count  # 8 hours/day * 30 days
        ir_cost = ir_hours_per_month * 0.274

        total_cost = activity_runs_cost + ir_cost
        return round(total_cost, 2)

    def _calculate_data_factory_optimization(
        self,
        age_days: int,
        pipelines_count: int,
        active_pipelines_count: int,
        ir_count: int,
        self_hosted_ir_count: int,
        estimated_monthly_runs: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Azure Data Factory.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - No active pipelines for 90+ days
        if active_pipelines_count == 0 and age_days >= 90:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append(
                f"Data Factory has 0 active pipelines for {age_days} days. "
                "Delete the Data Factory if no longer needed to eliminate all costs. "
                f"Potential savings: ${potential_savings:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Many inactive pipelines (>50% inactive)
        if pipelines_count > 0:
            inactive_pipelines = pipelines_count - active_pipelines_count
            inactive_ratio = (inactive_pipelines / pipelines_count) * 100
            if inactive_ratio > 50 and inactive_pipelines > 5:
                is_optimizable = True
                optimization_score = 75
                priority = "high"
                potential_savings = monthly_cost * 0.3  # Estimate 30% savings
                recommendations.append(
                    f"Data Factory has {inactive_pipelines} inactive pipelines out of {pipelines_count} total ({inactive_ratio:.1f}%). "
                    "Delete unused pipelines to reduce complexity and improve performance. "
                    f"Potential savings: ${potential_savings:.2f}/month."
                )
                return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Unused Integration Runtimes
        if ir_count > 1 and active_pipelines_count < ir_count:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Each unused IR costs ~$65/month (8h/day * 30 days * $0.274/hour)
            unused_irs = ir_count - max(active_pipelines_count, 1)
            potential_savings = unused_irs * 65
            recommendations.append(
                f"Data Factory has {ir_count} Integration Runtimes but only {active_pipelines_count} active pipelines. "
                f"Remove {unused_irs} unused Integration Runtimes to save ${potential_savings:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Low activity (< 100 runs/month)
        if estimated_monthly_runs < 100 and estimated_monthly_runs > 0:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            potential_savings = monthly_cost * 0.5
            recommendations.append(
                f"Data Factory has very low activity ({estimated_monthly_runs} runs/month). "
                "Consider consolidating with other Data Factories or using serverless alternatives (Logic Apps). "
                f"Potential savings: ${potential_savings:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - No monitoring/alerting configured
        if not is_optimizable:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = 0.0
            recommendations.append(
                "LOW: Configure monitoring and alerting for Data Factory pipelines. "
                "Enable diagnostic logs and set up alerts for pipeline failures and long-running activities."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_static_web_apps(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Static Web Apps for cost intelligence.

        Pricing (Azure Static Web Apps):
        - Free tier: $0/mois (100GB bandwidth, 250MB storage)
        - Standard tier: $9/mois per app + $0.20/GB bandwidth

        Typical cost: $0-100/month

        Returns:
            List of AllCloudResourceData (is_orphan=False, is_optimizable=True)
        """
        try:
            from azure.mgmt.web import WebSiteManagementClient
        except ImportError:
            logger.warning("azure-mgmt-web not installed, skipping Static Web Apps scan")
            return []

        resources = []

        try:
            credential = self._get_azure_credential()
            web_client = WebSiteManagementClient(credential, self.subscription_id)

            # Iterate over resource groups
            for rg_name in self.resource_groups:
                try:
                    # List Static Web Apps in resource group
                    static_sites = web_client.static_sites.list(rg_name)

                    for site in static_sites:
                        try:
                            # Extract location
                            site_location = getattr(site, "location", "unknown").lower()

                            # Filter by region if not global
                            if region.lower() != "global" and site_location != region.lower():
                                continue

                            # Extract metadata
                            site_id = getattr(site, "id", "")
                            site_name = getattr(site, "name", "unknown")
                            sku_tier = getattr(getattr(site, "sku", None), "tier", "Free")  # Free or Standard
                            created_time = getattr(site, "created_time", None)

                            # Default hostname
                            default_hostname = getattr(site, "default_hostname", None)

                            # Custom domains
                            custom_domains = []
                            try:
                                domains_list = web_client.static_sites.list_static_site_custom_domains(rg_name, site_name)
                                custom_domains = [d.name for d in domains_list]
                            except Exception:
                                pass

                            # Repository info
                            repository_url = getattr(site, "repository_url", None)
                            branch = getattr(site, "branch", None)

                            # Tags
                            tags = getattr(site, "tags", {}) or {}

                            # Calculate age
                            age_days = 0
                            if created_time:
                                age_days = (datetime.now(timezone.utc) - created_time).days

                            # Estimate bandwidth usage (no direct metric)
                            # Assume Standard tier = high traffic, Free tier = low traffic
                            estimated_monthly_bandwidth_gb = 50 if sku_tier == "Standard" else 10

                            # Estimate monthly cost
                            monthly_cost = self._estimate_static_web_app_cost(
                                sku_tier=sku_tier,
                                estimated_monthly_bandwidth_gb=estimated_monthly_bandwidth_gb
                            )

                            # Check optimization opportunities
                            is_optimizable, optimization_score, priority, potential_savings, recommendations = (
                                self._calculate_static_web_app_optimization(
                                    sku_tier=sku_tier,
                                    age_days=age_days,
                                    estimated_monthly_bandwidth_gb=estimated_monthly_bandwidth_gb,
                                    custom_domains_count=len(custom_domains),
                                    repository_url=repository_url,
                                    monthly_cost=monthly_cost,
                                )
                            )

                            # Build metadata
                            metadata = {
                                "site_id": site_id,
                                "site_name": site_name,
                                "sku_tier": sku_tier,
                                "location": site_location,
                                "resource_group": rg_name,
                                "age_days": age_days,
                                "default_hostname": default_hostname,
                                "custom_domains": custom_domains,
                                "repository_url": repository_url,
                                "branch": branch,
                                "estimated_monthly_bandwidth_gb": estimated_monthly_bandwidth_gb,
                                "tags": tags,
                            }

                            resource = AllCloudResourceData(
                                resource_id=site_id,
                                resource_type="azure_static_web_app",
                                resource_name=site_name,
                                region=site_location,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=metadata,
                                created_at_cloud=created_time,
                                is_orphan=False,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                            )

                            resources.append(resource)
                            logger.info(
                                f"Found Static Web App: {site_name} in {site_location} "
                                f"(SKU: {sku_tier}, Optimizable: {is_optimizable}, Score: {optimization_score})"
                            )

                        except Exception as e:
                            logger.error(f"Error processing Static Web App {getattr(site, 'name', 'unknown')}: {e}")
                            continue

                except Exception as e:
                    logger.error(f"Error listing Static Web Apps in resource group {rg_name}: {e}")
                    continue

        except Exception as e:
            logger.error(f"Error scanning Azure Static Web Apps: {e}")

        logger.info(f"Completed Azure Static Web Apps scan in region {region}: {len(resources)} sites found")
        return resources

    def _estimate_static_web_app_cost(
        self,
        sku_tier: str,
        estimated_monthly_bandwidth_gb: int,
    ) -> float:
        """
        Estimate monthly cost for Azure Static Web Apps.

        Pricing:
        - Free tier: $0 (100GB bandwidth, 250MB storage included)
        - Standard tier: $9/month + $0.20/GB bandwidth
        """
        if sku_tier.lower() == "free":
            # Free tier - no cost unless exceeding limits
            if estimated_monthly_bandwidth_gb > 100:
                # Overage charges (rare)
                overage_gb = estimated_monthly_bandwidth_gb - 100
                return round(overage_gb * 0.20, 2)
            return 0.0

        # Standard tier
        base_cost = 9.0  # $9/month
        bandwidth_cost = estimated_monthly_bandwidth_gb * 0.20
        return round(base_cost + bandwidth_cost, 2)

    def _calculate_static_web_app_optimization(
        self,
        sku_tier: str,
        age_days: int,
        estimated_monthly_bandwidth_gb: int,
        custom_domains_count: int,
        repository_url: str,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Azure Static Web Apps.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Standard tier with no traffic for 90+ days
        if sku_tier.lower() == "standard" and estimated_monthly_bandwidth_gb < 1 and age_days >= 90:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost  # Full savings by downgrading or deleting
            recommendations.append(
                f"Standard tier Static Web App has no traffic for {age_days} days. "
                "Downgrade to Free tier or delete if unused. "
                f"Potential savings: ${potential_savings:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Standard tier with very low traffic (<5GB/month)
        if sku_tier.lower() == "standard" and estimated_monthly_bandwidth_gb < 5:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            # Free tier would cost $0, Standard costs $9 + bandwidth
            potential_savings = 9.0  # Base cost savings
            recommendations.append(
                f"Standard tier Static Web App has very low traffic ({estimated_monthly_bandwidth_gb}GB/month). "
                "Free tier includes 100GB bandwidth. "
                f"Downgrade to Free tier to save ${potential_savings:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Free tier approaching bandwidth limit (>80GB/month)
        if sku_tier.lower() == "free" and estimated_monthly_bandwidth_gb > 80:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = 0.0  # No savings, but avoiding overage charges
            recommendations.append(
                f"Free tier Static Web App is approaching bandwidth limit ({estimated_monthly_bandwidth_gb}GB/100GB). "
                "Upgrade to Standard tier or optimize bandwidth usage (CDN, compression, caching) "
                "to avoid overage charges."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - No custom domain configured (best practice)
        if custom_domains_count == 0 and age_days > 30:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            potential_savings = 0.0
            recommendations.append(
                "Static Web App has no custom domain configured. "
                "Configure a custom domain for production apps to improve branding and SEO."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - No monitoring/alerting configured
        if not is_optimizable:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = 0.0
            recommendations.append(
                "LOW: Configure monitoring and alerting for Static Web App. "
                "Enable Application Insights to track performance, errors, and user behavior."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_dedicated_hsms(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Dedicated HSM instances for cost intelligence.

        Pricing (Azure Dedicated HSM):
        - Standard HSM: ~$4,500/mois (bare metal appliance)
        - High Performance HSM: ~$8,000/mois

        Typical cost: $4,500-8,000/month (VERY EXPENSIVE)

        Returns:
            List of AllCloudResourceData (is_orphan=False, is_optimizable=True)
        """
        try:
            from azure.mgmt.hardwaresecuritymodules import AzureDedicatedHSMResourceProvider
        except ImportError:
            logger.warning("azure-mgmt-hardwaresecuritymodules not installed, skipping Dedicated HSM scan")
            return []

        resources = []

        try:
            credential = self._get_azure_credential()
            hsm_client = AzureDedicatedHSMResourceProvider(credential, self.subscription_id)

            # Iterate over resource groups
            for rg_name in self.resource_groups:
                try:
                    # List Dedicated HSMs in resource group
                    hsms = hsm_client.dedicated_hsm.list_by_resource_group(rg_name)

                    for hsm in hsms:
                        try:
                            # Extract location
                            hsm_location = getattr(hsm, "location", "unknown").lower()

                            # Filter by region if not global
                            if region.lower() != "global" and hsm_location != region.lower():
                                continue

                            # Extract metadata
                            hsm_id = getattr(hsm, "id", "")
                            hsm_name = getattr(hsm, "name", "unknown")
                            sku_name = getattr(getattr(hsm, "sku", None), "name", "Standard")  # Standard or High Performance
                            provisioning_state = getattr(getattr(hsm, "properties", None), "provisioning_state", "Unknown")

                            # HSM properties
                            properties = getattr(hsm, "properties", None)
                            stamp_id = getattr(properties, "stamp_id", None) if properties else None
                            status_message = getattr(properties, "status_message", None) if properties else None

                            # Network profile
                            network_profile = getattr(properties, "network_profile", None) if properties else None
                            subnet_id = getattr(network_profile, "subnet", {}).get("id", None) if network_profile else None

                            # Tags
                            tags = getattr(hsm, "tags", {}) or {}

                            # Calculate age (estimate based on tags or assume old)
                            age_days = 90  # Default assumption for HSMs

                            # Estimate key usage (no direct metric available)
                            # Dedicated HSMs can store thousands of keys
                            estimated_keys_count = 100  # Estimate
                            estimated_keys_capacity = 10000  # Typical capacity

                            # Estimate monthly cost based on SKU
                            monthly_cost = self._estimate_dedicated_hsm_cost(sku_name=sku_name)

                            # Check optimization opportunities
                            is_optimizable, optimization_score, priority, potential_savings, recommendations = (
                                self._calculate_dedicated_hsm_optimization(
                                    hsm_name=hsm_name,
                                    sku_name=sku_name,
                                    provisioning_state=provisioning_state,
                                    age_days=age_days,
                                    estimated_keys_count=estimated_keys_count,
                                    estimated_keys_capacity=estimated_keys_capacity,
                                    status_message=status_message,
                                    monthly_cost=monthly_cost,
                                )
                            )

                            # Build metadata
                            metadata = {
                                "hsm_id": hsm_id,
                                "hsm_name": hsm_name,
                                "sku": sku_name,
                                "location": hsm_location,
                                "resource_group": rg_name,
                                "provisioning_state": provisioning_state,
                                "stamp_id": stamp_id,
                                "status_message": status_message,
                                "subnet_id": subnet_id,
                                "age_days": age_days,
                                "estimated_keys_count": estimated_keys_count,
                                "estimated_keys_capacity": estimated_keys_capacity,
                                "tags": tags,
                            }

                            resource = AllCloudResourceData(
                                resource_id=hsm_id,
                                resource_type="azure_dedicated_hsm",
                                resource_name=hsm_name,
                                region=hsm_location,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=metadata,
                                created_at_cloud=None,  # Not available in API
                                is_orphan=False,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                            )

                            resources.append(resource)
                            logger.info(
                                f"Found Dedicated HSM: {hsm_name} in {hsm_location} "
                                f"(SKU: {sku_name}, Optimizable: {is_optimizable}, Score: {optimization_score})"
                            )

                        except Exception as e:
                            logger.error(f"Error processing Dedicated HSM {getattr(hsm, 'name', 'unknown')}: {e}")
                            continue

                except Exception as e:
                    logger.error(f"Error listing Dedicated HSMs in resource group {rg_name}: {e}")
                    continue

        except Exception as e:
            logger.error(f"Error scanning Azure Dedicated HSMs: {e}")

        logger.info(f"Completed Azure Dedicated HSM scan in region {region}: {len(resources)} HSMs found")
        return resources

    def _estimate_dedicated_hsm_cost(self, sku_name: str) -> float:
        """
        Estimate monthly cost for Azure Dedicated HSM.

        Pricing:
        - Standard HSM: ~$4,500/month
        - High Performance HSM: ~$8,000/month
        """
        if "high" in sku_name.lower() or "performance" in sku_name.lower():
            return 8000.0
        return 4500.0  # Standard

    def _calculate_dedicated_hsm_optimization(
        self,
        hsm_name: str,
        sku_name: str,
        provisioning_state: str,
        age_days: int,
        estimated_keys_count: int,
        estimated_keys_capacity: int,
        status_message: str,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Azure Dedicated HSM.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - HSM provisioned but never connected for 30+ days
        if provisioning_state.lower() == "succeeded" and age_days >= 30:
            # Check if there's a status message indicating no connections
            if status_message and ("not connected" in status_message.lower() or "no activity" in status_message.lower()):
                is_optimizable = True
                optimization_score = 90
                priority = "critical"
                potential_savings = monthly_cost  # Full cost if unused
                recommendations.append(
                    f"Dedicated HSM is provisioned but never connected for {age_days} days. "
                    f"This is VERY EXPENSIVE (${monthly_cost:.2f}/month). "
                    "Delete immediately if not needed or investigate connectivity issues."
                )
                return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Utilization <10% of key capacity
        if estimated_keys_capacity > 0:
            utilization_pct = (estimated_keys_count / estimated_keys_capacity) * 100
            if utilization_pct < 10:
                is_optimizable = True
                optimization_score = 75
                priority = "high"
                potential_savings = monthly_cost * 0.7  # Partial waste
                recommendations.append(
                    f"Dedicated HSM has very low utilization ({utilization_pct:.1f}% of key capacity). "
                    f"This costs ${monthly_cost:.2f}/month. "
                    "Consider migrating to Azure Key Vault Managed HSM (much cheaper) or consolidating HSMs."
                )
                return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - No backup configured (risk of data loss)
        # Placeholder - would require checking backup settings
        if not is_optimizable:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = 0.0  # No cost savings, but critical risk
            recommendations.append(
                f"Dedicated HSM backup configuration not detected (costs ${monthly_cost:.2f}/month). "
                "Configure backup and disaster recovery to protect critical cryptographic keys. "
                "Data loss would be catastrophic."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Redundant HSM unused (DR not tested)
        if "dr" in hsm_name.lower() or "secondary" in hsm_name.lower():
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            potential_savings = 0.0
            recommendations.append(
                f"Dedicated HSM appears to be a DR/secondary instance (costs ${monthly_cost:.2f}/month). "
                "Verify DR testing is performed regularly. "
                "Untested DR is wasted money."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - No monitoring/alerting configured
        if not is_optimizable:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = 0.0
            recommendations.append(
                f"LOW: Configure monitoring and alerting for Dedicated HSM (costs ${monthly_cost:.2f}/month). "
                "Monitor key operations, connection status, and performance metrics. "
                "Set up alerts for failures and capacity limits."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_iot_hub_message_routing(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure IoT Hub message routing configurations for cost intelligence.

        This scans all IoT Hubs across all resource groups in the specified region
        and analyzes their message routing configuration for optimization opportunities.

        Pricing:
            - Free tier: Free (8,000 messages/day)
            - Basic tier: $10/month + $0.0004/1K messages
            - Standard tier: $25-200/month (S1-S3) + $0.0004/1K messages
            - Message routing: Additional $0.50/million routing operations

        Typical monthly cost: $10-500/month

        Optimization scenarios:
            1. CRITICAL (90): Custom routes configured but 0 messages routed for 90+ days
            2. HIGH (75): Fallback route activated with 100% of messages (routing misconfigured)
            3. HIGH (70): Redundant or conflicting routes (duplicate queries/endpoints)
            4. MEDIUM (50): Dead endpoints (Storage/Event Hub/Service Bus unavailable)
            5. LOW (30): No dead-letter endpoint configured (message loss risk)

        Returns:
            List of AllCloudResourceData objects with is_optimizable=True and optimization scores.
        """
        resources = []

        try:
            from azure.mgmt.iothub import IotHubClient
        except ImportError:
            logger.error("azure-mgmt-iothub not installed - cannot scan IoT Hub routing")
            return resources

        try:
            credential = DefaultAzureCredential()
            client = IotHubClient(credential, self.subscription_id)

            # Iterate through all resource groups
            for rg_name in await self._get_resource_group_names():
                try:
                    # List all IoT Hubs in resource group
                    iot_hubs = client.iot_hub_resource.list_by_resource_group(rg_name)

                    for hub in iot_hubs:
                        try:
                            # Filter by region
                            hub_region = hub.location.lower().replace(" ", "")
                            normalized_region = region.lower().replace(" ", "")
                            if hub_region != normalized_region:
                                continue

                            # Get routing configuration
                            routing_config = hub.properties.routing if hub.properties else None
                            sku = hub.sku.name if hub.sku else "unknown"
                            tier = hub.sku.tier if hub.sku else "unknown"

                            # Extract route metadata
                            custom_routes = routing_config.routes if routing_config and routing_config.routes else []
                            fallback_route_enabled = (
                                routing_config.fallback_route.is_enabled
                                if routing_config and routing_config.fallback_route
                                else False
                            )
                            endpoints = (
                                routing_config.endpoints if routing_config and routing_config.endpoints else None
                            )

                            # Count endpoints by type
                            storage_endpoints = len(endpoints.storage_containers) if endpoints and endpoints.storage_containers else 0
                            eventhub_endpoints = len(endpoints.event_hubs) if endpoints and endpoints.event_hubs else 0
                            servicebus_queue_endpoints = (
                                len(endpoints.service_bus_queues) if endpoints and endpoints.service_bus_queues else 0
                            )
                            servicebus_topic_endpoints = (
                                len(endpoints.service_bus_topics) if endpoints and endpoints.service_bus_topics else 0
                            )

                            total_custom_endpoints = (
                                storage_endpoints
                                + eventhub_endpoints
                                + servicebus_queue_endpoints
                                + servicebus_topic_endpoints
                            )

                            # Estimate message routing activity (placeholder - real implementation would query metrics)
                            # For MVP, we estimate based on tier and routes
                            estimated_monthly_messages = 0
                            if tier.lower() == "free":
                                estimated_monthly_messages = 8000 * 30  # 8K/day limit
                            elif tier.lower() == "basic":
                                estimated_monthly_messages = 100_000  # Conservative estimate
                            elif tier.lower() == "standard":
                                if "s1" in sku.lower():
                                    estimated_monthly_messages = 1_000_000
                                elif "s2" in sku.lower():
                                    estimated_monthly_messages = 10_000_000
                                elif "s3" in sku.lower():
                                    estimated_monthly_messages = 100_000_000

                            # Calculate routing efficiency
                            routes_count = len(custom_routes)
                            fallback_percentage = 100 if fallback_route_enabled and routes_count == 0 else 0
                            if routes_count > 0 and fallback_route_enabled:
                                # Estimate: if routes exist, assume 70% custom, 30% fallback
                                fallback_percentage = 30

                            # Calculate cost
                            monthly_cost = self._calculate_iot_hub_routing_cost(
                                tier, sku, estimated_monthly_messages, routes_count
                            )

                            # Calculate optimization
                            (
                                is_optimizable,
                                optimization_score,
                                priority,
                                potential_savings,
                                recommendations,
                            ) = self._calculate_iot_hub_routing_optimization(
                                tier,
                                routes_count,
                                total_custom_endpoints,
                                fallback_route_enabled,
                                fallback_percentage,
                                estimated_monthly_messages,
                                monthly_cost,
                            )

                            resource = AllCloudResourceData(
                                resource_id=hub.id,
                                resource_type="azure_iot_hub_routing",
                                resource_name=hub.name,
                                region=hub.location,
                                is_orphan=False,  # IoT Hub routing cannot be orphan
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata={
                                    "sku": sku,
                                    "tier": tier,
                                    "resource_group": rg_name,
                                    "custom_routes_count": routes_count,
                                    "fallback_route_enabled": fallback_route_enabled,
                                    "fallback_percentage": fallback_percentage,
                                    "storage_endpoints": storage_endpoints,
                                    "eventhub_endpoints": eventhub_endpoints,
                                    "servicebus_queue_endpoints": servicebus_queue_endpoints,
                                    "servicebus_topic_endpoints": servicebus_topic_endpoints,
                                    "total_custom_endpoints": total_custom_endpoints,
                                    "estimated_monthly_messages": estimated_monthly_messages,
                                },
                                last_used_at=None,
                                created_at_cloud=None,
                            )

                            resources.append(resource)
                            logger.info(
                                f"Scanned Azure IoT Hub routing: {hub.name} in {hub.location} "
                                f"(routes={routes_count}, endpoints={total_custom_endpoints}, "
                                f"optimizable={is_optimizable}, cost=${monthly_cost:.2f}/month)"
                            )

                        except Exception as e:
                            logger.error(
                                f"Error scanning IoT Hub {hub.name}: {e}",
                                exc_info=True,
                            )
                            continue

                except Exception as e:
                    logger.error(
                        f"Error listing IoT Hubs in resource group {rg_name}: {e}",
                        exc_info=True,
                    )
                    continue

        except Exception as e:
            logger.error(f"Error scanning Azure IoT Hub routing: {e}", exc_info=True)

        return resources

    def _calculate_iot_hub_routing_cost(
        self, tier: str, sku: str, monthly_messages: int, routes_count: int
    ) -> float:
        """
        Calculate monthly cost for IoT Hub message routing.

        Pricing breakdown (as of 2025):
            - Free tier: $0 (8,000 messages/day limit)
            - Basic B1: $10/month + $0.0004/1K messages
            - Standard S1: $25/month + $0.0004/1K messages (400K/day)
            - Standard S2: $250/month + $0.0004/1K messages (6M/day)
            - Standard S3: $2,500/month + $0.0004/1K messages (300M/day)
            - Message routing: $0.50/million routing operations

        Returns:
            Estimated monthly cost in USD
        """
        tier_lower = tier.lower()
        sku_lower = sku.lower()

        # Base cost by SKU
        base_cost = 0.0
        if tier_lower == "free":
            base_cost = 0.0
        elif tier_lower == "basic":
            base_cost = 10.0
        elif tier_lower == "standard":
            if "s1" in sku_lower:
                base_cost = 25.0
            elif "s2" in sku_lower:
                base_cost = 250.0
            elif "s3" in sku_lower:
                base_cost = 2500.0
            else:
                base_cost = 25.0  # Default to S1

        # Message cost ($0.0004 per 1K messages)
        message_cost = (monthly_messages / 1000) * 0.0004

        # Routing operations cost ($0.50 per million operations)
        # Each message can trigger multiple route evaluations
        routing_operations = monthly_messages * routes_count if routes_count > 0 else monthly_messages
        routing_cost = (routing_operations / 1_000_000) * 0.50

        total_cost = base_cost + message_cost + routing_cost
        return round(total_cost, 2)

    def _calculate_iot_hub_routing_optimization(
        self,
        tier: str,
        routes_count: int,
        total_custom_endpoints: int,
        fallback_route_enabled: bool,
        fallback_percentage: int,
        monthly_messages: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Azure IoT Hub message routing.

        5 optimization scenarios (ordered by severity):
            1. CRITICAL (90): Custom routes configured but 0 messages routed for 90+ days
            2. HIGH (75): Fallback route activated with 100% of messages (routing misconfigured)
            3. HIGH (70): Redundant or conflicting routes (>5 routes to same endpoint type)
            4. MEDIUM (50): Dead endpoints (custom endpoints > 0 but fallback 100%)
            5. LOW (30): No dead-letter endpoint configured (message loss risk)

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations: list[str] = []

        # Scenario 1: CRITICAL - Custom routes configured but 0 messages for 90+ days
        # (In real implementation, would check metrics for actual message counts over 90 days)
        if routes_count > 0 and monthly_messages == 0:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            # If no messages, can downgrade to Free tier
            if tier.lower() != "free":
                potential_savings = monthly_cost  # Save full cost by removing hub or downgrading
            recommendations.append(
                f"CRITICAL: IoT Hub has {routes_count} custom route(s) configured but 0 messages routed in 90+ days. "
                f"Consider deleting unused routes or removing the IoT Hub (saves ${monthly_cost:.2f}/month). "
                "Review device connectivity and message flow."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Fallback route with 100% of messages (routing misconfigured)
        if fallback_route_enabled and fallback_percentage >= 90 and routes_count > 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            # Routing cost wasted on ineffective rules
            potential_savings = round((monthly_messages * routes_count / 1_000_000) * 0.50, 2)
            recommendations.append(
                f"HIGH: Fallback route receives {fallback_percentage}% of messages despite {routes_count} custom route(s). "
                f"Review routing queries - they may never match (saves ${potential_savings:.2f}/month in routing costs). "
                "Fix route conditions or remove ineffective routes."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Redundant or conflicting routes
        if routes_count > 5:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Estimate 30% of routing operations are redundant
            redundant_routing_cost = round((monthly_messages * routes_count * 0.3 / 1_000_000) * 0.50, 2)
            potential_savings = redundant_routing_cost
            recommendations.append(
                f"HIGH: IoT Hub has {routes_count} custom routes (>5). "
                f"Review for redundant or conflicting routing queries (saves ${potential_savings:.2f}/month). "
                "Consolidate routes where possible to reduce routing overhead."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Dead endpoints (endpoints exist but fallback gets all messages)
        if total_custom_endpoints > 0 and fallback_route_enabled and fallback_percentage >= 70:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Endpoints configured but not receiving messages
            potential_savings = round(monthly_cost * 0.1, 2)  # 10% savings from cleanup
            recommendations.append(
                f"MEDIUM: {total_custom_endpoints} custom endpoint(s) configured but fallback route receives {fallback_percentage}% of messages. "
                f"Endpoints may be unreachable or misconfigured (saves ${potential_savings:.2f}/month). "
                "Verify endpoint connectivity (Storage/Event Hub/Service Bus)."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - No dead-letter endpoint configured
        if routes_count > 0 and not fallback_route_enabled:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = 0.0  # No direct cost savings, but prevents message loss
            recommendations.append(
                f"LOW: {routes_count} custom route(s) configured but no fallback route enabled. "
                "Messages that don't match any route will be dropped. "
                "Enable fallback route to prevent message loss and improve reliability."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_ml_online_endpoints(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure ML Online Endpoints for cost intelligence.

        This scans all Azure ML workspaces across all resource groups in the specified region
        and analyzes their online endpoints (real-time inference) for optimization opportunities.

        Pricing:
            - Basic instance (CPU): ~$50/month
            - Standard instance (CPU): ~$200/month
            - Premium instance (GPU): ~$500-1,000/month
            - Autoscaling: Additional costs based on instance count

        Typical monthly cost: $50-1,000/month

        Optimization scenarios:
            1. CRITICAL (90): Endpoint deployed but 0 inference requests for 90+ days
            2. HIGH (75): Premium GPU instance with <5% utilization (forecasting/recommendations models)
            3. HIGH (70): Multiple endpoints with same model version (consolidation opportunity)
            4. MEDIUM (50): Standard instance with <20% utilization
            5. LOW (30): No autoscaling configured (cost efficiency opportunity)

        Returns:
            List of AllCloudResourceData objects with is_optimizable=True and optimization scores.
        """
        resources = []

        try:
            from azure.ai.ml import MLClient
        except ImportError:
            logger.error("azure-ai-ml not installed - cannot scan ML Online Endpoints")
            return resources

        try:
            credential = DefaultAzureCredential()

            # Iterate through all resource groups
            for rg_name in await self._get_resource_group_names():
                try:
                    # List all ML workspaces in resource group
                    from azure.mgmt.machinelearningservices import AzureMachineLearningWorkspaces
                    ml_mgmt_client = AzureMachineLearningWorkspaces(credential, self.subscription_id)
                    workspaces = ml_mgmt_client.workspaces.list_by_resource_group(rg_name)

                    for workspace in workspaces:
                        try:
                            # Filter by region
                            workspace_region = workspace.location.lower().replace(" ", "")
                            normalized_region = region.lower().replace(" ", "")
                            if workspace_region != normalized_region:
                                continue

                            # Create ML Client for this workspace
                            ml_client = MLClient(
                                credential=credential,
                                subscription_id=self.subscription_id,
                                resource_group_name=rg_name,
                                workspace_name=workspace.name
                            )

                            # List online endpoints in workspace
                            online_endpoints = ml_client.online_endpoints.list()

                            for endpoint in online_endpoints:
                                try:
                                    # Get endpoint details
                                    endpoint_name = endpoint.name
                                    provisioning_state = endpoint.provisioning_state
                                    endpoint_type = endpoint.kind if hasattr(endpoint, 'kind') else "unknown"

                                    # Get deployments for this endpoint
                                    deployments = list(ml_client.online_deployments.list(endpoint_name=endpoint_name))
                                    deployment_count = len(deployments)

                                    # Extract instance info from first deployment (if exists)
                                    instance_type = "unknown"
                                    instance_count = 0
                                    if deployments:
                                        first_deployment = deployments[0]
                                        instance_type = first_deployment.instance_type if hasattr(first_deployment, 'instance_type') else "unknown"
                                        instance_count = first_deployment.instance_count if hasattr(first_deployment, 'instance_count') else 0

                                    # Estimate monthly messages/requests (placeholder - real implementation would query metrics)
                                    # For MVP, we estimate based on instance type
                                    estimated_monthly_requests = 0
                                    if "gpu" in instance_type.lower() or "premium" in instance_type.lower():
                                        estimated_monthly_requests = 100_000  # Premium instances expected high traffic
                                    elif "standard" in instance_type.lower():
                                        estimated_monthly_requests = 50_000
                                    else:
                                        estimated_monthly_requests = 10_000  # Basic

                                    # Calculate cost
                                    monthly_cost = self._calculate_ml_online_endpoint_cost(
                                        instance_type, instance_count
                                    )

                                    # Calculate optimization
                                    (
                                        is_optimizable,
                                        optimization_score,
                                        priority,
                                        potential_savings,
                                        recommendations,
                                    ) = self._calculate_ml_online_endpoint_optimization(
                                        instance_type,
                                        instance_count,
                                        deployment_count,
                                        estimated_monthly_requests,
                                        monthly_cost,
                                    )

                                    resource = AllCloudResourceData(
                                        resource_id=f"/subscriptions/{self.subscription_id}/resourceGroups/{rg_name}/providers/Microsoft.MachineLearningServices/workspaces/{workspace.name}/onlineEndpoints/{endpoint_name}",
                                        resource_type="azure_ml_online_endpoint",
                                        resource_name=endpoint_name,
                                        region=workspace.location,
                                        is_orphan=False,  # ML endpoints cannot be orphan
                                        is_optimizable=is_optimizable,
                                        optimization_score=optimization_score,
                                        optimization_priority=priority,
                                        potential_monthly_savings=potential_savings,
                                        optimization_recommendations=recommendations,
                                        estimated_monthly_cost=monthly_cost,
                                        currency="USD",
                                        resource_metadata={
                                            "workspace_name": workspace.name,
                                            "resource_group": rg_name,
                                            "provisioning_state": provisioning_state,
                                            "endpoint_type": endpoint_type,
                                            "instance_type": instance_type,
                                            "instance_count": instance_count,
                                            "deployment_count": deployment_count,
                                            "estimated_monthly_requests": estimated_monthly_requests,
                                        },
                                        last_used_at=None,
                                        created_at_cloud=None,
                                    )

                                    resources.append(resource)
                                    logger.info(
                                        f"Scanned Azure ML Online Endpoint: {endpoint_name} in workspace {workspace.name} "
                                        f"(instance={instance_type}, count={instance_count}, optimizable={is_optimizable}, cost=${monthly_cost:.2f}/month)"
                                    )

                                except Exception as e:
                                    logger.error(
                                        f"Error scanning ML Online Endpoint {endpoint.name}: {e}",
                                        exc_info=True,
                                    )
                                    continue

                        except Exception as e:
                            logger.error(
                                f"Error scanning ML workspace {workspace.name}: {e}",
                                exc_info=True,
                            )
                            continue

                except Exception as e:
                    logger.error(
                        f"Error listing ML workspaces in resource group {rg_name}: {e}",
                        exc_info=True,
                    )
                    continue

        except Exception as e:
            logger.error(f"Error scanning Azure ML Online Endpoints: {e}", exc_info=True)

        return resources

    def _calculate_ml_online_endpoint_cost(
        self, instance_type: str, instance_count: int
    ) -> float:
        """
        Calculate monthly cost for Azure ML Online Endpoint.

        Pricing breakdown (as of 2025):
            - Basic CPU instances: ~$50/month per instance
            - Standard CPU instances: ~$200/month per instance
            - Premium GPU instances: ~$500-1,000/month per instance
            - Data transfer: $0.087/GB (typically negligible)

        Returns:
            Estimated monthly cost in USD
        """
        instance_type_lower = instance_type.lower()

        # Base cost per instance per month
        cost_per_instance = 50.0  # Default to Basic

        if "gpu" in instance_type_lower or "premium" in instance_type_lower:
            cost_per_instance = 750.0  # Premium GPU average
        elif "standard" in instance_type_lower:
            cost_per_instance = 200.0  # Standard CPU
        elif "basic" in instance_type_lower or "cpu" in instance_type_lower:
            cost_per_instance = 50.0  # Basic CPU

        total_cost = cost_per_instance * max(instance_count, 1)
        return round(total_cost, 2)

    def _calculate_ml_online_endpoint_optimization(
        self,
        instance_type: str,
        instance_count: int,
        deployment_count: int,
        monthly_requests: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Azure ML Online Endpoints.

        5 optimization scenarios (ordered by severity):
            1. CRITICAL (90): Endpoint deployed but 0 inference requests for 90+ days
            2. HIGH (75): Premium GPU instance with <5% utilization (forecasting/recommendations)
            3. HIGH (70): Multiple deployments with same model (consolidation opportunity)
            4. MEDIUM (50): Standard instance with <20% utilization
            5. LOW (30): No autoscaling configured (cost efficiency)

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations: list[str] = []

        instance_type_lower = instance_type.lower()
        is_gpu = "gpu" in instance_type_lower or "premium" in instance_type_lower

        # Scenario 1: CRITICAL - Endpoint deployed but 0 requests for 90+ days
        if monthly_requests == 0:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost  # Save full cost by deleting endpoint
            recommendations.append(
                f"CRITICAL: ML Online Endpoint deployed but 0 inference requests for 90+ days (costs ${monthly_cost:.2f}/month). "
                f"Consider deleting the endpoint or putting it in standby mode. "
                "Review model usage and deployment necessity."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Premium GPU instance with <5% utilization
        if is_gpu and monthly_requests < 5_000:  # <5% of expected 100K
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            # Downgrade to Standard CPU saves ~75% cost
            potential_savings = round(monthly_cost * 0.75, 2)
            recommendations.append(
                f"HIGH: Premium GPU instance with very low utilization ({monthly_requests:,} requests/month, <5%). "
                f"Downgrade to Standard CPU instance (saves ${potential_savings:.2f}/month). "
                "GPU recommended only for complex deep learning models (forecasting, recommendations, image processing)."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Multiple deployments (consolidation opportunity)
        if deployment_count > 3:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Consolidate deployments saves ~50% cost
            potential_savings = round(monthly_cost * 0.5, 2)
            recommendations.append(
                f"HIGH: Endpoint has {deployment_count} deployments. "
                f"Consider consolidating similar model versions (saves ${potential_savings:.2f}/month). "
                "Use blue-green deployment strategy instead of keeping multiple versions live."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Standard instance with <20% utilization
        if "standard" in instance_type_lower and monthly_requests < 10_000:  # <20% of expected 50K
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Downgrade to Basic saves ~75% cost
            potential_savings = round(monthly_cost * 0.75, 2)
            recommendations.append(
                f"MEDIUM: Standard instance with low utilization ({monthly_requests:,} requests/month, <20%). "
                f"Downgrade to Basic instance (saves ${potential_savings:.2f}/month). "
                "Review traffic patterns and adjust instance type accordingly."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - No autoscaling configured (single instance)
        if instance_count == 1:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = round(monthly_cost * 0.2, 2)  # 20% savings from autoscaling
            recommendations.append(
                f"LOW: Single instance deployment without autoscaling (costs ${monthly_cost:.2f}/month). "
                f"Enable autoscaling to optimize costs during low-traffic periods (saves ${potential_savings:.2f}/month). "
                "Configure min=0 instances to scale down to zero when idle."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_ml_batch_endpoints(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure ML Batch Endpoints for cost intelligence.

        This scans all Azure ML workspaces across all resource groups in the specified region
        and analyzes their batch endpoints (batch scoring/inference) for optimization opportunities.

        Pricing:
            - Compute cluster: $0.10-2.00/hour per node (depends on VM size)
            - Storage: ~$10-50/month for batch outputs
            - Data transfer: $0.087/GB

        Typical monthly cost: $10-500/month (pay-per-use model)

        Optimization scenarios:
            1. CRITICAL (90): Batch endpoint deployed but 0 batch jobs for 90+ days
            2. HIGH (75): Compute cluster running 24/7 instead of on-demand
            3. HIGH (70): Oversized cluster (too many nodes for workload)
            4. MEDIUM (50): No job scheduling optimization (peak hour waste)
            5. LOW (30): Missing cost allocation tags

        Returns:
            List of AllCloudResourceData objects with is_optimizable=True and optimization scores.
        """
        resources = []

        try:
            from azure.ai.ml import MLClient
        except ImportError:
            logger.error("azure-ai-ml not installed - cannot scan ML Batch Endpoints")
            return resources

        try:
            credential = DefaultAzureCredential()

            # Iterate through all resource groups
            for rg_name in await self._get_resource_group_names():
                try:
                    # List all ML workspaces in resource group
                    from azure.mgmt.machinelearningservices import AzureMachineLearningWorkspaces
                    ml_mgmt_client = AzureMachineLearningWorkspaces(credential, self.subscription_id)
                    workspaces = ml_mgmt_client.workspaces.list_by_resource_group(rg_name)

                    for workspace in workspaces:
                        try:
                            # Filter by region
                            workspace_region = workspace.location.lower().replace(" ", "")
                            normalized_region = region.lower().replace(" ", "")
                            if workspace_region != normalized_region:
                                continue

                            # Create ML Client for this workspace
                            ml_client = MLClient(
                                credential=credential,
                                subscription_id=self.subscription_id,
                                resource_group_name=rg_name,
                                workspace_name=workspace.name
                            )

                            # List batch endpoints in workspace
                            batch_endpoints = ml_client.batch_endpoints.list()

                            for endpoint in batch_endpoints:
                                try:
                                    # Get endpoint details
                                    endpoint_name = endpoint.name
                                    provisioning_state = endpoint.provisioning_state

                                    # Get deployments for this endpoint
                                    deployments = list(ml_client.batch_deployments.list(endpoint_name=endpoint_name))
                                    deployment_count = len(deployments)

                                    # Extract compute info from first deployment (if exists)
                                    compute_type = "unknown"
                                    instance_count = 0
                                    if deployments:
                                        first_deployment = deployments[0]
                                        compute_name = first_deployment.compute if hasattr(first_deployment, 'compute') else None
                                        if compute_name:
                                            # Try to get compute cluster details
                                            try:
                                                compute = ml_client.compute.get(compute_name)
                                                compute_type = compute.type if hasattr(compute, 'type') else "unknown"
                                                if hasattr(compute, 'size'):
                                                    instance_count = compute.size.max_instances if hasattr(compute.size, 'max_instances') else 1
                                            except:
                                                pass

                                    # Estimate monthly batch jobs (placeholder - real implementation would query job history)
                                    # For MVP, assume some baseline activity
                                    estimated_monthly_jobs = 10  # Conservative estimate

                                    # Check if cluster is always running (critical waste)
                                    is_always_running = False
                                    if compute_type.lower() == "amlcompute":
                                        # AMLCompute clusters can be set to always-on or auto-scale to 0
                                        is_always_running = True  # Assume worst case for MVP

                                    # Calculate cost
                                    monthly_cost = self._calculate_ml_batch_endpoint_cost(
                                        compute_type, instance_count, is_always_running
                                    )

                                    # Calculate optimization
                                    (
                                        is_optimizable,
                                        optimization_score,
                                        priority,
                                        potential_savings,
                                        recommendations,
                                    ) = self._calculate_ml_batch_endpoint_optimization(
                                        compute_type,
                                        instance_count,
                                        deployment_count,
                                        estimated_monthly_jobs,
                                        is_always_running,
                                        monthly_cost,
                                    )

                                    resource = AllCloudResourceData(
                                        resource_id=f"/subscriptions/{self.subscription_id}/resourceGroups/{rg_name}/providers/Microsoft.MachineLearningServices/workspaces/{workspace.name}/batchEndpoints/{endpoint_name}",
                                        resource_type="azure_ml_batch_endpoint",
                                        resource_name=endpoint_name,
                                        region=workspace.location,
                                        is_orphan=False,  # ML batch endpoints cannot be orphan
                                        is_optimizable=is_optimizable,
                                        optimization_score=optimization_score,
                                        optimization_priority=priority,
                                        potential_monthly_savings=potential_savings,
                                        optimization_recommendations=recommendations,
                                        estimated_monthly_cost=monthly_cost,
                                        currency="USD",
                                        resource_metadata={
                                            "workspace_name": workspace.name,
                                            "resource_group": rg_name,
                                            "provisioning_state": provisioning_state,
                                            "compute_type": compute_type,
                                            "instance_count": instance_count,
                                            "deployment_count": deployment_count,
                                            "estimated_monthly_jobs": estimated_monthly_jobs,
                                            "is_always_running": is_always_running,
                                        },
                                        last_used_at=None,
                                        created_at_cloud=None,
                                    )

                                    resources.append(resource)
                                    logger.info(
                                        f"Scanned Azure ML Batch Endpoint: {endpoint_name} in workspace {workspace.name} "
                                        f"(compute={compute_type}, nodes={instance_count}, optimizable={is_optimizable}, cost=${monthly_cost:.2f}/month)"
                                    )

                                except Exception as e:
                                    logger.error(
                                        f"Error scanning ML Batch Endpoint {endpoint.name}: {e}",
                                        exc_info=True,
                                    )
                                    continue

                        except Exception as e:
                            logger.error(
                                f"Error scanning ML workspace {workspace.name}: {e}",
                                exc_info=True,
                            )
                            continue

                except Exception as e:
                    logger.error(
                        f"Error listing ML workspaces in resource group {rg_name}: {e}",
                        exc_info=True,
                    )
                    continue

        except Exception as e:
            logger.error(f"Error scanning Azure ML Batch Endpoints: {e}", exc_info=True)

        return resources

    def _calculate_ml_batch_endpoint_cost(
        self, compute_type: str, instance_count: int, is_always_running: bool
    ) -> float:
        """
        Calculate monthly cost for Azure ML Batch Endpoint.

        Pricing breakdown (as of 2025):
            - Standard D4s v3 (always running): ~$200/month per node
            - Standard D4s v3 (on-demand, 50 hours/month): ~$25/month per node
            - Storage for batch outputs: ~$10/month

        Returns:
            Estimated monthly cost in USD
        """
        compute_type_lower = compute_type.lower()

        if is_always_running:
            # Cluster running 24/7 - very expensive
            cost_per_node_monthly = 200.0  # Average for Standard D4s
            compute_cost = cost_per_node_monthly * max(instance_count, 1)
        else:
            # On-demand usage - assume 50 hours/month average
            cost_per_hour = 0.50  # Average for Standard D4s
            hours_per_month = 50
            compute_cost = cost_per_hour * hours_per_month * max(instance_count, 1)

        storage_cost = 10.0  # Batch output storage
        total_cost = compute_cost + storage_cost
        return round(total_cost, 2)

    def _calculate_ml_batch_endpoint_optimization(
        self,
        compute_type: str,
        instance_count: int,
        deployment_count: int,
        monthly_jobs: int,
        is_always_running: bool,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Azure ML Batch Endpoints.

        5 optimization scenarios (ordered by severity):
            1. CRITICAL (90): Batch endpoint deployed but 0 batch jobs for 90+ days
            2. HIGH (75): Compute cluster running 24/7 instead of on-demand
            3. HIGH (70): Oversized cluster (too many nodes for workload)
            4. MEDIUM (50): No job scheduling optimization
            5. LOW (30): Missing cost allocation tags

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations: list[str] = []

        # Scenario 1: CRITICAL - Endpoint deployed but 0 batch jobs for 90+ days
        if monthly_jobs == 0:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost  # Save full cost by deleting endpoint
            recommendations.append(
                f"CRITICAL: ML Batch Endpoint deployed but 0 batch jobs executed for 90+ days (costs ${monthly_cost:.2f}/month). "
                f"Consider deleting the endpoint or archiving unused batch models. "
                "Review batch scoring requirements and model lifecycle."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Cluster running 24/7 instead of on-demand
        if is_always_running:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            # Switch to on-demand saves ~75% cost
            potential_savings = round(monthly_cost * 0.75, 2)
            recommendations.append(
                f"HIGH: Compute cluster running 24/7 instead of on-demand (costs ${monthly_cost:.2f}/month). "
                f"Configure auto-scale to 0 nodes when idle (saves ${potential_savings:.2f}/month). "
                "Batch endpoints should only spin up compute when jobs are submitted."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Oversized cluster (too many nodes for workload)
        if instance_count > 10 and monthly_jobs < 5:  # High node count, low job frequency
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Reduce cluster size saves ~50% cost
            potential_savings = round(monthly_cost * 0.5, 2)
            recommendations.append(
                f"HIGH: Oversized cluster ({instance_count} nodes) for low workload ({monthly_jobs} jobs/month). "
                f"Reduce max cluster size to 3-5 nodes (saves ${potential_savings:.2f}/month). "
                "Right-size your cluster based on actual batch job requirements."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - No job scheduling optimization
        if monthly_jobs > 20:  # High job frequency
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Better scheduling saves ~20% cost
            potential_savings = round(monthly_cost * 0.2, 2)
            recommendations.append(
                f"MEDIUM: High batch job frequency ({monthly_jobs} jobs/month) without scheduling optimization. "
                f"Implement job batching and off-peak scheduling (saves ${potential_savings:.2f}/month). "
                "Schedule non-urgent jobs during low-cost hours to optimize compute usage."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - Missing cost allocation tags
        is_optimizable = True
        optimization_score = 30
        priority = "low"
        potential_savings = 0.0  # No direct savings, but improves cost tracking
        recommendations.append(
            f"LOW: Batch endpoint lacks cost allocation tags (costs ${monthly_cost:.2f}/month). "
            "Add tags for project, team, and environment to improve cost attribution. "
            "Proper tagging enables chargeback and budget tracking."
        )
        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_automation_accounts(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Automation Accounts for cost intelligence.

        This scans all Automation Accounts across all resource groups in the specified region
        and analyzes their runbooks, jobs, and schedules for optimization opportunities.

        Pricing:
            - Process automation: $0.002/minute runtime
            - Update management: $5/month per managed machine
            - Configuration management: $6/month per node
            - Storage: ~$1/month for job logs

        Typical monthly cost: $5-100/month

        Optimization scenarios:
            1. CRITICAL (90): Account active but 0 runbook executions for 90+ days
            2. HIGH (75): Scheduled runbooks disabled for 60+ days
            3. HIGH (70): Multiple accounts in same region (consolidation opportunity)
            4. MEDIUM (50): Runbooks with high failure rate (>30% wasted executions)
            5. LOW (30): No job monitoring configured

        Returns:
            List of AllCloudResourceData objects with is_optimizable=True and optimization scores.
        """
        resources = []

        try:
            from azure.mgmt.automation import AutomationClient
        except ImportError:
            logger.error("azure-mgmt-automation not installed - cannot scan Automation Accounts")
            return resources

        try:
            credential = DefaultAzureCredential()
            automation_client = AutomationClient(credential, self.subscription_id)

            # Iterate through all resource groups
            for rg_name in await self._get_resource_group_names():
                try:
                    # List all Automation Accounts in resource group
                    accounts = automation_client.automation_account.list_by_resource_group(rg_name)

                    for account in accounts:
                        try:
                            # Filter by region
                            account_region = account.location.lower().replace(" ", "")
                            normalized_region = region.lower().replace(" ", "")
                            if account_region != normalized_region:
                                continue

                            account_name = account.name
                            state = account.state if hasattr(account, 'state') else "unknown"

                            # Get runbooks count
                            runbooks = list(automation_client.runbook.list_by_automation_account(rg_name, account_name))
                            runbook_count = len(runbooks)

                            # Count enabled/disabled runbooks
                            enabled_runbooks = sum(1 for rb in runbooks if hasattr(rb, 'state') and rb.state.lower() == 'published')
                            disabled_runbooks = runbook_count - enabled_runbooks

                            # Get schedules count
                            try:
                                schedules = list(automation_client.schedule.list_by_automation_account(rg_name, account_name))
                                schedule_count = len(schedules)
                                disabled_schedules = sum(1 for s in schedules if hasattr(s, 'is_enabled') and not s.is_enabled)
                            except:
                                schedule_count = 0
                                disabled_schedules = 0

                            # Estimate monthly executions (placeholder - real implementation would query job history)
                            # For MVP, assume some baseline activity
                            estimated_monthly_executions = enabled_runbooks * 30  # 1 execution per day per runbook

                            # Estimate failure rate
                            failure_rate_percent = 0  # Placeholder

                            # Calculate cost
                            monthly_cost = self._calculate_automation_account_cost(
                                runbook_count, estimated_monthly_executions
                            )

                            # Calculate optimization
                            (
                                is_optimizable,
                                optimization_score,
                                priority,
                                potential_savings,
                                recommendations,
                            ) = self._calculate_automation_account_optimization(
                                runbook_count,
                                enabled_runbooks,
                                disabled_runbooks,
                                schedule_count,
                                disabled_schedules,
                                estimated_monthly_executions,
                                failure_rate_percent,
                                monthly_cost,
                            )

                            resource = AllCloudResourceData(
                                resource_id=account.id,
                                resource_type="azure_automation_account",
                                resource_name=account_name,
                                region=account.location,
                                is_orphan=False,  # Automation accounts cannot be orphan
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata={
                                    "resource_group": rg_name,
                                    "state": state,
                                    "runbook_count": runbook_count,
                                    "enabled_runbooks": enabled_runbooks,
                                    "disabled_runbooks": disabled_runbooks,
                                    "schedule_count": schedule_count,
                                    "disabled_schedules": disabled_schedules,
                                    "estimated_monthly_executions": estimated_monthly_executions,
                                    "failure_rate_percent": failure_rate_percent,
                                },
                                last_used_at=None,
                                created_at_cloud=None,
                            )

                            resources.append(resource)
                            logger.info(
                                f"Scanned Azure Automation Account: {account_name} in {account.location} "
                                f"(runbooks={runbook_count}, schedules={schedule_count}, optimizable={is_optimizable}, cost=${monthly_cost:.2f}/month)"
                            )

                        except Exception as e:
                            logger.error(
                                f"Error scanning Automation Account {account.name}: {e}",
                                exc_info=True,
                            )
                            continue

                except Exception as e:
                    logger.error(
                        f"Error listing Automation Accounts in resource group {rg_name}: {e}",
                        exc_info=True,
                    )
                    continue

        except Exception as e:
            logger.error(f"Error scanning Azure Automation Accounts: {e}", exc_info=True)

        return resources

    def _calculate_automation_account_cost(
        self, runbook_count: int, monthly_executions: int
    ) -> float:
        """
        Calculate monthly cost for Azure Automation Account.

        Pricing breakdown (as of 2025):
            - Process automation: $0.002/minute runtime
            - Assume average runbook runtime: 5 minutes
            - Storage: ~$1/month for job logs
            - Update management: $5/month (if used)

        Returns:
            Estimated monthly cost in USD
        """
        # Process automation cost
        avg_runtime_minutes = 5.0
        cost_per_minute = 0.002
        execution_cost = monthly_executions * avg_runtime_minutes * cost_per_minute

        # Storage cost for job logs
        storage_cost = 1.0

        # Base update management cost (if any runbooks exist)
        update_mgmt_cost = 5.0 if runbook_count > 0 else 0.0

        total_cost = execution_cost + storage_cost + update_mgmt_cost
        return round(total_cost, 2)

    def _calculate_automation_account_optimization(
        self,
        runbook_count: int,
        enabled_runbooks: int,
        disabled_runbooks: int,
        schedule_count: int,
        disabled_schedules: int,
        monthly_executions: int,
        failure_rate_percent: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Azure Automation Accounts.

        5 optimization scenarios (ordered by severity):
            1. CRITICAL (90): Account active but 0 runbook executions for 90+ days
            2. HIGH (75): Scheduled runbooks disabled for 60+ days
            3. HIGH (70): Multiple accounts in same region (consolidation opportunity)
            4. MEDIUM (50): Runbooks with high failure rate (>30%)
            5. LOW (30): No job monitoring configured

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations: list[str] = []

        # Scenario 1: CRITICAL - Account active but 0 runbook executions for 90+ days
        if monthly_executions == 0 and runbook_count > 0:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost  # Save full cost by deleting account
            recommendations.append(
                f"CRITICAL: Automation Account has {runbook_count} runbook(s) but 0 executions for 90+ days (costs ${monthly_cost:.2f}/month). "
                f"Consider deleting the account or archiving unused runbooks. "
                "Review automation requirements and runbook lifecycle."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Scheduled runbooks disabled for 60+ days
        if disabled_schedules > 5:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            # Cleanup saves ~50% cost
            potential_savings = round(monthly_cost * 0.5, 2)
            recommendations.append(
                f"HIGH: {disabled_schedules} schedule(s) disabled for 60+ days (costs ${monthly_cost:.2f}/month). "
                f"Delete unused schedules and associated runbooks (saves ${potential_savings:.2f}/month). "
                "Clean up deprecated automation workflows."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Too many disabled runbooks (consolidation opportunity)
        if disabled_runbooks > enabled_runbooks and disabled_runbooks > 5:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Consolidate accounts saves ~30% cost
            potential_savings = round(monthly_cost * 0.3, 2)
            recommendations.append(
                f"HIGH: {disabled_runbooks} disabled runbooks vs {enabled_runbooks} enabled. "
                f"Archive or delete unused runbooks (saves ${potential_savings:.2f}/month). "
                "Consolidate active automation workflows into fewer accounts."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - High failure rate (wasted executions)
        if failure_rate_percent > 30:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Fix failures saves ~30% cost
            potential_savings = round(monthly_cost * 0.3, 2)
            recommendations.append(
                f"MEDIUM: High runbook failure rate ({failure_rate_percent}% failures). "
                f"Fix failing runbooks to reduce wasted executions (saves ${potential_savings:.2f}/month). "
                "Review error logs and improve runbook error handling."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - No job monitoring configured
        if schedule_count > 0:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = 0.0  # No direct savings, but improves reliability
            recommendations.append(
                f"LOW: {schedule_count} schedule(s) without job monitoring alerts (costs ${monthly_cost:.2f}/month). "
                "Configure Azure Monitor alerts for job failures and long-running jobs. "
                "Proactive monitoring prevents waste from failed automation workflows."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_advisor_recommendations(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Advisor cost optimization recommendations for cost intelligence.

        Azure Advisor is a FREE native Azure service that provides personalized recommendations
        to optimize Azure deployments (cost, security, reliability, performance).

        Pricing: Azure Advisor is FREE (no additional cost)

        Detection Logic:
        - Scans Advisor recommendations with category="Cost" (subscription-wide, not region-specific)
        - Groups recommendations by impacted resource ID
        - Analyzes recommendation impact level and age (how long ignored)
        - Recommendations marked as optimizable if not acted upon

        Waste Detection (5 scenarios):
        1. CRITICAL (90): Critical-impact recommendation ignored for 90+ days (potential high waste)
        2. HIGH (75): High-impact recommendation not acted on for 30+ days (significant waste)
        3. HIGH (70): Multiple recommendations for same resource (indicates systemic issues)
        4. MEDIUM (50): Medium-impact recommendation ignored for 60+ days (moderate waste)
        5. LOW (30): Low-impact recommendation not reviewed for 90+ days (minor waste)

        Returns:
            List of ALL Advisor cost recommendations as AllCloudResourceData objects
        """
        logger.info("inventory.scanning_advisor_recommendations", region=region)

        try:
            from azure.mgmt.advisor import AdvisorManagementClient
            from datetime import datetime, timezone

            advisor_client = AdvisorManagementClient(
                credential=self.credentials,
                subscription_id=self.subscription_id
            )

            all_recommendations: list[AllCloudResourceData] = []

            # List all cost optimization recommendations (subscription-wide)
            # Note: Advisor recommendations are not region-specific, they apply to entire subscription
            recommendations_list = list(
                advisor_client.recommendations.list(filter="Category eq 'Cost'")
            )

            logger.info(
                "inventory.advisor_recommendations_found",
                region=region,
                recommendation_count=len(recommendations_list)
            )

            for rec in recommendations_list:
                try:
                    # Extract recommendation properties
                    recommendation_id = rec.name or "unknown"
                    recommendation_type = rec.recommendation_type_id or "unknown"
                    impacted_resource_id = rec.impacted_value or "unknown"
                    impacted_resource_type = rec.impacted_field or "unknown"
                    short_description = rec.short_description.problem if rec.short_description else "No description"
                    impact_level = rec.impact.lower() if rec.impact else "low"  # High, Medium, Low
                    last_updated = rec.last_updated

                    # Calculate age of recommendation (how long it's been ignored)
                    if last_updated:
                        age_days = (datetime.now(timezone.utc) - last_updated).days
                    else:
                        age_days = 0

                    # Estimate potential savings from extended_properties
                    potential_savings_amount = 0.0
                    currency = "USD"
                    if rec.extended_properties and "annualSavingsAmount" in rec.extended_properties:
                        try:
                            potential_savings_amount = float(rec.extended_properties["annualSavingsAmount"])
                            currency = rec.extended_properties.get("savingsCurrency", "USD")
                        except (ValueError, TypeError):
                            pass

                    # Monthly savings (annualSavingsAmount / 12)
                    monthly_savings = potential_savings_amount / 12.0 if potential_savings_amount > 0 else 10.0

                    # Determine optimization status
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        optimization_savings,
                        recommendations_list_output,
                    ) = self._calculate_advisor_recommendation_optimization(
                        impact_level=impact_level,
                        age_days=age_days,
                        monthly_savings=monthly_savings,
                        short_description=short_description,
                    )

                    # Create AllCloudResourceData
                    resource_data = AllCloudResourceData(
                        resource_id=recommendation_id,
                        resource_type="azure_advisor_recommendation",
                        resource_name=f"Advisor: {short_description[:50]}...",
                        region=region,  # Not region-specific, but required field
                        estimated_monthly_cost=monthly_savings,
                        currency=currency,
                        is_orphan=False,  # Advisor recommendations are not orphans
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_priority=priority,
                        potential_monthly_savings=optimization_savings,
                        optimization_recommendations=recommendations_list_output,
                        resource_metadata={
                            "recommendation_id": recommendation_id,
                            "recommendation_type": recommendation_type,
                            "impacted_resource_id": impacted_resource_id,
                            "impacted_resource_type": impacted_resource_type,
                            "short_description": short_description,
                            "impact_level": impact_level,
                            "age_days": age_days,
                            "last_updated": last_updated.isoformat() if last_updated else None,
                            "annual_savings_amount": potential_savings_amount,
                        },
                    )
                    all_recommendations.append(resource_data)

                except Exception as e:
                    logger.error(
                        "inventory.advisor_recommendation_processing_error",
                        recommendation_id=getattr(rec, "name", "unknown"),
                        error=str(e),
                    )
                    continue

            logger.info(
                "inventory.advisor_recommendations_scan_complete",
                region=region,
                total_recommendations=len(all_recommendations),
                optimizable_count=sum(1 for r in all_recommendations if r.is_optimizable),
            )

            return all_recommendations

        except ImportError:
            logger.error(
                "inventory.missing_advisor_sdk",
                region=region,
                error="azure-mgmt-advisor package not installed",
            )
            return []
        except Exception as e:
            logger.error("inventory.advisor_recommendations_scan_error", region=region, error=str(e))
            return []

    def _calculate_advisor_recommendation_optimization(
        self,
        impact_level: str,
        age_days: int,
        monthly_savings: float,
        short_description: str,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization score for Azure Advisor cost recommendations.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Critical-impact recommendation ignored for 90+ days
        if impact_level == "high" and age_days >= 90:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_savings
            recommendations.append(
                f"CRITICAL: High-impact Advisor recommendation ignored for {age_days} days (potential ${monthly_savings:.2f}/month savings). "
                f"Recommendation: {short_description}. "
                "Act immediately to implement this cost optimization. Long-ignored high-impact recommendations indicate significant waste."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - High-impact recommendation not acted on for 30+ days
        if impact_level == "high" and age_days >= 30:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_savings
            recommendations.append(
                f"HIGH: High-impact Advisor recommendation pending for {age_days} days (saves ${monthly_savings:.2f}/month). "
                f"Recommendation: {short_description}. "
                "Implement this recommendation to realize significant cost savings."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Medium-impact recommendation ignored for 60+ days
        if impact_level == "medium" and age_days >= 60:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = monthly_savings
            recommendations.append(
                f"HIGH: Medium-impact Advisor recommendation ignored for {age_days} days (saves ${monthly_savings:.2f}/month). "
                f"Recommendation: {short_description}. "
                "Review and implement to avoid continued waste. Extended inaction increases cumulative cost."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Medium-impact recommendation pending for 30+ days
        if impact_level == "medium" and age_days >= 30:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            potential_savings = monthly_savings
            recommendations.append(
                f"MEDIUM: Medium-impact Advisor recommendation pending for {age_days} days (saves ${monthly_savings:.2f}/month). "
                f"Recommendation: {short_description}. "
                "Schedule implementation to reduce ongoing costs."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - Low-impact recommendation not reviewed for 90+ days
        if impact_level == "low" and age_days >= 90:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = monthly_savings
            recommendations.append(
                f"LOW: Low-impact Advisor recommendation pending for {age_days} days (saves ${monthly_savings:.2f}/month). "
                f"Recommendation: {short_description}. "
                "Review recommendation and decide whether to implement or suppress."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_arm_deployments(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure ARM Template Deployments for cost intelligence.

        ARM (Azure Resource Manager) templates are JSON files that define infrastructure as code.
        Failed or long-running deployments can waste resources (compute time, storage for logs).

        Pricing: ARM deployments are FREE, but failed deployments waste engineer time and resources
        Estimated cost: $50-200/hour of engineer time wasted debugging failed deployments

        Detection Logic:
        - Scans ARM deployments at resource group level (region-specific)
        - Analyzes deployment status (Succeeded, Failed, Running)
        - Calculates deployment duration and identifies long-running deployments
        - Detects repeated failed deployments (same template failing multiple times)

        Waste Detection (5 scenarios):
        1. CRITICAL (90): Failed deployment running for 90+ days (indicates abandoned infrastructure)
        2. HIGH (75): Multiple failed deployments of same template in 30d (systemic issue)
        3. HIGH (70): Running deployment stuck for 7+ days (blocked deployment)
        4. MEDIUM (50): Failed deployment not cleaned up for 30+ days (technical debt)
        5. LOW (30): Successful deployment with warnings or issues (minor inefficiencies)

        Returns:
            List of ALL ARM deployments as AllCloudResourceData objects
        """
        logger.info("inventory.scanning_arm_deployments", region=region)

        try:
            from azure.mgmt.resource import ResourceManagementClient
            from datetime import datetime, timezone

            resource_client = ResourceManagementClient(
                credential=self.credentials,
                subscription_id=self.subscription_id
            )

            all_deployments: list[AllCloudResourceData] = []

            # List all resource groups (to scan deployments per RG)
            resource_groups = list(resource_client.resource_groups.list())

            # Filter resource groups by region
            region_rgs = [rg for rg in resource_groups if rg.location == region]

            logger.info(
                "inventory.arm_deployments_resource_groups_found",
                region=region,
                resource_group_count=len(region_rgs)
            )

            for rg in region_rgs:
                try:
                    rg_name = rg.name

                    # List deployments in this resource group
                    deployments_list = list(
                        resource_client.deployments.list_by_resource_group(
                            resource_group_name=rg_name
                        )
                    )

                    for deployment in deployments_list:
                        try:
                            deployment_name = deployment.name or "unknown"
                            deployment_id = deployment.id or "unknown"
                            deployment_state = deployment.properties.provisioning_state if deployment.properties else "Unknown"
                            timestamp = deployment.properties.timestamp if deployment.properties else None
                            duration = deployment.properties.duration if deployment.properties else None
                            mode = safe_get_value(deployment.properties.mode if deployment.properties else None, "Unknown")

                            # Calculate age of deployment
                            if timestamp:
                                age_days = (datetime.now(timezone.utc) - timestamp).days
                            else:
                                age_days = 0

                            # Parse duration (ISO 8601 format like "PT1H30M")
                            duration_seconds = 0
                            if duration:
                                try:
                                    # Simple parse for PT format (e.g., PT1H30M5S)
                                    import re
                                    hours = re.search(r'(\d+)H', duration)
                                    minutes = re.search(r'(\d+)M', duration)
                                    seconds = re.search(r'(\d+)S', duration)
                                    duration_seconds = (
                                        (int(hours.group(1)) * 3600 if hours else 0) +
                                        (int(minutes.group(1)) * 60 if minutes else 0) +
                                        (int(seconds.group(1)) if seconds else 0)
                                    )
                                except Exception:
                                    duration_seconds = 0

                            # Estimate cost (engineer time wasted debugging failed deployments)
                            # Failed deployment: $100/hour * hours wasted
                            # Assume 2 hours wasted per failed deployment
                            engineer_hourly_rate = 100.0
                            hours_wasted = 2.0 if deployment_state == "Failed" else 0.0
                            estimated_cost = engineer_hourly_rate * hours_wasted

                            # Monthly cost (amortized over 1 month if still present)
                            monthly_cost = estimated_cost if age_days <= 30 else 0.0

                            # Determine optimization status
                            (
                                is_optimizable,
                                optimization_score,
                                priority,
                                optimization_savings,
                                recommendations_list,
                            ) = self._calculate_arm_deployment_optimization(
                                deployment_state=deployment_state,
                                age_days=age_days,
                                duration_seconds=duration_seconds,
                                monthly_cost=monthly_cost,
                                deployment_name=deployment_name,
                            )

                            # Create AllCloudResourceData
                            resource_data = AllCloudResourceData(
                                resource_id=deployment_id,
                                resource_type="azure_arm_deployment",
                                resource_name=f"ARM: {deployment_name}",
                                region=region,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                is_orphan=False,  # ARM deployments are not orphans
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=optimization_savings,
                                optimization_recommendations=recommendations_list,
                                resource_metadata={
                                    "deployment_id": deployment_id,
                                    "deployment_name": deployment_name,
                                    "resource_group": rg_name,
                                    "provisioning_state": deployment_state,
                                    "deployment_mode": mode,
                                    "timestamp": timestamp.isoformat() if timestamp else None,
                                    "duration": duration,
                                    "duration_seconds": duration_seconds,
                                    "age_days": age_days,
                                },
                            )
                            all_deployments.append(resource_data)

                        except Exception as e:
                            logger.error(
                                "inventory.arm_deployment_processing_error",
                                deployment_name=getattr(deployment, "name", "unknown"),
                                error=str(e),
                            )
                            continue

                except Exception as e:
                    logger.error(
                        "inventory.arm_deployments_rg_error",
                        resource_group=getattr(rg, "name", "unknown"),
                        error=str(e),
                    )
                    continue

            logger.info(
                "inventory.arm_deployments_scan_complete",
                region=region,
                total_deployments=len(all_deployments),
                optimizable_count=sum(1 for r in all_deployments if r.is_optimizable),
            )

            return all_deployments

        except ImportError:
            logger.error(
                "inventory.missing_resource_sdk",
                region=region,
                error="azure-mgmt-resource package not installed",
            )
            return []
        except Exception as e:
            logger.error("inventory.arm_deployments_scan_error", region=region, error=str(e))
            return []

    def _calculate_arm_deployment_optimization(
        self,
        deployment_state: str,
        age_days: int,
        duration_seconds: int,
        monthly_cost: float,
        deployment_name: str,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization score for Azure ARM Template Deployments.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Failed deployment abandoned for 90+ days
        if deployment_state == "Failed" and age_days >= 90:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append(
                f"CRITICAL: Failed ARM deployment '{deployment_name}' abandoned for {age_days} days (wasted ${monthly_cost:.2f} in engineer time). "
                "Delete this failed deployment to clean up technical debt. "
                "Long-abandoned failed deployments indicate process issues and waste resources."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Multiple failed deployments (same template)
        # Note: This scenario requires tracking multiple deployments, simplified here
        if deployment_state == "Failed" and age_days >= 30:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.append(
                f"HIGH: Failed ARM deployment '{deployment_name}' not cleaned up for {age_days} days (wasted ${monthly_cost:.2f}). "
                "Investigate root cause of failure and delete failed deployment. "
                "Repeated failures indicate systemic infrastructure issues."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Running deployment stuck for 7+ days
        if deployment_state == "Running" and age_days >= 7:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = monthly_cost * 2  # Stuck deployments block other work
            recommendations.append(
                f"HIGH: ARM deployment '{deployment_name}' stuck in Running state for {age_days} days. "
                "Cancel this stuck deployment and investigate what's blocking it. "
                "Long-running deployments may indicate timeouts, dependency issues, or resource locks."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Failed deployment not cleaned up for 7+ days
        if deployment_state == "Failed" and age_days >= 7:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            potential_savings = monthly_cost
            recommendations.append(
                f"MEDIUM: Failed ARM deployment '{deployment_name}' not cleaned up for {age_days} days (wasted ${monthly_cost:.2f}). "
                "Delete this failed deployment and fix the underlying issue. "
                "Failed deployments create technical debt and clutter deployment history."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - Very long deployment duration (>1 hour)
        if deployment_state == "Succeeded" and duration_seconds > 3600:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = 10.0  # Small optimization potential
            recommendations.append(
                f"LOW: ARM deployment '{deployment_name}' took {duration_seconds/60:.1f} minutes to complete. "
                "Review template for optimization opportunities (parallel deployments, smaller batches). "
                "Long deployment times slow down development velocity."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_container_instances(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Container Instances (ACI) for cost intelligence.

        Azure Container Instances (ACI) is a serverless container runtime (like AWS Fargate).
        Charges are per-second based on CPU cores and memory allocated.

        Pricing: ~$0.0000012/vCPU-second + ~$0.00000013/GB-second
        Example: 1 vCPU + 1.5 GB = ~$40/month if running 24/7

        Detection Logic:
        - Scans all container groups (ACI deployment unit)
        - Analyzes container state (Running, Stopped, Succeeded, Failed)
        - Detects idle containers (Stopped but still allocated resources)
        - Identifies failed containers (exited with errors)
        - Calculates runtime and cost

        Waste Detection (5 scenarios):
        1. CRITICAL (90): Stopped container allocated for 90+ days (abandoned infrastructure)
        2. HIGH (75): Failed container not cleaned up for 30+ days (technical debt)
        3. HIGH (70): Running container with no restarts in 90d (potentially idle)
        4. MEDIUM (50): Succeeded container not deleted for 7+ days (completed job)
        5. LOW (30): Over-provisioned container (high CPU/memory, low utilization)

        Returns:
            List of ALL container instances as AllCloudResourceData objects
        """
        logger.info("inventory.scanning_container_instances", region=region)

        try:
            from azure.mgmt.containerinstance import ContainerInstanceManagementClient
            from datetime import datetime, timezone

            aci_client = ContainerInstanceManagementClient(
                credential=self.credentials,
                subscription_id=self.subscription_id
            )

            all_container_groups: list[AllCloudResourceData] = []

            # List all resource groups (to scan container groups per RG)
            from azure.mgmt.resource import ResourceManagementClient
            resource_client = ResourceManagementClient(
                credential=self.credentials,
                subscription_id=self.subscription_id
            )

            resource_groups = list(resource_client.resource_groups.list())

            # Filter resource groups by region
            region_rgs = [rg for rg in resource_groups if rg.location == region]

            logger.info(
                "inventory.container_instances_resource_groups_found",
                region=region,
                resource_group_count=len(region_rgs)
            )

            for rg in region_rgs:
                try:
                    rg_name = rg.name

                    # List container groups in this resource group
                    container_groups_list = list(
                        aci_client.container_groups.list_by_resource_group(
                            resource_group_name=rg_name
                        )
                    )

                    for cg in container_groups_list:
                        try:
                            cg_name = cg.name or "unknown"
                            cg_id = cg.id or "unknown"
                            cg_location = cg.location or region
                            provisioning_state = cg.provisioning_state or "Unknown"
                            instance_view = cg.instance_view

                            # Container state (from first container in group)
                            container_state = "Unknown"
                            restart_count = 0
                            if cg.containers and len(cg.containers) > 0:
                                first_container = cg.containers[0]
                                if hasattr(first_container, 'instance_view') and first_container.instance_view:
                                    container_state = first_container.instance_view.current_state.state if first_container.instance_view.current_state else "Unknown"
                                    restart_count = first_container.instance_view.restart_count if hasattr(first_container.instance_view, 'restart_count') else 0

                            # OS type and resources
                            os_type = safe_get_value(cg.os_type, "Linux")
                            cpu_count = 0.0
                            memory_gb = 0.0
                            if cg.containers and len(cg.containers) > 0:
                                for container in cg.containers:
                                    if container.resources and container.resources.requests:
                                        cpu_count += container.resources.requests.cpu or 0.0
                                        memory_gb += container.resources.requests.memory_in_gb or 0.0

                            # Calculate age (creation time)
                            # Note: ACI doesn't expose creation timestamp directly, use resource tags or assume recent
                            age_days = 30  # Default assumption for orphan detection

                            # Calculate monthly cost
                            # Pricing: $0.0000012/vCPU-second + $0.00000013/GB-second (US pricing)
                            cpu_cost_per_second = 0.0000012
                            memory_cost_per_second = 0.00000013
                            seconds_per_month = 30 * 24 * 3600  # ~2.6M seconds

                            # Only charge if container is running or stopped (not succeeded/failed)
                            if container_state in ["Running", "Stopped"]:
                                monthly_cost = (
                                    (cpu_count * cpu_cost_per_second * seconds_per_month) +
                                    (memory_gb * memory_cost_per_second * seconds_per_month)
                                )
                            else:
                                monthly_cost = 0.0

                            # Determine optimization status
                            (
                                is_optimizable,
                                optimization_score,
                                priority,
                                optimization_savings,
                                recommendations_list,
                            ) = self._calculate_container_instance_optimization(
                                container_state=container_state,
                                age_days=age_days,
                                restart_count=restart_count,
                                monthly_cost=monthly_cost,
                                cg_name=cg_name,
                                cpu_count=cpu_count,
                                memory_gb=memory_gb,
                            )

                            # Create AllCloudResourceData
                            resource_data = AllCloudResourceData(
                                resource_id=cg_id,
                                resource_type="azure_container_instance",
                                resource_name=f"ACI: {cg_name}",
                                region=cg_location,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                is_orphan=False,  # Will be determined by optimization logic
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=optimization_savings,
                                optimization_recommendations=recommendations_list,
                                resource_metadata={
                                    "container_group_id": cg_id,
                                    "container_group_name": cg_name,
                                    "resource_group": rg_name,
                                    "provisioning_state": provisioning_state,
                                    "container_state": container_state,
                                    "os_type": os_type,
                                    "cpu_count": cpu_count,
                                    "memory_gb": memory_gb,
                                    "restart_count": restart_count,
                                    "container_count": len(cg.containers) if cg.containers else 0,
                                    "age_days": age_days,
                                },
                            )
                            all_container_groups.append(resource_data)

                        except Exception as e:
                            logger.error(
                                "inventory.container_instance_processing_error",
                                container_group_name=getattr(cg, "name", "unknown"),
                                error=str(e),
                            )
                            continue

                except Exception as e:
                    logger.error(
                        "inventory.container_instances_rg_error",
                        resource_group=getattr(rg, "name", "unknown"),
                        error=str(e),
                    )
                    continue

            logger.info(
                "inventory.container_instances_scan_complete",
                region=region,
                total_container_groups=len(all_container_groups),
                optimizable_count=sum(1 for r in all_container_groups if r.is_optimizable),
            )

            return all_container_groups

        except ImportError:
            logger.error(
                "inventory.missing_container_instance_sdk",
                region=region,
                error="azure-mgmt-containerinstance package not installed",
            )
            return []
        except Exception as e:
            logger.error("inventory.container_instances_scan_error", region=region, error=str(e))
            return []

    def _calculate_container_instance_optimization(
        self,
        container_state: str,
        age_days: int,
        restart_count: int,
        monthly_cost: float,
        cg_name: str,
        cpu_count: float,
        memory_gb: float,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization score for Azure Container Instances.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Stopped container allocated for 90+ days (orphan)
        if container_state == "Stopped" and age_days >= 90:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append(
                f"CRITICAL: Container group '{cg_name}' stopped for {age_days} days but still allocated (wastes ${monthly_cost:.2f}/month). "
                "Delete this abandoned container group immediately. "
                "Long-stopped containers indicate forgotten infrastructure and waste resources."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Failed container not cleaned up for 30+ days
        if container_state == "Failed" and age_days >= 30:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.append(
                f"HIGH: Container group '{cg_name}' failed {age_days} days ago but not cleaned up (wasted ${monthly_cost:.2f}). "
                "Delete this failed container group and investigate the failure. "
                "Failed containers indicate process issues and waste resources."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Running container with no restarts in 90 days (potentially idle)
        if container_state == "Running" and restart_count == 0 and age_days >= 90:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = monthly_cost * 0.5  # Assume 50% waste
            recommendations.append(
                f"HIGH: Container group '{cg_name}' running for {age_days} days with 0 restarts (costs ${monthly_cost:.2f}/month). "
                "Verify this container is actually doing work. No restarts may indicate idle container. "
                "Consider monitoring CPU/memory utilization or switching to event-driven architecture."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Succeeded container not deleted for 7+ days
        if container_state == "Succeeded" and age_days >= 7:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            potential_savings = monthly_cost
            recommendations.append(
                f"MEDIUM: Container group '{cg_name}' completed successfully {age_days} days ago but not deleted (costs ${monthly_cost:.2f}/month). "
                "Delete this completed container group to free resources. "
                "Succeeded containers from batch jobs should be cleaned up automatically."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - Over-provisioned container (high CPU/memory)
        if cpu_count >= 4 or memory_gb >= 8:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = monthly_cost * 0.3  # Assume 30% over-provisioning
            recommendations.append(
                f"LOW: Container group '{cg_name}' provisioned with {cpu_count} vCPU / {memory_gb:.1f} GB (costs ${monthly_cost:.2f}/month). "
                "Review actual resource utilization to detect over-provisioning. "
                "Right-sizing containers can reduce costs by 30-50%. Consider Azure Monitor metrics."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_batch_jobs(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Batch Jobs for cost intelligence.

        Azure Batch is a cloud-based job scheduling service for large-scale parallel
        and high-performance computing (HPC) workloads.

        Pricing: Batch service is FREE, but you pay for compute nodes
        - VM pricing: $0.10-2.00/hour per node (depending on VM size)
        - Example: 10 nodes  Standard_D2s_v3  24h = ~$40/day

        Detection Logic:
        - Scans all Batch Accounts in subscription
        - Lists jobs (Active, Completed, Failed) per account
        - Analyzes pool allocation and node usage
        - Detects abandoned/stuck jobs with active pools
        - Calculates compute waste from idle nodes

        Waste Detection (5 scenarios):
        1. CRITICAL (90): Failed job with active pool for 90+ days (abandoned infrastructure)
        2. HIGH (75): Multiple failed jobs on same pool in 30d (systemic issue)
        3. HIGH (70): Active job stuck for 7+ days (blocked job, wasting resources)
        4. MEDIUM (50): Completed job with active pool for 7+ days (cleanup missing)
        5. LOW (30): Pool over-provisioned (more nodes than needed for workload)

        Returns:
            List of ALL Batch jobs as AllCloudResourceData objects
        """
        logger.info("inventory.scanning_batch_jobs", region=region)

        try:
            from azure.mgmt.batch import BatchManagementClient
            from datetime import datetime, timezone

            batch_client = BatchManagementClient(
                credential=self.credentials,
                subscription_id=self.subscription_id
            )

            all_batch_jobs: list[AllCloudResourceData] = []

            # List all Batch accounts in subscription
            batch_accounts = list(batch_client.batch_account.list())

            # Filter by region
            region_accounts = [acc for acc in batch_accounts if acc.location == region]

            logger.info(
                "inventory.batch_accounts_found",
                region=region,
                batch_account_count=len(region_accounts)
            )

            for account in region_accounts:
                try:
                    account_name = account.name or "unknown"
                    account_id = account.id or "unknown"
                    resource_group = account.id.split('/')[4] if account.id else "unknown"

                    # List pools in this Batch account
                    try:
                        pools_list = list(
                            batch_client.pool.list_by_batch_account(
                                resource_group_name=resource_group,
                                account_name=account_name
                            )
                        )
                    except Exception as e:
                        logger.warning(
                            "inventory.batch_pools_list_error",
                            account=account_name,
                            error=str(e)
                        )
                        pools_list = []

                    # Since we can't easily list individual jobs via Management API,
                    # we analyze pools as proxy for job activity
                    # Note: Full job details require Batch DataPlane API (azure.batch)
                    for pool in pools_list:
                        try:
                            pool_name = pool.name or "unknown"
                            pool_id = pool.id or "unknown"
                            allocation_state = pool.allocation_state if hasattr(pool, 'allocation_state') else "unknown"
                            vm_size = pool.vm_size if hasattr(pool, 'vm_size') else "unknown"

                            # Pool scale settings
                            target_dedicated_nodes = 0
                            target_low_priority_nodes = 0
                            current_dedicated_nodes = 0
                            current_low_priority_nodes = 0

                            if hasattr(pool, 'scale_settings') and pool.scale_settings:
                                if hasattr(pool.scale_settings, 'fixed_scale') and pool.scale_settings.fixed_scale:
                                    target_dedicated_nodes = pool.scale_settings.fixed_scale.target_dedicated_nodes or 0
                                    target_low_priority_nodes = pool.scale_settings.fixed_scale.target_low_priority_nodes or 0

                            if hasattr(pool, 'current_dedicated_nodes'):
                                current_dedicated_nodes = pool.current_dedicated_nodes or 0
                            if hasattr(pool, 'current_low_priority_nodes'):
                                current_low_priority_nodes = pool.current_low_priority_nodes or 0

                            total_nodes = current_dedicated_nodes + current_low_priority_nodes

                            # Estimate age (creation time)
                            creation_time = pool.creation_time if hasattr(pool, 'creation_time') else None
                            if creation_time:
                                age_days = (datetime.now(timezone.utc) - creation_time).days
                            else:
                                age_days = 30  # Default assumption

                            # Last modified time
                            last_modified = pool.last_modified if hasattr(pool, 'last_modified') else None

                            # Estimate monthly cost based on VM size and node count
                            # Simplified pricing (actual varies by region and VM size)
                            vm_hourly_cost = {
                                "standard_d2s_v3": 0.096,  # 2 vCPU, 8 GB
                                "standard_d4s_v3": 0.192,  # 4 vCPU, 16 GB
                                "standard_f2s_v2": 0.085,  # 2 vCPU, 4 GB
                                "standard_f4s_v2": 0.169,  # 4 vCPU, 8 GB
                            }

                            vm_size_normalized = vm_size.lower() if vm_size != "unknown" else "standard_d2s_v3"
                            hourly_cost_per_node = vm_hourly_cost.get(vm_size_normalized, 0.10)  # Default $0.10/hour

                            hours_per_month = 730  # ~30 days
                            monthly_cost = total_nodes * hourly_cost_per_node * hours_per_month

                            # Determine optimization status (using pool as proxy for job activity)
                            # Note: In real scenario, would use Batch DataPlane API to get actual job states
                            (
                                is_optimizable,
                                optimization_score,
                                priority,
                                optimization_savings,
                                recommendations_list,
                            ) = self._calculate_batch_job_optimization(
                                allocation_state=allocation_state,
                                age_days=age_days,
                                total_nodes=total_nodes,
                                target_nodes=target_dedicated_nodes + target_low_priority_nodes,
                                monthly_cost=monthly_cost,
                                pool_name=pool_name,
                                vm_size=vm_size,
                            )

                            # Create AllCloudResourceData
                            resource_data = AllCloudResourceData(
                                resource_id=pool_id,
                                resource_type="azure_batch_job",
                                resource_name=f"Batch Pool: {pool_name}",
                                region=region,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                is_orphan=False,  # Pools are not orphans (they're infrastructure)
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=optimization_savings,
                                optimization_recommendations=recommendations_list,
                                resource_metadata={
                                    "batch_account": account_name,
                                    "pool_id": pool_id,
                                    "pool_name": pool_name,
                                    "allocation_state": allocation_state,
                                    "vm_size": vm_size,
                                    "current_dedicated_nodes": current_dedicated_nodes,
                                    "current_low_priority_nodes": current_low_priority_nodes,
                                    "target_dedicated_nodes": target_dedicated_nodes,
                                    "target_low_priority_nodes": target_low_priority_nodes,
                                    "total_nodes": total_nodes,
                                    "age_days": age_days,
                                    "creation_time": creation_time.isoformat() if creation_time else None,
                                    "last_modified": last_modified.isoformat() if last_modified else None,
                                },
                            )
                            all_batch_jobs.append(resource_data)

                        except Exception as e:
                            logger.error(
                                "inventory.batch_pool_processing_error",
                                pool_name=getattr(pool, "name", "unknown"),
                                error=str(e),
                            )
                            continue

                except Exception as e:
                    logger.error(
                        "inventory.batch_account_error",
                        account=getattr(account, "name", "unknown"),
                        error=str(e),
                    )
                    continue

            logger.info(
                "inventory.batch_jobs_scan_complete",
                region=region,
                total_batch_pools=len(all_batch_jobs),
                optimizable_count=sum(1 for r in all_batch_jobs if r.is_optimizable),
            )

            return all_batch_jobs

        except ImportError:
            logger.error(
                "inventory.missing_batch_sdk",
                region=region,
                error="azure-mgmt-batch package not installed",
            )
            return []
        except Exception as e:
            logger.error("inventory.batch_jobs_scan_error", region=region, error=str(e))
            return []

    def _calculate_batch_job_optimization(
        self,
        allocation_state: str,
        age_days: int,
        total_nodes: int,
        target_nodes: int,
        monthly_cost: float,
        pool_name: str,
        vm_size: str,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization score for Azure Batch Jobs (using pool as proxy).

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Pool with nodes allocated for 90+ days (likely abandoned)
        if total_nodes > 0 and age_days >= 90:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append(
                f"CRITICAL: Batch pool '{pool_name}' with {total_nodes} node(s) allocated for {age_days} days (wastes ${monthly_cost:.2f}/month). "
                "This pool has been running for 90+ days, likely abandoned. Delete pool or reduce node allocation. "
                "Long-running Batch pools indicate forgotten infrastructure and significant waste."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Pool steady-state for 30+ days (not a typical batch workload)
        if allocation_state == "Steady" and total_nodes > 0 and age_days >= 30:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost * 0.7  # Assume 70% waste
            recommendations.append(
                f"HIGH: Batch pool '{pool_name}' in Steady state for {age_days} days with {total_nodes} node(s) (costs ${monthly_cost:.2f}/month). "
                "Batch pools should be ephemeral (created for jobs, deleted after). "
                "Consider switching to auto-scale or deleting pool when not in use. Potential savings: ${monthly_cost * 0.7:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Pool resizing (stuck in transition state)
        if allocation_state == "Resizing" and age_days >= 7:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = monthly_cost * 0.5  # Partial waste
            recommendations.append(
                f"HIGH: Batch pool '{pool_name}' stuck in Resizing state for {age_days} days (costs ${monthly_cost:.2f}/month). "
                "Pool may be stuck due to quota limits, networking issues, or API errors. "
                "Investigate and fix resizing issue, or delete and recreate pool."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Pool with idle nodes (current > target)
        if total_nodes > target_nodes and total_nodes > 0:
            excess_nodes = total_nodes - target_nodes
            excess_cost = (excess_nodes / total_nodes) * monthly_cost
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            potential_savings = excess_cost
            recommendations.append(
                f"MEDIUM: Batch pool '{pool_name}' has {excess_nodes} excess node(s) ({total_nodes} current vs {target_nodes} target, costs ${monthly_cost:.2f}/month). "
                "Pool is not scaling down properly. Review auto-scale formula or manually reduce nodes. "
                f"Removing excess nodes would save ${excess_cost:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - Large pool (potential over-provisioning)
        if total_nodes >= 10:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = monthly_cost * 0.2  # Assume 20% over-provisioning
            recommendations.append(
                f"LOW: Batch pool '{pool_name}' has {total_nodes} node(s) with VM size {vm_size} (costs ${monthly_cost:.2f}/month). "
                "Large pools should be monitored for utilization. "
                "Review job metrics to ensure nodes are fully utilized. Consider right-sizing VM or reducing node count."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_storage_lifecycle_policies(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Storage Lifecycle Management Policies for cost intelligence.

        Azure Storage Lifecycle Management allows automatic transition of blobs between
        access tiers (Hot  Cool  Archive) and automatic deletion based on rules.

        Pricing: Lifecycle Management is FREE (included with Azure Storage)
        BUT improper configuration leads to massive waste:
        - Hot tier: $0.018/GB/month
        - Cool tier: $0.010/GB/month (45% cheaper)
        - Archive tier: $0.002/GB/month (89% cheaper)

        Detection Logic:
        - Scans all Storage Accounts in subscription
        - Retrieves management policies (name='default')
        - Analyzes lifecycle rules for blob transitions
        - Detects missing or misconfigured policies
        - Calculates waste from data stored in wrong tier

        Waste Detection (5 scenarios):
        1. CRITICAL (90): No policy + >1TB data in Hot tier 90+ days (massive waste)
        2. HIGH (75): Policy exists but poorly configured (no Archive transition)
        3. HIGH (70): Large blobs in Hot tier >180 days with no access (should be Archive)
        4. MEDIUM (50): Transition to Cool delayed (>30d in Hot instead of Cool)
        5. LOW (30): Policy exists but suboptimal (can be improved)

        Returns:
            List of ALL Storage Accounts with lifecycle policy analysis as AllCloudResourceData objects
        """
        logger.info("inventory.scanning_storage_lifecycle_policies", region=region)

        try:
            from azure.mgmt.storage import StorageManagementClient
            from datetime import datetime, timezone

            storage_client = StorageManagementClient(
                credential=self.credentials,
                subscription_id=self.subscription_id
            )

            all_policies: list[AllCloudResourceData] = []

            # List all storage accounts
            storage_accounts = list(storage_client.storage_accounts.list())

            # Filter by region
            region_accounts = [acc for acc in storage_accounts if acc.location == region]

            logger.info(
                "inventory.storage_accounts_found",
                region=region,
                storage_account_count=len(region_accounts)
            )

            for account in region_accounts:
                try:
                    account_name = account.name or "unknown"
                    account_id = account.id or "unknown"
                    resource_group = account.id.split('/')[4] if account.id else "unknown"
                    sku_tier = safe_get_value(account.sku.tier if account.sku else None, "Standard")

                    # Try to get management policy (name is always 'default')
                    policy = None
                    has_policy = False
                    try:
                        policy = storage_client.management_policies.get(
                            resource_group_name=resource_group,
                            account_name=account_name,
                            management_policy_name='default'
                        )
                        has_policy = True
                    except Exception:
                        # No policy exists (404 is expected if no policy configured)
                        has_policy = False

                    # Estimate storage size (would need Azure Monitor API for actual usage)
                    # For now, assume moderate usage (100 GB) for cost calculation
                    estimated_storage_gb = 100.0

                    # Analyze policy configuration
                    policy_config_status = "none"
                    has_cool_transition = False
                    has_archive_transition = False
                    cool_transition_days = None
                    archive_transition_days = None

                    if has_policy and policy and hasattr(policy, 'policy') and policy.policy:
                        policy_config_status = "configured"
                        if hasattr(policy.policy, 'rules') and policy.policy.rules:
                            for rule in policy.policy.rules:
                                if not rule.enabled:
                                    continue

                                # Check for tier transitions in rule definition
                                if hasattr(rule, 'definition') and rule.definition:
                                    actions = rule.definition.actions if hasattr(rule.definition, 'actions') else None
                                    if actions and hasattr(actions, 'base_blob') and actions.base_blob:
                                        base_blob = actions.base_blob

                                        # Cool transition
                                        if hasattr(base_blob, 'tier_to_cool') and base_blob.tier_to_cool:
                                            has_cool_transition = True
                                            if hasattr(base_blob.tier_to_cool, 'days_after_modification_greater_than'):
                                                cool_transition_days = base_blob.tier_to_cool.days_after_modification_greater_than

                                        # Archive transition
                                        if hasattr(base_blob, 'tier_to_archive') and base_blob.tier_to_archive:
                                            has_archive_transition = True
                                            if hasattr(base_blob.tier_to_archive, 'days_after_modification_greater_than'):
                                                archive_transition_days = base_blob.tier_to_archive.days_after_modification_greater_than

                    # Calculate monthly cost (assume all data in Hot tier if no policy)
                    hot_tier_cost_per_gb = 0.018
                    cool_tier_cost_per_gb = 0.010
                    archive_tier_cost_per_gb = 0.002

                    # If no policy, all data in Hot tier
                    if not has_policy:
                        monthly_cost = estimated_storage_gb * hot_tier_cost_per_gb
                    else:
                        # With policy, assume 50% in Cool, 30% in Archive, 20% in Hot
                        monthly_cost = (
                            estimated_storage_gb * 0.2 * hot_tier_cost_per_gb +
                            estimated_storage_gb * 0.5 * cool_tier_cost_per_gb +
                            estimated_storage_gb * 0.3 * archive_tier_cost_per_gb
                        )

                    # Determine optimization status
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        optimization_savings,
                        recommendations_list,
                    ) = self._calculate_storage_lifecycle_optimization(
                        has_policy=has_policy,
                        has_cool_transition=has_cool_transition,
                        has_archive_transition=has_archive_transition,
                        cool_transition_days=cool_transition_days,
                        archive_transition_days=archive_transition_days,
                        estimated_storage_gb=estimated_storage_gb,
                        monthly_cost=monthly_cost,
                        account_name=account_name,
                    )

                    # Create AllCloudResourceData
                    resource_data = AllCloudResourceData(
                        resource_id=account_id,
                        resource_type="azure_storage_lifecycle_policy",
                        resource_name=f"Storage: {account_name}",
                        region=region,
                        estimated_monthly_cost=monthly_cost,
                        currency="USD",
                        is_orphan=False,  # Lifecycle policies are not orphans
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_priority=priority,
                        potential_monthly_savings=optimization_savings,
                        optimization_recommendations=recommendations_list,
                        resource_metadata={
                            "storage_account_id": account_id,
                            "storage_account_name": account_name,
                            "resource_group": resource_group,
                            "sku_tier": sku_tier,
                            "has_lifecycle_policy": has_policy,
                            "policy_config_status": policy_config_status,
                            "has_cool_transition": has_cool_transition,
                            "has_archive_transition": has_archive_transition,
                            "cool_transition_days": cool_transition_days,
                            "archive_transition_days": archive_transition_days,
                            "estimated_storage_gb": estimated_storage_gb,
                        },
                    )
                    all_policies.append(resource_data)

                except Exception as e:
                    logger.error(
                        "inventory.storage_lifecycle_policy_processing_error",
                        storage_account=getattr(account, "name", "unknown"),
                        error=str(e),
                    )
                    continue

            logger.info(
                "inventory.storage_lifecycle_policies_scan_complete",
                region=region,
                total_storage_accounts=len(all_policies),
                optimizable_count=sum(1 for r in all_policies if r.is_optimizable),
            )

            return all_policies

        except ImportError:
            logger.error(
                "inventory.missing_storage_sdk",
                region=region,
                error="azure-mgmt-storage package not installed",
            )
            return []
        except Exception as e:
            logger.error("inventory.storage_lifecycle_policies_scan_error", region=region, error=str(e))
            return []

    def _calculate_storage_lifecycle_optimization(
        self,
        has_policy: bool,
        has_cool_transition: bool,
        has_archive_transition: bool,
        cool_transition_days: int | None,
        archive_transition_days: int | None,
        estimated_storage_gb: float,
        monthly_cost: float,
        account_name: str,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization score for Azure Storage Lifecycle Policies.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - No policy + significant storage (>100 GB)
        if not has_policy and estimated_storage_gb >= 100:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            # Potential savings: transition 70% to Cool (45% cheaper) and 20% to Archive (89% cheaper)
            hot_cost = estimated_storage_gb * 0.018
            optimized_cost = (
                estimated_storage_gb * 0.1 * 0.018 +  # 10% Hot
                estimated_storage_gb * 0.7 * 0.010 +  # 70% Cool
                estimated_storage_gb * 0.2 * 0.002    # 20% Archive
            )
            potential_savings = hot_cost - optimized_cost
            recommendations.append(
                f"CRITICAL: Storage account '{account_name}' has NO lifecycle policy configured ({estimated_storage_gb:.0f} GB, costs ${monthly_cost:.2f}/month). "
                "All blobs likely in Hot tier (most expensive). Create lifecycle policy to transition old blobs to Cool (30d) and Archive (90d). "
                f"Implementing tiering would save ${potential_savings:.2f}/month (70% reduction)."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Policy exists but no Archive transition
        if has_policy and has_cool_transition and not has_archive_transition:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            # Assume 30% of data could move to Archive
            current_cool_cost = estimated_storage_gb * 0.3 * 0.010
            archive_cost = estimated_storage_gb * 0.3 * 0.002
            potential_savings = current_cool_cost - archive_cost
            recommendations.append(
                f"HIGH: Storage account '{account_name}' has lifecycle policy with Cool transition but NO Archive transition (costs ${monthly_cost:.2f}/month). "
                "Add Archive tier rules for blobs older than 180 days to maximize savings. "
                f"Archive tier is 80% cheaper than Cool. Potential savings: ${potential_savings:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Cool transition delayed (>60 days)
        if has_policy and has_cool_transition and cool_transition_days and cool_transition_days > 60:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Calculate waste from keeping data in Hot for too long
            extra_days = cool_transition_days - 30  # Optimal is 30 days
            extra_cost_per_gb = (0.018 - 0.010) * (extra_days / 30)
            potential_savings = estimated_storage_gb * extra_cost_per_gb
            recommendations.append(
                f"HIGH: Storage account '{account_name}' transitions to Cool after {cool_transition_days} days (costs ${monthly_cost:.2f}/month). "
                f"This is too late. Reduce to 30 days to save ${potential_savings:.2f}/month. "
                "Blobs rarely accessed after 30 days should move to Cool tier immediately."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Archive transition delayed (>180 days)
        if has_policy and has_archive_transition and archive_transition_days and archive_transition_days > 180:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            extra_days = archive_transition_days - 180
            extra_cost_per_gb = (0.010 - 0.002) * (extra_days / 90)
            potential_savings = estimated_storage_gb * 0.2 * extra_cost_per_gb
            recommendations.append(
                f"MEDIUM: Storage account '{account_name}' transitions to Archive after {archive_transition_days} days (costs ${monthly_cost:.2f}/month). "
                f"Reduce to 180 days to save ${potential_savings:.2f}/month. "
                "Long-term archival data should move to Archive tier sooner."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - Policy exists but could be optimized
        if has_policy and (not has_cool_transition or not has_archive_transition):
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = monthly_cost * 0.1  # Assume 10% improvement
            recommendations.append(
                f"LOW: Storage account '{account_name}' has lifecycle policy but incomplete (costs ${monthly_cost:.2f}/month). "
                "Review and optimize: ensure Cool transition (30d) and Archive transition (180d) are configured. "
                f"Fine-tuning could save ${potential_savings:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    # ============================================================================
    # AWS - EKS CLUSTER (Cost Intelligence / Inventory Mode)
    # ============================================================================

    def _calculate_eks_monthly_cost(
        self,
        node_group_costs: float,
        fargate_costs: float,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS EKS Cluster.

        Cost structure:
        - Control plane: $73/month (0.10/hour * 730 hours)
        - Node groups: Sum of EC2 instance costs
        - Fargate pods: vCPU + memory costs

        Args:
            node_group_costs: Total monthly cost of all node groups (EC2 instances)
            fargate_costs: Total monthly cost of Fargate pods
            region: AWS region

        Returns:
            Total estimated monthly cost
        """
        # EKS control plane cost (fixed)
        control_plane_cost = 73.0  # $0.10/hour * 730 hours/month

        total_cost = control_plane_cost + node_group_costs + fargate_costs

        return total_cost

    def _calculate_eks_optimization(
        self,
        cluster: dict[str, Any],
        node_groups: list[dict[str, Any]],
        fargate_profiles: list[dict[str, Any]],
        monthly_cost: float,
        avg_cpu_utilization: float,
        node_instance_types: list[str],
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Calculate AWS EKS Cluster optimization metrics.

        Optimization scenarios (5):
        1. No worker nodes - Cluster with 0 nodes (paying control plane only)
        2. All nodes unhealthy - All nodes in degraded/failed state
        3. Over-provisioned nodes - All nodes with CPU <20% (right-sizing opportunity)
        4. Old generation nodes - Using t2/m4/c4/r4 instance types
        5. Spot instances not used - 100% On-Demand nodes (Spot 70% cheaper)

        Args:
            cluster: EKS cluster details
            node_groups: List of node groups
            fargate_profiles: List of Fargate profiles
            monthly_cost: Total monthly cost
            avg_cpu_utilization: Average CPU utilization across all nodes
            node_instance_types: List of instance types used by nodes

        Returns:
            Tuple of (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations: list[dict[str, Any]] = []

        cluster_name = cluster.get("name", "Unknown")
        total_node_count = sum(ng.get("scalingConfig", {}).get("desiredSize", 0) for ng in node_groups)

        # Scenario 1: CRITICAL - No worker nodes (paying control plane only)
        if total_node_count == 0 and not fargate_profiles:
            is_optimizable = True
            optimization_score = 95
            priority = "critical"
            potential_savings = monthly_cost  # Full cluster cost is waste
            recommendations.append({
                "type": "no_nodes",
                "severity": "critical",
                "message": (
                    f"CRITICAL: EKS cluster '{cluster_name}' has NO worker nodes or Fargate profiles (costs ${monthly_cost:.2f}/month). "
                    f"You're paying ${73:.2f}/month for control plane with no compute resources. "
                    "Delete this cluster immediately or add node groups/Fargate profiles if needed."
                ),
                "impact": "Paying full EKS costs with zero workload capacity",
                "action": "Delete cluster or add worker nodes/Fargate profiles",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - All nodes unhealthy (dead cluster)
        unhealthy_nodes = sum(
            1 for ng in node_groups
            if ng.get("health", {}).get("issues", [])
        )
        if total_node_count > 0 and unhealthy_nodes == len(node_groups):
            is_optimizable = True
            optimization_score = 90
            priority = "high"
            potential_savings = monthly_cost  # Entire cluster is waste
            recommendations.append({
                "type": "all_unhealthy",
                "severity": "high",
                "message": (
                    f"HIGH: EKS cluster '{cluster_name}' has ALL node groups unhealthy (costs ${monthly_cost:.2f}/month). "
                    f"All {len(node_groups)} node groups are in degraded/failed state. "
                    "This cluster is not serving any workloads. Investigate node group health or delete cluster."
                ),
                "impact": "Cluster not operational, all costs are waste",
                "action": "Fix node group health issues or delete cluster",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: MEDIUM - Over-provisioned nodes (CPU <20%)
        if avg_cpu_utilization > 0 and avg_cpu_utilization < 20.0:
            is_optimizable = True
            optimization_score = 70
            priority = "medium"
            # Assume 30% savings from right-sizing (reduce node count or instance types)
            potential_savings = monthly_cost * 0.30
            recommendations.append({
                "type": "over_provisioned",
                "severity": "medium",
                "message": (
                    f"MEDIUM: EKS cluster '{cluster_name}' has very low CPU utilization ({avg_cpu_utilization:.1f}%, costs ${monthly_cost:.2f}/month). "
                    f"Nodes are under-utilized. Right-size node groups: reduce node count or use smaller instance types. "
                    f"Potential savings: ${potential_savings:.2f}/month (30% reduction)."
                ),
                "impact": f"Wasting {100 - avg_cpu_utilization:.1f}% of node capacity",
                "action": "Reduce node count or downgrade instance types",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Old generation nodes (t2/m4/c4/r4)
        old_generation_types = ["t2", "m4", "c4", "r4"]
        old_instances = [it for it in node_instance_types if any(it.startswith(old) for old in old_generation_types)]
        if old_instances:
            is_optimizable = True
            optimization_score = 65
            priority = "medium"
            # Assume 15% savings from upgrading to newer generation
            potential_savings = monthly_cost * 0.15
            recommendations.append({
                "type": "old_generation",
                "severity": "medium",
                "message": (
                    f"MEDIUM: EKS cluster '{cluster_name}' uses old generation instance types (costs ${monthly_cost:.2f}/month). "
                    f"Found: {', '.join(set(old_instances))}. "
                    "Upgrade to newer generations (t3, m5, c5, r5) for 15% cost savings and better performance."
                ),
                "impact": "Missing out on 15% cost savings and improved performance",
                "action": f"Upgrade to newer generation: {', '.join(set(old_instances)).replace('t2', 't3').replace('m4', 'm5').replace('c4', 'c5').replace('r4', 'r5')}",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: MEDIUM - Spot instances not used (100% On-Demand)
        # Check if any node group uses Spot instances
        has_spot = any(ng.get("capacityType", "ON_DEMAND") == "SPOT" for ng in node_groups)
        if not has_spot and total_node_count >= 3:
            is_optimizable = True
            optimization_score = 60
            priority = "medium"
            # Spot instances are typically 70% cheaper
            # Recommend 60% of nodes on Spot (conservative mix)
            potential_savings = monthly_cost * 0.60 * 0.70
            recommendations.append({
                "type": "no_spot",
                "severity": "medium",
                "message": (
                    f"MEDIUM: EKS cluster '{cluster_name}' uses 100% On-Demand nodes (costs ${monthly_cost:.2f}/month). "
                    f"Cluster has {total_node_count} nodes. "
                    "Use Spot instances for 60% of nodes to save 70% on those nodes. "
                    f"Potential savings: ${potential_savings:.2f}/month (42% total reduction)."
                ),
                "impact": "Missing out on 42% cost savings from Spot instances",
                "action": "Create Spot node groups for stateless/fault-tolerant workloads",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # No optimization opportunities found
        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_eks_clusters(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS EKS Clusters for cost intelligence.

        This method scans EKS clusters to provide cost visibility and optimization
        recommendations. Unlike orphan detection, this scans ALL clusters regardless
        of usage patterns.

        Args:
            region: AWS region to scan

        Returns:
            List of AllCloudResourceData objects for all EKS clusters
        """
        resources: list[AllCloudResourceData] = []

        try:
            async with self.session.client("eks", region_name=region) as eks:
                async with self.session.client("ec2", region_name=region) as ec2:
                    async with self.session.client("cloudwatch", region_name=region) as cloudwatch:
                        # List all clusters
                        response = await eks.list_clusters()
                        cluster_names = response.get("clusters", [])

                        for cluster_name in cluster_names:
                            try:
                                # Get cluster details
                                cluster_response = await eks.describe_cluster(name=cluster_name)
                                cluster = cluster_response["cluster"]

                                cluster_arn = cluster.get("arn", "")
                                created_at = cluster.get("createdAt")
                                status = cluster.get("status", "UNKNOWN")

                                # List node groups
                                node_groups_response = await eks.list_nodegroups(clusterName=cluster_name)
                                node_group_names = node_groups_response.get("nodegroups", [])

                                node_groups = []
                                node_group_costs = 0.0
                                node_instance_types = []
                                total_nodes = 0

                                for ng_name in node_group_names:
                                    ng_response = await eks.describe_nodegroup(
                                        clusterName=cluster_name,
                                        nodegroupName=ng_name
                                    )
                                    ng = ng_response["nodegroup"]
                                    node_groups.append(ng)

                                    # Calculate node group cost
                                    scaling_config = ng.get("scalingConfig", {})
                                    desired_size = scaling_config.get("desiredSize", 0)
                                    total_nodes += desired_size

                                    instance_types = ng.get("instanceTypes", [])
                                    node_instance_types.extend(instance_types)

                                    # Simplified cost calculation (assume $0.05/hour per node avg)
                                    # In reality, this varies by instance type
                                    node_group_costs += desired_size * 0.05 * 730  # $0.05/hour * 730 hours

                                # List Fargate profiles
                                fargate_response = await eks.list_fargate_profiles(clusterName=cluster_name)
                                fargate_profile_names = fargate_response.get("fargateProfileNames", [])

                                fargate_profiles = []
                                fargate_costs = 0.0

                                for fp_name in fargate_profile_names:
                                    fp_response = await eks.describe_fargate_profile(
                                        clusterName=cluster_name,
                                        fargateProfileName=fp_name
                                    )
                                    fp = fp_response["fargateProfile"]
                                    fargate_profiles.append(fp)

                                    # Simplified Fargate cost (hard to estimate without pod metrics)
                                    # Assume minimal cost for now
                                    fargate_costs += 10.0  # Placeholder

                                # Get CloudWatch CPU metrics (if nodes exist)
                                avg_cpu_utilization = 0.0
                                if total_nodes > 0:
                                    try:
                                        # Get average CPU utilization for the cluster (last 14 days)
                                        end_time = datetime.utcnow()
                                        start_time = end_time - timedelta(days=14)

                                        cpu_response = await cloudwatch.get_metric_statistics(
                                            Namespace="ContainerInsights",
                                            MetricName="node_cpu_utilization",
                                            Dimensions=[
                                                {"Name": "ClusterName", "Value": cluster_name},
                                            ],
                                            StartTime=start_time,
                                            EndTime=end_time,
                                            Period=86400,  # 1 day
                                            Statistics=["Average"],
                                        )

                                        datapoints = cpu_response.get("Datapoints", [])
                                        if datapoints:
                                            avg_cpu_utilization = sum(dp["Average"] for dp in datapoints) / len(datapoints)
                                    except Exception as e:
                                        logger.warning(
                                            "eks.cloudwatch_metrics_failed",
                                            cluster_name=cluster_name,
                                            error=str(e),
                                        )

                                # Calculate total monthly cost
                                monthly_cost = self._calculate_eks_monthly_cost(
                                    node_group_costs=node_group_costs,
                                    fargate_costs=fargate_costs,
                                    region=region,
                                )

                                # Calculate optimization metrics
                                (
                                    is_optimizable,
                                    optimization_score,
                                    optimization_priority,
                                    potential_savings,
                                    optimization_recommendations,
                                ) = self._calculate_eks_optimization(
                                    cluster=cluster,
                                    node_groups=node_groups,
                                    fargate_profiles=fargate_profiles,
                                    monthly_cost=monthly_cost,
                                    avg_cpu_utilization=avg_cpu_utilization,
                                    node_instance_types=node_instance_types,
                                )

                                # Build metadata
                                metadata = {
                                    "cluster_name": cluster_name,
                                    "cluster_arn": cluster_arn,
                                    "status": status,
                                    "kubernetes_version": cluster.get("version", "unknown"),
                                    "node_groups_count": len(node_groups),
                                    "total_nodes": total_nodes,
                                    "fargate_profiles_count": len(fargate_profiles),
                                    "avg_cpu_utilization": round(avg_cpu_utilization, 2),
                                    "node_instance_types": list(set(node_instance_types)),
                                }

                                # Create resource entry
                                resource = AllCloudResourceData(
                                    resource_id=cluster_arn,
                                    resource_type="eks_cluster",
                                    resource_name=cluster_name,
                                    region=region,
                                    estimated_monthly_cost=monthly_cost,
                                    currency="USD",
                                    resource_metadata=metadata,
                                    created_at_cloud=created_at,
                                    is_optimizable=is_optimizable,
                                    optimization_score=optimization_score,
                                    optimization_priority=optimization_priority,
                                    potential_monthly_savings=potential_savings,
                                    optimization_recommendations=optimization_recommendations,
                                )

                                resources.append(resource)

                            except Exception as e:
                                logger.error(
                                    "eks.cluster_scan_failed",
                                    cluster_name=cluster_name,
                                    region=region,
                                    error=str(e),
                                )
                                continue

        except Exception as e:
            logger.error(
                "eks.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # ============================================================================
    # AWS - LAMBDA FUNCTION (Cost Intelligence / Inventory Mode)
    # ============================================================================

    def _calculate_lambda_monthly_cost(
        self,
        invocations_monthly: int,
        avg_duration_ms: float,
        memory_mb: int,
        provisioned_concurrency: int,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS Lambda Function.

        Cost structure:
        - Request cost: $0.20 per 1 million requests
        - Compute cost: $0.0000166667 per GB-second (us-east-1 pricing)
        - Provisioned concurrency: $0.0000041667 per GB-hour

        Args:
            invocations_monthly: Number of invocations per month
            avg_duration_ms: Average duration in milliseconds
            memory_mb: Memory allocation in MB
            provisioned_concurrency: Provisioned concurrency units
            region: AWS region

        Returns:
            Total estimated monthly cost
        """
        # Request cost
        request_cost = (invocations_monthly / 1_000_000) * 0.20

        # Compute cost
        compute_seconds = (invocations_monthly * avg_duration_ms) / 1000
        memory_gb = memory_mb / 1024
        compute_cost = compute_seconds * memory_gb * 0.0000166667

        # Provisioned concurrency cost (if configured)
        provisioned_cost = 0.0
        if provisioned_concurrency > 0:
            # $0.0000041667 per GB-hour * 730 hours/month
            provisioned_cost = provisioned_concurrency * memory_gb * 0.0000041667 * 730

        total_cost = request_cost + compute_cost + provisioned_cost

        return total_cost

    def _calculate_lambda_optimization(
        self,
        function: dict[str, Any],
        invocations_monthly: int,
        error_count: int,
        avg_duration_ms: float,
        memory_mb: int,
        timeout_seconds: int,
        provisioned_concurrency: int,
        runtime: str,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Calculate AWS Lambda Function optimization metrics.

        Optimization scenarios (5):
        1. Unused provisioned concurrency - VERY EXPENSIVE (highest priority)
        2. Never invoked - Function created but never executed
        3. 100% failures - All invocations fail (dead function)
        4. Over-provisioned memory - >50% unused memory
        5. Old/deprecated runtime - Security risk + no support

        Args:
            function: Lambda function details
            invocations_monthly: Monthly invocation count
            error_count: Number of errors in the period
            avg_duration_ms: Average duration in milliseconds
            memory_mb: Memory allocation in MB
            timeout_seconds: Function timeout in seconds
            provisioned_concurrency: Provisioned concurrency units
            runtime: Lambda runtime (python3.11, nodejs20.x, etc.)
            monthly_cost: Total monthly cost

        Returns:
            Tuple of (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations: list[dict[str, Any]] = []

        function_name = function.get("FunctionName", "Unknown")

        # Scenario 1: CRITICAL - Unused provisioned concurrency (VERY EXPENSIVE)
        if provisioned_concurrency > 0 and invocations_monthly < 100:
            is_optimizable = True
            optimization_score = 95
            priority = "critical"
            # Provisioned concurrency is typically 90% of the cost for low-traffic functions
            memory_gb = memory_mb / 1024
            provisioned_monthly_cost = provisioned_concurrency * memory_gb * 0.0000041667 * 730
            potential_savings = provisioned_monthly_cost
            recommendations.append({
                "type": "unused_provisioned",
                "severity": "critical",
                "message": (
                    f"CRITICAL: Lambda function '{function_name}' has PROVISIONED CONCURRENCY configured but very low usage (costs ${monthly_cost:.2f}/month). "
                    f"Provisioned concurrency: {provisioned_concurrency} units, Invocations: {invocations_monthly}/month. "
                    f"Provisioned concurrency costs ${provisioned_monthly_cost:.2f}/month. "
                    "This is the MOST EXPENSIVE Lambda configuration. Remove provisioned concurrency immediately."
                ),
                "impact": f"Wasting ${provisioned_monthly_cost:.2f}/month on unused provisioned capacity",
                "action": "Remove provisioned concurrency configuration",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Never invoked (function created but never used)
        if invocations_monthly == 0:
            is_optimizable = True
            optimization_score = 90
            priority = "high"
            potential_savings = monthly_cost  # All cost is waste
            recommendations.append({
                "type": "never_invoked",
                "severity": "high",
                "message": (
                    f"HIGH: Lambda function '{function_name}' has NEVER been invoked (costs ${monthly_cost:.2f}/month). "
                    "This function is not being used by any service. "
                    "Delete this function or integrate it if it was meant to be used."
                ),
                "impact": "Paying for a function that is never used",
                "action": "Delete function or integrate into application",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - 100% failures (all invocations fail)
        if invocations_monthly > 0 and error_count > 0:
            error_rate = (error_count / invocations_monthly) * 100
            if error_rate >= 95.0:
                is_optimizable = True
                optimization_score = 85
                priority = "high"
                potential_savings = monthly_cost  # Dead function is waste
                recommendations.append({
                    "type": "all_failures",
                    "severity": "high",
                    "message": (
                        f"HIGH: Lambda function '{function_name}' has {error_rate:.1f}% error rate (costs ${monthly_cost:.2f}/month). "
                        f"Out of {invocations_monthly} invocations, {error_count} failed. "
                        "This function is effectively broken. Fix errors or delete function."
                    ),
                    "impact": "Paying for a broken function that always fails",
                    "action": "Fix errors in function code or delete function",
                })
                return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Over-provisioned memory (>50% unused)
        # Estimate: If avg_duration is very low relative to timeout, memory is likely over-provisioned
        # This is a simplified heuristic (real memory usage requires custom metrics)
        if invocations_monthly > 100 and avg_duration_ms < (timeout_seconds * 1000 * 0.3):
            is_optimizable = True
            optimization_score = 65
            priority = "medium"
            # Assume 30% savings from right-sizing memory
            potential_savings = monthly_cost * 0.30
            recommendations.append({
                "type": "over_provisioned_memory",
                "severity": "medium",
                "message": (
                    f"MEDIUM: Lambda function '{function_name}' may have over-provisioned memory (costs ${monthly_cost:.2f}/month). "
                    f"Memory: {memory_mb}MB, Avg duration: {avg_duration_ms:.0f}ms, Timeout: {timeout_seconds}s. "
                    f"Function completes very quickly ({avg_duration_ms:.0f}ms) suggesting memory may be excessive. "
                    f"Test with lower memory (e.g., {int(memory_mb * 0.7)}MB) to save ${potential_savings:.2f}/month (30% reduction)."
                ),
                "impact": "Paying for unused memory capacity",
                "action": f"Reduce memory allocation from {memory_mb}MB to {int(memory_mb * 0.7)}MB",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: MEDIUM - Old/deprecated runtime (security risk)
        deprecated_runtimes = [
            "python2.7", "python3.6", "python3.7",
            "nodejs", "nodejs4.3", "nodejs6.10", "nodejs8.10", "nodejs10.x", "nodejs12.x", "nodejs14.x",
            "ruby2.5", "ruby2.7",
            "java8",
            "dotnetcore2.0", "dotnetcore2.1", "dotnetcore3.1",
            "go1.x"
        ]
        if runtime in deprecated_runtimes:
            is_optimizable = True
            optimization_score = 60
            priority = "medium"
            potential_savings = 0.0  # No direct cost savings, but security risk
            recommendations.append({
                "type": "old_runtime",
                "severity": "medium",
                "message": (
                    f"MEDIUM: Lambda function '{function_name}' uses DEPRECATED runtime '{runtime}' (costs ${monthly_cost:.2f}/month). "
                    "This runtime is no longer supported by AWS and has known security vulnerabilities. "
                    "Upgrade to latest runtime: python3.11+, nodejs20.x+, java17+, etc."
                ),
                "impact": "Security vulnerabilities, no AWS support, future breaking changes",
                "action": f"Upgrade runtime from '{runtime}' to latest version",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # No optimization opportunities found
        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_lambda_functions(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS Lambda Functions for cost intelligence.

        This method scans Lambda functions to provide cost visibility and optimization
        recommendations. Unlike orphan detection, this scans ALL functions regardless
        of usage patterns.

        Args:
            region: AWS region to scan

        Returns:
            List of AllCloudResourceData objects for all Lambda functions
        """
        resources: list[AllCloudResourceData] = []

        try:
            async with self.session.client("lambda", region_name=region) as lambda_client:
                async with self.session.client("cloudwatch", region_name=region) as cloudwatch:
                    # List all Lambda functions
                    paginator = lambda_client.get_paginator("list_functions")
                    async for page in paginator.paginate():
                        functions = page.get("Functions", [])

                        for function in functions:
                            try:
                                function_name = function.get("FunctionName", "")
                                function_arn = function.get("FunctionArn", "")
                                runtime = function.get("Runtime", "unknown")
                                memory_mb = function.get("MemorySize", 128)
                                timeout_seconds = function.get("Timeout", 3)

                                # Get last modified date
                                last_modified = function.get("LastModified", "")
                                try:
                                    created_at = datetime.fromisoformat(last_modified.replace("Z", "+00:00"))
                                except Exception:
                                    created_at = None

                                # Check for provisioned concurrency
                                provisioned_concurrency = 0
                                try:
                                    pc_response = await lambda_client.list_provisioned_concurrency_configs(
                                        FunctionName=function_name
                                    )
                                    configs = pc_response.get("ProvisionedConcurrencyConfigs", [])
                                    if configs:
                                        # Sum all provisioned concurrency
                                        provisioned_concurrency = sum(c.get("AllocatedConcurrentExecutions", 0) for c in configs)
                                except Exception:
                                    pass

                                # Get CloudWatch metrics (last 30 days)
                                end_time = datetime.utcnow()
                                start_time = end_time - timedelta(days=30)

                                # Get invocations count
                                invocations_monthly = 0
                                try:
                                    invocations_response = await cloudwatch.get_metric_statistics(
                                        Namespace="AWS/Lambda",
                                        MetricName="Invocations",
                                        Dimensions=[
                                            {"Name": "FunctionName", "Value": function_name},
                                        ],
                                        StartTime=start_time,
                                        EndTime=end_time,
                                        Period=2592000,  # 30 days
                                        Statistics=["Sum"],
                                    )

                                    datapoints = invocations_response.get("Datapoints", [])
                                    if datapoints:
                                        invocations_monthly = int(datapoints[0].get("Sum", 0))
                                except Exception as e:
                                    logger.warning(
                                        "lambda.invocations_metrics_failed",
                                        function_name=function_name,
                                        error=str(e),
                                    )

                                # Get error count
                                error_count = 0
                                try:
                                    errors_response = await cloudwatch.get_metric_statistics(
                                        Namespace="AWS/Lambda",
                                        MetricName="Errors",
                                        Dimensions=[
                                            {"Name": "FunctionName", "Value": function_name},
                                        ],
                                        StartTime=start_time,
                                        EndTime=end_time,
                                        Period=2592000,  # 30 days
                                        Statistics=["Sum"],
                                    )

                                    datapoints = errors_response.get("Datapoints", [])
                                    if datapoints:
                                        error_count = int(datapoints[0].get("Sum", 0))
                                except Exception:
                                    pass

                                # Get average duration
                                avg_duration_ms = 0.0
                                try:
                                    duration_response = await cloudwatch.get_metric_statistics(
                                        Namespace="AWS/Lambda",
                                        MetricName="Duration",
                                        Dimensions=[
                                            {"Name": "FunctionName", "Value": function_name},
                                        ],
                                        StartTime=start_time,
                                        EndTime=end_time,
                                        Period=2592000,  # 30 days
                                        Statistics=["Average"],
                                    )

                                    datapoints = duration_response.get("Datapoints", [])
                                    if datapoints:
                                        avg_duration_ms = datapoints[0].get("Average", 0.0)
                                except Exception:
                                    # Fallback to a reasonable default
                                    avg_duration_ms = 100.0

                                # Calculate monthly cost
                                monthly_cost = self._calculate_lambda_monthly_cost(
                                    invocations_monthly=invocations_monthly,
                                    avg_duration_ms=avg_duration_ms,
                                    memory_mb=memory_mb,
                                    provisioned_concurrency=provisioned_concurrency,
                                    region=region,
                                )

                                # Calculate optimization metrics
                                (
                                    is_optimizable,
                                    optimization_score,
                                    optimization_priority,
                                    potential_savings,
                                    optimization_recommendations,
                                ) = self._calculate_lambda_optimization(
                                    function=function,
                                    invocations_monthly=invocations_monthly,
                                    error_count=error_count,
                                    avg_duration_ms=avg_duration_ms,
                                    memory_mb=memory_mb,
                                    timeout_seconds=timeout_seconds,
                                    provisioned_concurrency=provisioned_concurrency,
                                    runtime=runtime,
                                    monthly_cost=monthly_cost,
                                )

                                # Build metadata
                                metadata = {
                                    "function_name": function_name,
                                    "function_arn": function_arn,
                                    "runtime": runtime,
                                    "memory_mb": memory_mb,
                                    "timeout_seconds": timeout_seconds,
                                    "invocations_monthly": invocations_monthly,
                                    "error_count": error_count,
                                    "error_rate": round((error_count / invocations_monthly * 100) if invocations_monthly > 0 else 0.0, 2),
                                    "avg_duration_ms": round(avg_duration_ms, 2),
                                    "provisioned_concurrency": provisioned_concurrency,
                                }

                                # Create resource entry
                                resource = AllCloudResourceData(
                                    resource_id=function_arn,
                                    resource_type="lambda_function",
                                    resource_name=function_name,
                                    region=region,
                                    estimated_monthly_cost=monthly_cost,
                                    currency="USD",
                                    resource_metadata=metadata,
                                    created_at_cloud=created_at,
                                    is_optimizable=is_optimizable,
                                    optimization_score=optimization_score,
                                    optimization_priority=optimization_priority,
                                    potential_monthly_savings=potential_savings,
                                    optimization_recommendations=optimization_recommendations,
                                )

                                resources.append(resource)

                            except Exception as e:
                                logger.error(
                                    "lambda.function_scan_failed",
                                    function_name=function.get("FunctionName", "Unknown"),
                                    region=region,
                                    error=str(e),
                                )
                                continue

        except Exception as e:
            logger.error(
                "lambda.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # ============================================================================
    # AWS - DYNAMODB TABLE (Cost Intelligence / Inventory Mode)
    # ============================================================================

    def _calculate_dynamodb_monthly_cost(
        self,
        billing_mode: str,
        read_capacity: int,
        write_capacity: int,
        storage_gb: float,
        gsi_count: int,
        gsi_read_capacity: int,
        gsi_write_capacity: int,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS DynamoDB Table.

        Cost structure (us-east-1 pricing):

        Provisioned mode:
        - Read Capacity Unit (RCU): $0.00065/hour = $0.47/month per RCU
        - Write Capacity Unit (WCU): $0.00013/hour = $0.095/month per WCU
        - Storage: $0.25/GB-month
        - GSI: Same pricing as base table (doubles cost if same capacity)

        On-Demand mode:
        - Write Request Unit (WRU): $1.25 per million writes
        - Read Request Unit (RRU): $0.25 per million reads
        - Storage: $0.25/GB-month
        - GSI: Same pricing as base table

        Args:
            billing_mode: "PROVISIONED" or "PAY_PER_REQUEST" (On-Demand)
            read_capacity: Provisioned read capacity units (0 for On-Demand)
            write_capacity: Provisioned write capacity units (0 for On-Demand)
            storage_gb: Table size in GB
            gsi_count: Number of Global Secondary Indexes
            gsi_read_capacity: Total GSI read capacity (Provisioned only)
            gsi_write_capacity: Total GSI write capacity (Provisioned only)
            region: AWS region

        Returns:
            Total estimated monthly cost
        """
        storage_cost = storage_gb * 0.25  # $0.25/GB-month

        if billing_mode == "PROVISIONED":
            # Base table RCU/WCU costs
            base_read_cost = read_capacity * 0.00065 * 730  # $0.00065/hour * 730 hours
            base_write_cost = write_capacity * 0.00013 * 730  # $0.00013/hour * 730 hours

            # GSI RCU/WCU costs (same pricing as base table)
            gsi_read_cost = gsi_read_capacity * 0.00065 * 730
            gsi_write_cost = gsi_write_capacity * 0.00013 * 730

            total_cost = base_read_cost + base_write_cost + gsi_read_cost + gsi_write_cost + storage_cost
        else:  # PAY_PER_REQUEST (On-Demand)
            # For On-Demand, we don't know the actual request count without CloudWatch metrics
            # Return storage cost only (actual cost calculated later with metrics)
            total_cost = storage_cost

        return total_cost

    def _calculate_dynamodb_optimization(
        self,
        table: dict[str, Any],
        billing_mode: str,
        read_capacity: int,
        write_capacity: int,
        consumed_read_capacity: float,
        consumed_write_capacity: float,
        gsi_count: int,
        gsi_read_capacity: int,
        gsi_write_capacity: int,
        item_count: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Calculate AWS DynamoDB Table optimization metrics.

        Optimization scenarios (5):
        1. Over-provisioned capacity - <10% utilization (VERY EXPENSIVE)
        2. Unused Global Secondary Indexes - GSI never queried (doubles cost)
        3. Never used tables (Provisioned) - 0 usage since creation
        4. Never used tables (On-Demand) - 0 usage in 60 days
        5. Empty tables - 0 items for 90+ days

        Args:
            table: DynamoDB table details
            billing_mode: "PROVISIONED" or "PAY_PER_REQUEST"
            read_capacity: Provisioned read capacity units
            write_capacity: Provisioned write capacity units
            consumed_read_capacity: Actual consumed RCU (from CloudWatch)
            consumed_write_capacity: Actual consumed WCU (from CloudWatch)
            gsi_count: Number of GSIs
            gsi_read_capacity: Total GSI read capacity
            gsi_write_capacity: Total GSI write capacity
            item_count: Number of items in table
            monthly_cost: Total monthly cost

        Returns:
            Tuple of (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations: list[dict[str, Any]] = []

        table_name = table.get("TableName", "Unknown")

        # Scenario 1: CRITICAL - Over-provisioned capacity (<10% utilization)
        if billing_mode == "PROVISIONED":
            # Calculate utilization percentages
            read_utilization = (consumed_read_capacity / read_capacity * 100) if read_capacity > 0 else 0.0
            write_utilization = (consumed_write_capacity / write_capacity * 100) if write_capacity > 0 else 0.0
            avg_utilization = (read_utilization + write_utilization) / 2

            if avg_utilization < 10.0 and (read_capacity > 0 or write_capacity > 0):
                is_optimizable = True
                optimization_score = 95
                priority = "critical"
                # Assume 70% savings from right-sizing to actual usage
                potential_savings = monthly_cost * 0.70
                recommendations.append({
                    "type": "over_provisioned",
                    "severity": "critical",
                    "message": (
                        f"CRITICAL: DynamoDB table '{table_name}' has VERY LOW capacity utilization (costs ${monthly_cost:.2f}/month). "
                        f"Read: {read_utilization:.1f}% ({consumed_read_capacity:.0f}/{read_capacity} RCU), "
                        f"Write: {write_utilization:.1f}% ({consumed_write_capacity:.0f}/{write_capacity} WCU). "
                        f"You're wasting {100 - avg_utilization:.1f}% of provisioned capacity. "
                        f"Right-size to actual usage or switch to On-Demand mode. Potential savings: ${potential_savings:.2f}/month (70% reduction)."
                    ),
                    "impact": f"Wasting {100 - avg_utilization:.1f}% of provisioned capacity",
                    "action": f"Reduce capacity to {int(consumed_read_capacity * 1.5)} RCU / {int(consumed_write_capacity * 1.5)} WCU or switch to On-Demand",
                })
                return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Unused Global Secondary Indexes (doubles cost)
        if gsi_count > 0 and (gsi_read_capacity > 0 or gsi_write_capacity > 0):
            is_optimizable = True
            optimization_score = 85
            priority = "high"
            # GSI cost is typically 50% of total cost for tables with GSIs
            gsi_monthly_cost = monthly_cost * 0.50
            potential_savings = gsi_monthly_cost
            recommendations.append({
                "type": "unused_gsi",
                "severity": "high",
                "message": (
                    f"HIGH: DynamoDB table '{table_name}' has {gsi_count} Global Secondary Indexes (costs ${monthly_cost:.2f}/month). "
                    f"GSIs double your costs ({gsi_count} GSIs = ~${gsi_monthly_cost:.2f}/month). "
                    f"Verify all GSIs are actively used in queries. Delete unused GSIs immediately. "
                    f"Each unused GSI wastes ~${gsi_monthly_cost / gsi_count:.2f}/month."
                ),
                "impact": f"GSIs cost ${gsi_monthly_cost:.2f}/month - verify usage",
                "action": f"Delete unused GSIs (check CloudWatch GSI metrics)",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Never used tables (Provisioned mode)
        if billing_mode == "PROVISIONED" and consumed_read_capacity == 0 and consumed_write_capacity == 0:
            is_optimizable = True
            optimization_score = 90
            priority = "high"
            potential_savings = monthly_cost  # All cost is waste
            recommendations.append({
                "type": "never_used_provisioned",
                "severity": "high",
                "message": (
                    f"HIGH: DynamoDB table '{table_name}' in PROVISIONED mode has NEVER been used (costs ${monthly_cost:.2f}/month). "
                    f"0 reads and 0 writes detected. "
                    "This table is not integrated with any application. Delete table or integrate if needed."
                ),
                "impact": "Paying for provisioned capacity with zero usage",
                "action": "Delete table or integrate into application",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: HIGH - Never used tables (On-Demand mode)
        if billing_mode == "PAY_PER_REQUEST" and consumed_read_capacity == 0 and consumed_write_capacity == 0:
            is_optimizable = True
            optimization_score = 85
            priority = "high"
            potential_savings = monthly_cost  # Storage cost is waste
            recommendations.append({
                "type": "never_used_ondemand",
                "severity": "high",
                "message": (
                    f"HIGH: DynamoDB table '{table_name}' in ON-DEMAND mode has NO usage (costs ${monthly_cost:.2f}/month). "
                    f"0 reads and 0 writes in last 60 days. "
                    "This table is not being accessed. Delete table if no longer needed."
                ),
                "impact": f"Paying storage cost (${monthly_cost:.2f}/month) with no access",
                "action": "Delete table or verify integration",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: MEDIUM - Empty tables (0 items for 90+ days)
        if item_count == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "medium"
            potential_savings = monthly_cost  # Empty table is waste
            recommendations.append({
                "type": "empty_table",
                "severity": "medium",
                "message": (
                    f"MEDIUM: DynamoDB table '{table_name}' is EMPTY (costs ${monthly_cost:.2f}/month). "
                    f"Table has 0 items. "
                    "Empty table has been idle for extended period. Delete if no longer needed."
                ),
                "impact": f"Paying ${monthly_cost:.2f}/month for empty table",
                "action": "Delete table if no longer needed",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # No optimization opportunities found
        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_dynamodb_tables(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS DynamoDB Tables for cost intelligence.

        This method scans DynamoDB tables to provide cost visibility and optimization
        recommendations. Unlike orphan detection, this scans ALL tables regardless
        of usage patterns.

        Args:
            region: AWS region to scan

        Returns:
            List of AllCloudResourceData objects for all DynamoDB tables
        """
        resources: list[AllCloudResourceData] = []

        try:
            async with self.session.client("dynamodb", region_name=region) as dynamodb:
                async with self.session.client("cloudwatch", region_name=region) as cloudwatch:
                    # List all tables
                    paginator = dynamodb.get_paginator("list_tables")
                    async for page in paginator.paginate():
                        table_names = page.get("TableNames", [])

                        for table_name in table_names:
                            try:
                                # Get table details
                                response = await dynamodb.describe_table(TableName=table_name)
                                table = response["Table"]

                                table_arn = table.get("TableArn", "")
                                billing_mode_summary = table.get("BillingModeSummary", {})
                                billing_mode = billing_mode_summary.get("BillingMode", "PROVISIONED")

                                # Get creation date
                                created_at = table.get("CreationDateTime")

                                # Get provisioned throughput (for Provisioned mode)
                                provisioned_throughput = table.get("ProvisionedThroughput", {})
                                read_capacity = provisioned_throughput.get("ReadCapacityUnits", 0)
                                write_capacity = provisioned_throughput.get("WriteCapacityUnits", 0)

                                # Get table size and item count
                                storage_gb = table.get("TableSizeBytes", 0) / (1024 ** 3)  # Convert bytes to GB
                                item_count = table.get("ItemCount", 0)

                                # Get GSI information
                                global_secondary_indexes = table.get("GlobalSecondaryIndexes", [])
                                gsi_count = len(global_secondary_indexes)
                                gsi_read_capacity = 0
                                gsi_write_capacity = 0

                                for gsi in global_secondary_indexes:
                                    gsi_throughput = gsi.get("ProvisionedThroughput", {})
                                    gsi_read_capacity += gsi_throughput.get("ReadCapacityUnits", 0)
                                    gsi_write_capacity += gsi_throughput.get("WriteCapacityUnits", 0)

                                # Get CloudWatch metrics (last 14 days)
                                end_time = datetime.utcnow()
                                start_time = end_time - timedelta(days=14)

                                # Get consumed read capacity
                                consumed_read_capacity = 0.0
                                try:
                                    read_response = await cloudwatch.get_metric_statistics(
                                        Namespace="AWS/DynamoDB",
                                        MetricName="ConsumedReadCapacityUnits",
                                        Dimensions=[
                                            {"Name": "TableName", "Value": table_name},
                                        ],
                                        StartTime=start_time,
                                        EndTime=end_time,
                                        Period=1209600,  # 14 days
                                        Statistics=["Average"],
                                    )

                                    datapoints = read_response.get("Datapoints", [])
                                    if datapoints:
                                        consumed_read_capacity = datapoints[0].get("Average", 0.0)
                                except Exception:
                                    pass

                                # Get consumed write capacity
                                consumed_write_capacity = 0.0
                                try:
                                    write_response = await cloudwatch.get_metric_statistics(
                                        Namespace="AWS/DynamoDB",
                                        MetricName="ConsumedWriteCapacityUnits",
                                        Dimensions=[
                                            {"Name": "TableName", "Value": table_name},
                                        ],
                                        StartTime=start_time,
                                        EndTime=end_time,
                                        Period=1209600,  # 14 days
                                        Statistics=["Average"],
                                    )

                                    datapoints = write_response.get("Datapoints", [])
                                    if datapoints:
                                        consumed_write_capacity = datapoints[0].get("Average", 0.0)
                                except Exception:
                                    pass

                                # Calculate monthly cost
                                monthly_cost = self._calculate_dynamodb_monthly_cost(
                                    billing_mode=billing_mode,
                                    read_capacity=read_capacity,
                                    write_capacity=write_capacity,
                                    storage_gb=storage_gb,
                                    gsi_count=gsi_count,
                                    gsi_read_capacity=gsi_read_capacity,
                                    gsi_write_capacity=gsi_write_capacity,
                                    region=region,
                                )

                                # Calculate optimization metrics
                                (
                                    is_optimizable,
                                    optimization_score,
                                    optimization_priority,
                                    potential_savings,
                                    optimization_recommendations,
                                ) = self._calculate_dynamodb_optimization(
                                    table=table,
                                    billing_mode=billing_mode,
                                    read_capacity=read_capacity,
                                    write_capacity=write_capacity,
                                    consumed_read_capacity=consumed_read_capacity,
                                    consumed_write_capacity=consumed_write_capacity,
                                    gsi_count=gsi_count,
                                    gsi_read_capacity=gsi_read_capacity,
                                    gsi_write_capacity=gsi_write_capacity,
                                    item_count=item_count,
                                    monthly_cost=monthly_cost,
                                )

                                # Build metadata
                                metadata = {
                                    "table_name": table_name,
                                    "table_arn": table_arn,
                                    "billing_mode": billing_mode,
                                    "read_capacity": read_capacity,
                                    "write_capacity": write_capacity,
                                    "consumed_read_capacity": round(consumed_read_capacity, 2),
                                    "consumed_write_capacity": round(consumed_write_capacity, 2),
                                    "storage_gb": round(storage_gb, 2),
                                    "item_count": item_count,
                                    "gsi_count": gsi_count,
                                    "gsi_read_capacity": gsi_read_capacity,
                                    "gsi_write_capacity": gsi_write_capacity,
                                }

                                # Create resource entry
                                resource = AllCloudResourceData(
                                    resource_id=table_arn,
                                    resource_type="dynamodb_table",
                                    resource_name=table_name,
                                    region=region,
                                    estimated_monthly_cost=monthly_cost,
                                    currency="USD",
                                    resource_metadata=metadata,
                                    created_at_cloud=created_at,
                                    is_optimizable=is_optimizable,
                                    optimization_score=optimization_score,
                                    optimization_priority=optimization_priority,
                                    potential_monthly_savings=potential_savings,
                                    optimization_recommendations=optimization_recommendations,
                                )

                                resources.append(resource)

                            except Exception as e:
                                logger.error(
                                    "dynamodb.table_scan_failed",
                                    table_name=table_name,
                                    region=region,
                                    error=str(e),
                                )
                                continue

        except Exception as e:
            logger.error(
                "dynamodb.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # ============================================================================
    # AWS - FARGATE TASK (Cost Intelligence / Inventory Mode)
    # ============================================================================

    def _calculate_fargate_monthly_cost(
        self,
        vcpu: float,
        memory_gb: float,
        hours_running: float,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS Fargate Task.

        Cost structure (us-east-1 pricing, Linux x86_64):
        - vCPU: $0.04048 per vCPU-hour
        - Memory: $0.004445 per GB-hour

        Args:
            vcpu: Number of vCPUs (e.g., 0.25, 0.5, 1, 2, 4)
            memory_gb: Memory in GB (e.g., 0.5, 1, 2, 4, 8, 16)
            hours_running: Hours running in the month (0-730)
            region: AWS region

        Returns:
            Total estimated monthly cost
        """
        vcpu_cost = vcpu * 0.04048 * hours_running
        memory_cost = memory_gb * 0.004445 * hours_running

        total_cost = vcpu_cost + memory_cost

        return total_cost

    def _calculate_fargate_optimization(
        self,
        task: dict[str, Any],
        task_definition: dict[str, Any],
        last_status: str,
        avg_cpu_utilization: float,
        avg_memory_utilization: float,
        hours_running_monthly: float,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Calculate AWS Fargate Task optimization metrics.

        Optimization scenarios (4):
        1. Stopped tasks - Task stopped but still allocated (critical)
        2. Low CPU utilization - CPU <10% (high)
        3. EC2 would be cheaper - Long-running tasks (medium)
        4. Over-provisioned vCPU/memory - Underutilized (low)

        Args:
            task: ECS task details
            task_definition: Task definition details
            last_status: Task status (RUNNING, STOPPED, etc.)
            avg_cpu_utilization: Average CPU utilization percentage
            avg_memory_utilization: Average memory utilization percentage
            hours_running_monthly: Hours running in the month
            monthly_cost: Total monthly cost

        Returns:
            Tuple of (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations: list[dict[str, Any]] = []

        task_arn = task.get("taskArn", "")
        task_id = task_arn.split("/")[-1][:8] if task_arn else "Unknown"

        # Extract vCPU and memory from task definition
        vcpu = float(task_definition.get("cpu", "256")) / 1024  # Convert CPU units to vCPU
        memory_gb = float(task_definition.get("memory", "512")) / 1024  # Convert MB to GB

        # Scenario 1: CRITICAL - Stopped tasks (allocated but not running)
        if last_status != "RUNNING" and hours_running_monthly > 0:
            is_optimizable = True
            optimization_score = 95
            priority = "critical"
            potential_savings = monthly_cost  # All cost is waste
            recommendations.append({
                "type": "stopped_task",
                "severity": "critical",
                "message": (
                    f"CRITICAL: Fargate task '{task_id}' is STOPPED but still allocated (costs ${monthly_cost:.2f}/month). "
                    f"Status: {last_status}. "
                    "Stopped tasks should not be allocated. Stop or delete this task immediately."
                ),
                "impact": "Paying for stopped task capacity",
                "action": "Stop or delete Fargate task",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Low CPU utilization (CPU <10%)
        if avg_cpu_utilization > 0 and avg_cpu_utilization < 10.0:
            is_optimizable = True
            optimization_score = 85
            priority = "high"
            # Assume 40% savings from right-sizing CPU
            potential_savings = monthly_cost * 0.40
            recommendations.append({
                "type": "low_cpu",
                "severity": "high",
                "message": (
                    f"HIGH: Fargate task '{task_id}' has VERY LOW CPU utilization (costs ${monthly_cost:.2f}/month). "
                    f"CPU: {avg_cpu_utilization:.1f}% (vCPU: {vcpu}). "
                    f"Task is using only {avg_cpu_utilization:.1f}% of allocated CPU. "
                    f"Reduce vCPU allocation to save ${potential_savings:.2f}/month (40% reduction)."
                ),
                "impact": f"Wasting {100 - avg_cpu_utilization:.1f}% of CPU capacity",
                "action": f"Reduce vCPU from {vcpu} to {vcpu * 0.5:.2f}",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: MEDIUM - EC2 would be cheaper (long-running tasks)
        # Fargate break-even is ~10-15 pods; for long-running tasks, EC2 is cheaper
        if hours_running_monthly >= 720:  # Running 24/7
            is_optimizable = True
            optimization_score = 70
            priority = "medium"
            # EC2 is typically 30% cheaper for 24/7 workloads
            potential_savings = monthly_cost * 0.30
            recommendations.append({
                "type": "ec2_cheaper",
                "severity": "medium",
                "message": (
                    f"MEDIUM: Fargate task '{task_id}' runs 24/7 (costs ${monthly_cost:.2f}/month). "
                    f"Running {hours_running_monthly:.0f} hours/month. "
                    "Fargate is optimized for sporadic/burst workloads. "
                    f"For 24/7 workloads, EC2 instances are 30% cheaper. Consider migrating to ECS on EC2 to save ${potential_savings:.2f}/month."
                ),
                "impact": "Fargate is 30% more expensive than EC2 for 24/7 workloads",
                "action": "Migrate to ECS on EC2 instances",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: LOW - Over-provisioned vCPU/memory (underutilized)
        if avg_cpu_utilization > 0 and avg_cpu_utilization < 30.0 and avg_memory_utilization < 30.0:
            is_optimizable = True
            optimization_score = 50
            priority = "low"
            # Assume 20% savings from right-sizing
            potential_savings = monthly_cost * 0.20
            recommendations.append({
                "type": "over_provisioned",
                "severity": "low",
                "message": (
                    f"LOW: Fargate task '{task_id}' has low CPU and memory utilization (costs ${monthly_cost:.2f}/month). "
                    f"CPU: {avg_cpu_utilization:.1f}%, Memory: {avg_memory_utilization:.1f}%. "
                    f"Current: {vcpu} vCPU, {memory_gb:.2f} GB memory. "
                    f"Right-size to {vcpu * 0.8:.2f} vCPU / {memory_gb * 0.8:.2f} GB to save ${potential_savings:.2f}/month (20% reduction)."
                ),
                "impact": "Underutilized CPU and memory resources",
                "action": f"Reduce to {vcpu * 0.8:.2f} vCPU / {memory_gb * 0.8:.2f} GB",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # No optimization opportunities found
        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_fargate_tasks(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS Fargate Tasks for cost intelligence.

        This method scans Fargate tasks running in ECS clusters to provide cost
        visibility and optimization recommendations. Unlike orphan detection, this
        scans ALL tasks regardless of usage patterns.

        Args:
            region: AWS region to scan

        Returns:
            List of AllCloudResourceData objects for all Fargate tasks
        """
        resources: list[AllCloudResourceData] = []

        try:
            async with self.session.client("ecs", region_name=region) as ecs:
                async with self.session.client("cloudwatch", region_name=region) as cloudwatch:
                    # List all ECS clusters
                    clusters_response = await ecs.list_clusters()
                    cluster_arns = clusters_response.get("clusterArns", [])

                    for cluster_arn in cluster_arns:
                        try:
                            # List tasks in cluster with Fargate launch type
                            tasks_response = await ecs.list_tasks(
                                cluster=cluster_arn,
                                launchType="FARGATE",
                            )
                            task_arns = tasks_response.get("taskArns", [])

                            if not task_arns:
                                continue

                            # Describe tasks
                            described_tasks_response = await ecs.describe_tasks(
                                cluster=cluster_arn,
                                tasks=task_arns,
                            )
                            tasks = described_tasks_response.get("tasks", [])

                            for task in tasks:
                                try:
                                    task_arn = task.get("taskArn", "")
                                    task_definition_arn = task.get("taskDefinitionArn", "")
                                    last_status = task.get("lastStatus", "UNKNOWN")
                                    created_at = task.get("createdAt")

                                    # Get task definition details
                                    task_def_response = await ecs.describe_task_definition(
                                        taskDefinition=task_definition_arn
                                    )
                                    task_definition = task_def_response["taskDefinition"]

                                    # Extract vCPU and memory
                                    cpu_str = task_definition.get("cpu", "256")  # CPU units
                                    memory_str = task_definition.get("memory", "512")  # MB
                                    vcpu = float(cpu_str) / 1024  # Convert to vCPU
                                    memory_gb = float(memory_str) / 1024  # Convert to GB

                                    # Calculate hours running (estimate from creation time)
                                    hours_running_monthly = 0.0
                                    if created_at:
                                        age = datetime.utcnow() - created_at.replace(tzinfo=None)
                                        hours_running_monthly = min(age.total_seconds() / 3600, 730)

                                    # Get CloudWatch CPU metrics (last 7 days)
                                    avg_cpu_utilization = 0.0
                                    avg_memory_utilization = 0.0

                                    if last_status == "RUNNING":
                                        try:
                                            end_time = datetime.utcnow()
                                            start_time = end_time - timedelta(days=7)

                                            # Get CPU utilization
                                            cpu_response = await cloudwatch.get_metric_statistics(
                                                Namespace="ECS/ContainerInsights",
                                                MetricName="CpuUtilized",
                                                Dimensions=[
                                                    {"Name": "ClusterName", "Value": cluster_arn.split("/")[-1]},
                                                    {"Name": "TaskId", "Value": task_arn.split("/")[-1]},
                                                ],
                                                StartTime=start_time,
                                                EndTime=end_time,
                                                Period=604800,  # 7 days
                                                Statistics=["Average"],
                                            )

                                            datapoints = cpu_response.get("Datapoints", [])
                                            if datapoints:
                                                avg_cpu_utilization = datapoints[0].get("Average", 0.0)
                                        except Exception:
                                            pass

                                        try:
                                            # Get Memory utilization
                                            memory_response = await cloudwatch.get_metric_statistics(
                                                Namespace="ECS/ContainerInsights",
                                                MetricName="MemoryUtilized",
                                                Dimensions=[
                                                    {"Name": "ClusterName", "Value": cluster_arn.split("/")[-1]},
                                                    {"Name": "TaskId", "Value": task_arn.split("/")[-1]},
                                                ],
                                                StartTime=start_time,
                                                EndTime=end_time,
                                                Period=604800,  # 7 days
                                                Statistics=["Average"],
                                            )

                                            datapoints = memory_response.get("Datapoints", [])
                                            if datapoints:
                                                avg_memory_utilization = datapoints[0].get("Average", 0.0)
                                        except Exception:
                                            pass

                                    # Calculate monthly cost
                                    monthly_cost = self._calculate_fargate_monthly_cost(
                                        vcpu=vcpu,
                                        memory_gb=memory_gb,
                                        hours_running=hours_running_monthly,
                                        region=region,
                                    )

                                    # Calculate optimization metrics
                                    (
                                        is_optimizable,
                                        optimization_score,
                                        optimization_priority,
                                        potential_savings,
                                        optimization_recommendations,
                                    ) = self._calculate_fargate_optimization(
                                        task=task,
                                        task_definition=task_definition,
                                        last_status=last_status,
                                        avg_cpu_utilization=avg_cpu_utilization,
                                        avg_memory_utilization=avg_memory_utilization,
                                        hours_running_monthly=hours_running_monthly,
                                        monthly_cost=monthly_cost,
                                    )

                                    # Build metadata
                                    task_id = task_arn.split("/")[-1] if task_arn else "unknown"
                                    metadata = {
                                        "task_id": task_id,
                                        "task_arn": task_arn,
                                        "cluster_arn": cluster_arn,
                                        "last_status": last_status,
                                        "vcpu": vcpu,
                                        "memory_gb": memory_gb,
                                        "hours_running_monthly": round(hours_running_monthly, 2),
                                        "avg_cpu_utilization": round(avg_cpu_utilization, 2),
                                        "avg_memory_utilization": round(avg_memory_utilization, 2),
                                    }

                                    # Create resource entry
                                    resource = AllCloudResourceData(
                                        resource_id=task_arn,
                                        resource_type="fargate_task",
                                        resource_name=f"fargate-{task_id}",
                                        region=region,
                                        estimated_monthly_cost=monthly_cost,
                                        currency="USD",
                                        resource_metadata=metadata,
                                        created_at_cloud=created_at,
                                        is_optimizable=is_optimizable,
                                        optimization_score=optimization_score,
                                        optimization_priority=optimization_priority,
                                        potential_monthly_savings=potential_savings,
                                        optimization_recommendations=optimization_recommendations,
                                    )

                                    resources.append(resource)

                                except Exception as e:
                                    logger.error(
                                        "fargate.task_scan_failed",
                                        task_arn=task.get("taskArn", "Unknown"),
                                        cluster_arn=cluster_arn,
                                        region=region,
                                        error=str(e),
                                    )
                                    continue

                        except Exception as e:
                            logger.error(
                                "fargate.cluster_scan_failed",
                                cluster_arn=cluster_arn,
                                region=region,
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "fargate.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # ============================================================================
    # AWS - ELASTICACHE CLUSTER (Cost Intelligence / Inventory Mode)
    # ============================================================================

    def _calculate_elasticache_monthly_cost(
        self,
        node_type: str,
        num_nodes: int,
        engine: str,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS ElastiCache Cluster.

        Cost structure (us-east-1 pricing):
        - Node cost varies by type (t3.micro to r6g.xlarge)
        - Charged per node per hour
        - Redis and Memcached same pricing

        Args:
            node_type: Node type (e.g., cache.t3.micro, cache.m5.large)
            num_nodes: Number of cache nodes
            engine: redis or memcached
            region: AWS region

        Returns:
            Total estimated monthly cost
        """
        # Map node_type to pricing key
        # cache.t3.micro -> elasticache_t3_micro
        pricing_key = f"elasticache_{node_type.replace('cache.', '').replace('.', '_')}"

        # Get hourly cost from pricing dict, fallback to m5.large if not found
        hourly_cost = self.PRICING.get(pricing_key, 0.126)

        # Calculate monthly cost (730 hours/month)
        monthly_cost = hourly_cost * 730 * num_nodes

        return monthly_cost

    def _calculate_elasticache_optimization(
        self,
        cluster: dict[str, Any],
        cache_hits: float,
        cache_misses: float,
        curr_connections: float,
        memory_usage_percent: float,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Calculate AWS ElastiCache Cluster optimization metrics.

        Optimization scenarios (4):
        1. Zero cache hits - No activity (critical)
        2. Low hit rate - <50% hits, cache inefficient (high)
        3. No connections - Nobody connects (high)
        4. Over-provisioned memory - <20% memory used (medium)

        Args:
            cluster: ElastiCache cluster details
            cache_hits: Total cache hits in period
            cache_misses: Total cache misses in period
            curr_connections: Current connections count
            memory_usage_percent: Memory usage percentage
            monthly_cost: Total monthly cost

        Returns:
            Tuple of (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations: list[dict[str, Any]] = []

        cluster_id = cluster.get("CacheClusterId", "Unknown")

        # Scenario 1: CRITICAL - Zero cache hits (no activity)
        if cache_hits == 0.0 and cache_misses == 0.0:
            is_optimizable = True
            optimization_score = 95
            priority = "critical"
            potential_savings = monthly_cost  # All cost is waste
            recommendations.append({
                "type": "zero_cache_hits",
                "severity": "critical",
                "message": (
                    f"CRITICAL: ElastiCache cluster '{cluster_id}' has ZERO cache activity (costs ${monthly_cost:.2f}/month). "
                    "0 cache hits and 0 cache misses detected. "
                    "This cluster is not being used by any application. Delete cluster or integrate if needed."
                ),
                "impact": "Paying for a cache that is never accessed",
                "action": "Delete cluster or integrate into application",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Low hit rate (<50% hits, cache inefficient)
        total_requests = cache_hits + cache_misses
        if total_requests > 0:
            hit_rate = (cache_hits / total_requests) * 100
            if hit_rate < 50.0:
                is_optimizable = True
                optimization_score = 85
                priority = "high"
                # Low hit rate means cache is not effective, consider alternative solutions
                potential_savings = monthly_cost * 0.50  # 50% of cost could be saved
                recommendations.append({
                    "type": "low_hit_rate",
                    "severity": "high",
                    "message": (
                        f"HIGH: ElastiCache cluster '{cluster_id}' has LOW cache hit rate (costs ${monthly_cost:.2f}/month). "
                        f"Hit rate: {hit_rate:.1f}% (Hits: {cache_hits:.0f}, Misses: {cache_misses:.0f}). "
                        "Cache hit rate below 50% indicates ineffective caching. "
                        f"Review cache key design or consider deleting cluster to save ${potential_savings:.2f}/month (50% of cost)."
                    ),
                    "impact": "Cache is ineffective with less than 50% hit rate",
                    "action": "Optimize cache key design or delete cluster",
                })
                return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - No connections (nobody connects)
        if curr_connections == 0.0 and total_requests > 0:
            is_optimizable = True
            optimization_score = 90
            priority = "high"
            potential_savings = monthly_cost  # Cluster not actively used
            recommendations.append({
                "type": "no_connections",
                "severity": "high",
                "message": (
                    f"HIGH: ElastiCache cluster '{cluster_id}' has NO active connections (costs ${monthly_cost:.2f}/month). "
                    "0 current connections detected. "
                    "No applications are actively connected to this cluster. Delete or verify integration."
                ),
                "impact": "Paying for a cluster with no active connections",
                "action": "Delete cluster or verify application integration",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Over-provisioned memory (<20% memory used)
        if memory_usage_percent > 0 and memory_usage_percent < 20.0:
            is_optimizable = True
            optimization_score = 65
            priority = "medium"
            # Assume 40% savings from right-sizing to smaller node type
            potential_savings = monthly_cost * 0.40
            recommendations.append({
                "type": "over_provisioned_memory",
                "severity": "medium",
                "message": (
                    f"MEDIUM: ElastiCache cluster '{cluster_id}' has LOW memory usage (costs ${monthly_cost:.2f}/month). "
                    f"Memory usage: {memory_usage_percent:.1f}%. "
                    f"Cluster is using only {memory_usage_percent:.1f}% of allocated memory. "
                    f"Downgrade to smaller node type to save ${potential_savings:.2f}/month (40% reduction)."
                ),
                "impact": f"Wasting {100 - memory_usage_percent:.1f}% of memory capacity",
                "action": "Downgrade to smaller node type (e.g., t3.small  t3.micro)",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # No optimization opportunities found
        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_elasticache_clusters(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS ElastiCache Clusters for cost intelligence.

        This method scans ElastiCache clusters (Redis + Memcached) to provide cost
        visibility and optimization recommendations. Unlike orphan detection, this
        scans ALL clusters regardless of usage patterns.

        Args:
            region: AWS region to scan

        Returns:
            List of AllCloudResourceData objects for all ElastiCache clusters
        """
        resources: list[AllCloudResourceData] = []

        try:
            async with self.session.client("elasticache", region_name=region) as elasticache:
                async with self.session.client("cloudwatch", region_name=region) as cloudwatch:
                    # List all ElastiCache clusters (Redis + Memcached)
                    response = await elasticache.describe_cache_clusters()
                    clusters = response.get("CacheClusters", [])

                    for cluster in clusters:
                        try:
                            cluster_id = cluster.get("CacheClusterId", "")
                            cluster_status = cluster.get("CacheClusterStatus", "unknown")

                            # Skip non-available clusters
                            if cluster_status != "available":
                                continue

                            cluster_arn = cluster.get("ARN", "")
                            created_at = cluster.get("CacheClusterCreateTime")

                            # Extract cluster metadata
                            node_type = cluster.get("CacheNodeType", "cache.m5.large")
                            num_nodes = cluster.get("NumCacheNodes", 1)
                            engine = cluster.get("Engine", "redis")
                            engine_version = cluster.get("EngineVersion", "unknown")

                            # Get CloudWatch metrics (last 7 days)
                            end_time = datetime.utcnow()
                            start_time = end_time - timedelta(days=7)

                            # Get cache hits
                            cache_hits = 0.0
                            try:
                                hits_response = await cloudwatch.get_metric_statistics(
                                    Namespace="AWS/ElastiCache",
                                    MetricName="CacheHits",
                                    Dimensions=[
                                        {"Name": "CacheClusterId", "Value": cluster_id},
                                    ],
                                    StartTime=start_time,
                                    EndTime=end_time,
                                    Period=604800,  # 7 days
                                    Statistics=["Sum"],
                                )
                                datapoints = hits_response.get("Datapoints", [])
                                if datapoints:
                                    cache_hits = datapoints[0].get("Sum", 0.0)
                            except Exception:
                                pass

                            # Get cache misses
                            cache_misses = 0.0
                            try:
                                misses_response = await cloudwatch.get_metric_statistics(
                                    Namespace="AWS/ElastiCache",
                                    MetricName="CacheMisses",
                                    Dimensions=[
                                        {"Name": "CacheClusterId", "Value": cluster_id},
                                    ],
                                    StartTime=start_time,
                                    EndTime=end_time,
                                    Period=604800,  # 7 days
                                    Statistics=["Sum"],
                                )
                                datapoints = misses_response.get("Datapoints", [])
                                if datapoints:
                                    cache_misses = datapoints[0].get("Sum", 0.0)
                            except Exception:
                                pass

                            # Get current connections
                            curr_connections = 0.0
                            try:
                                conn_response = await cloudwatch.get_metric_statistics(
                                    Namespace="AWS/ElastiCache",
                                    MetricName="CurrConnections",
                                    Dimensions=[
                                        {"Name": "CacheClusterId", "Value": cluster_id},
                                    ],
                                    StartTime=start_time,
                                    EndTime=end_time,
                                    Period=604800,  # 7 days
                                    Statistics=["Average"],
                                )
                                datapoints = conn_response.get("Datapoints", [])
                                if datapoints:
                                    curr_connections = datapoints[0].get("Average", 0.0)
                            except Exception:
                                pass

                            # Get memory usage
                            memory_usage_percent = 0.0
                            try:
                                memory_response = await cloudwatch.get_metric_statistics(
                                    Namespace="AWS/ElastiCache",
                                    MetricName="DatabaseMemoryUsagePercentage",
                                    Dimensions=[
                                        {"Name": "CacheClusterId", "Value": cluster_id},
                                    ],
                                    StartTime=start_time,
                                    EndTime=end_time,
                                    Period=604800,  # 7 days
                                    Statistics=["Average"],
                                )
                                datapoints = memory_response.get("Datapoints", [])
                                if datapoints:
                                    memory_usage_percent = datapoints[0].get("Average", 0.0)
                            except Exception:
                                pass

                            # Calculate monthly cost
                            monthly_cost = self._calculate_elasticache_monthly_cost(
                                node_type=node_type,
                                num_nodes=num_nodes,
                                engine=engine,
                                region=region,
                            )

                            # Calculate optimization metrics
                            (
                                is_optimizable,
                                optimization_score,
                                optimization_priority,
                                potential_savings,
                                optimization_recommendations,
                            ) = self._calculate_elasticache_optimization(
                                cluster=cluster,
                                cache_hits=cache_hits,
                                cache_misses=cache_misses,
                                curr_connections=curr_connections,
                                memory_usage_percent=memory_usage_percent,
                                monthly_cost=monthly_cost,
                            )

                            # Calculate cache hit rate
                            total_requests = cache_hits + cache_misses
                            hit_rate = (cache_hits / total_requests * 100) if total_requests > 0 else 0.0

                            # Build metadata
                            metadata = {
                                "cluster_id": cluster_id,
                                "cluster_arn": cluster_arn,
                                "cluster_status": cluster_status,
                                "node_type": node_type,
                                "num_nodes": num_nodes,
                                "engine": engine,
                                "engine_version": engine_version,
                                "cache_hits": round(cache_hits, 2),
                                "cache_misses": round(cache_misses, 2),
                                "hit_rate_percent": round(hit_rate, 2),
                                "curr_connections": round(curr_connections, 2),
                                "memory_usage_percent": round(memory_usage_percent, 2),
                            }

                            # Create resource entry
                            resource = AllCloudResourceData(
                                resource_id=cluster_arn,
                                resource_type="elasticache_cluster",
                                resource_name=cluster_id,
                                region=region,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=metadata,
                                created_at_cloud=created_at,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=optimization_priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=optimization_recommendations,
                            )

                            resources.append(resource)

                        except Exception as e:
                            logger.error(
                                "elasticache.cluster_scan_failed",
                                cluster_id=cluster.get("CacheClusterId", "Unknown"),
                                region=region,
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "elasticache.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # =========================================================================
    # AWS Kinesis Stream - Cost Optimization Scanning
    # =========================================================================

    def _calculate_kinesis_monthly_cost(
        self,
        shard_count: int,
        retention_hours: int,
        enhanced_fanout_consumers: int,
        incoming_bytes_per_month: float,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS Kinesis Stream.

        Pricing model:
        - Shard cost: $0.015/hour = $10.80/month per shard
        - Data retention (3 tiers):
          * Free: First 24 hours (default)
          * Extended: 25-168 hours ($0.020/GB/month)
          * Long-term: >168 hours ($0.026/GB/month)
        - Enhanced fan-out: $0.015/hour = $10.95/month per consumer

        Args:
            shard_count: Number of shards
            retention_hours: Retention period (24-8760 hours)
            enhanced_fanout_consumers: Number of enhanced fan-out consumers
            incoming_bytes_per_month: Monthly incoming data volume (bytes)
            region: AWS region

        Returns:
            Estimated monthly cost in USD
        """
        # Shard cost
        shard_monthly_cost = self.PRICING.get("kinesis_shard", 10.80)
        total_shard_cost = shard_monthly_cost * shard_count

        # Retention cost (beyond free 24h)
        retention_cost = 0.0
        if retention_hours > 24:
            # Convert bytes to GB
            data_gb = incoming_bytes_per_month / (1024**3)

            if retention_hours <= 168:  # 25-168 hours (Extended)
                retention_rate = self.PRICING.get("kinesis_retention_extended_per_gb", 0.020)
                retention_cost = data_gb * retention_rate
            else:  # >168 hours (Long-term)
                retention_rate = self.PRICING.get("kinesis_retention_long_per_gb", 0.026)
                retention_cost = data_gb * retention_rate

        # Enhanced fan-out cost
        fanout_monthly_cost = self.PRICING.get("kinesis_enhanced_fanout_per_consumer", 10.95)
        total_fanout_cost = fanout_monthly_cost * enhanced_fanout_consumers

        total_cost = total_shard_cost + retention_cost + total_fanout_cost
        return total_cost

    def _calculate_kinesis_optimization(
        self,
        stream: dict[str, Any],
        incoming_records: float,
        incoming_bytes: float,
        get_records_count: float,
        iterator_age_ms: float,
        shard_count: int,
        retention_hours: int,
        enhanced_fanout_consumers: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Analyze AWS Kinesis Stream for cost optimization opportunities.

        Optimization scenarios (5):
        1. Completely inactive - No incoming records (critical)
        2. Written but never read - Data written, never consumed (high)
        3. Under-utilized - <1% throughput used (high)
        4. Excessive retention - >24h when not needed (medium)
        5. Unused enhanced fan-out - Consumers not reading data (medium)

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Completely inactive (no incoming records)
        if incoming_records == 0.0:
            is_optimizable = True
            optimization_score = 95
            priority = "critical"
            potential_savings = monthly_cost  # Full savings
            recommendations.append(
                {
                    "type": "delete_stream",
                    "title": "Delete Completely Inactive Kinesis Stream",
                    "description": f"Stream '{stream.get('StreamName', '')}' has received NO records in the last 7 days. "
                    f"This stream is not being used and costs ${monthly_cost:.2f}/month.",
                    "impact": "high",
                    "estimated_savings": f"${monthly_cost:.2f}/month",
                    "action": "Delete this unused Kinesis Stream",
                    "risk": "low",
                }
            )
            return (is_optimizable, optimization_score, priority, potential_savings, recommendations)

        # Scenario 2: HIGH - Written but never read (waste of shards)
        if incoming_records > 0 and get_records_count == 0.0:
            is_optimizable = True
            optimization_score = 85
            priority = "high"
            potential_savings = monthly_cost * 0.80  # 80% savings
            recommendations.append(
                {
                    "type": "unused_stream",
                    "title": "Stream Receives Data But Never Consumed",
                    "description": f"Stream '{stream.get('StreamName', '')}' receives {incoming_records:.0f} records/week "
                    f"but has ZERO GetRecords API calls. Data is written but never read.",
                    "impact": "high",
                    "estimated_savings": f"${potential_savings:.2f}/month",
                    "action": "Review if this stream is still needed, or implement consumers",
                    "risk": "medium",
                }
            )

        # Scenario 3: HIGH - Under-utilized (<1% throughput)
        # Kinesis shard capacity: 1 MB/s incoming, 2 MB/s outgoing, 1000 records/s
        # For 7 days, max capacity = 1 MB/s * 604800s = ~575 GB incoming
        max_incoming_bytes_7d = shard_count * 1_000_000 * 604800  # bytes
        utilization_percent = (incoming_bytes / max_incoming_bytes_7d) * 100 if max_incoming_bytes_7d > 0 else 0

        if utilization_percent < 1.0 and incoming_records > 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority != "critical" else priority

            # Estimate right-sized shard count
            recommended_shards = max(1, int(shard_count * utilization_percent / 100))
            shard_savings = (shard_count - recommended_shards) * self.PRICING.get("kinesis_shard", 10.80)
            potential_savings = max(potential_savings, shard_savings)

            recommendations.append(
                {
                    "type": "reduce_shards",
                    "title": f"Kinesis Stream Under-Utilized ({utilization_percent:.2f}% Capacity)",
                    "description": f"Stream '{stream.get('StreamName', '')}' uses only {utilization_percent:.2f}% "
                    f"of its {shard_count} shard(s) capacity. Recommend reducing to {recommended_shards} shard(s).",
                    "impact": "medium",
                    "estimated_savings": f"${shard_savings:.2f}/month",
                    "action": f"Reduce shard count from {shard_count} to {recommended_shards}",
                    "risk": "low",
                }
            )

        # Scenario 4: MEDIUM - Excessive retention (>24h when not needed)
        # If iterator age is low (<1h), consumers are reading data quickly
        # Extended retention (>24h) may be unnecessary
        if retention_hours > 24 and iterator_age_ms < 3_600_000:  # <1 hour lag
            is_optimizable = True
            optimization_score = max(optimization_score, 60)
            priority = "medium" if priority == "low" else priority

            # Calculate retention cost savings
            incoming_bytes_monthly = incoming_bytes * 4.33  # 7d  30d estimate
            data_gb = incoming_bytes_monthly / (1024**3)
            retention_rate = self.PRICING.get("kinesis_retention_extended_per_gb", 0.020)
            retention_savings = data_gb * retention_rate
            potential_savings = max(potential_savings, retention_savings)

            recommendations.append(
                {
                    "type": "reduce_retention",
                    "title": f"Excessive Data Retention ({retention_hours} Hours)",
                    "description": f"Stream '{stream.get('StreamName', '')}' retains data for {retention_hours} hours "
                    f"but consumers process data within 1 hour. Default 24h retention is sufficient.",
                    "impact": "low",
                    "estimated_savings": f"${retention_savings:.2f}/month",
                    "action": f"Reduce retention period from {retention_hours}h to 24h",
                    "risk": "low",
                }
            )

        # Scenario 5: MEDIUM - Unused enhanced fan-out (paying for consumers not reading)
        if enhanced_fanout_consumers > 0 and get_records_count == 0.0:
            is_optimizable = True
            optimization_score = max(optimization_score, 65)
            priority = "medium" if priority == "low" else priority

            fanout_cost = enhanced_fanout_consumers * self.PRICING.get("kinesis_enhanced_fanout_per_consumer", 10.95)
            potential_savings = max(potential_savings, fanout_cost)

            recommendations.append(
                {
                    "type": "remove_fanout",
                    "title": f"Unused Enhanced Fan-Out Consumers ({enhanced_fanout_consumers})",
                    "description": f"Stream '{stream.get('StreamName', '')}' has {enhanced_fanout_consumers} "
                    f"enhanced fan-out consumer(s) but zero GetRecords calls. These consumers are not reading data.",
                    "impact": "medium",
                    "estimated_savings": f"${fanout_cost:.2f}/month",
                    "action": "Remove unused enhanced fan-out consumers",
                    "risk": "low",
                }
            )

        return (is_optimizable, optimization_score, priority, potential_savings, recommendations)

    async def scan_kinesis_streams(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS Kinesis Streams in a region for cost intelligence.

        For each stream:
        - Calculate monthly cost (shards + retention + enhanced fan-out)
        - Fetch CloudWatch metrics (IncomingRecords, IncomingBytes, GetRecords, IteratorAge)
        - Analyze optimization opportunities (5 scenarios)

        Returns:
            List of AllCloudResourceData entries for all Kinesis Streams
        """
        resources: list[AllCloudResourceData] = []

        try:
            async with self.session.client("kinesis", region_name=region) as kinesis:
                async with self.session.client("cloudwatch", region_name=region) as cloudwatch:
                    # List all streams
                    paginator = kinesis.get_paginator("list_streams")
                    stream_names = []

                    async for page in paginator.paginate():
                        stream_names.extend(page.get("StreamNames", []))

                    logger.info(
                        "inventory.kinesis.streams_found",
                        region=region,
                        count=len(stream_names),
                    )

                    # Process each stream
                    for stream_name in stream_names:
                        try:
                            # Get stream details
                            stream_response = await kinesis.describe_stream(StreamName=stream_name)
                            stream_desc = stream_response.get("StreamDescription", {})

                            stream_arn = stream_desc.get("StreamARN", "")
                            stream_status = stream_desc.get("StreamStatus", "UNKNOWN")
                            shard_count = len(stream_desc.get("Shards", []))
                            retention_hours = stream_desc.get("RetentionPeriodHours", 24)
                            encryption_type = stream_desc.get("EncryptionType", "NONE")
                            created_timestamp = stream_desc.get("StreamCreationTimestamp")

                            # Get enhanced fan-out consumers
                            consumers_response = await kinesis.list_stream_consumers(StreamARN=stream_arn)
                            consumers = consumers_response.get("Consumers", [])
                            enhanced_fanout_consumers = len(consumers)

                            # Get CloudWatch metrics (7 days)
                            end_time = datetime.utcnow()
                            start_time = end_time - timedelta(days=7)

                            # Metric 1: IncomingRecords (sum)
                            incoming_records_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/Kinesis",
                                MetricName="IncomingRecords",
                                Dimensions=[{"Name": "StreamName", "Value": stream_name}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,  # 7 days
                                Statistics=["Sum"],
                            )
                            incoming_records = (
                                incoming_records_response["Datapoints"][0]["Sum"]
                                if incoming_records_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 2: IncomingBytes (sum)
                            incoming_bytes_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/Kinesis",
                                MetricName="IncomingBytes",
                                Dimensions=[{"Name": "StreamName", "Value": stream_name}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            incoming_bytes = (
                                incoming_bytes_response["Datapoints"][0]["Sum"]
                                if incoming_bytes_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 3: GetRecords.Records (sum) - Consumption activity
                            get_records_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/Kinesis",
                                MetricName="GetRecords.Records",
                                Dimensions=[{"Name": "StreamName", "Value": stream_name}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            get_records_count = (
                                get_records_response["Datapoints"][0]["Sum"]
                                if get_records_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 4: GetRecords.IteratorAgeMilliseconds (avg) - Consumer lag
                            iterator_age_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/Kinesis",
                                MetricName="GetRecords.IteratorAgeMilliseconds",
                                Dimensions=[{"Name": "StreamName", "Value": stream_name}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Average"],
                            )
                            iterator_age_ms = (
                                iterator_age_response["Datapoints"][0]["Average"]
                                if iterator_age_response.get("Datapoints")
                                else 0.0
                            )

                            # Calculate cost
                            incoming_bytes_monthly = incoming_bytes * 4.33  # 7d  30d estimate
                            monthly_cost = self._calculate_kinesis_monthly_cost(
                                shard_count=shard_count,
                                retention_hours=retention_hours,
                                enhanced_fanout_consumers=enhanced_fanout_consumers,
                                incoming_bytes_per_month=incoming_bytes_monthly,
                                region=region,
                            )

                            # Calculate optimization
                            (
                                is_optimizable,
                                optimization_score,
                                priority,
                                potential_savings,
                                recommendations,
                            ) = self._calculate_kinesis_optimization(
                                stream=stream_desc,
                                incoming_records=incoming_records,
                                incoming_bytes=incoming_bytes,
                                get_records_count=get_records_count,
                                iterator_age_ms=iterator_age_ms,
                                shard_count=shard_count,
                                retention_hours=retention_hours,
                                enhanced_fanout_consumers=enhanced_fanout_consumers,
                                monthly_cost=monthly_cost,
                            )

                            # Build metadata
                            resource_metadata = {
                                "stream_name": stream_name,
                                "stream_arn": stream_arn,
                                "stream_status": stream_status,
                                "shard_count": shard_count,
                                "retention_hours": retention_hours,
                                "encryption_type": encryption_type,
                                "enhanced_fanout_consumers": enhanced_fanout_consumers,
                                "created_timestamp": created_timestamp.isoformat() if created_timestamp else None,
                                "metrics": {
                                    "incoming_records_7d": incoming_records,
                                    "incoming_bytes_7d": incoming_bytes,
                                    "get_records_count_7d": get_records_count,
                                    "iterator_age_ms_avg": iterator_age_ms,
                                },
                            }

                            # Create resource entry
                            resource = AllCloudResourceData(
                                resource_id=stream_arn,
                                resource_type="kinesis_stream",
                                resource_name=stream_name,
                                region=region,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=resource_metadata,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                            )

                            resources.append(resource)

                            logger.info(
                                "inventory.kinesis.stream_scanned",
                                stream_name=stream_name,
                                region=region,
                                shard_count=shard_count,
                                monthly_cost=monthly_cost,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                            )

                        except Exception as e:
                            logger.error(
                                "inventory.kinesis.stream_scan_error",
                                stream_name=stream_name,
                                region=region,
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "kinesis.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # =========================================================================
    # AWS FSx File System - Cost Optimization Scanning
    # =========================================================================

    def _calculate_fsx_monthly_cost(
        self,
        file_system_type: str,
        storage_capacity_gb: int,
        throughput_capacity_mbps: int,
        backup_storage_gb: float,
        deployment_type: str,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS FSx File System.

        Pricing model (4 file system types):
        1. FSx for Lustre: $0.145/GB/month + $2.20/MB/s throughput
        2. FSx for Windows (SSD): $0.13/GB/month + $2.20/MB/s throughput
        3. FSx for Windows (HDD): $0.013/GB/month + $2.20/MB/s throughput
        4. FSx for ONTAP: $0.144/GB/month + $2.20/MB/s throughput
        5. FSx for OpenZFS: $0.0996/GB/month + $2.20/MB/s throughput
        6. Backup: $0.05/GB/month

        Args:
            file_system_type: Type (LUSTRE, WINDOWS, ONTAP, OPENZFS)
            storage_capacity_gb: Storage size in GB
            throughput_capacity_mbps: Throughput capacity in MB/s
            backup_storage_gb: Backup storage size in GB
            deployment_type: SINGLE_AZ_1, SINGLE_AZ_2, MULTI_AZ_1
            region: AWS region

        Returns:
            Estimated monthly cost in USD
        """
        # Storage cost
        storage_cost = 0.0
        if file_system_type == "LUSTRE":
            storage_rate = self.PRICING.get("fsx_lustre_per_gb", 0.145)
            storage_cost = storage_capacity_gb * storage_rate
        elif file_system_type == "WINDOWS":
            # Detect HDD vs SSD based on deployment_type or assume SSD
            storage_rate = self.PRICING.get("fsx_windows_per_gb", 0.13)
            storage_cost = storage_capacity_gb * storage_rate
        elif file_system_type == "ONTAP":
            storage_rate = self.PRICING.get("fsx_ontap_per_gb", 0.144)
            storage_cost = storage_capacity_gb * storage_rate
        elif file_system_type == "OPENZFS":
            storage_rate = self.PRICING.get("fsx_openzfs_per_gb", 0.0996)
            storage_cost = storage_capacity_gb * storage_rate
        else:
            # Fallback
            storage_cost = storage_capacity_gb * 0.13

        # Throughput cost (if applicable)
        throughput_cost = 0.0
        if throughput_capacity_mbps > 0:
            throughput_rate = self.PRICING.get("fsx_throughput_per_mbps", 2.20)
            throughput_cost = throughput_capacity_mbps * throughput_rate

        # Backup cost
        backup_rate = self.PRICING.get("fsx_backup_per_gb", 0.05)
        backup_cost = backup_storage_gb * backup_rate

        total_cost = storage_cost + throughput_cost + backup_cost
        return total_cost

    def _calculate_fsx_optimization(
        self,
        file_system: dict[str, Any],
        data_read_ops: float,
        data_write_ops: float,
        data_read_bytes: float,
        data_write_bytes: float,
        storage_capacity_gb: int,
        throughput_capacity_mbps: int,
        file_system_type: str,
        deployment_type: str,
        backup_storage_gb: float,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Analyze AWS FSx File System for cost optimization opportunities.

        Optimization scenarios (5):
        1. Completely inactive - No read/write operations (critical)
        2. Over-provisioned storage - >50% unused (high)
        3. Over-provisioned throughput - <20% utilization (high)
        4. Wrong storage type - Expensive SSD when HDD works (medium)
        5. Excessive backup retention - >90 days backup (medium)

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Completely inactive (no operations)
        if data_read_ops == 0.0 and data_write_ops == 0.0:
            is_optimizable = True
            optimization_score = 95
            priority = "critical"
            potential_savings = monthly_cost  # Full savings
            recommendations.append(
                {
                    "type": "delete_filesystem",
                    "title": "Delete Completely Inactive FSx File System",
                    "description": f"File system '{file_system.get('FileSystemId', '')}' has had NO read or write "
                    f"operations in the last 7 days. This file system is completely unused and costs ${monthly_cost:.2f}/month.",
                    "impact": "high",
                    "estimated_savings": f"${monthly_cost:.2f}/month",
                    "action": "Delete this unused FSx file system after verifying no critical data",
                    "risk": "low",
                }
            )
            return (is_optimizable, optimization_score, priority, potential_savings, recommendations)

        # Scenario 2: HIGH - Over-provisioned storage (>50% unused)
        # Estimate storage usage based on write operations
        total_ops = data_read_ops + data_write_ops
        if total_ops > 0:
            # Rough estimate: If very few write operations, storage likely under-utilized
            # This is a simplification - real storage usage requires detailed analysis
            write_ratio = data_write_ops / total_ops if total_ops > 0 else 0
            if write_ratio < 0.1 and storage_capacity_gb > 1024:  # <10% writes, >1TB storage
                is_optimizable = True
                optimization_score = max(optimization_score, 75)
                priority = "high" if priority != "critical" else priority

                # Estimate 40% reduction in storage
                recommended_storage_gb = int(storage_capacity_gb * 0.6)
                storage_rate = 0.13  # Default
                if file_system_type == "LUSTRE":
                    storage_rate = self.PRICING.get("fsx_lustre_per_gb", 0.145)
                elif file_system_type == "ONTAP":
                    storage_rate = self.PRICING.get("fsx_ontap_per_gb", 0.144)
                elif file_system_type == "OPENZFS":
                    storage_rate = self.PRICING.get("fsx_openzfs_per_gb", 0.0996)

                storage_savings = (storage_capacity_gb - recommended_storage_gb) * storage_rate
                potential_savings = max(potential_savings, storage_savings)

                recommendations.append(
                    {
                        "type": "reduce_storage",
                        "title": f"FSx File System Over-Provisioned (Low Write Activity)",
                        "description": f"File system '{file_system.get('FileSystemId', '')}' has only {write_ratio*100:.1f}% "
                        f"write operations, indicating potential over-provisioning of {storage_capacity_gb} GB storage.",
                        "impact": "medium",
                        "estimated_savings": f"${storage_savings:.2f}/month",
                        "action": f"Consider reducing storage capacity from {storage_capacity_gb} GB to {recommended_storage_gb} GB",
                        "risk": "medium",
                    }
                )

        # Scenario 3: HIGH - Over-provisioned throughput (<20% utilization)
        # Estimate throughput usage based on read/write bytes
        # FSx throughput capacity measured in MB/s, convert 7-day bytes to avg MB/s
        if throughput_capacity_mbps > 0:
            seven_days_seconds = 604800
            total_bytes_7d = data_read_bytes + data_write_bytes
            avg_throughput_mbps = (total_bytes_7d / seven_days_seconds) / (1024 * 1024)  # Bytes/s to MB/s
            utilization_percent = (avg_throughput_mbps / throughput_capacity_mbps) * 100

            if utilization_percent < 20.0 and throughput_capacity_mbps > 8:  # <20% utilization, >8 MB/s capacity
                is_optimizable = True
                optimization_score = max(optimization_score, 80)
                priority = "high" if priority != "critical" else priority

                # Estimate right-sized throughput
                recommended_throughput = max(8, int(throughput_capacity_mbps * 0.5))  # Min 8 MB/s
                throughput_rate = self.PRICING.get("fsx_throughput_per_mbps", 2.20)
                throughput_savings = (throughput_capacity_mbps - recommended_throughput) * throughput_rate
                potential_savings = max(potential_savings, throughput_savings)

                recommendations.append(
                    {
                        "type": "reduce_throughput",
                        "title": f"FSx Throughput Under-Utilized ({utilization_percent:.1f}% Used)",
                        "description": f"File system '{file_system.get('FileSystemId', '')}' uses only {utilization_percent:.1f}% "
                        f"of its {throughput_capacity_mbps} MB/s throughput capacity. Recommend reducing to {recommended_throughput} MB/s.",
                        "impact": "medium",
                        "estimated_savings": f"${throughput_savings:.2f}/month",
                        "action": f"Reduce throughput capacity from {throughput_capacity_mbps} to {recommended_throughput} MB/s",
                        "risk": "low",
                    }
                )

        # Scenario 4: MEDIUM - Wrong storage type (SSD when HDD works)
        # Windows File Server supports both SSD and HDD
        # If low IOPS requirements, HDD is 10x cheaper
        if file_system_type == "WINDOWS" and total_ops > 0:
            # If total operations per second < 100 IOPS, HDD is sufficient
            seven_days_seconds = 604800
            avg_iops = total_ops / seven_days_seconds

            if avg_iops < 100:  # Low IOPS workload
                is_optimizable = True
                optimization_score = max(optimization_score, 65)
                priority = "medium" if priority == "low" else priority

                # Calculate savings by switching SSD to HDD
                ssd_rate = self.PRICING.get("fsx_windows_per_gb", 0.13)
                hdd_rate = self.PRICING.get("fsx_windows_hdd_per_gb", 0.013)
                storage_savings = storage_capacity_gb * (ssd_rate - hdd_rate)
                potential_savings = max(potential_savings, storage_savings)

                recommendations.append(
                    {
                        "type": "switch_to_hdd",
                        "title": f"FSx Windows File Server - Switch SSD to HDD (Low IOPS)",
                        "description": f"File system '{file_system.get('FileSystemId', '')}' has only {avg_iops:.1f} IOPS "
                        f"on average. For low-IOPS workloads, HDD storage is 10x cheaper than SSD.",
                        "impact": "low",
                        "estimated_savings": f"${storage_savings:.2f}/month (90% cost reduction)",
                        "action": f"Migrate from SSD to HDD storage for {storage_capacity_gb} GB",
                        "risk": "low",
                    }
                )

        # Scenario 5: MEDIUM - Excessive backup retention (>90 days)
        # High backup storage relative to primary storage indicates long retention
        if backup_storage_gb > 0:
            backup_ratio = backup_storage_gb / storage_capacity_gb if storage_capacity_gb > 0 else 0

            if backup_ratio > 3.0:  # >3x primary storage (suggests 90+ days retention)
                is_optimizable = True
                optimization_score = max(optimization_score, 55)
                priority = "medium" if priority == "low" else priority

                # Estimate 50% reduction in backup storage
                recommended_backup_gb = backup_storage_gb * 0.5
                backup_rate = self.PRICING.get("fsx_backup_per_gb", 0.05)
                backup_savings = (backup_storage_gb - recommended_backup_gb) * backup_rate
                potential_savings = max(potential_savings, backup_savings)

                recommendations.append(
                    {
                        "type": "reduce_backup_retention",
                        "title": f"Excessive Backup Retention ({backup_ratio:.1f}x Primary Storage)",
                        "description": f"File system '{file_system.get('FileSystemId', '')}' has {backup_storage_gb:.0f} GB "
                        f"of backups, which is {backup_ratio:.1f}x the primary storage. Consider shorter retention period.",
                        "impact": "low",
                        "estimated_savings": f"${backup_savings:.2f}/month",
                        "action": f"Reduce backup retention to free up {backup_storage_gb - recommended_backup_gb:.0f} GB",
                        "risk": "low",
                    }
                )

        return (is_optimizable, optimization_score, priority, potential_savings, recommendations)

    async def scan_fsx_file_systems(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS FSx File Systems in a region for cost intelligence.

        Supports 4 FSx types:
        - FSx for Lustre (HPC, machine learning)
        - FSx for Windows File Server (SSD/HDD)
        - FSx for NetApp ONTAP (enterprise NAS)
        - FSx for OpenZFS (Linux workloads)

        For each file system:
        - Calculate monthly cost (storage + throughput + backup)
        - Fetch CloudWatch metrics (DataReadOperations, DataWriteOperations, DataReadBytes, DataWriteBytes)
        - Analyze optimization opportunities (5 scenarios)

        Returns:
            List of AllCloudResourceData entries for all FSx File Systems
        """
        resources: list[AllCloudResourceData] = []

        try:
            async with self.session.client("fsx", region_name=region) as fsx:
                async with self.session.client("cloudwatch", region_name=region) as cloudwatch:
                    # List all file systems
                    paginator = fsx.get_paginator("describe_file_systems")
                    file_systems = []

                    async for page in paginator.paginate():
                        file_systems.extend(page.get("FileSystems", []))

                    logger.info(
                        "inventory.fsx.file_systems_found",
                        region=region,
                        count=len(file_systems),
                    )

                    # Process each file system
                    for fs in file_systems:
                        try:
                            filesystem_id = fs.get("FileSystemId", "")
                            filesystem_type = fs.get("FileSystemType", "UNKNOWN")  # LUSTRE, WINDOWS, ONTAP, OPENZFS
                            lifecycle = fs.get("Lifecycle", "UNKNOWN")
                            storage_capacity_gb = fs.get("StorageCapacity", 0)
                            created_time = fs.get("CreationTime")

                            # Get type-specific details
                            deployment_type = "SINGLE_AZ_1"  # Default
                            throughput_capacity_mbps = 0

                            if filesystem_type == "WINDOWS":
                                windows_config = fs.get("WindowsConfiguration", {})
                                deployment_type = windows_config.get("DeploymentType", "SINGLE_AZ_1")
                                throughput_capacity_mbps = windows_config.get("ThroughputCapacity", 0)
                            elif filesystem_type == "LUSTRE":
                                lustre_config = fs.get("LustreConfiguration", {})
                                deployment_type = lustre_config.get("DeploymentType", "SCRATCH_1")
                                throughput_capacity_mbps = lustre_config.get("PerUnitStorageThroughput", 0) * storage_capacity_gb
                            elif filesystem_type == "ONTAP":
                                ontap_config = fs.get("OntapConfiguration", {})
                                deployment_type = ontap_config.get("DeploymentType", "SINGLE_AZ_1")
                                throughput_capacity_mbps = ontap_config.get("ThroughputCapacity", 0)
                            elif filesystem_type == "OPENZFS":
                                openzfs_config = fs.get("OpenZFSConfiguration", {})
                                deployment_type = openzfs_config.get("DeploymentType", "SINGLE_AZ_1")
                                throughput_capacity_mbps = openzfs_config.get("ThroughputCapacity", 0)

                            # Get backup information
                            # Note: BackupStorage is not directly available in describe_file_systems
                            # Would need to call describe_backups to get exact backup storage
                            # For now, estimate as 0 (can be enhanced later)
                            backup_storage_gb = 0.0

                            # Get CloudWatch metrics (7 days)
                            end_time = datetime.utcnow()
                            start_time = end_time - timedelta(days=7)

                            # Metric 1: DataReadOperations (sum)
                            data_read_ops_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/FSx",
                                MetricName="DataReadOperations",
                                Dimensions=[{"Name": "FileSystemId", "Value": filesystem_id}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,  # 7 days
                                Statistics=["Sum"],
                            )
                            data_read_ops = (
                                data_read_ops_response["Datapoints"][0]["Sum"]
                                if data_read_ops_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 2: DataWriteOperations (sum)
                            data_write_ops_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/FSx",
                                MetricName="DataWriteOperations",
                                Dimensions=[{"Name": "FileSystemId", "Value": filesystem_id}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            data_write_ops = (
                                data_write_ops_response["Datapoints"][0]["Sum"]
                                if data_write_ops_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 3: DataReadBytes (sum)
                            data_read_bytes_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/FSx",
                                MetricName="DataReadBytes",
                                Dimensions=[{"Name": "FileSystemId", "Value": filesystem_id}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            data_read_bytes = (
                                data_read_bytes_response["Datapoints"][0]["Sum"]
                                if data_read_bytes_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 4: DataWriteBytes (sum)
                            data_write_bytes_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/FSx",
                                MetricName="DataWriteBytes",
                                Dimensions=[{"Name": "FileSystemId", "Value": filesystem_id}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            data_write_bytes = (
                                data_write_bytes_response["Datapoints"][0]["Sum"]
                                if data_write_bytes_response.get("Datapoints")
                                else 0.0
                            )

                            # Calculate cost
                            monthly_cost = self._calculate_fsx_monthly_cost(
                                file_system_type=filesystem_type,
                                storage_capacity_gb=storage_capacity_gb,
                                throughput_capacity_mbps=throughput_capacity_mbps,
                                backup_storage_gb=backup_storage_gb,
                                deployment_type=deployment_type,
                                region=region,
                            )

                            # Calculate optimization
                            (
                                is_optimizable,
                                optimization_score,
                                priority,
                                potential_savings,
                                recommendations,
                            ) = self._calculate_fsx_optimization(
                                file_system=fs,
                                data_read_ops=data_read_ops,
                                data_write_ops=data_write_ops,
                                data_read_bytes=data_read_bytes,
                                data_write_bytes=data_write_bytes,
                                storage_capacity_gb=storage_capacity_gb,
                                throughput_capacity_mbps=throughput_capacity_mbps,
                                file_system_type=filesystem_type,
                                deployment_type=deployment_type,
                                backup_storage_gb=backup_storage_gb,
                                monthly_cost=monthly_cost,
                            )

                            # Build metadata
                            resource_metadata = {
                                "filesystem_id": filesystem_id,
                                "filesystem_type": filesystem_type,
                                "lifecycle": lifecycle,
                                "storage_capacity_gb": storage_capacity_gb,
                                "throughput_capacity_mbps": throughput_capacity_mbps,
                                "deployment_type": deployment_type,
                                "backup_storage_gb": backup_storage_gb,
                                "created_time": created_time.isoformat() if created_time else None,
                                "metrics": {
                                    "data_read_ops_7d": data_read_ops,
                                    "data_write_ops_7d": data_write_ops,
                                    "data_read_bytes_7d": data_read_bytes,
                                    "data_write_bytes_7d": data_write_bytes,
                                },
                            }

                            # Create resource entry
                            resource = AllCloudResourceData(
                                resource_id=filesystem_id,
                                resource_type="fsx_file_system",
                                resource_name=filesystem_id,  # FSx uses ID as name
                                region=region,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=resource_metadata,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                            )

                            resources.append(resource)

                            logger.info(
                                "inventory.fsx.filesystem_scanned",
                                filesystem_id=filesystem_id,
                                region=region,
                                filesystem_type=filesystem_type,
                                storage_capacity_gb=storage_capacity_gb,
                                monthly_cost=monthly_cost,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                            )

                        except Exception as e:
                            logger.error(
                                "inventory.fsx.filesystem_scan_error",
                                filesystem_id=fs.get("FileSystemId", "unknown"),
                                region=region,
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "fsx.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # =========================================================================
    # AWS OpenSearch Domain - Cost Optimization Scanning
    # =========================================================================

    def _calculate_opensearch_monthly_cost(
        self,
        instance_type: str,
        instance_count: int,
        storage_size_gb: int,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS OpenSearch Domain.

        Pricing model:
        - Instance cost: t3.small ($0.036/h) to r5.large ($0.228/h)  instance count
        - Storage cost: $0.10/GB/month (EBS GP2 SSD)

        Args:
            instance_type: Instance type (e.g., t3.small.search, m5.large.search)
            instance_count: Number of data nodes
            storage_size_gb: EBS storage size in GB
            region: AWS region

        Returns:
            Estimated monthly cost in USD
        """
        # Map instance type to pricing key
        # instance_type format: "t3.small.search"  pricing key: "opensearch_t3_small"
        pricing_key = f"opensearch_{instance_type.replace('.search', '').replace('.', '_')}"
        hourly_cost = self.PRICING.get(pricing_key, 0.161)  # Fallback to m5.large
        instance_monthly_cost = hourly_cost * 730 * instance_count

        # Storage cost
        storage_rate = self.PRICING.get("opensearch_storage_per_gb", 0.10)
        storage_monthly_cost = storage_size_gb * storage_rate

        total_cost = instance_monthly_cost + storage_monthly_cost
        return total_cost

    def _calculate_opensearch_optimization(
        self,
        domain: dict[str, Any],
        search_rate: float,
        indexing_rate: float,
        cpu_utilization: float,
        jvm_memory_pressure: float,
        free_storage_gb: float,
        total_storage_gb: int,
        instance_type: str,
        instance_count: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Analyze AWS OpenSearch Domain for cost optimization opportunities.

        Optimization scenarios (5):
        1. Completely idle - No search/indexing activity (critical)
        2. Very low usage - <100 requests/day (high)
        3. Over-provisioned instance - Low CPU/JVM (high)
        4. Over-provisioned storage - >50% free (medium)
        5. Wrong instance family - Memory vs compute optimized (medium)

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Completely idle (no activity)
        if search_rate == 0.0 and indexing_rate == 0.0:
            is_optimizable = True
            optimization_score = 95
            priority = "critical"
            potential_savings = monthly_cost  # Full savings
            recommendations.append(
                {
                    "type": "delete_domain",
                    "title": "Delete Completely Idle OpenSearch Domain",
                    "description": f"Domain '{domain.get('DomainName', '')}' has had NO search or indexing "
                    f"activity in the last 7 days. This domain is completely unused and costs ${monthly_cost:.2f}/month.",
                    "impact": "high",
                    "estimated_savings": f"${monthly_cost:.2f}/month",
                    "action": "Delete this unused OpenSearch domain after verifying no critical data",
                    "risk": "low",
                }
            )
            return (is_optimizable, optimization_score, priority, potential_savings, recommendations)

        # Scenario 2: HIGH - Very low usage (<100 requests/day)
        total_requests_7d = search_rate + indexing_rate
        requests_per_day = total_requests_7d / 7

        if requests_per_day > 0 and requests_per_day < 100:
            is_optimizable = True
            optimization_score = max(optimization_score, 80)
            priority = "high" if priority != "critical" else priority

            # Recommend downsizing to t3.small (dev/test tier)
            if "t3.small" not in instance_type:
                t3_hourly = self.PRICING.get("opensearch_t3_small", 0.036)
                t3_monthly_cost = t3_hourly * 730 * instance_count
                storage_cost = total_storage_gb * self.PRICING.get("opensearch_storage_per_gb", 0.10)
                new_total_cost = t3_monthly_cost + storage_cost
                savings = monthly_cost - new_total_cost
                potential_savings = max(potential_savings, savings)

                recommendations.append(
                    {
                        "type": "downsize_to_dev_tier",
                        "title": f"Very Low Usage ({requests_per_day:.0f} Requests/Day)",
                        "description": f"Domain '{domain.get('DomainName', '')}' has only {requests_per_day:.0f} requests/day. "
                        f"Consider downsizing from {instance_type} to t3.small.search (dev/test tier).",
                        "impact": "medium",
                        "estimated_savings": f"${savings:.2f}/month",
                        "action": f"Downsize instance type to t3.small.search ({instance_count} node(s))",
                        "risk": "low",
                    }
                )

        # Scenario 3: HIGH - Over-provisioned instance (low CPU + low JVM)
        if cpu_utilization > 0 and cpu_utilization < 20.0 and jvm_memory_pressure < 30.0:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority != "critical" else priority

            # Estimate 40% cost reduction by downsizing
            instance_savings = monthly_cost * 0.40
            potential_savings = max(potential_savings, instance_savings)

            recommendations.append(
                {
                    "type": "downsize_instance",
                    "title": f"Over-Provisioned Instance (CPU {cpu_utilization:.1f}%, JVM {jvm_memory_pressure:.1f}%)",
                    "description": f"Domain '{domain.get('DomainName', '')}' uses only {cpu_utilization:.1f}% CPU "
                    f"and {jvm_memory_pressure:.1f}% JVM memory. Recommend downsizing instance type.",
                    "impact": "medium",
                    "estimated_savings": f"${instance_savings:.2f}/month",
                    "action": f"Downsize from {instance_type} ({instance_count} nodes) to smaller instance",
                    "risk": "low",
                }
            )

        # Scenario 4: MEDIUM - Over-provisioned storage (>50% free)
        if total_storage_gb > 0 and free_storage_gb > 0:
            storage_utilization_percent = ((total_storage_gb - free_storage_gb) / total_storage_gb) * 100

            if storage_utilization_percent < 50.0:
                is_optimizable = True
                optimization_score = max(optimization_score, 60)
                priority = "medium" if priority == "low" else priority

                # Recommend reducing storage by 30%
                recommended_storage_gb = int(total_storage_gb * 0.7)
                storage_rate = self.PRICING.get("opensearch_storage_per_gb", 0.10)
                storage_savings = (total_storage_gb - recommended_storage_gb) * storage_rate
                potential_savings = max(potential_savings, storage_savings)

                recommendations.append(
                    {
                        "type": "reduce_storage",
                        "title": f"Over-Provisioned Storage ({storage_utilization_percent:.1f}% Used)",
                        "description": f"Domain '{domain.get('DomainName', '')}' uses only {storage_utilization_percent:.1f}% "
                        f"of its {total_storage_gb} GB storage. {free_storage_gb:.0f} GB is free.",
                        "impact": "low",
                        "estimated_savings": f"${storage_savings:.2f}/month",
                        "action": f"Reduce storage from {total_storage_gb} GB to {recommended_storage_gb} GB",
                        "risk": "low",
                    }
                )

        # Scenario 5: MEDIUM - Wrong instance family (r5 vs m5)
        # If JVM memory pressure is low (<50%), don't need memory-optimized (r5)
        # If JVM memory pressure is high (>75%), need memory-optimized (r5)
        if jvm_memory_pressure > 0:
            if "r5" in instance_type and jvm_memory_pressure < 50.0:
                # Using r5 (memory-optimized) but low memory usage  switch to m5 (general purpose)
                is_optimizable = True
                optimization_score = max(optimization_score, 65)
                priority = "medium" if priority == "low" else priority

                # Estimate 30% savings by switching r5  m5
                family_savings = monthly_cost * 0.30
                potential_savings = max(potential_savings, family_savings)

                recommendations.append(
                    {
                        "type": "switch_instance_family",
                        "title": f"Memory-Optimized Instance Unnecessary (JVM {jvm_memory_pressure:.1f}%)",
                        "description": f"Domain '{domain.get('DomainName', '')}' uses r5 (memory-optimized) instance "
                        f"but only {jvm_memory_pressure:.1f}% JVM memory. Switch to m5 (general purpose) to save 30%.",
                        "impact": "low",
                        "estimated_savings": f"${family_savings:.2f}/month",
                        "action": f"Switch from {instance_type} to m5 equivalent",
                        "risk": "low",
                    }
                )
            elif "m5" in instance_type and jvm_memory_pressure > 75.0:
                # Using m5 (general purpose) but high memory usage  recommend r5 (memory-optimized)
                # This is not a cost optimization, but a performance recommendation
                # We don't add it to potential_savings
                recommendations.append(
                    {
                        "type": "upgrade_instance_family_performance",
                        "title": f"Consider Memory-Optimized Instance (JVM {jvm_memory_pressure:.1f}%)",
                        "description": f"Domain '{domain.get('DomainName', '')}' uses m5 (general purpose) but has "
                        f"{jvm_memory_pressure:.1f}% JVM memory pressure. Consider r5 for better performance.",
                        "impact": "low",
                        "estimated_savings": "$0.00/month (performance recommendation, not cost savings)",
                        "action": f"Evaluate switching to r5 instance for better performance",
                        "risk": "low",
                    }
                )

        return (is_optimizable, optimization_score, priority, potential_savings, recommendations)

    async def scan_opensearch_domains(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS OpenSearch Domains in a region for cost intelligence.

        For each domain:
        - Calculate monthly cost (instance + storage)
        - Fetch CloudWatch metrics (SearchRate, IndexingRate, CPU, JVM, Storage)
        - Analyze optimization opportunities (5 scenarios)

        Returns:
            List of AllCloudResourceData entries for all OpenSearch Domains
        """
        resources: list[AllCloudResourceData] = []

        try:
            async with self.session.client("opensearch", region_name=region) as opensearch:
                async with self.session.client("cloudwatch", region_name=region) as cloudwatch:
                    # List all domains
                    response = await opensearch.list_domain_names()
                    domain_names = [d["DomainName"] for d in response.get("DomainNames", [])]

                    logger.info(
                        "inventory.opensearch.domains_found",
                        region=region,
                        count=len(domain_names),
                    )

                    # Process each domain
                    for domain_name in domain_names:
                        try:
                            # Get domain details
                            domain_response = await opensearch.describe_domain(DomainName=domain_name)
                            domain = domain_response.get("DomainStatus", {})

                            domain_arn = domain.get("ARN", "")
                            domain_status = domain.get("Processing", False)  # Processing = updating
                            created_at = domain.get("Created")

                            # Get cluster config
                            cluster_config = domain.get("ClusterConfig", {})
                            instance_type = cluster_config.get("InstanceType", "m5.large.search")
                            instance_count = cluster_config.get("InstanceCount", 1)

                            # Get EBS storage
                            ebs_options = domain.get("EBSOptions", {})
                            ebs_enabled = ebs_options.get("EBSEnabled", False)
                            storage_size_gb = ebs_options.get("VolumeSize", 0) if ebs_enabled else 0

                            # Get CloudWatch metrics (7 days)
                            end_time = datetime.utcnow()
                            start_time = end_time - timedelta(days=7)

                            # Metric 1: SearchRate (sum over 7 days)
                            search_rate_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/ES",
                                MetricName="SearchRate",
                                Dimensions=[
                                    {"Name": "DomainName", "Value": domain_name},
                                    {"Name": "ClientId", "Value": domain_arn.split(":")[4]},
                                ],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,  # 7 days
                                Statistics=["Sum"],
                            )
                            search_rate = (
                                search_rate_response["Datapoints"][0]["Sum"]
                                if search_rate_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 2: IndexingRate (sum over 7 days)
                            indexing_rate_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/ES",
                                MetricName="IndexingRate",
                                Dimensions=[
                                    {"Name": "DomainName", "Value": domain_name},
                                    {"Name": "ClientId", "Value": domain_arn.split(":")[4]},
                                ],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            indexing_rate = (
                                indexing_rate_response["Datapoints"][0]["Sum"]
                                if indexing_rate_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 3: CPUUtilization (average over 7 days)
                            cpu_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/ES",
                                MetricName="CPUUtilization",
                                Dimensions=[
                                    {"Name": "DomainName", "Value": domain_name},
                                    {"Name": "ClientId", "Value": domain_arn.split(":")[4]},
                                ],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Average"],
                            )
                            cpu_utilization = (
                                cpu_response["Datapoints"][0]["Average"]
                                if cpu_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 4: JVMMemoryPressure (average)
                            jvm_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/ES",
                                MetricName="JVMMemoryPressure",
                                Dimensions=[
                                    {"Name": "DomainName", "Value": domain_name},
                                    {"Name": "ClientId", "Value": domain_arn.split(":")[4]},
                                ],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Average"],
                            )
                            jvm_memory_pressure = (
                                jvm_response["Datapoints"][0]["Average"]
                                if jvm_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 5: FreeStorageSpace (minimum, to detect low storage)
                            storage_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/ES",
                                MetricName="FreeStorageSpace",
                                Dimensions=[
                                    {"Name": "DomainName", "Value": domain_name},
                                    {"Name": "ClientId", "Value": domain_arn.split(":")[4]},
                                ],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Average"],
                            )
                            free_storage_mb = (
                                storage_response["Datapoints"][0]["Average"]
                                if storage_response.get("Datapoints")
                                else 0.0
                            )
                            free_storage_gb = free_storage_mb / 1024  # Convert MB to GB

                            # Calculate cost
                            monthly_cost = self._calculate_opensearch_monthly_cost(
                                instance_type=instance_type,
                                instance_count=instance_count,
                                storage_size_gb=storage_size_gb,
                                region=region,
                            )

                            # Calculate optimization
                            (
                                is_optimizable,
                                optimization_score,
                                priority,
                                potential_savings,
                                recommendations,
                            ) = self._calculate_opensearch_optimization(
                                domain=domain,
                                search_rate=search_rate,
                                indexing_rate=indexing_rate,
                                cpu_utilization=cpu_utilization,
                                jvm_memory_pressure=jvm_memory_pressure,
                                free_storage_gb=free_storage_gb,
                                total_storage_gb=storage_size_gb,
                                instance_type=instance_type,
                                instance_count=instance_count,
                                monthly_cost=monthly_cost,
                            )

                            # Build metadata
                            resource_metadata = {
                                "domain_name": domain_name,
                                "domain_arn": domain_arn,
                                "instance_type": instance_type,
                                "instance_count": instance_count,
                                "storage_size_gb": storage_size_gb,
                                "engine_version": domain.get("EngineVersion", "Unknown"),
                                "processing": domain_status,
                                "created_at": created_at.isoformat() if created_at else None,
                                "metrics": {
                                    "search_rate_7d": search_rate,
                                    "indexing_rate_7d": indexing_rate,
                                    "cpu_utilization_avg": cpu_utilization,
                                    "jvm_memory_pressure_avg": jvm_memory_pressure,
                                    "free_storage_gb": free_storage_gb,
                                },
                            }

                            # Create resource entry
                            resource = AllCloudResourceData(
                                resource_id=domain_arn,
                                resource_type="opensearch_domain",
                                resource_name=domain_name,
                                region=region,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=resource_metadata,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                            )

                            resources.append(resource)

                            logger.info(
                                "inventory.opensearch.domain_scanned",
                                domain_name=domain_name,
                                region=region,
                                instance_type=instance_type,
                                instance_count=instance_count,
                                monthly_cost=monthly_cost,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                            )

                        except Exception as e:
                            logger.error(
                                "inventory.opensearch.domain_scan_error",
                                domain_name=domain_name,
                                region=region,
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "opensearch.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # =========================================================================
    # AWS API Gateway - Cost Optimization Scanning
    # =========================================================================

    def _calculate_apigateway_monthly_cost(
        self,
        api_type: str,
        request_count_per_month: float,
        data_transfer_gb_per_month: float,
        cache_enabled: bool,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS API Gateway.

        Pricing model (3 API types):
        - REST API: $3.50 per million requests + $0.09/GB data transfer
        - HTTP API: $1.00 per million requests (70% cheaper, no caching)
        - WebSocket API: $1.00 per million messages + $0.25 per million connection minutes

        Args:
            api_type: API type (REST, HTTP, WEBSOCKET)
            request_count_per_month: Monthly request count (estimated from 7d)
            data_transfer_gb_per_month: Monthly data transfer in GB
            cache_enabled: Whether API cache is enabled (REST only)
            region: AWS region

        Returns:
            Estimated monthly cost in USD
        """
        # Request cost
        request_cost = 0.0
        if api_type == "REST":
            request_rate = self.PRICING.get("apigateway_rest_per_million", 3.50)
            request_cost = (request_count_per_month / 1_000_000) * request_rate
        elif api_type == "HTTP":
            request_rate = self.PRICING.get("apigateway_http_per_million", 1.00)
            request_cost = (request_count_per_month / 1_000_000) * request_rate
        elif api_type == "WEBSOCKET":
            message_rate = self.PRICING.get("apigateway_websocket_per_million", 1.00)
            request_cost = (request_count_per_month / 1_000_000) * message_rate
            # WebSocket also charges for connection minutes (not included here, would need additional metric)

        # Data transfer cost
        data_transfer_rate = self.PRICING.get("apigateway_data_transfer_per_gb", 0.09)
        data_transfer_cost = data_transfer_gb_per_month * data_transfer_rate

        # Cache cost (REST API only, if enabled)
        # Not included in basic pricing (would need cache size info)
        cache_cost = 0.0

        total_cost = request_cost + data_transfer_cost + cache_cost
        return total_cost

    def _calculate_apigateway_optimization(
        self,
        api: dict[str, Any],
        api_type: str,
        request_count: float,
        error_4xx_count: float,
        error_5xx_count: float,
        cache_hit_count: float,
        cache_miss_count: float,
        cache_enabled: bool,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Analyze AWS API Gateway for cost optimization opportunities.

        Optimization scenarios (6):
        1. Completely inactive - No requests (critical)
        2. Very low usage - <1000 req/day (high)
        3. High error rate - >10% 4XX/5XX (high)
        4. REST API when HTTP API sufficient - 70% savings (medium)
        5. Cache enabled but low hit rate - <50% (medium)
        6. WebSocket API with no active connections - Delete if unused (low)

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Completely inactive (no requests)
        if request_count == 0.0:
            is_optimizable = True
            optimization_score = 95
            priority = "critical"
            potential_savings = monthly_cost  # Full savings
            recommendations.append(
                {
                    "type": "delete_api",
                    "title": "Delete Completely Inactive API Gateway",
                    "description": f"API '{api.get('Name', api.get('name', ''))}' has received NO requests "
                    f"in the last 7 days. This API is completely unused and costs ${monthly_cost:.2f}/month.",
                    "impact": "high",
                    "estimated_savings": f"${monthly_cost:.2f}/month",
                    "action": "Delete this unused API Gateway after verifying no dependencies",
                    "risk": "low",
                }
            )
            return (is_optimizable, optimization_score, priority, potential_savings, recommendations)

        # Scenario 2: HIGH - Very low usage (<1000 requests/day)
        requests_per_day = request_count / 7

        if requests_per_day > 0 and requests_per_day < 1000:
            is_optimizable = True
            optimization_score = max(optimization_score, 80)
            priority = "high" if priority != "critical" else priority

            recommendations.append(
                {
                    "type": "low_usage",
                    "title": f"Very Low Usage ({requests_per_day:.0f} Requests/Day)",
                    "description": f"API '{api.get('Name', api.get('name', ''))}' receives only {requests_per_day:.0f} requests/day. "
                    f"Consider consolidating with other APIs or using serverless alternatives (AWS Lambda URL).",
                    "impact": "medium",
                    "estimated_savings": f"${monthly_cost * 0.50:.2f}/month (if consolidated)",
                    "action": "Review if this API can be consolidated or replaced with simpler alternatives",
                    "risk": "medium",
                }
            )
            potential_savings = max(potential_savings, monthly_cost * 0.50)

        # Scenario 3: HIGH - High error rate (>10% 4XX/5XX)
        total_errors = error_4xx_count + error_5xx_count
        if request_count > 0:
            error_rate = (total_errors / request_count) * 100

            if error_rate > 10.0:
                is_optimizable = True
                optimization_score = max(optimization_score, 85)
                priority = "high" if priority != "critical" else priority

                recommendations.append(
                    {
                        "type": "high_error_rate",
                        "title": f"High Error Rate ({error_rate:.1f}% Errors)",
                        "description": f"API '{api.get('Name', api.get('name', ''))}' has {error_rate:.1f}% error rate "
                        f"({error_4xx_count:.0f} 4XX + {error_5xx_count:.0f} 5XX errors). "
                        f"This indicates integration issues or broken endpoints.",
                        "impact": "high",
                        "estimated_savings": f"${monthly_cost * 0.30:.2f}/month (if errors indicate unused endpoints)",
                        "action": "Review and fix integration errors, or delete broken API if no longer needed",
                        "risk": "medium",
                    }
                )
                potential_savings = max(potential_savings, monthly_cost * 0.30)

        # Scenario 4: MEDIUM - REST API when HTTP API sufficient (70% savings)
        if api_type == "REST":
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "medium" if priority == "low" else priority

            # Calculate savings by migrating REST  HTTP API
            rest_rate = self.PRICING.get("apigateway_rest_per_million", 3.50)
            http_rate = self.PRICING.get("apigateway_http_per_million", 1.00)
            request_count_monthly = request_count * 4.33  # 7d  30d
            current_request_cost = (request_count_monthly / 1_000_000) * rest_rate
            new_request_cost = (request_count_monthly / 1_000_000) * http_rate
            migration_savings = current_request_cost - new_request_cost

            if migration_savings > 0:
                potential_savings = max(potential_savings, migration_savings)

                recommendations.append(
                    {
                        "type": "migrate_to_http_api",
                        "title": f"Migrate REST API to HTTP API (Save 70%)",
                        "description": f"API '{api.get('Name', '')}' uses REST API ($3.50/M requests). "
                        f"If you don't need API caching, request/response transformations, or usage plans, "
                        f"migrate to HTTP API ($1.00/M requests) to save 70%.",
                        "impact": "low",
                        "estimated_savings": f"${migration_savings:.2f}/month",
                        "action": "Evaluate migrating from REST API to HTTP API if features not needed",
                        "risk": "low",
                    }
                )

        # Scenario 5: MEDIUM - Cache enabled but low hit rate (<50%)
        if cache_enabled and (cache_hit_count + cache_miss_count) > 0:
            cache_total = cache_hit_count + cache_miss_count
            cache_hit_rate = (cache_hit_count / cache_total) * 100

            if cache_hit_rate < 50.0:
                is_optimizable = True
                optimization_score = max(optimization_score, 65)
                priority = "medium" if priority == "low" else priority

                # Estimate cache cost (varies by cache size, assume 0.5 GB = $0.020/hour = ~$14/month)
                estimated_cache_cost = 14.00
                potential_savings = max(potential_savings, estimated_cache_cost)

                recommendations.append(
                    {
                        "type": "disable_cache",
                        "title": f"Low Cache Hit Rate ({cache_hit_rate:.1f}%)",
                        "description": f"API '{api.get('Name', '')}' has caching enabled but only {cache_hit_rate:.1f}% hit rate. "
                        f"Cache is ineffective and costs ~$14/month. Consider disabling cache to save money.",
                        "impact": "low",
                        "estimated_savings": f"${estimated_cache_cost:.2f}/month",
                        "action": "Disable API Gateway cache if hit rate remains below 50%",
                        "risk": "low",
                    }
                )

        # Scenario 6: LOW - WebSocket API with no active connections
        # This would require additional metrics (WebSocket connection count)
        # For now, we detect via request_count = 0 (covered in Scenario 1)

        return (is_optimizable, optimization_score, priority, potential_savings, recommendations)

    async def scan_api_gateways(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS API Gateways in a region for cost intelligence.

        Scans 3 API types:
        1. REST API (apigateway client)
        2. HTTP API (apigatewayv2 client)
        3. WebSocket API (apigatewayv2 client)

        For each API:
        - Calculate monthly cost (requests + data transfer)
        - Fetch CloudWatch metrics (Count, 4XXError, 5XXError, Cache)
        - Analyze optimization opportunities (6 scenarios)

        Returns:
            List of AllCloudResourceData entries for all API Gateways
        """
        resources: list[AllCloudResourceData] = []

        try:
            # Scan REST APIs (apigateway client)
            async with self.session.client("apigateway", region_name=region) as apigw:
                async with self.session.client("cloudwatch", region_name=region) as cloudwatch:
                    # List REST APIs
                    rest_response = await apigw.get_rest_apis()
                    rest_apis = rest_response.get("items", [])

                    logger.info(
                        "inventory.apigateway.rest_apis_found",
                        region=region,
                        count=len(rest_apis),
                    )

                    for api in rest_apis:
                        try:
                            api_id = api.get("id", "")
                            api_name = api.get("name", "")
                            created_date = api.get("createdDate")

                            # Get CloudWatch metrics (7 days)
                            end_time = datetime.utcnow()
                            start_time = end_time - timedelta(days=7)

                            # Metric 1: Count (total requests)
                            count_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/ApiGateway",
                                MetricName="Count",
                                Dimensions=[{"Name": "ApiName", "Value": api_name}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            request_count = (
                                count_response["Datapoints"][0]["Sum"]
                                if count_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 2: 4XXError
                            error_4xx_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/ApiGateway",
                                MetricName="4XXError",
                                Dimensions=[{"Name": "ApiName", "Value": api_name}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            error_4xx_count = (
                                error_4xx_response["Datapoints"][0]["Sum"]
                                if error_4xx_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 3: 5XXError
                            error_5xx_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/ApiGateway",
                                MetricName="5XXError",
                                Dimensions=[{"Name": "ApiName", "Value": api_name}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            error_5xx_count = (
                                error_5xx_response["Datapoints"][0]["Sum"]
                                if error_5xx_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 4: CacheHitCount (if caching enabled)
                            cache_hit_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/ApiGateway",
                                MetricName="CacheHitCount",
                                Dimensions=[{"Name": "ApiName", "Value": api_name}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            cache_hit_count = (
                                cache_hit_response["Datapoints"][0]["Sum"]
                                if cache_hit_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 5: CacheMissCount
                            cache_miss_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/ApiGateway",
                                MetricName="CacheMissCount",
                                Dimensions=[{"Name": "ApiName", "Value": api_name}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            cache_miss_count = (
                                cache_miss_response["Datapoints"][0]["Sum"]
                                if cache_miss_response.get("Datapoints")
                                else 0.0
                            )

                            cache_enabled = (cache_hit_count + cache_miss_count) > 0

                            # Estimate monthly metrics from 7-day data
                            request_count_monthly = request_count * 4.33
                            data_transfer_gb_monthly = (request_count * 5) / (1024 ** 3)  # Assume 5KB avg response

                            # Calculate cost
                            monthly_cost = self._calculate_apigateway_monthly_cost(
                                api_type="REST",
                                request_count_per_month=request_count_monthly,
                                data_transfer_gb_per_month=data_transfer_gb_monthly,
                                cache_enabled=cache_enabled,
                                region=region,
                            )

                            # Calculate optimization
                            (
                                is_optimizable,
                                optimization_score,
                                priority,
                                potential_savings,
                                recommendations,
                            ) = self._calculate_apigateway_optimization(
                                api=api,
                                api_type="REST",
                                request_count=request_count,
                                error_4xx_count=error_4xx_count,
                                error_5xx_count=error_5xx_count,
                                cache_hit_count=cache_hit_count,
                                cache_miss_count=cache_miss_count,
                                cache_enabled=cache_enabled,
                                monthly_cost=monthly_cost,
                            )

                            # Build metadata
                            resource_metadata = {
                                "api_id": api_id,
                                "api_name": api_name,
                                "api_type": "REST",
                                "endpoint_type": api.get("endpointConfiguration", {}).get("types", ["REGIONAL"])[0],
                                "cache_enabled": cache_enabled,
                                "created_date": created_date.isoformat() if created_date else None,
                                "metrics": {
                                    "request_count_7d": request_count,
                                    "error_4xx_count_7d": error_4xx_count,
                                    "error_5xx_count_7d": error_5xx_count,
                                    "cache_hit_count_7d": cache_hit_count,
                                    "cache_miss_count_7d": cache_miss_count,
                                },
                            }

                            # Create resource entry
                            resource = AllCloudResourceData(
                                resource_id=api_id,
                                resource_type="api_gateway",
                                resource_name=api_name,
                                region=region,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=resource_metadata,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                            )

                            resources.append(resource)

                            logger.info(
                                "inventory.apigateway.rest_api_scanned",
                                api_name=api_name,
                                region=region,
                                request_count=request_count,
                                monthly_cost=monthly_cost,
                                is_optimizable=is_optimizable,
                            )

                        except Exception as e:
                            logger.error(
                                "inventory.apigateway.rest_api_scan_error",
                                api_id=api.get("id", "unknown"),
                                region=region,
                                error=str(e),
                            )
                            continue

            # Scan HTTP + WebSocket APIs (apigatewayv2 client)
            async with self.session.client("apigatewayv2", region_name=region) as apigw2:
                async with self.session.client("cloudwatch", region_name=region) as cloudwatch:
                    # List HTTP + WebSocket APIs
                    v2_response = await apigw2.get_apis()
                    v2_apis = v2_response.get("Items", [])

                    logger.info(
                        "inventory.apigateway.v2_apis_found",
                        region=region,
                        count=len(v2_apis),
                    )

                    for api in v2_apis:
                        try:
                            api_id = api.get("ApiId", "")
                            api_name = api.get("Name", "")
                            api_protocol = api.get("ProtocolType", "HTTP")  # HTTP or WEBSOCKET
                            created_date = api.get("CreatedDate")

                            # CloudWatch metrics (note: different namespace for v2)
                            end_time = datetime.utcnow()
                            start_time = end_time - timedelta(days=7)

                            # For HTTP/WebSocket APIs, metrics use ApiId dimension
                            count_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/ApiGateway",
                                MetricName="Count",
                                Dimensions=[{"Name": "ApiId", "Value": api_id}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            request_count = (
                                count_response["Datapoints"][0]["Sum"]
                                if count_response.get("Datapoints")
                                else 0.0
                            )

                            # Errors
                            error_4xx_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/ApiGateway",
                                MetricName="4XXError",
                                Dimensions=[{"Name": "ApiId", "Value": api_id}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            error_4xx_count = (
                                error_4xx_response["Datapoints"][0]["Sum"]
                                if error_4xx_response.get("Datapoints")
                                else 0.0
                            )

                            error_5xx_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/ApiGateway",
                                MetricName="5XXError",
                                Dimensions=[{"Name": "ApiId", "Value": api_id}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            error_5xx_count = (
                                error_5xx_response["Datapoints"][0]["Sum"]
                                if error_5xx_response.get("Datapoints")
                                else 0.0
                            )

                            # HTTP/WebSocket APIs don't support caching
                            cache_hit_count = 0.0
                            cache_miss_count = 0.0
                            cache_enabled = False

                            # Estimate monthly metrics
                            request_count_monthly = request_count * 4.33
                            data_transfer_gb_monthly = (request_count * 5) / (1024 ** 3)

                            # Calculate cost
                            monthly_cost = self._calculate_apigateway_monthly_cost(
                                api_type=api_protocol,
                                request_count_per_month=request_count_monthly,
                                data_transfer_gb_per_month=data_transfer_gb_monthly,
                                cache_enabled=False,
                                region=region,
                            )

                            # Calculate optimization
                            (
                                is_optimizable,
                                optimization_score,
                                priority,
                                potential_savings,
                                recommendations,
                            ) = self._calculate_apigateway_optimization(
                                api=api,
                                api_type=api_protocol,
                                request_count=request_count,
                                error_4xx_count=error_4xx_count,
                                error_5xx_count=error_5xx_count,
                                cache_hit_count=cache_hit_count,
                                cache_miss_count=cache_miss_count,
                                cache_enabled=cache_enabled,
                                monthly_cost=monthly_cost,
                            )

                            # Build metadata
                            resource_metadata = {
                                "api_id": api_id,
                                "api_name": api_name,
                                "api_type": api_protocol,
                                "api_endpoint": api.get("ApiEndpoint", ""),
                                "created_date": created_date.isoformat() if created_date else None,
                                "metrics": {
                                    "request_count_7d": request_count,
                                    "error_4xx_count_7d": error_4xx_count,
                                    "error_5xx_count_7d": error_5xx_count,
                                },
                            }

                            # Create resource entry
                            resource = AllCloudResourceData(
                                resource_id=api_id,
                                resource_type="api_gateway",
                                resource_name=api_name,
                                region=region,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=resource_metadata,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                            )

                            resources.append(resource)

                            logger.info(
                                "inventory.apigateway.v2_api_scanned",
                                api_name=api_name,
                                api_type=api_protocol,
                                region=region,
                                request_count=request_count,
                                monthly_cost=monthly_cost,
                                is_optimizable=is_optimizable,
                            )

                        except Exception as e:
                            logger.error(
                                "inventory.apigateway.v2_api_scan_error",
                                api_id=api.get("ApiId", "unknown"),
                                region=region,
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "apigateway.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # =========================================================================
    # AWS CloudFront Distribution - Cost Optimization Scanning (GLOBAL SERVICE)
    # =========================================================================

    def _calculate_cloudfront_monthly_cost(
        self,
        requests_per_month: float,
        data_transfer_gb_per_month: float,
        invalidations_per_month: int,
        price_class: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS CloudFront Distribution.

        Pricing model (varies by region/price class):
        - Data transfer out: $0.085/GB (US/Europe), $0.140/GB (Asia), $0.170/GB (Other)
        - HTTP requests: $0.0075 per 10,000 requests
        - HTTPS requests: $0.010 per 10,000 requests (assume 100% HTTPS)
        - Invalidations: First 1,000 paths free, $0.005 per path after

        Args:
            requests_per_month: Monthly request count (estimated from 7d)
            data_transfer_gb_per_month: Monthly data transfer out in GB
            invalidations_per_month: Monthly invalidation requests
            price_class: Price class (PriceClass_All, PriceClass_200, PriceClass_100)

        Returns:
            Estimated monthly cost in USD
        """
        # Data transfer cost (varies by price class)
        if price_class == "PriceClass_100":  # US, Canada, Europe
            data_transfer_rate = self.PRICING.get("cloudfront_data_transfer_us_europe_per_gb", 0.085)
        elif price_class == "PriceClass_200":  # US, Canada, Europe, Asia, Middle East, Africa
            data_transfer_rate = self.PRICING.get("cloudfront_data_transfer_asia_per_gb", 0.140)
        else:  # PriceClass_All (all edge locations)
            data_transfer_rate = self.PRICING.get("cloudfront_data_transfer_other_per_gb", 0.170)

        data_transfer_cost = data_transfer_gb_per_month * data_transfer_rate

        # Request cost (assume 100% HTTPS)
        https_rate = self.PRICING.get("cloudfront_https_requests_per_10k", 0.010)
        request_cost = (requests_per_month / 10_000) * https_rate

        # Invalidation cost (first 1000 free)
        invalidation_rate = self.PRICING.get("cloudfront_invalidation_per_path", 0.005)
        invalidation_cost = max(0, invalidations_per_month - 1000) * invalidation_rate

        total_cost = data_transfer_cost + request_cost + invalidation_cost
        return total_cost

    def _calculate_cloudfront_optimization(
        self,
        distribution: dict[str, Any],
        requests_per_month: float,
        data_transfer_gb: float,
        error_rate_4xx: float,
        error_rate_5xx: float,
        origin_latency_ms: float,
        price_class: str,
        invalidations_per_month: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Analyze AWS CloudFront Distribution for cost optimization opportunities.

        Optimization scenarios (5):
        1. Completely inactive - No requests (critical)
        2. Very low usage - <10K requests/month (high)
        3. High origin latency - >500ms avg, poor caching (high)
        4. Price class optimization - All Edges when NA/EU sufficient (medium)
        5. Excessive invalidations - >1000/month, use versioning (medium)

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Completely inactive (no requests)
        if requests_per_month == 0.0:
            is_optimizable = True
            optimization_score = 95
            priority = "critical"
            potential_savings = monthly_cost  # Full savings
            recommendations.append(
                {
                    "type": "delete_distribution",
                    "title": "Delete Completely Inactive CloudFront Distribution",
                    "description": f"Distribution '{distribution.get('Id', '')}' has received NO requests "
                    f"in the last 7 days. This distribution is completely unused and costs ${monthly_cost:.2f}/month.",
                    "impact": "high",
                    "estimated_savings": f"${monthly_cost:.2f}/month",
                    "action": "Delete this unused CloudFront distribution",
                    "risk": "low",
                }
            )
            return (is_optimizable, optimization_score, priority, potential_savings, recommendations)

        # Scenario 2: HIGH - Very low usage (<10K requests/month)
        if requests_per_month > 0 and requests_per_month < 10_000:
            is_optimizable = True
            optimization_score = max(optimization_score, 85)
            priority = "high" if priority != "critical" else priority

            # For very low traffic, direct S3 access may be cheaper
            recommendations.append(
                {
                    "type": "low_usage_consolidate",
                    "title": f"Very Low Usage ({requests_per_month:.0f} Requests/Month)",
                    "description": f"Distribution '{distribution.get('Id', '')}' receives only {requests_per_month:.0f} requests/month. "
                    f"For such low traffic, consider direct S3 access or consolidating with another distribution.",
                    "impact": "medium",
                    "estimated_savings": f"${monthly_cost * 0.70:.2f}/month (if using direct S3 access)",
                    "action": "Review if CloudFront is necessary for this low traffic volume",
                    "risk": "medium",
                }
            )
            potential_savings = max(potential_savings, monthly_cost * 0.70)

        # Scenario 3: HIGH - High origin latency (>500ms avg) indicates poor caching
        if origin_latency_ms > 500.0:
            is_optimizable = True
            optimization_score = max(optimization_score, 80)
            priority = "high" if priority != "critical" else priority

            # High latency = going to origin too often = cache not effective
            recommendations.append(
                {
                    "type": "optimize_caching",
                    "title": f"High Origin Latency ({origin_latency_ms:.0f}ms Average)",
                    "description": f"Distribution '{distribution.get('Id', '')}' has high origin latency ({origin_latency_ms:.0f}ms avg), "
                    f"indicating poor cache hit rate. Optimize Cache-Control headers or use Lambda@Edge for better caching.",
                    "impact": "high",
                    "estimated_savings": f"${monthly_cost * 0.30:.2f}/month (by improving cache hit rate)",
                    "action": "Review and optimize caching strategy (TTL, Cache-Control headers)",
                    "risk": "low",
                }
            )
            potential_savings = max(potential_savings, monthly_cost * 0.30)

        # Scenario 4: MEDIUM - Price class optimization
        # PriceClass_All = most expensive, PriceClass_100 = cheapest (US/Europe only)
        if price_class == "PriceClass_All":
            is_optimizable = True
            optimization_score = max(optimization_score, 65)
            priority = "medium" if priority == "low" else priority

            # Estimate savings by switching from All Edges to US/Europe only
            all_edges_rate = self.PRICING.get("cloudfront_data_transfer_other_per_gb", 0.170)
            us_europe_rate = self.PRICING.get("cloudfront_data_transfer_us_europe_per_gb", 0.085)
            data_transfer_savings = data_transfer_gb * (all_edges_rate - us_europe_rate)
            potential_savings = max(potential_savings, data_transfer_savings)

            recommendations.append(
                {
                    "type": "optimize_price_class",
                    "title": f"Price Class 'All Edges' May Be Overkill",
                    "description": f"Distribution '{distribution.get('Id', '')}' uses 'All Edges' price class. "
                    f"If most users are in North America/Europe, switch to 'Use Only US, Canada and Europe' to save 50% on data transfer.",
                    "impact": "low",
                    "estimated_savings": f"${data_transfer_savings:.2f}/month",
                    "action": "Change price class from 'All' to 'US/Europe' if traffic is mostly NA/EU",
                    "risk": "low",
                }
            )

        # Scenario 5: MEDIUM - Excessive invalidations (>1000/month)
        if invalidations_per_month > 1000:
            is_optimizable = True
            optimization_score = max(optimization_score, 60)
            priority = "medium" if priority == "low" else priority

            # Calculate invalidation cost savings
            invalidation_rate = self.PRICING.get("cloudfront_invalidation_per_path", 0.005)
            invalidation_cost = (invalidations_per_month - 1000) * invalidation_rate
            potential_savings = max(potential_savings, invalidation_cost)

            recommendations.append(
                {
                    "type": "reduce_invalidations",
                    "title": f"Excessive Invalidations ({invalidations_per_month} Paths/Month)",
                    "description": f"Distribution '{distribution.get('Id', '')}' has {invalidations_per_month} invalidation "
                    f"requests/month. After first 1,000 free, you're charged ${invalidation_cost:.2f}/month. "
                    f"Use versioned file names (e.g., app.v2.js) instead of invalidations.",
                    "impact": "low",
                    "estimated_savings": f"${invalidation_cost:.2f}/month",
                    "action": "Implement versioning strategy (file names with versions) to avoid invalidations",
                    "risk": "low",
                }
            )

        return (is_optimizable, optimization_score, priority, potential_savings, recommendations)

    async def scan_cloudfront_distributions(self) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS CloudFront Distributions for cost intelligence.

         IMPORTANT: CloudFront is a GLOBAL service (not regional).
        This method scans all distributions regardless of region.

        For each distribution:
        - Calculate monthly cost (requests + data transfer + invalidations)
        - Fetch CloudWatch metrics (Requests, BytesDownloaded, ErrorRate, OriginLatency)
        - Analyze optimization opportunities (5 scenarios)

        Returns:
            List of AllCloudResourceData entries for all CloudFront Distributions
        """
        resources: list[AllCloudResourceData] = []

        try:
            # CloudFront is a global service, client doesn't take region parameter
            async with self.session.client("cloudfront") as cloudfront:
                async with self.session.client("cloudwatch", region_name="us-east-1") as cloudwatch:
                    # CloudWatch metrics for CloudFront are ONLY in us-east-1

                    # List all distributions
                    paginator = cloudfront.get_paginator("list_distributions")
                    distributions = []

                    async for page in paginator.paginate():
                        dist_list = page.get("DistributionList", {})
                        distributions.extend(dist_list.get("Items", []))

                    logger.info(
                        "inventory.cloudfront.distributions_found",
                        count=len(distributions),
                    )

                    # Process each distribution
                    for dist in distributions:
                        try:
                            dist_id = dist.get("Id", "")
                            dist_domain_name = dist.get("DomainName", "")
                            dist_status = dist.get("Status", "")
                            dist_enabled = dist.get("Enabled", False)
                            price_class = dist.get("PriceClass", "PriceClass_All")

                            # Get CloudWatch metrics (7 days)
                            # NOTE: CloudFront metrics are in us-east-1 regardless of distribution location
                            end_time = datetime.utcnow()
                            start_time = end_time - timedelta(days=7)

                            # Metric 1: Requests (sum over 7 days)
                            requests_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/CloudFront",
                                MetricName="Requests",
                                Dimensions=[
                                    {"Name": "DistributionId", "Value": dist_id},
                                    {"Name": "Region", "Value": "Global"},
                                ],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,  # 7 days
                                Statistics=["Sum"],
                            )
                            requests_7d = (
                                requests_response["Datapoints"][0]["Sum"]
                                if requests_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 2: BytesDownloaded (sum)
                            bytes_downloaded_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/CloudFront",
                                MetricName="BytesDownloaded",
                                Dimensions=[
                                    {"Name": "DistributionId", "Value": dist_id},
                                    {"Name": "Region", "Value": "Global"},
                                ],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            bytes_downloaded = (
                                bytes_downloaded_response["Datapoints"][0]["Sum"]
                                if bytes_downloaded_response.get("Datapoints")
                                else 0.0
                            )
                            data_transfer_gb_7d = bytes_downloaded / (1024 ** 3)  # Convert to GB

                            # Metric 3: 4xxErrorRate (average)
                            error_4xx_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/CloudFront",
                                MetricName="4xxErrorRate",
                                Dimensions=[
                                    {"Name": "DistributionId", "Value": dist_id},
                                    {"Name": "Region", "Value": "Global"},
                                ],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Average"],
                            )
                            error_rate_4xx = (
                                error_4xx_response["Datapoints"][0]["Average"]
                                if error_4xx_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 4: 5xxErrorRate (average)
                            error_5xx_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/CloudFront",
                                MetricName="5xxErrorRate",
                                Dimensions=[
                                    {"Name": "DistributionId", "Value": dist_id},
                                    {"Name": "Region", "Value": "Global"},
                                ],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Average"],
                            )
                            error_rate_5xx = (
                                error_5xx_response["Datapoints"][0]["Average"]
                                if error_5xx_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 5: OriginLatency (average)
                            origin_latency_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/CloudFront",
                                MetricName="OriginLatency",
                                Dimensions=[
                                    {"Name": "DistributionId", "Value": dist_id},
                                    {"Name": "Region", "Value": "Global"},
                                ],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Average"],
                            )
                            origin_latency_ms = (
                                origin_latency_response["Datapoints"][0]["Average"]
                                if origin_latency_response.get("Datapoints")
                                else 0.0
                            )

                            # Estimate monthly metrics from 7-day data
                            requests_per_month = requests_7d * 4.33
                            data_transfer_gb_per_month = data_transfer_gb_7d * 4.33

                            # Estimate invalidations (not available via CloudWatch, use placeholder)
                            # In real scenario, would need to track via CloudFront API or logs
                            invalidations_per_month = 0

                            # Calculate cost
                            monthly_cost = self._calculate_cloudfront_monthly_cost(
                                requests_per_month=requests_per_month,
                                data_transfer_gb_per_month=data_transfer_gb_per_month,
                                invalidations_per_month=invalidations_per_month,
                                price_class=price_class,
                            )

                            # Calculate optimization
                            (
                                is_optimizable,
                                optimization_score,
                                priority,
                                potential_savings,
                                recommendations,
                            ) = self._calculate_cloudfront_optimization(
                                distribution=dist,
                                requests_per_month=requests_per_month,
                                data_transfer_gb=data_transfer_gb_7d,
                                error_rate_4xx=error_rate_4xx,
                                error_rate_5xx=error_rate_5xx,
                                origin_latency_ms=origin_latency_ms,
                                price_class=price_class,
                                invalidations_per_month=invalidations_per_month,
                                monthly_cost=monthly_cost,
                            )

                            # Build metadata
                            resource_metadata = {
                                "distribution_id": dist_id,
                                "domain_name": dist_domain_name,
                                "status": dist_status,
                                "enabled": dist_enabled,
                                "price_class": price_class,
                                "metrics": {
                                    "requests_7d": requests_7d,
                                    "data_transfer_gb_7d": data_transfer_gb_7d,
                                    "error_rate_4xx_avg": error_rate_4xx,
                                    "error_rate_5xx_avg": error_rate_5xx,
                                    "origin_latency_ms_avg": origin_latency_ms,
                                },
                            }

                            # Create resource entry (region = "global" for CloudFront)
                            resource = AllCloudResourceData(
                                resource_id=dist_id,
                                resource_type="cloudfront_distribution",
                                resource_name=dist_domain_name,
                                region="global",  # CloudFront is global
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=resource_metadata,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                            )

                            resources.append(resource)

                            logger.info(
                                "inventory.cloudfront.distribution_scanned",
                                distribution_id=dist_id,
                                domain_name=dist_domain_name,
                                requests_7d=requests_7d,
                                monthly_cost=monthly_cost,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                            )

                        except Exception as e:
                            logger.error(
                                "inventory.cloudfront.distribution_scan_error",
                                distribution_id=dist.get("Id", "unknown"),
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "cloudfront.scan_failed",
                error=str(e),
            )

        return resources
"""Inventory scanner service for complete cloud resource scanning.

This service scans ALL cloud resources (not just orphans) to provide
cost intelligence and optimization recommendations.
"""

import uuid
import structlog
from datetime import datetime, timedelta
from typing import Any, Optional

from sqlalchemy.ext.asyncio import AsyncSession

from app.providers.base import AllCloudResourceData, OptimizationScenario
from app.models.detection_rule import DEFAULT_DETECTION_RULES
from app.crud import detection_rule as detection_rule_crud

logger = structlog.get_logger()


def safe_get_value(obj: Any, default: Any = None) -> Any:
    """
    Safely extract .value from Azure SDK Enum objects.

    Azure SDK inconsistently returns Enum objects (with .value property) or
    strings directly depending on API version and region. This helper handles both cases.

    Args:
        obj: Azure SDK object (Enum or string)
        default: Default value if obj is None

    Returns:
        The actual value (obj.value if Enum, obj if string, default if None)

    Example:
        >>> safe_get_value(OperatingSystemTypes.LINUX)  # Enum
        'Linux'
        >>> safe_get_value("Linux")  # String
        'Linux'
        >>> safe_get_value(None, "Unknown")
        'Unknown'
    """
    if obj is None:
        return default
    return obj.value if hasattr(obj, 'value') else obj


class AWSInventoryScanner:
    """AWS-specific inventory scanner for cost intelligence."""

    def __init__(
        self,
        provider: Any,
        user_id: Optional[uuid.UUID] = None,
        db: Optional[AsyncSession] = None,
    ) -> None:
        """
        Initialize AWS inventory scanner.

        Args:
            provider: AWS provider instance with authenticated session
            user_id: Optional user UUID for loading custom detection rules
            db: Optional database session for loading detection rules
        """
        self.provider = provider
        self.session = provider.session
        self.user_id = user_id
        self.db = db
        self.user_detection_rules: dict[str, dict] = {}  # Will be loaded async

    async def _load_detection_rules(self) -> None:
        """
        Load user's custom detection rules from database.

        This method loads all detection rules for the user if user_id and db are provided.
        Rules are stored in self.user_detection_rules for quick access during scanning.
        """
        if not self.user_id or not self.db:
            logger.info("inventory.detection_rules_skip", reason="no_user_or_db")
            return

        try:
            # Load all user rules from database
            rules = await detection_rule_crud.get_user_rules(self.db, self.user_id)

            # Convert to dict for quick lookup
            for rule in rules:
                self.user_detection_rules[rule.resource_type] = rule.rules

            logger.info(
                "inventory.detection_rules_loaded",
                user_id=str(self.user_id),
                rules_count=len(self.user_detection_rules),
            )
        except Exception as e:
            logger.error(
                "inventory.detection_rules_load_error",
                error=str(e),
                user_id=str(self.user_id),
            )

    def _should_include_resource(
        self,
        resource_type: str,
        resource_age_days: Optional[float],
    ) -> bool:
        """
        Determine if a resource should be included based on detection rules.

        Args:
            resource_type: Type of resource (e.g., 'ec2_instance', 'ebs_volume')
            resource_age_days: Age of resource in days (None if unknown)

        Returns:
            True if resource should be included, False otherwise
        """
        # Get effective rules (user custom or default)
        effective_rules = self.user_detection_rules.get(
            resource_type, DEFAULT_DETECTION_RULES.get(resource_type, {})
        )

        # Check if detection is enabled
        if not effective_rules.get("enabled", True):
            logger.debug(
                "inventory.resource_filtered_disabled",
                resource_type=resource_type,
            )
            return False

        # Check min_age_days requirement
        min_age_days = effective_rules.get("min_age_days", 0)
        if resource_age_days is not None and resource_age_days < min_age_days:
            logger.debug(
                "inventory.resource_filtered_age",
                resource_type=resource_type,
                resource_age_days=resource_age_days,
                min_age_days=min_age_days,
            )
            return False

        return True

    async def scan_ec2_instances(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL EC2 instances (running, stopped, etc.) for cost intelligence.

        Unlike orphan detection, this returns ALL instances with utilization metrics
        and optimization recommendations.

        Args:
            region: AWS region to scan

        Returns:
            List of all EC2 instance resources
        """
        logger.info("inventory.scan_ec2_start", region=region)
        all_instances: list[AllCloudResourceData] = []

        try:
            async with self.session.client("ec2", region_name=region) as ec2:
                # Describe ALL instances (no filters)
                response = await ec2.describe_instances()

                for reservation in response.get("Reservations", []):
                    for instance in reservation.get("Instances", []):
                        instance_id = instance["InstanceId"]
                        instance_type = instance["InstanceType"]
                        state = instance["State"]["Name"]

                        # Extract instance name from tags
                        instance_name = None
                        tags = {}
                        for tag in instance.get("Tags", []):
                            tags[tag["Key"]] = tag["Value"]
                            if tag["Key"] == "Name":
                                instance_name = tag["Value"]

                        # Get CloudWatch metrics (last 14 days)
                        cpu_util = await self._get_cpu_utilization(instance_id, region)
                        network_in = await self._get_network_in(instance_id, region)

                        # Calculate monthly cost
                        monthly_cost = self._calculate_ec2_monthly_cost(
                            instance_type, state
                        )

                        # Determine utilization status
                        utilization_status = self._determine_utilization_status(
                            cpu_util, state
                        )

                        # Calculate optimization score and recommendations
                        (
                            is_optimizable,
                            optimization_score,
                            optimization_priority,
                            potential_savings,
                            recommendations,
                        ) = self._calculate_ec2_optimization(
                            instance,
                            cpu_util,
                            monthly_cost,
                            state,
                        )

                        # Check if instance is also detected as orphan
                        is_orphan = state == "stopped" or (
                            state == "running" and cpu_util < 5.0
                        )

                        # Create resource data
                        resource = AllCloudResourceData(
                            resource_type="ec2_instance",
                            resource_id=instance_id,
                            resource_name=instance_name,
                            region=region,
                            estimated_monthly_cost=monthly_cost,
                            resource_metadata={
                                "instance_type": instance_type,
                                "state": state,
                                "availability_zone": instance.get(
                                    "Placement", {}
                                ).get("AvailabilityZone"),
                                "launch_time": instance.get("LaunchTime").isoformat()
                                if instance.get("LaunchTime")
                                else None,
                                "platform": instance.get("Platform", "linux"),
                                "vpc_id": instance.get("VpcId"),
                                "subnet_id": instance.get("SubnetId"),
                            },
                            currency="USD",
                            utilization_status=utilization_status,
                            cpu_utilization_percent=cpu_util,
                            memory_utilization_percent=None,  # TODO: Fetch from CloudWatch agent
                            network_utilization_mbps=network_in,
                            is_optimizable=is_optimizable,
                            optimization_priority=optimization_priority,
                            optimization_score=optimization_score,
                            potential_monthly_savings=potential_savings,
                            optimization_recommendations=recommendations,
                            tags=tags,
                            resource_status=state,
                            is_orphan=is_orphan,
                            created_at_cloud=instance.get("LaunchTime").replace(tzinfo=None) if instance.get("LaunchTime") else None,
                            last_used_at=None,  # TODO: Estimate from CloudWatch
                        )

                        all_instances.append(resource)

                logger.info(
                    "inventory.scan_ec2_complete",
                    region=region,
                    total_instances=len(all_instances),
                )

        except Exception as e:
            logger.error(
                "inventory.scan_ec2_error",
                region=region,
                error=str(e),
                exc_info=True,
            )
            raise

        return all_instances

    async def scan_ebs_volumes(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL EBS volumes (attached + unattached) for cost intelligence.

        Unlike orphan detection, this returns ALL volumes with I/O metrics
        and optimization recommendations.

        Args:
            region: AWS region to scan

        Returns:
            List of all EBS volume resources
        """
        logger.info("inventory.scan_ebs_start", region=region)
        all_volumes: list[AllCloudResourceData] = []

        try:
            async with self.session.client("ec2", region_name=region) as ec2:
                # Describe ALL volumes (no filters)
                response = await ec2.describe_volumes()

                for volume in response.get("Volumes", []):
                    volume_id = volume["VolumeId"]
                    volume_type = volume.get("VolumeType", "gp3")
                    state = volume["State"]
                    size_gb = volume["Size"]
                    iops = volume.get("Iops")
                    throughput = volume.get("Throughput")

                    # Extract volume name from tags
                    volume_name = None
                    tags = {}
                    for tag in volume.get("Tags", []):
                        tags[tag["Key"]] = tag["Value"]
                        if tag["Key"] == "Name":
                            volume_name = tag["Value"]

                    # Get CloudWatch metrics (last 14 days)
                    read_ops = await self._get_volume_read_ops(volume_id, region)
                    write_ops = await self._get_volume_write_ops(volume_id, region)

                    # Calculate monthly cost
                    monthly_cost = self._calculate_ebs_monthly_cost(
                        volume_type, size_gb, iops, throughput, region
                    )

                    # Determine utilization status
                    if state == "available":
                        utilization_status = "idle"
                    elif read_ops + write_ops == 0:
                        utilization_status = "idle"
                    elif read_ops + write_ops < 1000:  # < 1000 ops/day
                        utilization_status = "low"
                    elif read_ops + write_ops < 10000:  # < 10K ops/day
                        utilization_status = "medium"
                    else:
                        utilization_status = "high"

                    # Calculate optimization score and recommendations
                    (
                        is_optimizable,
                        optimization_score,
                        optimization_priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_ebs_optimization(
                        volume,
                        read_ops,
                        write_ops,
                        monthly_cost,
                        state,
                    )

                    # Check if volume is also detected as orphan
                    is_orphan = state == "available" or (
                        len(volume.get("Attachments", [])) > 0 and read_ops + write_ops == 0
                    )

                    # Get attachment info
                    attachments = volume.get("Attachments", [])
                    attached_to = None
                    if attachments:
                        attached_to = attachments[0].get("InstanceId")

                    # Create resource data
                    resource = AllCloudResourceData(
                        resource_type="ebs_volume",
                        resource_id=volume_id,
                        resource_name=volume_name,
                        region=region,
                        estimated_monthly_cost=monthly_cost,
                        resource_metadata={
                            "volume_type": volume_type,
                            "state": state,
                            "size_gb": size_gb,
                            "iops": iops,
                            "throughput": throughput,
                            "availability_zone": volume.get("AvailabilityZone"),
                            "encrypted": volume.get("Encrypted", False),
                            "multi_attach_enabled": volume.get("MultiAttachEnabled", False),
                            "attached_to": attached_to,
                            "attachment_count": len(attachments),
                        },
                        currency="USD",
                        utilization_status=utilization_status,
                        cpu_utilization_percent=None,
                        memory_utilization_percent=None,
                        storage_utilization_percent=None,
                        network_utilization_mbps=None,
                        is_optimizable=is_optimizable,
                        optimization_priority=optimization_priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations,
                        tags=tags,
                        resource_status=state,
                        is_orphan=is_orphan,
                        created_at_cloud=volume.get("CreateTime").replace(tzinfo=None) if volume.get("CreateTime") else None,
                        last_used_at=None,  # TODO: Estimate from CloudWatch metrics
                    )

                    all_volumes.append(resource)

                logger.info(
                    "inventory.scan_ebs_complete",
                    region=region,
                    total_volumes=len(all_volumes),
                )

        except Exception as e:
            logger.error(
                "inventory.scan_ebs_error",
                region=region,
                error=str(e),
                exc_info=True,
            )
            raise

        return all_volumes

    async def scan_elastic_ips(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Elastic IPs (associated + unassociated) for cost intelligence.

        Unlike orphan detection, this returns ALL Elastic IPs with
        utilization status and optimization recommendations.

        Args:
            region: AWS region to scan

        Returns:
            List of all Elastic IP resources
        """
        logger.info("inventory.scan_elastic_ips_start", region=region)
        all_eips: list[AllCloudResourceData] = []

        try:
            async with self.session.client("ec2", region_name=region) as ec2:
                # Describe ALL Elastic IPs (no filters)
                response = await ec2.describe_addresses()

                for eip in response.get("Addresses", []):
                    allocation_id = eip.get("AllocationId", "N/A")
                    public_ip = eip.get("PublicIp", "N/A")
                    association_id = eip.get("AssociationId")
                    instance_id = eip.get("InstanceId")
                    network_interface_id = eip.get("NetworkInterfaceId")

                    # Extract EIP name from tags
                    eip_name = None
                    tags = {}
                    for tag in eip.get("Tags", []):
                        tags[tag["Key"]] = tag["Value"]
                        if tag["Key"] == "Name":
                            eip_name = tag["Value"]

                    # Determine if EIP is associated (attached to resource)
                    is_associated = bool(association_id or instance_id or network_interface_id)

                    # Calculate monthly cost
                    # Unassociated EIPs cost $3.60/month, associated are free
                    if is_associated:
                        monthly_cost = 0.0
                        utilization_status = "active"
                        state = "associated"
                    else:
                        # Use dynamic pricing service with fallback
                        try:
                            eip_price = await self.pricing_service.get_aws_price("elastic_ip", region)
                            monthly_cost = eip_price if eip_price else 3.60
                        except Exception:
                            monthly_cost = 3.60
                        utilization_status = "idle"
                        state = "unassociated"

                    # Calculate optimization score and recommendations
                    recommendations = []
                    is_optimizable = False
                    optimization_score = 0
                    potential_savings = 0.0
                    priority = "none"

                    if not is_associated:
                        # Unassociated EIP - critical priority
                        is_optimizable = True
                        optimization_score = 95
                        priority = "critical"
                        potential_savings = monthly_cost
                        recommendations.append({
                            "action": "Release unassociated Elastic IP",
                            "details": f"EIP is not associated with any resource. Release to save ${potential_savings:.2f}/month",
                            "priority": "critical",
                        })
                    elif network_interface_id and not instance_id:
                        # Associated with ENI but not instance - might be detached
                        is_optimizable = True
                        optimization_score = 70
                        priority = "high"
                        potential_savings = monthly_cost
                        recommendations.append({
                            "action": "Review EIP on detached ENI",
                            "details": "EIP is attached to ENI but not directly to instance. Verify if ENI is in use",
                            "priority": "high",
                        })

                    # Check if EIP is also detected as orphan
                    is_orphan = not is_associated

                    # Create resource data
                    resource = AllCloudResourceData(
                        resource_type="elastic_ip",
                        resource_id=allocation_id,
                        resource_name=eip_name,
                        region=region,
                        estimated_monthly_cost=monthly_cost,
                        resource_metadata={
                            "public_ip": public_ip,
                            "allocation_id": allocation_id,
                            "association_id": association_id,
                            "instance_id": instance_id,
                            "network_interface_id": network_interface_id,
                            "domain": eip.get("Domain", "vpc"),
                            "private_ip_address": eip.get("PrivateIpAddress"),
                            "is_associated": is_associated,
                        },
                        currency="USD",
                        utilization_status=utilization_status,
                        cpu_utilization_percent=None,
                        memory_utilization_percent=None,
                        storage_utilization_percent=None,
                        network_utilization_mbps=None,
                        is_optimizable=is_optimizable,
                        optimization_priority=priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations,
                        tags=tags,
                        resource_status=state,
                        is_orphan=is_orphan,
                        created_at_cloud=None,  # EIPs don't have creation time in API
                        last_used_at=None,
                    )

                    all_eips.append(resource)

                logger.info(
                    "inventory.scan_elastic_ips_complete",
                    region=region,
                    total_eips=len(all_eips),
                )

        except Exception as e:
            logger.error(
                "inventory.scan_elastic_ips_error",
                region=region,
                error=str(e),
                exc_info=True,
            )
            raise

        return all_eips

    async def scan_aws_load_balancers(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS Load Balancers (ALB, NLB, GLB, CLB) for cost intelligence.

        Unlike orphan detection, this returns ALL load balancers with
        utilization status and optimization recommendations.

        Args:
            region: AWS region to scan

        Returns:
            List of all load balancer resources
        """
        logger.info("inventory.scan_aws_lb_start", region=region)
        all_lbs: list[AllCloudResourceData] = []

        try:
            # ================================================================
            # Scan Application/Network/Gateway Load Balancers (ELBv2)
            # ================================================================
            async with self.session.client("elbv2", region_name=region) as elbv2:
                response = await elbv2.describe_load_balancers()

                for lb in response.get("LoadBalancers", []):
                    lb_arn = lb["LoadBalancerArn"]
                    lb_name = lb["LoadBalancerName"]
                    lb_type = lb["Type"]  # 'application', 'network', or 'gateway'
                    lb_state = lb["State"]["Code"]  # 'active', 'provisioning', 'failed'
                    created_at = lb["CreatedTime"]

                    # Extract tags
                    tags = {}
                    try:
                        tags_response = await elbv2.describe_tags(ResourceArns=[lb_arn])
                        for tag_desc in tags_response.get("TagDescriptions", []):
                            for tag in tag_desc.get("Tags", []):
                                tags[tag["Key"]] = tag["Value"]
                    except Exception:
                        pass

                    # Get listeners count
                    try:
                        listeners_response = await elbv2.describe_listeners(
                            LoadBalancerArn=lb_arn
                        )
                        listener_count = len(listeners_response.get("Listeners", []))
                    except Exception:
                        listener_count = 0

                    # Get target groups for this LB
                    try:
                        tg_response = await elbv2.describe_target_groups(
                            LoadBalancerArn=lb_arn
                        )
                        target_groups = tg_response.get("TargetGroups", [])
                        target_group_count = len(target_groups)
                    except Exception:
                        target_groups = []
                        target_group_count = 0

                    # Get target health
                    healthy_target_count = 0
                    total_target_count = 0
                    for tg in target_groups:
                        tg_arn = tg["TargetGroupArn"]
                        try:
                            health_response = await elbv2.describe_target_health(
                                TargetGroupArn=tg_arn
                            )
                            targets = health_response.get("TargetHealthDescriptions", [])
                            total_target_count += len(targets)
                            for target in targets:
                                if target["TargetHealth"]["State"] == "healthy":
                                    healthy_target_count += 1
                        except Exception:
                            pass

                    # Calculate monthly cost
                    monthly_cost = self._calculate_alb_monthly_cost(lb_type, region)

                    # Determine utilization status
                    if lb_state != "active":
                        utilization_status = "idle"
                    elif listener_count == 0 or target_group_count == 0:
                        utilization_status = "idle"
                    elif total_target_count > 0 and healthy_target_count == 0:
                        utilization_status = "idle"
                    elif healthy_target_count > 0:
                        utilization_status = "active"
                    else:
                        utilization_status = "unknown"

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        optimization_priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_alb_optimization(
                        lb,
                        listener_count,
                        target_group_count,
                        healthy_target_count,
                        total_target_count,
                        monthly_cost,
                        lb_type,
                    )

                    # Check if LB is orphan
                    is_orphan = (
                        listener_count == 0
                        or target_group_count == 0
                        or (total_target_count > 0 and healthy_target_count == 0)
                    )

                    # Create resource data
                    resource = AllCloudResourceData(
                        resource_type="load_balancer",
                        resource_id=lb_arn,
                        resource_name=lb_name,
                        region=region,
                        estimated_monthly_cost=monthly_cost,
                        resource_metadata={
                            "lb_type": lb_type,
                            "lb_state": lb_state,
                            "dns_name": lb.get("DNSName"),
                            "scheme": lb.get("Scheme", "internet-facing"),
                            "vpc_id": lb.get("VpcId"),
                            "availability_zones": [
                                az.get("ZoneName") for az in lb.get("AvailabilityZones", [])
                            ],
                            "listener_count": listener_count,
                            "target_group_count": target_group_count,
                            "healthy_target_count": healthy_target_count,
                            "total_target_count": total_target_count,
                        },
                        currency="USD",
                        utilization_status=utilization_status,
                        cpu_utilization_percent=None,
                        memory_utilization_percent=None,
                        storage_utilization_percent=None,
                        network_utilization_mbps=None,
                        is_optimizable=is_optimizable,
                        optimization_priority=optimization_priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations,
                        tags=tags,
                        resource_status=lb_state,
                        is_orphan=is_orphan,
                        created_at_cloud=created_at.replace(tzinfo=None) if created_at else None,
                        last_used_at=None,
                    )

                    all_lbs.append(resource)

            # ================================================================
            # Scan Classic Load Balancers (ELB)
            # ================================================================
            async with self.session.client("elb", region_name=region) as elb:
                response = await elb.describe_load_balancers()

                for lb in response.get("LoadBalancerDescriptions", []):
                    lb_name = lb["LoadBalancerName"]
                    created_at = lb.get("CreatedTime")
                    dns_name = lb.get("DNSName")
                    scheme = lb.get("Scheme", "internet-facing")
                    vpc_id = lb.get("VPCId")
                    availability_zones = lb.get("AvailabilityZones", [])

                    # Extract tags
                    tags = {}
                    try:
                        tags_response = await elb.describe_tags(
                            LoadBalancerNames=[lb_name]
                        )
                        for tag_desc in tags_response.get("TagDescriptions", []):
                            for tag in tag_desc.get("Tags", []):
                                tags[tag["Key"]] = tag["Value"]
                    except Exception:
                        pass

                    # Get listeners count
                    listeners = lb.get("ListenerDescriptions", [])
                    listener_count = len(listeners)

                    # Get registered instances (CLB doesn't use target groups)
                    instances = lb.get("Instances", [])
                    total_target_count = len(instances)

                    # Get instance health
                    healthy_target_count = 0
                    if instances:
                        try:
                            health_response = await elb.describe_instance_health(
                                LoadBalancerName=lb_name
                            )
                            for instance_state in health_response.get("InstanceStates", []):
                                if instance_state["State"] == "InService":
                                    healthy_target_count += 1
                        except Exception:
                            pass

                    # Calculate monthly cost for CLB
                    monthly_cost = self._calculate_alb_monthly_cost("classic", region)

                    # Determine utilization status
                    if listener_count == 0 or total_target_count == 0:
                        utilization_status = "idle"
                    elif total_target_count > 0 and healthy_target_count == 0:
                        utilization_status = "idle"
                    elif healthy_target_count > 0:
                        utilization_status = "active"
                    else:
                        utilization_status = "unknown"

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        optimization_priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_alb_optimization(
                        {"LoadBalancerName": lb_name, "Name": lb_name},
                        listener_count,
                        0,  # CLB doesn't have target groups
                        healthy_target_count,
                        total_target_count,
                        monthly_cost,
                        "classic",
                    )

                    # Check if CLB is orphan
                    is_orphan = (
                        listener_count == 0
                        or total_target_count == 0
                        or (total_target_count > 0 and healthy_target_count == 0)
                    )

                    # Create resource data
                    resource = AllCloudResourceData(
                        resource_type="load_balancer",
                        resource_id=lb_name,  # CLB uses name as ID
                        resource_name=lb_name,
                        region=region,
                        estimated_monthly_cost=monthly_cost,
                        resource_metadata={
                            "lb_type": "classic",
                            "lb_state": "active",  # CLB doesn't have state field
                            "dns_name": dns_name,
                            "scheme": scheme,
                            "vpc_id": vpc_id,
                            "availability_zones": availability_zones,
                            "listener_count": listener_count,
                            "target_group_count": 0,  # CLB doesn't use target groups
                            "healthy_target_count": healthy_target_count,
                            "total_target_count": total_target_count,
                        },
                        currency="USD",
                        utilization_status=utilization_status,
                        cpu_utilization_percent=None,
                        memory_utilization_percent=None,
                        storage_utilization_percent=None,
                        network_utilization_mbps=None,
                        is_optimizable=is_optimizable,
                        optimization_priority=optimization_priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations,
                        tags=tags,
                        resource_status="active",
                        is_orphan=is_orphan,
                        created_at_cloud=created_at,
                        last_used_at=None,
                    )

                    all_lbs.append(resource)

            logger.info(
                "inventory.scan_aws_lb_complete",
                region=region,
                total_lbs=len(all_lbs),
            )

        except Exception as e:
            logger.error(
                "inventory.scan_aws_lb_error",
                region=region,
                error=str(e),
                exc_info=True,
            )
            raise

        return all_lbs

    async def scan_ebs_snapshots(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL EBS snapshots for cost intelligence.

        Unlike orphan detection, this returns ALL snapshots owned by account
        with optimization recommendations.

        Args:
            region: AWS region to scan

        Returns:
            List of all EBS snapshot resources
        """
        logger.info("inventory.scan_ebs_snapshots_start", region=region)
        all_snapshots: list[AllCloudResourceData] = []

        try:
            async with self.session.client("ec2", region_name=region) as ec2:
                async with self.session.client("sts", region_name=region) as sts:
                    # Get account ID
                    identity = await sts.get_caller_identity()
                    account_id = identity["Account"]

                # Describe all snapshots owned by this account
                response = await ec2.describe_snapshots(OwnerIds=[account_id])
                snapshots = response.get("Snapshots", [])

                # Get all volumes to check if snapshot source still exists
                volumes_response = await ec2.describe_volumes()
                volume_ids = {vol["VolumeId"] for vol in volumes_response.get("Volumes", [])}

                # Count snapshots per volume
                snapshot_counts: dict[str, int] = {}
                for snap in snapshots:
                    vol_id = snap.get("VolumeId")
                    if vol_id:
                        snapshot_counts[vol_id] = snapshot_counts.get(vol_id, 0) + 1

                for snapshot in snapshots:
                    snapshot_id = snapshot["SnapshotId"]
                    volume_id = snapshot.get("VolumeId")
                    size_gb = snapshot["VolumeSize"]
                    start_time = snapshot["StartTime"]
                    description = snapshot.get("Description", "")
                    state = snapshot["State"]
                    encrypted = snapshot.get("Encrypted", False)

                    # Calculate age
                    age_days = (datetime.now(start_time.tzinfo) - start_time).days

                    # Check if volume still exists
                    volume_exists = volume_id in volume_ids if volume_id else False
                    is_orphaned = not volume_exists and age_days > 90

                    # Get snapshot count for this volume
                    snapshot_count_for_volume = snapshot_counts.get(volume_id, 1) if volume_id else 1

                    # Calculate monthly cost
                    monthly_cost = self._calculate_ebs_snapshot_monthly_cost(size_gb, region)

                    # Extract tags
                    tags = {}
                    for tag in snapshot.get("Tags", []):
                        tags[tag["Key"]] = tag["Value"]

                    # Extract name from tags
                    snapshot_name = tags.get("Name")

                    # Calculate optimization (reuse existing function)
                    (
                        is_optimizable,
                        optimization_score,
                        optimization_priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_snapshot_optimization(
                        snapshot,
                        age_days,
                        size_gb,
                        is_orphaned,
                        snapshot_count_for_volume,
                        True,  # AWS snapshots are incremental by default
                        monthly_cost,
                    )

                    # Determine utilization status
                    if age_days > 365:
                        utilization_status = "idle"
                    elif is_orphaned:
                        utilization_status = "idle"
                    elif snapshot_count_for_volume > 10:
                        utilization_status = "low"
                    else:
                        utilization_status = "active"

                    # Create resource data
                    resource = AllCloudResourceData(
                        resource_type="ebs_snapshot",
                        resource_id=snapshot_id,
                        resource_name=snapshot_name,
                        region=region,
                        estimated_monthly_cost=monthly_cost,
                        resource_metadata={
                            "snapshot_id": snapshot_id,
                            "volume_id": volume_id,
                            "volume_exists": volume_exists,
                            "size_gb": size_gb,
                            "start_time": start_time.isoformat(),
                            "age_days": age_days,
                            "description": description,
                            "state": state,
                            "encrypted": encrypted,
                            "progress": snapshot.get("Progress", "100%"),
                            "snapshot_count_for_volume": snapshot_count_for_volume,
                        },
                        currency="USD",
                        utilization_status=utilization_status,
                        cpu_utilization_percent=None,
                        memory_utilization_percent=None,
                        storage_utilization_percent=None,
                        network_utilization_mbps=None,
                        is_optimizable=is_optimizable,
                        optimization_priority=optimization_priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations,
                        tags=tags,
                        resource_status=state,
                        is_orphan=is_orphaned,
                        created_at_cloud=start_time.replace(tzinfo=None) if start_time else None,
                        last_used_at=None,
                    )

                    all_snapshots.append(resource)

                logger.info(
                    "inventory.scan_ebs_snapshots_complete",
                    region=region,
                    total_snapshots=len(all_snapshots),
                )

        except Exception as e:
            logger.error(
                "inventory.scan_ebs_snapshots_error",
                region=region,
                error=str(e),
                exc_info=True,
            )
            raise

        return all_snapshots

    async def scan_aws_nat_gateways(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS NAT Gateways for cost intelligence.

        Unlike orphan detection, this returns ALL NAT Gateways with
        utilization metrics and optimization recommendations.

        Args:
            region: AWS region to scan

        Returns:
            List of all NAT Gateway resources
        """
        logger.info("inventory.scan_aws_nat_gateways_start", region=region)
        all_nat_gateways: list[AllCloudResourceData] = []

        try:
            async with self.session.client("ec2", region_name=region) as ec2:
                # Describe all NAT Gateways
                response = await ec2.describe_nat_gateways()
                nat_gateways = response.get("NatGateways", [])

                async with self.session.client("cloudwatch", region_name=region) as cloudwatch:
                    for nat in nat_gateways:
                        nat_gateway_id = nat["NatGatewayId"]
                        state = nat["State"]  # 'pending', 'available', 'deleting', 'deleted', 'failed'
                        vpc_id = nat.get("VpcId")
                        subnet_id = nat.get("SubnetId")
                        created_at = nat.get("CreateTime")

                        # Get Elastic IP addresses
                        nat_gateway_addresses = nat.get("NatGatewayAddresses", [])
                        elastic_ip_count = len(nat_gateway_addresses)
                        public_ip = None
                        private_ip = None
                        if nat_gateway_addresses:
                            public_ip = nat_gateway_addresses[0].get("PublicIp")
                            private_ip = nat_gateway_addresses[0].get("PrivateIp")

                        # Get CloudWatch metrics for bytes processed (last 30 days)
                        bytes_processed = 0.0
                        try:
                            end_time = datetime.utcnow()
                            start_time = end_time - timedelta(days=30)

                            # BytesOutToDestination metric
                            metrics_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/NATGateway",
                                MetricName="BytesOutToDestination",
                                Dimensions=[{"Name": "NatGatewayId", "Value": nat_gateway_id}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=86400 * 30,  # 30 days
                                Statistics=["Sum"],
                            )

                            datapoints = metrics_response.get("Datapoints", [])
                            if datapoints:
                                bytes_processed = datapoints[0].get("Sum", 0.0)
                        except Exception:
                            # CloudWatch metrics may not be available, continue with 0
                            pass

                        # Calculate data processed in GB
                        data_processed_gb = bytes_processed / (1024 ** 3)

                        # Calculate monthly cost
                        monthly_cost = self._calculate_aws_nat_monthly_cost(
                            data_processed_gb, region
                        )

                        # Determine utilization status
                        if state == "failed":
                            utilization_status = "idle"
                        elif state == "deleted" or state == "deleting":
                            utilization_status = "idle"
                        elif bytes_processed == 0:
                            utilization_status = "idle"
                        elif data_processed_gb < 10:
                            utilization_status = "low"
                        elif data_processed_gb < 100:
                            utilization_status = "medium"
                        else:
                            utilization_status = "high"

                        # Calculate optimization
                        (
                            is_optimizable,
                            optimization_score,
                            optimization_priority,
                            potential_savings,
                            recommendations,
                        ) = self._calculate_aws_nat_optimization(
                            nat,
                            bytes_processed,
                            monthly_cost,
                            state,
                        )

                        # Check if NAT Gateway is orphan (failed or no traffic)
                        is_orphan = state == "failed" or (
                            state == "available" and bytes_processed == 0
                        )

                        # Extract tags
                        tags = {}
                        for tag in nat.get("Tags", []):
                            tags[tag["Key"]] = tag["Value"]

                        # Extract name from tags
                        nat_gateway_name = tags.get("Name")

                        # Calculate network utilization in Mbps (approximate)
                        # bytes_processed is for 30 days, convert to Mbps
                        network_mbps = None
                        if bytes_processed > 0:
                            bytes_per_second = bytes_processed / (30 * 24 * 3600)
                            network_mbps = (bytes_per_second * 8) / (1024 * 1024)  # Convert to Mbps

                        # Create resource data
                        resource = AllCloudResourceData(
                            resource_type="nat_gateway",
                            resource_id=nat_gateway_id,
                            resource_name=nat_gateway_name,
                            region=region,
                            estimated_monthly_cost=monthly_cost,
                            resource_metadata={
                                "nat_gateway_id": nat_gateway_id,
                                "state": state,
                                "vpc_id": vpc_id,
                                "subnet_id": subnet_id,
                                "elastic_ip_count": elastic_ip_count,
                                "public_ip": public_ip,
                                "private_ip": private_ip,
                                "data_processed_gb": round(data_processed_gb, 2),
                                "bytes_processed": int(bytes_processed),
                            },
                            currency="USD",
                            utilization_status=utilization_status,
                            cpu_utilization_percent=None,
                            memory_utilization_percent=None,
                            storage_utilization_percent=None,
                            network_utilization_mbps=network_mbps,
                            is_optimizable=is_optimizable,
                            optimization_priority=optimization_priority,
                            optimization_score=optimization_score,
                            potential_monthly_savings=potential_savings,
                            optimization_recommendations=recommendations,
                            tags=tags,
                            resource_status=state,
                            is_orphan=is_orphan,
                            created_at_cloud=created_at.replace(tzinfo=None) if created_at else None,
                            last_used_at=None,
                        )

                        all_nat_gateways.append(resource)

                logger.info(
                    "inventory.scan_aws_nat_gateways_complete",
                    region=region,
                    total_nat_gateways=len(all_nat_gateways),
                )

        except Exception as e:
            logger.error(
                "inventory.scan_aws_nat_gateways_error",
                region=region,
                error=str(e),
                exc_info=True,
            )
            raise

        return all_nat_gateways

    async def scan_rds_instances(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL RDS instances for cost intelligence.

        Args:
            region: AWS region to scan

        Returns:
            List of all RDS instance resources
        """
        logger.info("inventory.scan_rds_start", region=region)
        all_rds: list[AllCloudResourceData] = []

        try:
            async with self.session.client("rds", region_name=region) as rds:
                response = await rds.describe_db_instances()

                for db_instance in response.get("DBInstances", []):
                    db_identifier = db_instance["DBInstanceIdentifier"]
                    db_instance_class = db_instance["DBInstanceClass"]
                    db_engine = db_instance["Engine"]
                    status = db_instance["DBInstanceStatus"]

                    # Get CloudWatch metrics
                    cpu_util = await self._get_rds_cpu_utilization(
                        db_identifier, region
                    )
                    connections = await self._get_rds_connections(
                        db_identifier, region
                    )

                    # Calculate monthly cost
                    monthly_cost = self._calculate_rds_monthly_cost(
                        db_instance_class, db_engine, status
                    )

                    # Determine utilization
                    utilization_status = self._determine_utilization_status(
                        cpu_util, status
                    )

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        optimization_priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_rds_optimization(
                        db_instance,
                        cpu_util,
                        connections,
                        monthly_cost,
                        status,
                    )

                    # Check if RDS is orphan (stopped or very low activity)
                    is_orphan = status == "stopped" or (
                        status == "available" and cpu_util < 1.0 and connections < 1
                    )

                    # Extract tags
                    tags = {}
                    tag_list = db_instance.get("TagList", [])
                    for tag in tag_list:
                        tags[tag["Key"]] = tag["Value"]

                    resource = AllCloudResourceData(
                        resource_type="rds_instance",
                        resource_id=db_identifier,
                        resource_name=db_identifier,
                        region=region,
                        estimated_monthly_cost=monthly_cost,
                        resource_metadata={
                            "db_instance_class": db_instance_class,
                            "engine": db_engine,
                            "engine_version": db_instance.get("EngineVersion"),
                            "status": status,
                            "allocated_storage": db_instance.get("AllocatedStorage"),
                            "storage_type": db_instance.get("StorageType"),
                            "multi_az": db_instance.get("MultiAZ", False),
                            "availability_zone": db_instance.get("AvailabilityZone"),
                            "instance_create_time": db_instance.get(
                                "InstanceCreateTime"
                            ).isoformat()
                            if db_instance.get("InstanceCreateTime")
                            else None,
                        },
                        currency="USD",
                        utilization_status=utilization_status,
                        cpu_utilization_percent=cpu_util,
                        network_utilization_mbps=None,
                        is_optimizable=is_optimizable,
                        optimization_priority=optimization_priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations,
                        tags=tags,
                        resource_status=status,
                        is_orphan=is_orphan,
                        created_at_cloud=db_instance.get("InstanceCreateTime").replace(tzinfo=None) if db_instance.get("InstanceCreateTime") else None,
                        last_used_at=None,
                    )

                    all_rds.append(resource)

                logger.info(
                    "inventory.scan_rds_complete",
                    region=region,
                    total_rds=len(all_rds),
                )

        except Exception as e:
            logger.error(
                "inventory.scan_rds_error",
                region=region,
                error=str(e),
                exc_info=True,
            )
            raise

        return all_rds

    async def scan_s3_buckets(self) -> list[AllCloudResourceData]:
        """
        Scan ALL S3 buckets for cost intelligence.

        Note: S3 is global, so this is called once per account (not per region).

        Returns:
            List of all S3 bucket resources
        """
        logger.info("inventory.scan_s3_start")
        all_buckets: list[AllCloudResourceData] = []

        try:
            async with self.session.client("s3") as s3:
                response = await s3.list_buckets()

                for bucket in response.get("Buckets", []):
                    bucket_name = bucket["Name"]

                    # Get bucket region
                    try:
                        location_response = await s3.get_bucket_location(
                            Bucket=bucket_name
                        )
                        region = location_response.get("LocationConstraint") or "us-east-1"
                    except Exception:
                        region = "us-east-1"

                    # Get bucket size and object count (from CloudWatch metrics)
                    bucket_size_gb, object_count = await self._get_s3_bucket_size(
                        bucket_name, region
                    )

                    # Calculate monthly cost (storage + requests)
                    monthly_cost = self._calculate_s3_monthly_cost(
                        bucket_size_gb, region
                    )

                    # Determine if bucket is empty or rarely used
                    is_empty = bucket_size_gb < 0.001  # Less than 1 MB
                    utilization_status = "idle" if is_empty else "medium"

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        optimization_priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_s3_optimization(
                        bucket_name,
                        bucket_size_gb,
                        object_count,
                        monthly_cost,
                        region,
                    )

                    # Check if S3 bucket is orphan (empty for >90 days)
                    is_orphan = is_empty

                    # Get tags
                    tags = {}
                    try:
                        tag_response = await s3.get_bucket_tagging(Bucket=bucket_name)
                        for tag in tag_response.get("TagSet", []):
                            tags[tag["Key"]] = tag["Value"]
                    except Exception:
                        pass  # Bucket may not have tags

                    resource = AllCloudResourceData(
                        resource_type="s3_bucket",
                        resource_id=bucket_name,
                        resource_name=bucket_name,
                        region=region,
                        estimated_monthly_cost=monthly_cost,
                        resource_metadata={
                            "bucket_size_gb": bucket_size_gb,
                            "object_count": object_count,
                            "creation_date": bucket["CreationDate"].isoformat()
                            if bucket.get("CreationDate")
                            else None,
                        },
                        currency="USD",
                        utilization_status=utilization_status,
                        storage_utilization_percent=(
                            min(bucket_size_gb / 100, 100) if bucket_size_gb else 0
                        ),
                        is_optimizable=is_optimizable,
                        optimization_priority=optimization_priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations,
                        tags=tags,
                        resource_status="active",
                        is_orphan=is_orphan,
                        created_at_cloud=bucket.get("CreationDate").replace(tzinfo=None) if bucket.get("CreationDate") else None,
                        last_used_at=None,
                    )

                    all_buckets.append(resource)

                logger.info(
                    "inventory.scan_s3_complete",
                    total_buckets=len(all_buckets),
                )

        except Exception as e:
            logger.error(
                "inventory.scan_s3_error",
                error=str(e),
                exc_info=True,
            )
            raise

        return all_buckets

    # ========== Helper Methods ==========

    async def _get_cpu_utilization(
        self, instance_id: str, region: str
    ) -> float:
        """Get average CPU utilization from CloudWatch (last 14 days)."""
        try:
            async with self.session.client("cloudwatch", region_name=region) as cw:
                end_time = datetime.utcnow()
                start_time = end_time - timedelta(days=14)

                response = await cw.get_metric_statistics(
                    Namespace="AWS/EC2",
                    MetricName="CPUUtilization",
                    Dimensions=[{"Name": "InstanceId", "Value": instance_id}],
                    StartTime=start_time,
                    EndTime=end_time,
                    Period=86400,  # 1 day
                    Statistics=["Average"],
                )

                datapoints = response.get("Datapoints", [])
                if not datapoints:
                    return 0.0

                avg_cpu = sum(dp["Average"] for dp in datapoints) / len(datapoints)
                return round(avg_cpu, 2)

        except Exception as e:
            logger.warning(
                "cloudwatch.cpu_fetch_error",
                instance_id=instance_id,
                error=str(e),
            )
            return 0.0

    async def _get_network_in(
        self, instance_id: str, region: str
    ) -> float | None:
        """Get average network in (Mbps) from CloudWatch."""
        try:
            async with self.session.client("cloudwatch", region_name=region) as cw:
                end_time = datetime.utcnow()
                start_time = end_time - timedelta(days=14)

                response = await cw.get_metric_statistics(
                    Namespace="AWS/EC2",
                    MetricName="NetworkIn",
                    Dimensions=[{"Name": "InstanceId", "Value": instance_id}],
                    StartTime=start_time,
                    EndTime=end_time,
                    Period=86400,
                    Statistics=["Average"],
                )

                datapoints = response.get("Datapoints", [])
                if not datapoints:
                    return None

                # Convert bytes to Mbps (average over period)
                avg_bytes = sum(dp["Average"] for dp in datapoints) / len(datapoints)
                mbps = (avg_bytes * 8) / (1024 * 1024)  # bytes to Mbps
                return round(mbps, 2)

        except Exception:
            return None

    async def _get_rds_cpu_utilization(
        self, db_identifier: str, region: str
    ) -> float:
        """Get average RDS CPU utilization from CloudWatch."""
        try:
            async with self.session.client("cloudwatch", region_name=region) as cw:
                end_time = datetime.utcnow()
                start_time = end_time - timedelta(days=14)

                response = await cw.get_metric_statistics(
                    Namespace="AWS/RDS",
                    MetricName="CPUUtilization",
                    Dimensions=[{"Name": "DBInstanceIdentifier", "Value": db_identifier}],
                    StartTime=start_time,
                    EndTime=end_time,
                    Period=86400,
                    Statistics=["Average"],
                )

                datapoints = response.get("Datapoints", [])
                if not datapoints:
                    return 0.0

                avg_cpu = sum(dp["Average"] for dp in datapoints) / len(datapoints)
                return round(avg_cpu, 2)

        except Exception:
            return 0.0

    async def _get_rds_connections(
        self, db_identifier: str, region: str
    ) -> float:
        """Get average RDS database connections from CloudWatch."""
        try:
            async with self.session.client("cloudwatch", region_name=region) as cw:
                end_time = datetime.utcnow()
                start_time = end_time - timedelta(days=14)

                response = await cw.get_metric_statistics(
                    Namespace="AWS/RDS",
                    MetricName="DatabaseConnections",
                    Dimensions=[{"Name": "DBInstanceIdentifier", "Value": db_identifier}],
                    StartTime=start_time,
                    EndTime=end_time,
                    Period=86400,
                    Statistics=["Average"],
                )

                datapoints = response.get("Datapoints", [])
                if not datapoints:
                    return 0.0

                avg_conn = sum(dp["Average"] for dp in datapoints) / len(datapoints)
                return round(avg_conn, 2)

        except Exception:
            return 0.0

    async def _get_s3_bucket_size(
        self, bucket_name: str, region: str
    ) -> tuple[float, int]:
        """Get S3 bucket size (GB) and object count from CloudWatch."""
        try:
            async with self.session.client("cloudwatch", region_name=region) as cw:
                end_time = datetime.utcnow()
                start_time = end_time - timedelta(days=2)

                # Get bucket size
                size_response = await cw.get_metric_statistics(
                    Namespace="AWS/S3",
                    MetricName="BucketSizeBytes",
                    Dimensions=[
                        {"Name": "BucketName", "Value": bucket_name},
                        {"Name": "StorageType", "Value": "StandardStorage"},
                    ],
                    StartTime=start_time,
                    EndTime=end_time,
                    Period=86400,
                    Statistics=["Average"],
                )

                size_datapoints = size_response.get("Datapoints", [])
                size_bytes = size_datapoints[0]["Average"] if size_datapoints else 0
                size_gb = size_bytes / (1024**3)

                # Get object count
                count_response = await cw.get_metric_statistics(
                    Namespace="AWS/S3",
                    MetricName="NumberOfObjects",
                    Dimensions=[
                        {"Name": "BucketName", "Value": bucket_name},
                        {"Name": "StorageType", "Value": "AllStorageTypes"},
                    ],
                    StartTime=start_time,
                    EndTime=end_time,
                    Period=86400,
                    Statistics=["Average"],
                )

                count_datapoints = count_response.get("Datapoints", [])
                object_count = int(count_datapoints[0]["Average"]) if count_datapoints else 0

                return round(size_gb, 3), object_count

        except Exception:
            return 0.0, 0

    def _calculate_ec2_monthly_cost(
        self, instance_type: str, state: str
    ) -> float:
        """
        Calculate estimated monthly cost for EC2 instance.

        Simplified pricing - in production, use AWS Pricing API.
        """
        # Hardcoded prices per hour (us-east-1, on-demand)
        INSTANCE_PRICES = {
            "t2.micro": 0.0116,
            "t2.small": 0.023,
            "t2.medium": 0.0464,
            "t3.micro": 0.0104,
            "t3.small": 0.0208,
            "t3.medium": 0.0416,
            "m5.large": 0.096,
            "m5.xlarge": 0.192,
            "c5.large": 0.085,
            "r5.large": 0.126,
        }

        hourly_rate = INSTANCE_PRICES.get(instance_type, 0.10)  # Default $0.10/hr

        if state == "stopped":
            # Stopped instances still incur EBS costs (~$0.10/GB/month)
            # Estimate 30GB EBS volume
            return 30 * 0.10
        else:
            # Running instances: hourly rate * 730 hours/month
            return hourly_rate * 730

    def _calculate_rds_monthly_cost(
        self, db_instance_class: str, engine: str, status: str
    ) -> float:
        """Calculate estimated monthly cost for RDS instance."""
        # Simplified RDS pricing
        RDS_PRICES = {
            "db.t2.micro": 0.017,
            "db.t3.micro": 0.016,
            "db.t3.small": 0.032,
            "db.t3.medium": 0.064,
            "db.m5.large": 0.17,
            "db.r5.large": 0.24,
        }

        hourly_rate = RDS_PRICES.get(db_instance_class, 0.15)

        if status == "stopped":
            # Stopped RDS instances incur storage costs
            return 50 * 0.10  # Estimate 50GB storage
        else:
            return hourly_rate * 730

    def _calculate_s3_monthly_cost(self, size_gb: float, region: str) -> float:
        """Calculate estimated monthly cost for S3 bucket."""
        # S3 Standard pricing: $0.023/GB/month (first 50TB)
        storage_cost = size_gb * 0.023

        # Add request costs (estimate)
        request_cost = 1.0  # Flat $1/month estimate

        return storage_cost + request_cost

    def _determine_utilization_status(
        self, cpu_util: float | None, state: str
    ) -> str:
        """Determine utilization status based on CPU and state."""
        if state in ["stopped", "stopping"]:
            return "idle"
        elif cpu_util is None:
            return "unknown"
        elif cpu_util < 10:
            return "idle"
        elif cpu_util < 30:
            return "low"
        elif cpu_util < 70:
            return "medium"
        else:
            return "high"

    def _calculate_ec2_optimization(
        self,
        instance: dict[str, Any],
        cpu_util: float,
        monthly_cost: float,
        state: str,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Calculate EC2 optimization metrics.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        recommendations = []
        is_optimizable = False
        optimization_score = 0
        potential_savings = 0.0
        priority = "none"

        instance_type = instance["InstanceType"]

        # Scenario 1: Stopped instance (critical)
        if state == "stopped":
            is_optimizable = True
            optimization_score = 80
            priority = "critical"
            potential_savings = monthly_cost * 0.7  # Save 70% (only EBS remains)
            recommendations.append({
                "action": "Terminate or restart stopped instance",
                "details": f"Instance has been stopped. Terminate to save ${potential_savings:.2f}/month",
                "priority": "critical",
            })

        # Scenario 2: Very low CPU (<10%)
        elif cpu_util < 10:
            is_optimizable = True
            optimization_score = 60
            priority = "high"
            # Suggest downgrading to smaller instance
            potential_savings = monthly_cost * 0.5
            recommendations.append({
                "action": "Downgrade instance type",
                "details": f"CPU utilization is {cpu_util}%. Consider t3.micro or t3.small",
                "alternatives": [
                    {"name": "t3.micro", "cost": 7.60, "savings": monthly_cost - 7.60},
                    {"name": "t3.small", "cost": 15.20, "savings": monthly_cost - 15.20},
                ],
                "priority": "high",
            })

        # Scenario 3: Old generation instance
        elif instance_type.startswith(("t2.", "m4.", "c4.", "r4.")):
            is_optimizable = True
            optimization_score = 40
            priority = "medium"
            potential_savings = monthly_cost * 0.2  # 20% savings
            new_type = instance_type.replace("t2.", "t3.").replace("m4.", "m5.").replace("c4.", "c5.").replace("r4.", "r5.")
            recommendations.append({
                "action": "Upgrade to newer generation",
                "details": f"Migrate {instance_type}  {new_type} for better price/performance",
                "priority": "medium",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def _get_volume_read_ops(
        self, volume_id: str, region: str
    ) -> float:
        """Get average volume read operations from CloudWatch (last 14 days)."""
        try:
            async with self.session.client("cloudwatch", region_name=region) as cw:
                end_time = datetime.utcnow()
                start_time = end_time - timedelta(days=14)

                response = await cw.get_metric_statistics(
                    Namespace="AWS/EBS",
                    MetricName="VolumeReadOps",
                    Dimensions=[{"Name": "VolumeId", "Value": volume_id}],
                    StartTime=start_time,
                    EndTime=end_time,
                    Period=86400,  # 1 day
                    Statistics=["Sum"],
                )

                datapoints = response.get("Datapoints", [])
                if not datapoints:
                    return 0.0

                # Calculate average daily operations
                total_ops = sum(dp["Sum"] for dp in datapoints)
                avg_daily_ops = total_ops / len(datapoints) if datapoints else 0.0
                return round(avg_daily_ops, 2)

        except Exception as e:
            logger.warning(
                "cloudwatch.volume_read_ops_error",
                volume_id=volume_id,
                error=str(e),
            )
            return 0.0

    async def _get_volume_write_ops(
        self, volume_id: str, region: str
    ) -> float:
        """Get average volume write operations from CloudWatch (last 14 days)."""
        try:
            async with self.session.client("cloudwatch", region_name=region) as cw:
                end_time = datetime.utcnow()
                start_time = end_time - timedelta(days=14)

                response = await cw.get_metric_statistics(
                    Namespace="AWS/EBS",
                    MetricName="VolumeWriteOps",
                    Dimensions=[{"Name": "VolumeId", "Value": volume_id}],
                    StartTime=start_time,
                    EndTime=end_time,
                    Period=86400,  # 1 day
                    Statistics=["Sum"],
                )

                datapoints = response.get("Datapoints", [])
                if not datapoints:
                    return 0.0

                # Calculate average daily operations
                total_ops = sum(dp["Sum"] for dp in datapoints)
                avg_daily_ops = total_ops / len(datapoints) if datapoints else 0.0
                return round(avg_daily_ops, 2)

        except Exception as e:
            logger.warning(
                "cloudwatch.volume_write_ops_error",
                volume_id=volume_id,
                error=str(e),
            )
            return 0.0

    def _calculate_ebs_monthly_cost(
        self,
        volume_type: str,
        size_gb: int,
        iops: int | None,
        throughput: int | None,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for EBS volume.

        Uses dynamic pricing from pricing service with hardcoded fallback.
        Includes storage, IOPS, and throughput costs.

        Args:
            volume_type: Volume type (gp3, gp2, io2, io1, st1, sc1)
            size_gb: Volume size in GB
            iops: Provisioned IOPS (for io1/io2/gp3)
            throughput: Provisioned throughput in MBps (for gp3/st1)
            region: AWS region

        Returns:
            Estimated monthly cost in USD
        """
        # Base storage cost per GB/month (fallback prices)
        STORAGE_PRICES = {
            "gp3": 0.08,
            "gp2": 0.10,
            "io2": 0.125,
            "io1": 0.125,
            "st1": 0.045,
            "sc1": 0.015,
        }

        storage_price = STORAGE_PRICES.get(volume_type, 0.08)
        storage_cost = size_gb * storage_price

        # Additional IOPS cost (above baseline)
        iops_cost = 0.0
        if volume_type == "gp3" and iops and iops > 3000:
            # gp3: $0.005/IOPS/month above 3,000 baseline
            extra_iops = iops - 3000
            iops_cost = extra_iops * 0.005
        elif volume_type in ["io1", "io2"] and iops:
            # io1/io2: $0.065/IOPS/month
            iops_cost = iops * 0.065

        # Additional throughput cost (above baseline)
        throughput_cost = 0.0
        if volume_type == "gp3" and throughput and throughput > 125:
            # gp3: $0.04/MBps/month above 125 MBps baseline
            extra_throughput = throughput - 125
            throughput_cost = extra_throughput * 0.04

        total_cost = storage_cost + iops_cost + throughput_cost
        return round(total_cost, 2)

    def _calculate_ebs_optimization(
        self,
        volume: dict[str, Any],
        read_ops: float,
        write_ops: float,
        monthly_cost: float,
        state: str,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Calculate EBS volume optimization metrics.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        recommendations = []
        is_optimizable = False
        optimization_score = 0
        potential_savings = 0.0
        priority = "none"

        volume_type = volume.get("VolumeType", "gp3")
        size_gb = volume.get("Size", 0)
        iops = volume.get("Iops")
        attachments = volume.get("Attachments", [])

        # Scenario 1: Unattached volume (critical)
        if state == "available":
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost  # 100% savings
            recommendations.append({
                "action": "Delete unattached volume",
                "details": f"Volume is unattached. Create snapshot if needed, then delete to save ${potential_savings:.2f}/month",
                "priority": "critical",
            })

        # Scenario 2: Attached to stopped instance (high)
        elif attachments and read_ops == 0 and write_ops == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost * 0.9  # Assume snapshot cost is 10%
            recommendations.append({
                "action": "Snapshot and delete",
                "details": f"Volume has no I/O activity. Consider snapshot (${monthly_cost * 0.1:.2f}/month) and delete",
                "priority": "high",
            })

        # Scenario 3: gp2  gp3 migration opportunity (medium)
        elif volume_type == "gp2":
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # gp3 is ~20% cheaper than gp2
            gp3_cost = size_gb * 0.08
            potential_savings = monthly_cost - gp3_cost
            recommendations.append({
                "action": "Migrate gp2  gp3",
                "details": f"Migrate to gp3 for better price/performance. Save ~${potential_savings:.2f}/month",
                "alternatives": [
                    {
                        "name": "gp3",
                        "cost": gp3_cost,
                        "savings": potential_savings,
                        "performance": "Better (20% more baseline throughput)"
                    }
                ],
                "priority": "medium",
            })

        # Scenario 4: Overprovisioned IOPS (medium)
        elif volume_type in ["io1", "io2", "gp3"] and iops:
            # If total daily ops < 10% of provisioned IOPS, it's overprovisioned
            daily_ops = read_ops + write_ops
            if iops > 3000 and daily_ops < (iops * 0.1 * 86400):  # 86400 seconds/day
                is_optimizable = True
                optimization_score = 60
                priority = "medium"

                # Suggest reducing to 3000 IOPS (gp3 baseline)
                if volume_type == "gp3":
                    reduced_iops = 3000
                    iops_savings = (iops - reduced_iops) * 0.005
                else:  # io1/io2
                    reduced_iops = max(3000, int(iops * 0.5))
                    iops_savings = (iops - reduced_iops) * 0.065

                potential_savings = iops_savings
                recommendations.append({
                    "action": "Reduce provisioned IOPS",
                    "details": f"IOPS utilization <10%. Reduce {iops}  {reduced_iops} IOPS to save ${potential_savings:.2f}/month",
                    "priority": "medium",
                })

        # Scenario 5: io2  gp3 downgrade opportunity (low)
        elif volume_type in ["io1", "io2"] and daily_ops < 16000 * 86400:  # 16K IOPS baseline for gp3
            is_optimizable = True
            optimization_score = 40
            priority = "low"
            gp3_cost = size_gb * 0.08
            potential_savings = monthly_cost - gp3_cost
            if potential_savings > 0:
                recommendations.append({
                    "action": "Downgrade to gp3",
                    "details": f"Low IOPS usage. Consider gp3 (3K-16K IOPS) to save ${potential_savings:.2f}/month",
                    "priority": "low",
                })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    def _calculate_alb_monthly_cost(
        self,
        lb_type: str,
        region: str,
        data_processed_gb: float = 0.0,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS Load Balancer.

        AWS Load Balancer pricing:
        - ALB: $22.61/month base + $0.008/LCU-hour (~$5.76/month for 1 LCU-hour average)
        - NLB: $22.61/month base + $0.006/LCU-hour (~$4.32/month for 1 LCU-hour average)
        - CLB: $18.03/month base + $0.008/GB data processed

        Args:
            lb_type: Load balancer type ('application', 'network', 'classic')
            region: AWS region
            data_processed_gb: Data processed in GB (for CLB pricing)

        Returns:
            Estimated monthly cost in USD
        """
        # Base hourly rates * 730 hours/month
        if lb_type == "application":
            base_cost = 22.61
            # Simplified LCU cost: assume 1 LCU-hour average
            # 1 LCU-hour = $0.008/hour * 730 hours = $5.84/month
            lcu_cost = 5.84
            return base_cost + lcu_cost
        elif lb_type == "network":
            base_cost = 22.61
            # 1 LCU-hour for NLB = $0.006/hour * 730 hours = $4.38/month
            lcu_cost = 4.38
            return base_cost + lcu_cost
        elif lb_type == "classic":
            base_cost = 18.03
            # Data processing: $0.008/GB
            data_cost = data_processed_gb * 0.008
            return base_cost + data_cost
        else:
            # Default to ALB pricing
            return 28.45

    def _calculate_alb_optimization(
        self,
        lb: dict[str, Any],
        listener_count: int,
        target_group_count: int,
        healthy_target_count: int,
        total_target_count: int,
        monthly_cost: float,
        lb_type: str,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Calculate AWS Load Balancer optimization metrics.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        recommendations = []
        is_optimizable = False
        optimization_score = 0
        potential_savings = 0.0
        priority = "none"

        lb_name = lb.get("LoadBalancerName") or lb.get("Name", "Unknown")

        # Scenario 1: No listeners configured (Critical)
        if listener_count == 0:
            is_optimizable = True
            optimization_score = 95
            priority = "critical"
            potential_savings = monthly_cost  # 100% savings
            recommendations.append({
                "action": "Delete load balancer",
                "details": f"Load balancer has no listeners configured. Delete to save ${potential_savings:.2f}/month",
                "priority": "critical",
            })

        # Scenario 2: No target groups (Critical)
        elif target_group_count == 0 and lb_type != "classic":
            is_optimizable = True
            optimization_score = 95
            priority = "critical"
            potential_savings = monthly_cost  # 100% savings
            recommendations.append({
                "action": "Delete load balancer",
                "details": f"Load balancer has no target groups. No backends configured. Delete to save ${potential_savings:.2f}/month",
                "priority": "critical",
            })

        # Scenario 3: All targets unhealthy (High)
        elif total_target_count > 0 and healthy_target_count == 0:
            is_optimizable = True
            optimization_score = 85
            priority = "high"
            potential_savings = monthly_cost * 0.9  # Assume 90% savings if deleted
            recommendations.append({
                "action": "Fix or delete load balancer",
                "details": f"All {total_target_count} targets are unhealthy. Fix targets or delete LB to save ~${potential_savings:.2f}/month",
                "priority": "high",
            })

        # Scenario 4: No targets at all for CLB (High)
        elif lb_type == "classic" and total_target_count == 0:
            is_optimizable = True
            optimization_score = 90
            priority = "high"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete Classic Load Balancer",
                "details": f"CLB has no registered instances. Delete to save ${potential_savings:.2f}/month",
                "priority": "high",
            })

        # Scenario 5: CLB migration opportunity (Medium)
        elif lb_type == "classic" and healthy_target_count > 0:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # ALB costs ~$4/month more but offers better features
            alb_cost = 28.45
            potential_savings = -(alb_cost - monthly_cost)  # Negative = additional cost
            recommendations.append({
                "action": "Migrate Classic LB to Application LB",
                "details": f"Upgrade to ALB for better features (HTTP/2, WebSocket, path-based routing). Additional cost: ${abs(potential_savings):.2f}/month",
                "alternatives": [
                    {
                        "name": "Application Load Balancer",
                        "cost": alb_cost,
                        "savings": potential_savings,
                        "benefits": "HTTP/2, WebSocket, path-based routing, host-based routing, better health checks"
                    }
                ],
                "priority": "medium",
            })

        # Scenario 6: Very low utilization (Low)
        # This would require CloudWatch metrics, which we can add later
        # For now, we focus on structural issues

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    def _calculate_ebs_snapshot_monthly_cost(
        self,
        size_gb: int,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for EBS snapshot.

        AWS EBS Snapshot pricing: $0.05/GB/month (standard snapshot storage)

        Args:
            size_gb: Snapshot size in GB
            region: AWS region

        Returns:
            Estimated monthly cost in USD
        """
        # EBS snapshot pricing is simple: $0.05/GB/month
        return size_gb * 0.05

    def _calculate_aws_nat_monthly_cost(
        self,
        data_processed_gb: float,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS NAT Gateway.

        AWS NAT Gateway pricing:
        - Base: $32.40/month ($0.045/hour * 720 hours)
        - Data processing: $0.045/GB

        Args:
            data_processed_gb: Data processed per month in GB
            region: AWS region

        Returns:
            Estimated monthly cost in USD
        """
        base_cost = 32.40  # Base hourly cost * 720 hours/month
        data_cost = data_processed_gb * 0.045  # $0.045/GB
        return base_cost + data_cost

    def _calculate_aws_nat_optimization(
        self,
        nat_gateway: dict[str, Any],
        bytes_processed: float,
        monthly_cost: float,
        state: str,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Calculate AWS NAT Gateway optimization metrics.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        recommendations = []
        is_optimizable = False
        optimization_score = 0
        potential_savings = 0.0
        priority = "none"

        nat_gateway_id = nat_gateway.get("NatGatewayId", "Unknown")
        data_processed_gb = bytes_processed / (1024 ** 3)  # Convert bytes to GB

        # Scenario 1: NAT Gateway in failed state (Critical)
        if state == "failed":
            is_optimizable = True
            optimization_score = 95
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete failed NAT Gateway",
                "details": f"NAT Gateway is in failed state. Delete to save ${potential_savings:.2f}/month",
                "priority": "critical",
            })

        # Scenario 2: Zero traffic for 30+ days (High)
        elif bytes_processed == 0:
            is_optimizable = True
            optimization_score = 85
            priority = "high"
            potential_savings = monthly_cost * 0.9  # Assume 90% savings
            recommendations.append({
                "action": "Delete unused NAT Gateway",
                "details": f"NAT Gateway has no traffic. Delete to save ~${potential_savings:.2f}/month",
                "priority": "high",
            })

        # Scenario 3: Very low traffic (<10GB/month) (Medium)
        elif data_processed_gb < 10:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Suggest using instance with public IP instead
            potential_savings = 32.40  # Save base cost only
            recommendations.append({
                "action": "Consider using EC2 instance with public IP",
                "details": f"Very low traffic ({data_processed_gb:.2f}GB/month). EC2 with public IP may be more cost-effective. Save ~${potential_savings:.2f}/month",
                "alternatives": [
                    {
                        "name": "EC2 with Public IP",
                        "cost": data_processed_gb * 0.01,  # Approximate data transfer cost
                        "savings": potential_savings,
                    }
                ],
                "priority": "medium",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    def _calculate_rds_optimization(
        self,
        db_instance: dict[str, Any],
        cpu_util: float,
        connections: float,
        monthly_cost: float,
        status: str,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """Calculate RDS optimization metrics."""
        recommendations = []
        is_optimizable = False
        optimization_score = 0
        potential_savings = 0.0
        priority = "none"

        # Scenario 1: Stopped RDS
        if status == "stopped":
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost * 0.8
            recommendations.append({
                "action": "Delete or restart stopped RDS instance",
                "details": f"RDS instance is stopped. Terminate to save ${potential_savings:.2f}/month",
                "priority": "critical",
            })

        # Scenario 2: Very low activity
        elif cpu_util < 5 and connections < 2:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = monthly_cost * 0.5
            recommendations.append({
                "action": "Downgrade RDS instance or move to Aurora Serverless",
                "details": f"CPU: {cpu_util}%, Connections: {connections}. Consider smaller instance or serverless",
                "priority": "high",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    def _calculate_s3_optimization(
        self,
        bucket_name: str,
        bucket_size_gb: float,
        object_count: int,
        monthly_cost: float,
        region: str,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """Calculate S3 optimization metrics."""
        recommendations = []
        is_optimizable = False
        optimization_score = 0
        potential_savings = 0.0
        priority = "none"

        # Scenario 1: Empty bucket
        if bucket_size_gb < 0.001:
            is_optimizable = True
            optimization_score = 50
            priority = "low"
            potential_savings = 1.0  # Request costs
            recommendations.append({
                "action": "Delete empty S3 bucket",
                "details": f"Bucket is empty. Delete to save ${potential_savings:.2f}/month",
                "priority": "low",
            })

        # Scenario 2: Large bucket on Standard storage
        elif bucket_size_gb > 1000:  # > 1TB
            is_optimizable = True
            optimization_score = 30
            priority = "medium"
            # Suggest Glacier for archival
            potential_savings = bucket_size_gb * (0.023 - 0.004)  # Standard  Glacier
            recommendations.append({
                "action": "Move to S3 Glacier for archival data",
                "details": f"Bucket size: {bucket_size_gb:.1f}GB. Move infrequently accessed data to Glacier",
                "alternatives": [
                    {"name": "S3 Glacier", "cost": bucket_size_gb * 0.004, "savings": potential_savings},
                ],
                "priority": "medium",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    def _calculate_snapshot_optimization(
        self,
        snap,
        age_days: int,
        disk_size_gb: int,
        is_orphaned: bool,
        snapshot_count_for_disk: int,
        incremental: bool,
        monthly_cost: float
    ) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate EBS Snapshot optimization opportunities.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: Snapshot trs ancien (>365 jours) - CRITICAL (90 score)
        if age_days > 365:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete old snapshot (>1 year old)",
                "details": f"Snapshot is {age_days} days old ({disk_size_gb}GB). Consider deleting if no longer needed. Saving ${monthly_cost}/month.",
                "alternatives": [
                    {"name": "Delete snapshot", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "critical",
            })

        # Scenario 2: Snapshot orphelin (volume source supprim) - HIGH (75 score)
        elif is_orphaned:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete orphaned snapshot (source volume deleted)",
                "details": f"Source volume no longer exists. Snapshot is orphaned ({disk_size_gb}GB). Delete to save ${monthly_cost}/month.",
                "alternatives": [
                    {"name": "Delete orphaned snapshot", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "high",
            })

        # Scenario 3: Snapshots multiples du mme volume (>10) - MEDIUM (50 score)
        elif snapshot_count_for_disk > 10:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Estimate savings: delete oldest 50% of snapshots
            potential_savings = monthly_cost * 0.5
            recommendations.append({
                "action": f"Reduce number of snapshots ({snapshot_count_for_disk} snapshots)",
                "details": f"Source volume has {snapshot_count_for_disk} snapshots. Consider retention policy to delete old snapshots. Save ~${potential_savings}/month.",
                "alternatives": [
                    {"name": "Delete oldest 50% of snapshots", "cost": round(monthly_cost * 0.5, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "medium",
            })

        # Scenario 4: Snapshot non incrmentiel (LOW - 30 score)
        # Note: AWS EBS snapshots are always incremental, so this scenario rarely applies
        elif not incremental and disk_size_gb > 128:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            # Savings: incremental snapshots are ~80% cheaper
            potential_savings = monthly_cost * 0.8
            recommendations.append({
                "action": "Switch to incremental snapshots",
                "details": f"Using full snapshot ({disk_size_gb}GB). Incremental snapshots could save ~${potential_savings}/month (80% reduction).",
                "alternatives": [
                    {"name": "Use incremental snapshots", "cost": round(monthly_cost * 0.2, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations


class AzureInventoryScanner:
    """Azure-specific inventory scanner for cost intelligence."""

    def __init__(self, provider: Any) -> None:
        """
        Initialize Azure inventory scanner.

        Args:
            provider: Azure provider instance with authenticated credentials
        """
        self.provider = provider
        self.tenant_id = provider.tenant_id
        self.client_id = provider.client_id
        self.client_secret = provider.client_secret
        self.subscription_id = provider.subscription_id
        self.regions = provider.regions or []
        self.resource_groups = provider.resource_groups or []
        self.logger = structlog.get_logger()

    async def scan_virtual_machines(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Virtual Machines for cost intelligence.

        Unlike orphan detection, this returns ALL VMs (running, stopped, deallocated)
        with utilization metrics and optimization recommendations.

        Args:
            region: Azure region to scan

        Returns:
            List of all VM resources
        """
        logger.info("inventory.scan_azure_vms_start", region=region)
        all_vms: list[AllCloudResourceData] = []

        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.compute import ComputeManagementClient

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            compute_client = ComputeManagementClient(credential, self.subscription_id)

            # Get ALL VMs
            vms = list(compute_client.virtual_machines.list_all())

            for vm in vms:
                # Filter by region
                if vm.location != region:
                    continue

                # Filter by resource group (if specified)
                if not self.provider._is_resource_in_scope(vm.id):
                    continue

                # Extract resource group name
                resource_group = vm.id.split('/')[4]

                # Get instance view for power state
                instance_view = compute_client.virtual_machines.instance_view(
                    resource_group_name=resource_group,
                    vm_name=vm.name
                )

                # Determine power state
                power_state = "unknown"
                for status in instance_view.statuses:
                    if status.code and status.code.startswith('PowerState/'):
                        power_state = status.code.split('/')[-1]

                # Extract tags
                tags = vm.tags if vm.tags else {}

                # Get VM metrics (CPU utilization from Azure Monitor)
                cpu_util = await self._get_vm_cpu_utilization(
                    resource_group, vm.name, region
                )
                network_in = await self._get_vm_network_in(
                    resource_group, vm.name, region
                )

                # Calculate monthly cost (VM + disks)
                monthly_cost = self._calculate_vm_monthly_cost(
                    vm.hardware_profile.vm_size if vm.hardware_profile else "Standard_D2s_v3",
                    power_state,
                    vm.storage_profile if vm.storage_profile else None,
                    compute_client,
                    resource_group
                )

                # Determine utilization status
                utilization_status = self._determine_vm_utilization_status(
                    cpu_util, power_state
                )

                # Calculate optimization
                (
                    is_optimizable,
                    optimization_score,
                    optimization_priority,
                    potential_savings,
                    recommendations,
                ) = self._calculate_vm_optimization(
                    vm,
                    power_state,
                    cpu_util,
                    monthly_cost,
                )

                # Check if VM is orphan (deallocated or very low CPU)
                is_orphan = power_state == "deallocated" or (
                    power_state == "running" and cpu_util < 5.0
                )

                # Create resource data
                resource = AllCloudResourceData(
                    resource_type="azure_vm",
                    resource_id=vm.id,
                    resource_name=vm.name,
                    region=region,
                    estimated_monthly_cost=monthly_cost,
                    resource_metadata={
                        "vm_size": vm.hardware_profile.vm_size if vm.hardware_profile else None,
                        "power_state": power_state,
                        "os_type": vm.storage_profile.os_disk.os_type if vm.storage_profile and vm.storage_profile.os_disk else None,
                        "resource_group": resource_group,
                        "availability_zone": vm.zones[0] if vm.zones else None,
                    },
                    currency="USD",
                    utilization_status=utilization_status,
                    cpu_utilization_percent=cpu_util,
                    memory_utilization_percent=None,  # TODO: Fetch from Azure Monitor
                    network_utilization_mbps=network_in,
                    is_optimizable=is_optimizable,
                    optimization_priority=optimization_priority,
                    optimization_score=optimization_score,
                    potential_monthly_savings=potential_savings,
                    optimization_recommendations=recommendations,
                    tags=tags,
                    resource_status=power_state,
                    is_orphan=is_orphan,
                    created_at_cloud=None,  # Azure doesn't provide creation time easily
                    last_used_at=None,
                )

                all_vms.append(resource)

            logger.info(
                "inventory.scan_azure_vms_complete",
                region=region,
                total_vms=len(all_vms),
            )

        except Exception as e:
            logger.error(
                "inventory.scan_azure_vms_error",
                region=region,
                error=str(e),
                exc_info=True,
            )
            raise

        return all_vms

    async def scan_managed_disks(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Managed Disks for cost intelligence.

        Args:
            region: Azure region to scan

        Returns:
            List of all disk resources
        """
        logger.info("inventory.scan_azure_disks_start", region=region)
        all_disks: list[AllCloudResourceData] = []

        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.compute import ComputeManagementClient

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            compute_client = ComputeManagementClient(credential, self.subscription_id)

            # Get ALL disks
            disks = list(compute_client.disks.list())

            for disk in disks:
                # Filter by region
                if disk.location != region:
                    continue

                # Filter by resource group
                if not self.provider._is_resource_in_scope(disk.id):
                    continue

                # Determine if attached or unattached
                is_attached = disk.managed_by is not None
                disk_state = "attached" if is_attached else "unattached"

                # Extract tags
                tags = disk.tags if disk.tags else {}

                # Calculate monthly cost using provider's helper
                monthly_cost = self.provider._calculate_disk_cost(disk)

                # Determine utilization status
                utilization_status = "idle" if not is_attached else "active"

                # Calculate optimization
                (
                    is_optimizable,
                    optimization_score,
                    optimization_priority,
                    potential_savings,
                    recommendations,
                ) = self._calculate_disk_optimization(
                    disk,
                    is_attached,
                    monthly_cost,
                )

                # Check if disk is orphan (unattached)
                is_orphan = not is_attached

                # Create resource data
                resource = AllCloudResourceData(
                    resource_type="azure_managed_disk",
                    resource_id=disk.id,
                    resource_name=disk.name,
                    region=region,
                    estimated_monthly_cost=monthly_cost,
                    resource_metadata={
                        "disk_size_gb": disk.disk_size_gb,
                        "disk_state": disk_state,
                        "sku_name": disk.sku.name if disk.sku else None,
                        "sku_tier": disk.sku.tier if disk.sku else None,
                        "disk_iops": getattr(disk, 'disk_iops_read_write', None),
                        "disk_mbps": getattr(disk, 'disk_mbps_read_write', None),
                        "encryption_type": disk.encryption.type if disk.encryption else None,
                    },
                    currency="USD",
                    utilization_status=utilization_status,
                    cpu_utilization_percent=None,
                    memory_utilization_percent=None,
                    storage_utilization_percent=None,  # TODO: Calculate from IOPS metrics
                    network_utilization_mbps=None,
                    is_optimizable=is_optimizable,
                    optimization_priority=optimization_priority,
                    optimization_score=optimization_score,
                    potential_monthly_savings=potential_savings,
                    optimization_recommendations=recommendations,
                    tags=tags,
                    resource_status=disk_state,
                    is_orphan=is_orphan,
                    created_at_cloud=disk.time_created.replace(tzinfo=None) if hasattr(disk, 'time_created') and disk.time_created else None,
                    last_used_at=None,
                )

                all_disks.append(resource)

            logger.info(
                "inventory.scan_azure_disks_complete",
                region=region,
                total_disks=len(all_disks),
            )

        except Exception as e:
            logger.error(
                "inventory.scan_azure_disks_error",
                region=region,
                error=str(e),
                exc_info=True,
            )
            raise

        return all_disks

    async def scan_public_ips(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Public IPs for cost intelligence.

        Args:
            region: Azure region to scan

        Returns:
            List of all public IP resources
        """
        logger.info("inventory.scan_azure_ips_start", region=region)
        all_ips: list[AllCloudResourceData] = []

        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.network import NetworkManagementClient

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            network_client = NetworkManagementClient(credential, self.subscription_id)

            # Get ALL public IPs
            public_ips = list(network_client.public_ip_addresses.list_all())

            for ip in public_ips:
                # Filter by region
                if ip.location != region:
                    continue

                # Filter by resource group
                if not self.provider._is_resource_in_scope(ip.id):
                    continue

                # Determine if assigned or unassigned
                is_assigned = ip.ip_configuration is not None
                ip_state = "assigned" if is_assigned else "unassigned"

                # Extract tags
                tags = ip.tags if ip.tags else {}

                # Calculate monthly cost
                sku_name = ip.sku.name if ip.sku else "Basic"
                monthly_cost = 3.65 if sku_name == "Basic" else 4.00  # Standard SKU costs more

                # Determine utilization status
                utilization_status = "active" if is_assigned else "idle"

                # Calculate optimization
                (
                    is_optimizable,
                    optimization_score,
                    optimization_priority,
                    potential_savings,
                    recommendations,
                ) = self._calculate_ip_optimization(
                    ip,
                    is_assigned,
                    sku_name,
                    monthly_cost,
                )

                # Check if IP is orphan (unassigned)
                is_orphan = not is_assigned

                # Create resource data
                resource = AllCloudResourceData(
                    resource_type="azure_public_ip",
                    resource_id=ip.id,
                    resource_name=ip.name,
                    region=region,
                    estimated_monthly_cost=monthly_cost,
                    resource_metadata={
                        "ip_address": ip.ip_address,
                        "ip_state": ip_state,
                        "sku_name": sku_name,
                        "allocation_method": safe_get_value(ip.public_ip_allocation_method),
                        "ip_version": safe_get_value(ip.public_ip_address_version),
                        "idle_timeout_minutes": ip.idle_timeout_in_minutes,
                    },
                    currency="USD",
                    utilization_status=utilization_status,
                    cpu_utilization_percent=None,
                    memory_utilization_percent=None,
                    storage_utilization_percent=None,
                    network_utilization_mbps=None,  # TODO: Fetch from Azure Monitor
                    is_optimizable=is_optimizable,
                    optimization_priority=optimization_priority,
                    optimization_score=optimization_score,
                    potential_monthly_savings=potential_savings,
                    optimization_recommendations=recommendations,
                    tags=tags,
                    resource_status=ip_state,
                    is_orphan=is_orphan,
                    created_at_cloud=None,
                    last_used_at=None,
                )

                all_ips.append(resource)

            logger.info(
                "inventory.scan_azure_ips_complete",
                region=region,
                total_ips=len(all_ips),
            )

        except Exception as e:
            logger.error(
                "inventory.scan_azure_ips_error",
                region=region,
                error=str(e),
                exc_info=True,
            )
            raise

        return all_ips

    # Helper methods for Azure Monitor metrics

    async def _get_vm_cpu_utilization(
        self, resource_group: str, vm_name: str, region: str
    ) -> float:
        """
        Get CPU utilization percentage from Azure Monitor (last 14 days average).

        Args:
            resource_group: Resource group name
            vm_name: VM name
            region: Azure region

        Returns:
            Average CPU utilization percentage (0-100)
        """
        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.monitor import MonitorManagementClient
            from datetime import datetime, timedelta

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            monitor_client = MonitorManagementClient(credential, self.subscription_id)

            # Build resource ID
            resource_id = f"/subscriptions/{self.subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.Compute/virtualMachines/{vm_name}"

            # Get metrics for last 14 days
            end_time = datetime.utcnow()
            start_time = end_time - timedelta(days=14)

            metrics_data = monitor_client.metrics.list(
                resource_id,
                timespan=f"{start_time.isoformat()}/{end_time.isoformat()}",
                interval='PT1H',  # 1 hour granularity
                metricnames='Percentage CPU',
                aggregation='Average'
            )

            # Calculate average
            total_cpu = 0.0
            count = 0
            for metric in metrics_data.value:
                for timeseries in metric.timeseries:
                    for data in timeseries.data:
                        if data.average is not None:
                            total_cpu += data.average
                            count += 1

            return total_cpu / count if count > 0 else 0.0

        except Exception as e:
            logger.warning(
                "azure.monitor.cpu_error",
                vm_name=vm_name,
                error=str(e),
            )
            return 0.0  # Default to 0 if metrics unavailable

    async def _get_vm_network_in(
        self, resource_group: str, vm_name: str, region: str
    ) -> float:
        """
        Get network in (MB) from Azure Monitor (last 14 days average).

        Args:
            resource_group: Resource group name
            vm_name: VM name
            region: Azure region

        Returns:
            Average network in MB/s
        """
        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.monitor import MonitorManagementClient
            from datetime import datetime, timedelta

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            monitor_client = MonitorManagementClient(credential, self.subscription_id)

            # Build resource ID
            resource_id = f"/subscriptions/{self.subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.Compute/virtualMachines/{vm_name}"

            # Get metrics for last 14 days
            end_time = datetime.utcnow()
            start_time = end_time - timedelta(days=14)

            metrics_data = monitor_client.metrics.list(
                resource_id,
                timespan=f"{start_time.isoformat()}/{end_time.isoformat()}",
                interval='PT1H',
                metricnames='Network In Total',
                aggregation='Average'
            )

            # Calculate average (convert from bytes to MB/s)
            total_network = 0.0
            count = 0
            for metric in metrics_data.value:
                for timeseries in metric.timeseries:
                    for data in timeseries.data:
                        if data.average is not None:
                            total_network += data.average / (1024 * 1024)  # bytes to MB
                            count += 1

            return total_network / count if count > 0 else 0.0

        except Exception as e:
            logger.warning(
                "azure.monitor.network_error",
                vm_name=vm_name,
                error=str(e),
            )
            return 0.0

    # Helper methods for cost calculation

    def _calculate_vm_monthly_cost(
        self,
        vm_size: str,
        power_state: str,
        storage_profile: Any,
        compute_client: Any,
        resource_group: str,
    ) -> float:
        """
        Calculate monthly cost for Azure VM (VM compute + disks).

        Args:
            vm_size: VM size (e.g., Standard_D2s_v3)
            power_state: VM power state
            storage_profile: VM storage profile
            compute_client: Compute management client
            resource_group: Resource group name

        Returns:
            Estimated monthly cost in USD
        """
        # Hardcoded VM pricing (simplified, can be improved with Azure Retail Prices API)
        vm_pricing = {
            "Standard_B1s": 7.59,
            "Standard_B1ms": 15.18,
            "Standard_B2s": 30.37,
            "Standard_B2ms": 60.74,
            "Standard_D2s_v3": 70.08,
            "Standard_D4s_v3": 140.16,
            "Standard_D8s_v3": 280.32,
            "Standard_D16s_v3": 560.64,
            "Standard_E2s_v3": 88.32,
            "Standard_E4s_v3": 176.64,
            "Standard_F2s_v2": 59.20,
            "Standard_F4s_v2": 118.40,
        }

        # If deallocated, compute cost is $0 (but disks still charge)
        vm_cost = 0.0 if power_state == "deallocated" else vm_pricing.get(vm_size, 70.0)

        # Add disk costs
        disk_cost = 0.0
        if storage_profile:
            # OS disk
            if storage_profile.os_disk and storage_profile.os_disk.managed_disk:
                try:
                    os_disk_id = storage_profile.os_disk.managed_disk.id
                    os_disk_name = os_disk_id.split('/')[-1]
                    os_disk_rg = os_disk_id.split('/')[4]
                    os_disk = compute_client.disks.get(os_disk_rg, os_disk_name)
                    disk_cost += self.provider._calculate_disk_cost(os_disk)
                except Exception:
                    pass

            # Data disks
            if storage_profile.data_disks:
                for data_disk in storage_profile.data_disks:
                    if data_disk.managed_disk:
                        try:
                            disk_id = data_disk.managed_disk.id
                            disk_name = disk_id.split('/')[-1]
                            disk_rg = disk_id.split('/')[4]
                            disk = compute_client.disks.get(disk_rg, disk_name)
                            disk_cost += self.provider._calculate_disk_cost(disk)
                        except Exception:
                            pass

        return vm_cost + disk_cost

    def _determine_vm_utilization_status(self, cpu_util: float, power_state: str) -> str:
        """
        Determine VM utilization status.

        Args:
            cpu_util: CPU utilization percentage
            power_state: VM power state

        Returns:
            Utilization status string
        """
        if power_state == "deallocated":
            return "idle"
        elif cpu_util < 5.0:
            return "underutilized"
        elif cpu_util < 30.0:
            return "low_usage"
        elif cpu_util < 70.0:
            return "moderate_usage"
        else:
            return "high_usage"

    # Helper methods for optimization calculation

    def _calculate_vm_optimization(
        self,
        vm: Any,
        power_state: str,
        cpu_util: float,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Calculate VM optimization score and recommendations.

        IMPORTANT: Orphan resources (deallocated VMs or very low CPU <5%) are NOT "optimizable".
        They are WASTE that should be deleted, not optimized.
        Only non-orphan resources can be optimizable (e.g., low CPU 5-30%).

        Args:
            vm: Azure VM object
            power_state: VM power state
            cpu_util: Average CPU utilization
            monthly_cost: Monthly cost

        Returns:
            Tuple of (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: Deallocated VM  This is an ORPHAN (waste), not optimizable
        # It should be deleted, not optimized. Return no optimization.
        if power_state == "deallocated":
            return False, 0, "none", 0.0, []

        # Scenario 2: Running VM with very low CPU (<5%)  This is also an ORPHAN (waste), not optimizable
        # It should be stopped/deleted, not optimized. Return no optimization.
        elif power_state == "running" and cpu_util < 5.0:
            return False, 0, "none", 0.0, []

        # Scenario 3: Running VM with low CPU (5-30%)  This IS optimizable!
        # These VMs are being used but could be downsized to save costs.
        elif power_state == "running" and cpu_util < 30.0:
            is_optimizable = True
            optimization_score = 40
            priority = "medium"
            potential_savings = monthly_cost * 0.3
            recommendations.append({
                "action": "Consider downsizing this VM",
                "details": f"CPU utilization is {cpu_util:.1f}%. You may be able to use a smaller VM size.",
                "alternatives": [
                    {"name": "Downsize to smaller SKU", "cost": monthly_cost * 0.7, "savings": monthly_cost * 0.3},
                ],
                "priority": "medium",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    def _calculate_disk_optimization(
        self,
        disk: Any,
        is_attached: bool,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Calculate disk optimization score and recommendations.

        Args:
            disk: Azure disk object
            is_attached: Whether disk is attached
            monthly_cost: Monthly cost

        Returns:
            Tuple of (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: Unattached disk (Critical waste)
        if not is_attached:
            is_optimizable = True
            optimization_score = 85
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete this unattached disk",
                "details": "Disk is not attached to any VM. Consider deleting if no longer needed.",
                "alternatives": [
                    {"name": "Delete disk", "cost": 0, "savings": monthly_cost},
                    {"name": "Create snapshot and delete", "cost": monthly_cost * 0.1, "savings": monthly_cost * 0.9},
                ],
                "priority": "critical",
            })

        # Scenario 2: Premium disk when Standard SSD might suffice
        elif is_attached and disk.sku and disk.sku.name.startswith("Premium"):
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = monthly_cost * 0.4
            recommendations.append({
                "action": "Consider downgrading to Standard SSD",
                "details": "Using Premium SSD. Evaluate if Standard SSD performance is sufficient.",
                "alternatives": [
                    {"name": "Downgrade to Standard SSD", "cost": monthly_cost * 0.6, "savings": monthly_cost * 0.4},
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    def _calculate_ip_optimization(
        self,
        ip: Any,
        is_assigned: bool,
        sku_name: str,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Calculate public IP optimization score and recommendations.

        Args:
            ip: Azure public IP object
            is_assigned: Whether IP is assigned
            sku_name: SKU name (Basic or Standard)
            monthly_cost: Monthly cost

        Returns:
            Tuple of (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: Unassigned IP (Critical waste)
        if not is_assigned:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Release this unassigned public IP",
                "details": "Public IP is not assigned to any resource.",
                "alternatives": [
                    {"name": "Release IP", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "critical",
            })

        # Scenario 2: Standard SKU when Basic might suffice
        elif is_assigned and sku_name == "Standard":
            is_optimizable = True
            optimization_score = 20
            priority = "low"
            potential_savings = 0.35  # Difference between Standard and Basic
            recommendations.append({
                "action": "Consider using Basic SKU public IP",
                "details": "Using Standard SKU. Evaluate if Basic SKU is sufficient for your needs.",
                "alternatives": [
                    {"name": "Downgrade to Basic SKU", "cost": 3.65, "savings": 0.35},
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_load_balancers(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Load Balancers for cost intelligence.

        Args:
            region: Azure region to scan

        Returns:
            List of all load balancer resources
        """
        logger.info("inventory.scan_azure_lbs_start", region=region)
        all_lbs: list[AllCloudResourceData] = []

        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.network import NetworkManagementClient

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            network_client = NetworkManagementClient(credential, self.subscription_id)

            # Get ALL load balancers
            load_balancers = list(network_client.load_balancers.list_all())

            for lb in load_balancers:
                # Filter by region
                if lb.location != region:
                    continue

                # Filter by resource group
                if not self.provider._is_resource_in_scope(lb.id):
                    continue

                # Extract resource group
                resource_group = lb.id.split('/')[4]

                # Determine if load balancer is being used
                has_backend_pools = lb.backend_address_pools and len(lb.backend_address_pools) > 0
                has_probes = lb.probes and len(lb.probes) > 0
                has_rules = lb.load_balancing_rules and len(lb.load_balancing_rules) > 0

                # Extract tags
                tags = lb.tags if lb.tags else {}

                # Calculate monthly cost
                sku_name = lb.sku.name if lb.sku else "Basic"
                monthly_cost = 25.55 if sku_name == "Standard" else 18.25

                # Determine utilization status
                if not has_backend_pools and not has_probes:
                    utilization_status = "idle"
                elif not has_rules:
                    utilization_status = "underutilized"
                else:
                    utilization_status = "active"

                # Calculate optimization
                (
                    is_optimizable,
                    optimization_score,
                    optimization_priority,
                    potential_savings,
                    recommendations,
                ) = self._calculate_lb_optimization(
                    lb,
                    has_backend_pools,
                    has_probes,
                    has_rules,
                    sku_name,
                    monthly_cost,
                )

                # Check if LB is orphan (no backend pools)
                is_orphan = not has_backend_pools

                # Create resource data
                resource = AllCloudResourceData(
                    resource_type="azure_load_balancer",
                    resource_id=lb.id,
                    resource_name=lb.name,
                    region=region,
                    estimated_monthly_cost=monthly_cost,
                    resource_metadata={
                        "sku_name": sku_name,
                        "resource_group": resource_group,
                        "backend_pool_count": len(lb.backend_address_pools) if lb.backend_address_pools else 0,
                        "probe_count": len(lb.probes) if lb.probes else 0,
                        "rule_count": len(lb.load_balancing_rules) if lb.load_balancing_rules else 0,
                        "frontend_ip_count": len(lb.frontend_ip_configurations) if lb.frontend_ip_configurations else 0,
                    },
                    currency="USD",
                    utilization_status=utilization_status,
                    cpu_utilization_percent=None,
                    memory_utilization_percent=None,
                    storage_utilization_percent=None,
                    network_utilization_mbps=None,
                    is_optimizable=is_optimizable,
                    optimization_priority=optimization_priority,
                    optimization_score=optimization_score,
                    potential_monthly_savings=potential_savings,
                    optimization_recommendations=recommendations,
                    tags=tags,
                    resource_status="active",
                    is_orphan=is_orphan,
                    created_at_cloud=None,
                    last_used_at=None,
                )

                all_lbs.append(resource)

            logger.info(
                "inventory.scan_azure_lbs_complete",
                region=region,
                total_lbs=len(all_lbs),
            )

        except Exception as e:
            logger.error(
                "inventory.scan_azure_lbs_error",
                region=region,
                error=str(e),
                exc_info=True,
            )
            raise

        return all_lbs

    def _calculate_lb_optimization(
        self,
        lb: Any,
        has_backend_pools: bool,
        has_probes: bool,
        has_rules: bool,
        sku_name: str,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Calculate load balancer optimization score and recommendations.

        Args:
            lb: Azure load balancer object
            has_backend_pools: Whether LB has backend pools
            has_probes: Whether LB has health probes
            has_rules: Whether LB has load balancing rules
            sku_name: SKU name (Basic or Standard)
            monthly_cost: Monthly cost

        Returns:
            Tuple of (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: No backend pools (Critical waste)
        if not has_backend_pools:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete unused load balancer",
                "details": "Load balancer has no backend pools configured. Not serving any traffic.",
                "alternatives": [
                    {"name": "Delete load balancer", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "critical",
            })

        # Scenario 2: No health probes (High priority)
        elif not has_probes:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete load balancer without health probes",
                "details": "Load balancer has backend pools but no health probes. Likely misconfigured or unused.",
                "alternatives": [
                    {"name": "Delete load balancer", "cost": 0, "savings": monthly_cost},
                    {"name": "Configure health probes", "cost": monthly_cost, "savings": 0},
                ],
                "priority": "high",
            })

        # Scenario 3: Standard SKU with low traffic
        elif sku_name == "Standard" and has_backend_pools and has_rules:
            is_optimizable = True
            optimization_score = 40
            priority = "medium"
            potential_savings = 7.30  # Difference between Standard and Basic
            recommendations.append({
                "action": "Consider downgrading to Basic SKU",
                "details": "Using Standard SKU. Evaluate if Basic SKU features are sufficient.",
                "alternatives": [
                    {"name": "Downgrade to Basic SKU", "cost": 18.25, "savings": 7.30},
                ],
                "priority": "medium",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_app_gateways(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Application Gateways (WAF v2) for cost intelligence.

        Detection criteria:
        - Stopped/Deallocated instances (CRITICAL - 90 score)
        - No backend pools configured (CRITICAL - 80 score)
        - Oversized tier (Large when Medium/Small sufficient) (HIGH - 60 score)
        - WAF enabled but no custom rules (MEDIUM - 40 score)

        Args:
            region: Azure region to scan (e.g., 'eastus')

        Returns:
            List of all Application Gateways with optimization recommendations
        """
        logger.info("inventory.scan_azure_app_gateways_start", region=region)
        all_app_gateways: list[AllCloudResourceData] = []

        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.network import NetworkManagementClient

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            network_client = NetworkManagementClient(credential, self.subscription_id)
            app_gateways = list(network_client.application_gateways.list_all())

            logger.info(
                "inventory.azure_app_gateways_fetched",
                region=region,
                total_app_gateways=len(app_gateways)
            )

            for ag in app_gateways:
                # Filter by region
                if ag.location != region:
                    continue

                # Filter by resource group scope
                if not self.provider._is_resource_in_scope(ag.id):
                    continue

                # Detect usage patterns
                is_running = ag.operational_state and ag.operational_state.lower() == "running"
                has_backend_pools = ag.backend_address_pools and len(ag.backend_address_pools) > 0
                has_http_listeners = ag.http_listeners and len(ag.http_listeners) > 0
                has_request_routing_rules = ag.request_routing_rules and len(ag.request_routing_rules) > 0

                # WAF analysis
                has_waf = ag.web_application_firewall_configuration is not None
                waf_rule_count = 0
                if has_waf and ag.web_application_firewall_configuration.rule_set_type:
                    waf_rule_count = len(ag.web_application_firewall_configuration.disabled_rule_groups or [])

                # SKU analysis (tier + size)
                sku_tier = ag.sku.tier if ag.sku else "Standard_v2"
                sku_size = ag.sku.name if ag.sku else "Standard_Medium"

                # Pricing calculation (Azure Application Gateway v2 pricing - US East 2025)
                # SKU cost (per gateway per month)
                sku_cost_map = {
                    "Standard_Small": 142.00,    # Small (2 CU baseline)
                    "Standard_Medium": 372.00,   # Medium (10 CU baseline)
                    "Standard_Large": 745.00,    # Large (50 CU baseline)
                    "WAF_Small": 160.00,         # WAF Small (2 CU baseline + WAF cost)
                    "WAF_Medium": 390.00,        # WAF Medium (10 CU baseline + WAF cost)
                    "WAF_Large": 763.00,         # WAF Large (50 CU baseline + WAF cost)
                }

                monthly_cost = sku_cost_map.get(sku_size, 372.00)  # Default to Medium

                # Capacity units (additional cost beyond baseline)
                capacity = ag.sku.capacity if ag.sku and ag.sku.capacity else 2
                if capacity > 2:  # Beyond baseline 2 CU
                    monthly_cost += (capacity - 2) * 8.76  # $8.76 per CU per month

                # Calculate optimization
                (is_optimizable, optimization_score, optimization_priority,
                 potential_savings, recommendations) = self._calculate_ag_optimization(
                    ag, is_running, has_backend_pools, has_http_listeners,
                    has_request_routing_rules, has_waf, waf_rule_count,
                    sku_tier, sku_size, monthly_cost
                )

                # Build resource metadata
                resource_metadata = {
                    "sku_tier": sku_tier,
                    "sku_size": sku_size,
                    "capacity": capacity,
                    "operational_state": ag.operational_state,
                    "backend_pools_count": len(ag.backend_address_pools) if ag.backend_address_pools else 0,
                    "http_listeners_count": len(ag.http_listeners) if ag.http_listeners else 0,
                    "routing_rules_count": len(ag.request_routing_rules) if ag.request_routing_rules else 0,
                    "waf_enabled": has_waf,
                    "waf_rule_count": waf_rule_count,
                }

                resource = AllCloudResourceData(
                    resource_type="azure_app_gateway",
                    resource_id=ag.id,
                    resource_name=ag.name,
                    region=region,
                    estimated_monthly_cost=round(monthly_cost, 2),
                    currency="USD",
                    is_optimizable=is_optimizable,
                    optimization_priority=optimization_priority,
                    optimization_score=optimization_score,
                    potential_monthly_savings=round(potential_savings, 2),
                    optimization_recommendations=recommendations,
                    resource_metadata=resource_metadata,
                    created_at_cloud=ag.etag,  # Use etag as proxy for creation time
                    last_used_at=None,  # No last-used timestamp available
                    status="active",
                )

                all_app_gateways.append(resource)

            logger.info(
                "inventory.azure_app_gateways_scanned",
                region=region,
                total_scanned=len(all_app_gateways),
                optimizable=sum(1 for r in all_app_gateways if r.is_optimizable)
            )

        except ImportError:
            logger.error(
                "inventory.azure_sdk_missing",
                region=region,
                sdk="azure-mgmt-network",
                message="Install azure-mgmt-network to scan Application Gateways"
            )
        except Exception as e:
            logger.exception(
                "inventory.azure_app_gateways_scan_failed",
                region=region,
                error=str(e)
            )

        return all_app_gateways

    def _calculate_ag_optimization(
        self,
        ag,
        is_running: bool,
        has_backend_pools: bool,
        has_http_listeners: bool,
        has_request_routing_rules: bool,
        has_waf: bool,
        waf_rule_count: int,
        sku_tier: str,
        sku_size: str,
        monthly_cost: float
    ) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate Application Gateway optimization opportunities.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: Stopped or Deallocated (CRITICAL - 90 score)
        if not is_running:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete stopped Application Gateway",
                "details": f"Application Gateway is in '{ag.operational_state}' state and not serving traffic. You're still paying ${monthly_cost}/month.",
                "alternatives": [
                    {"name": "Delete Application Gateway", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "critical",
            })

        # Scenario 2: No backend pools (CRITICAL - 80 score)
        elif not has_backend_pools:
            is_optimizable = True
            optimization_score = 80
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete Application Gateway without backend pools",
                "details": "Application Gateway has no backend pools configured. Not routing any traffic.",
                "alternatives": [
                    {"name": "Delete Application Gateway", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "critical",
            })

        # Scenario 3: No HTTP listeners or routing rules (HIGH - 70 score)
        elif not has_http_listeners or not has_request_routing_rules:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete Application Gateway with no active routing",
                "details": "Application Gateway has backend pools but no HTTP listeners or routing rules. Not accepting traffic.",
                "alternatives": [
                    {"name": "Delete Application Gateway", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "high",
            })

        # Scenario 4: Oversized SKU (HIGH - 60 score)
        elif sku_size in ["Standard_Large", "WAF_Large"] and has_backend_pools:
            is_optimizable = True
            optimization_score = 60
            priority = "high"
            # Savings: Large  Medium (50% reduction)
            medium_cost = 372.00 if "WAF" not in sku_size else 390.00
            potential_savings = monthly_cost - medium_cost
            recommendations.append({
                "action": "Downsize Application Gateway from Large to Medium SKU",
                "details": f"Using Large SKU (${monthly_cost}/mo). Evaluate if Medium SKU is sufficient (${medium_cost}/mo).",
                "alternatives": [
                    {"name": "Downgrade to Medium SKU", "cost": medium_cost, "savings": potential_savings},
                ],
                "priority": "high",
            })

        # Scenario 5: WAF enabled but no custom rules (MEDIUM - 40 score)
        elif has_waf and waf_rule_count == 0 and "WAF" in sku_size:
            is_optimizable = True
            optimization_score = 40
            priority = "medium"
            # Savings: WAF  Standard (WAF adds ~$18/mo overhead)
            standard_cost = monthly_cost - 18.00
            potential_savings = 18.00
            recommendations.append({
                "action": "Consider disabling WAF or switch to Standard SKU",
                "details": f"WAF is enabled but no custom rules configured. Paying extra ${potential_savings}/month for unused feature.",
                "alternatives": [
                    {"name": "Switch to Standard SKU", "cost": standard_cost, "savings": potential_savings},
                ],
                "priority": "medium",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_storage_accounts(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Storage Accounts for cost intelligence.

        Detection criteria:
        - Zero storage used (CRITICAL - 90 score)
        - Premium tier underutilized (<100GB) (HIGH - 70 score)
        - Hot tier with cold access patterns (MEDIUM - 50 score)
        - GRS replication without geo-redundancy need (LOW - 30 score)

        Args:
            region: Azure region to scan (e.g., 'eastus')

        Returns:
            List of all Storage Accounts with optimization recommendations
        """
        logger.info("inventory.scan_azure_storage_accounts_start", region=region)
        all_storage_accounts: list[AllCloudResourceData] = []

        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.storage import StorageManagementClient

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            storage_client = StorageManagementClient(credential, self.subscription_id)
            storage_accounts = list(storage_client.storage_accounts.list())

            logger.info(
                "inventory.azure_storage_accounts_fetched",
                region=region,
                total_storage_accounts=len(storage_accounts)
            )

            for sa in storage_accounts:
                # Filter by region
                if sa.location != region:
                    continue

                # Filter by resource group scope
                if not self.provider._is_resource_in_scope(sa.id):
                    continue

                # Get account properties
                account_kind = sa.kind if sa.kind else "StorageV2"
                sku_name = sa.sku.name if sa.sku else "Standard_LRS"
                sku_tier = sa.sku.tier if sa.sku else "Standard"
                access_tier = sa.access_tier if sa.access_tier else "Hot"

                # Get usage metrics (requires additional API call)
                usage_gb = 0.0
                try:
                    # Get resource group from storage account ID
                    resource_group = sa.id.split("/")[4]
                    usage_metrics = storage_client.storage_accounts.list_account_sas(
                        resource_group_name=resource_group,
                        account_name=sa.name
                    )
                    # Note: Actual usage requires Azure Monitor API, hardcoded for now
                    usage_gb = 50.0  # Placeholder - would need Monitor API
                except Exception:
                    usage_gb = 50.0  # Default assumption

                # Pricing calculation (Azure Storage pricing - US East 2025)
                # Base cost per GB per month
                pricing_map = {
                    # Standard tier
                    "Standard_LRS": 0.0184,      # Locally redundant
                    "Standard_GRS": 0.0368,      # Geo-redundant
                    "Standard_RAGRS": 0.046,     # Read-access geo-redundant
                    "Standard_ZRS": 0.0225,      # Zone-redundant
                    # Premium tier
                    "Premium_LRS": 0.15,         # Premium locally redundant
                    "Premium_ZRS": 0.1875,       # Premium zone-redundant
                }

                price_per_gb = pricing_map.get(sku_name, 0.0184)
                storage_cost = usage_gb * price_per_gb

                # Additional costs (transactions, bandwidth) - estimate 20% overhead
                monthly_cost = storage_cost * 1.2

                # Calculate optimization
                (is_optimizable, optimization_score, optimization_priority,
                 potential_savings, recommendations) = self._calculate_sa_optimization(
                    sa, usage_gb, sku_name, sku_tier, access_tier,
                    account_kind, monthly_cost, price_per_gb
                )

                # Build resource metadata
                resource_metadata = {
                    "sku_name": sku_name,
                    "sku_tier": sku_tier,
                    "account_kind": account_kind,
                    "access_tier": access_tier,
                    "usage_gb": round(usage_gb, 2),
                    "price_per_gb": price_per_gb,
                    "provisioning_state": sa.provisioning_state,
                }

                resource = AllCloudResourceData(
                    resource_type="azure_storage_account",
                    resource_id=sa.id,
                    resource_name=sa.name,
                    region=region,
                    estimated_monthly_cost=round(monthly_cost, 2),
                    currency="USD",
                    is_optimizable=is_optimizable,
                    optimization_priority=optimization_priority,
                    optimization_score=optimization_score,
                    potential_monthly_savings=round(potential_savings, 2),
                    optimization_recommendations=recommendations,
                    resource_metadata=resource_metadata,
                    created_at_cloud=sa.creation_time,
                    last_used_at=None,  # No last-used timestamp available
                    status="active",
                )

                all_storage_accounts.append(resource)

            logger.info(
                "inventory.azure_storage_accounts_scanned",
                region=region,
                total_scanned=len(all_storage_accounts),
                optimizable=sum(1 for r in all_storage_accounts if r.is_optimizable)
            )

        except ImportError:
            logger.error(
                "inventory.azure_sdk_missing",
                region=region,
                sdk="azure-mgmt-storage",
                message="Install azure-mgmt-storage to scan Storage Accounts"
            )
        except Exception as e:
            logger.exception(
                "inventory.azure_storage_accounts_scan_failed",
                region=region,
                error=str(e)
            )

        return all_storage_accounts

    def _calculate_sa_optimization(
        self,
        sa,
        usage_gb: float,
        sku_name: str,
        sku_tier: str,
        access_tier: str,
        account_kind: str,
        monthly_cost: float,
        price_per_gb: float
    ) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate Storage Account optimization opportunities.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: Zero storage used (CRITICAL - 90 score)
        if usage_gb < 0.1:  # Less than 100 MB
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete empty Storage Account",
                "details": f"Storage Account has no data stored (<100 MB). You're paying ${monthly_cost}/month for unused storage.",
                "alternatives": [
                    {"name": "Delete Storage Account", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "critical",
            })

        # Scenario 2: Premium tier underutilized (HIGH - 70 score)
        elif sku_tier == "Premium" and usage_gb < 100:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Savings: Premium  Standard (80% reduction in per-GB cost)
            standard_cost = usage_gb * 0.0184 * 1.2
            potential_savings = monthly_cost - standard_cost
            recommendations.append({
                "action": "Downgrade from Premium to Standard tier",
                "details": f"Using Premium tier (${price_per_gb}/GB) but only {usage_gb}GB stored. Standard tier (${0.0184}/GB) would save ${potential_savings}/month.",
                "alternatives": [
                    {"name": "Downgrade to Standard LRS", "cost": round(standard_cost, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "high",
            })

        # Scenario 3: Hot tier with cold access (MEDIUM - 50 score)
        # Note: Would need actual access metrics from Monitor API
        elif access_tier == "Hot" and usage_gb > 100:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Savings: Hot  Cool (30% reduction in storage cost)
            cool_cost = usage_gb * 0.01 * 1.2
            potential_savings = monthly_cost - cool_cost
            recommendations.append({
                "action": "Consider switching from Hot to Cool access tier",
                "details": f"Using Hot tier for {usage_gb}GB. If access is infrequent, Cool tier could save ${potential_savings}/month.",
                "alternatives": [
                    {"name": "Switch to Cool tier", "cost": round(cool_cost, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "medium",
            })

        # Scenario 4: GRS replication without geo-redundancy need (LOW - 30 score)
        elif "GRS" in sku_name or "RAGRS" in sku_name:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            # Savings: GRS  LRS (50% reduction)
            lrs_cost = usage_gb * 0.0184 * 1.2
            potential_savings = monthly_cost - lrs_cost
            recommendations.append({
                "action": "Evaluate if geo-redundancy is necessary",
                "details": f"Using {sku_name} replication. If geo-redundancy isn't required, switch to LRS to save ${potential_savings}/month.",
                "alternatives": [
                    {"name": "Downgrade to LRS", "cost": round(lrs_cost, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_expressroute_circuits(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure ExpressRoute Circuits for cost intelligence.

        Detection criteria:
        - Not provisioned (CRITICAL - 90 score)
        - No peerings configured (HIGH - 75 score)
        - Premium tier underutilized (MEDIUM - 50 score)
        - Metered data plan with low usage (LOW - 30 score)

        Args:
            region: Azure region to scan (e.g., 'eastus')

        Returns:
            List of all ExpressRoute Circuits with optimization recommendations
        """
        logger.info("inventory.scan_azure_expressroute_start", region=region)
        all_expressroute: list[AllCloudResourceData] = []

        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.network import NetworkManagementClient

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            network_client = NetworkManagementClient(credential, self.subscription_id)
            expressroute_circuits = list(network_client.express_route_circuits.list_all())

            logger.info(
                "inventory.azure_expressroute_fetched",
                region=region,
                total_expressroute=len(expressroute_circuits)
            )

            for er in expressroute_circuits:
                # Filter by region
                if er.location != region:
                    continue

                # Filter by resource group scope
                if not self.provider._is_resource_in_scope(er.id):
                    continue

                # Get circuit properties
                sku_tier = er.sku.tier if er.sku else "Standard"
                sku_family = er.sku.family if er.sku else "MeteredData"
                bandwidth_mbps = er.service_provider_properties.bandwidth_in_mbps if er.service_provider_properties else 50

                # Circuit state
                provisioning_state = er.provisioning_state if er.provisioning_state else "Unknown"
                circuit_provisioning_state = er.circuit_provisioning_state if er.circuit_provisioning_state else "Disabled"

                # Peerings analysis
                has_peerings = er.peerings and len(er.peerings) > 0
                peering_count = len(er.peerings) if er.peerings else 0

                # Pricing calculation (Azure ExpressRoute pricing - US East 2025)
                # Port fee per month
                port_fees = {
                    50: 55.00,     # 50 Mbps
                    100: 120.00,   # 100 Mbps
                    200: 230.00,   # 200 Mbps
                    500: 560.00,   # 500 Mbps
                    1000: 1100.00, # 1 Gbps
                    2000: 2200.00, # 2 Gbps
                    5000: 5500.00, # 5 Gbps
                    10000: 11000.00, # 10 Gbps
                }

                # Find closest bandwidth tier
                port_cost = port_fees.get(bandwidth_mbps, 560.00)  # Default to 500 Mbps
                for tier_mbps, cost in sorted(port_fees.items()):
                    if bandwidth_mbps <= tier_mbps:
                        port_cost = cost
                        break

                # Premium add-on (if Premium tier)
                premium_cost = 1150.00 if sku_tier == "Premium" else 0.0

                # Metered data (if MeteredData family) - estimate $0.025/GB outbound
                # Assume 10% bandwidth utilization for cost estimation
                estimated_gb_outbound = (bandwidth_mbps / 8) * 0.1 * 730 * 3600 / 1024  # GB/month
                metered_cost = estimated_gb_outbound * 0.025 if sku_family == "MeteredData" else 0.0

                monthly_cost = port_cost + premium_cost + metered_cost

                # Calculate optimization
                (is_optimizable, optimization_score, optimization_priority,
                 potential_savings, recommendations) = self._calculate_er_optimization(
                    er, provisioning_state, circuit_provisioning_state, has_peerings,
                    peering_count, sku_tier, sku_family, bandwidth_mbps,
                    monthly_cost, port_cost, premium_cost
                )

                # Build resource metadata
                resource_metadata = {
                    "sku_tier": sku_tier,
                    "sku_family": sku_family,
                    "bandwidth_mbps": bandwidth_mbps,
                    "provisioning_state": provisioning_state,
                    "circuit_provisioning_state": circuit_provisioning_state,
                    "peering_count": peering_count,
                    "service_provider": er.service_provider_properties.service_provider_name if er.service_provider_properties else None,
                }

                resource = AllCloudResourceData(
                    resource_type="azure_expressroute_circuit",
                    resource_id=er.id,
                    resource_name=er.name,
                    region=region,
                    estimated_monthly_cost=round(monthly_cost, 2),
                    currency="USD",
                    is_optimizable=is_optimizable,
                    optimization_priority=optimization_priority,
                    optimization_score=optimization_score,
                    potential_monthly_savings=round(potential_savings, 2),
                    optimization_recommendations=recommendations,
                    resource_metadata=resource_metadata,
                    created_at_cloud=er.etag,  # Use etag as proxy
                    last_used_at=None,  # No last-used timestamp available
                    status="active",
                )

                all_expressroute.append(resource)

            logger.info(
                "inventory.azure_expressroute_scanned",
                region=region,
                total_scanned=len(all_expressroute),
                optimizable=sum(1 for r in all_expressroute if r.is_optimizable)
            )

        except ImportError:
            logger.error(
                "inventory.azure_sdk_missing",
                region=region,
                sdk="azure-mgmt-network",
                message="Install azure-mgmt-network to scan ExpressRoute Circuits"
            )
        except Exception as e:
            logger.exception(
                "inventory.azure_expressroute_scan_failed",
                region=region,
                error=str(e)
            )

        return all_expressroute

    def _calculate_er_optimization(
        self,
        er,
        provisioning_state: str,
        circuit_provisioning_state: str,
        has_peerings: bool,
        peering_count: int,
        sku_tier: str,
        sku_family: str,
        bandwidth_mbps: int,
        monthly_cost: float,
        port_cost: float,
        premium_cost: float
    ) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate ExpressRoute Circuit optimization opportunities.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: Not provisioned (CRITICAL - 90 score)
        if circuit_provisioning_state in ["Disabled", "NotProvisioned"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete unprovisioned ExpressRoute Circuit",
                "details": f"Circuit is '{circuit_provisioning_state}' and not serving traffic. You're paying ${monthly_cost}/month for unused circuit.",
                "alternatives": [
                    {"name": "Delete ExpressRoute Circuit", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "critical",
            })

        # Scenario 2: No peerings configured (HIGH - 75 score)
        elif not has_peerings or peering_count == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete ExpressRoute Circuit without peerings",
                "details": f"Circuit has no peerings configured. Not routing any traffic. Paying ${monthly_cost}/month.",
                "alternatives": [
                    {"name": "Delete ExpressRoute Circuit", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "high",
            })

        # Scenario 3: Premium tier underutilized (MEDIUM - 50 score)
        elif sku_tier == "Premium" and peering_count < 2:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Savings: Premium  Standard ($1150/mo savings)
            standard_cost = monthly_cost - premium_cost
            potential_savings = premium_cost
            recommendations.append({
                "action": "Downgrade from Premium to Standard tier",
                "details": f"Using Premium tier (${premium_cost}/mo add-on) but only {peering_count} peering(s) configured. Standard tier may be sufficient.",
                "alternatives": [
                    {"name": "Downgrade to Standard tier", "cost": round(standard_cost, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "medium",
            })

        # Scenario 4: Oversized bandwidth (MEDIUM - 40 score)
        elif bandwidth_mbps >= 1000 and has_peerings:
            is_optimizable = True
            optimization_score = 40
            priority = "medium"
            # Savings: Downsize bandwidth (estimate 50% reduction)
            lower_tier_cost = port_cost * 0.5
            potential_savings = port_cost - lower_tier_cost
            recommendations.append({
                "action": "Evaluate if lower bandwidth tier is sufficient",
                "details": f"Using {bandwidth_mbps} Mbps circuit (${port_cost}/mo). Review actual usage to see if downsizing is possible.",
                "alternatives": [
                    {"name": "Downsize to lower bandwidth", "cost": round(lower_tier_cost, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "medium",
            })

        # Scenario 5: Metered data with low usage (LOW - 30 score)
        elif sku_family == "MeteredData" and bandwidth_mbps <= 200:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            # Savings: Switch to UnlimitedData (may save on overage charges)
            # Note: UnlimitedData is typically more cost-effective for high usage
            potential_savings = monthly_cost * 0.1  # Estimate 10% savings
            recommendations.append({
                "action": "Evaluate UnlimitedData plan",
                "details": f"Using MeteredData plan on {bandwidth_mbps} Mbps circuit. If outbound data is high, UnlimitedData may be more cost-effective.",
                "alternatives": [
                    {"name": "Switch to UnlimitedData plan", "cost": round(monthly_cost * 0.9, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_disk_snapshots(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Disk Snapshots for cost intelligence.

        Detection criteria:
        - Snapshot trs ancien (>365 jours) (CRITICAL - 90 score)
        - Snapshot orphelin (disque source supprim) (HIGH - 75 score)
        - Snapshots multiples du mme disque (>10) (MEDIUM - 50 score)
        - Snapshot non incrmentiel (LOW - 30 score)

        Args:
            region: Azure region to scan (e.g., 'eastus')

        Returns:
            List of all Disk Snapshots with optimization recommendations
        """
        logger.info("inventory.scan_azure_snapshots_start", region=region)
        all_snapshots: list[AllCloudResourceData] = []

        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.compute import ComputeManagementClient
            from datetime import datetime, timezone

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            compute_client = ComputeManagementClient(credential, self.subscription_id)
            snapshots = list(compute_client.snapshots.list())

            logger.info(
                "inventory.azure_snapshots_fetched",
                region=region,
                total_snapshots=len(snapshots)
            )

            # Get all disks to check for orphaned snapshots
            all_disks = list(compute_client.disks.list())
            disk_ids = {disk.id for disk in all_disks}

            # Group snapshots by source disk
            snapshots_by_disk: dict[str, list] = {}
            for snapshot in snapshots:
                source_id = snapshot.creation_data.source_resource_id if snapshot.creation_data else None
                if source_id:
                    if source_id not in snapshots_by_disk:
                        snapshots_by_disk[source_id] = []
                    snapshots_by_disk[source_id].append(snapshot)

            for snap in snapshots:
                # Filter by region
                if snap.location != region:
                    continue

                # Filter by resource group scope
                if not self.provider._is_resource_in_scope(snap.id):
                    continue

                # Snapshot properties
                sku_name = snap.sku.name if snap.sku else "Standard_LRS"
                disk_size_gb = snap.disk_size_gb if snap.disk_size_gb else 128
                incremental = snap.incremental if hasattr(snap, 'incremental') else False

                # Calculate age
                time_created = snap.time_created if snap.time_created else datetime.now(timezone.utc)
                age_days = (datetime.now(timezone.utc) - time_created).days

                # Check if orphaned (source disk deleted)
                source_disk_id = snap.creation_data.source_resource_id if snap.creation_data else None
                is_orphaned = source_disk_id and source_disk_id not in disk_ids

                # Count snapshots from same source disk
                snapshot_count_for_disk = len(snapshots_by_disk.get(source_disk_id, [])) if source_disk_id else 1

                # Pricing calculation (Azure Snapshot pricing - US East 2025)
                # Price per GB per month
                pricing_map = {
                    "Standard_LRS": 0.05,      # Standard HDD snapshots
                    "Premium_LRS": 0.10,       # Premium SSD snapshots
                    "StandardSSD_LRS": 0.065,  # Standard SSD snapshots
                }

                price_per_gb = pricing_map.get(sku_name, 0.05)
                monthly_cost = disk_size_gb * price_per_gb

                # Calculate optimization
                (is_optimizable, optimization_score, optimization_priority,
                 potential_savings, recommendations) = self._calculate_snapshot_optimization(
                    snap, age_days, disk_size_gb, is_orphaned,
                    snapshot_count_for_disk, incremental, monthly_cost
                )

                # Build resource metadata
                resource_metadata = {
                    "sku_name": sku_name,
                    "disk_size_gb": disk_size_gb,
                    "incremental": incremental,
                    "age_days": age_days,
                    "is_orphaned": is_orphaned,
                    "snapshot_count_for_disk": snapshot_count_for_disk,
                    "source_disk_id": source_disk_id,
                    "provisioning_state": snap.provisioning_state,
                }

                resource = AllCloudResourceData(
                    resource_type="azure_disk_snapshot",
                    resource_id=snap.id,
                    resource_name=snap.name,
                    region=region,
                    estimated_monthly_cost=round(monthly_cost, 2),
                    currency="USD",
                    is_optimizable=is_optimizable,
                    optimization_priority=optimization_priority,
                    optimization_score=optimization_score,
                    potential_monthly_savings=round(potential_savings, 2),
                    optimization_recommendations=recommendations,
                    resource_metadata=resource_metadata,
                    created_at_cloud=time_created,
                    last_used_at=None,  # No last-used timestamp available
                    status="active",
                )

                all_snapshots.append(resource)

            logger.info(
                "inventory.azure_snapshots_scanned",
                region=region,
                total_scanned=len(all_snapshots),
                optimizable=sum(1 for r in all_snapshots if r.is_optimizable)
            )

        except ImportError:
            logger.error(
                "inventory.azure_sdk_missing",
                region=region,
                sdk="azure-mgmt-compute",
                message="Install azure-mgmt-compute to scan Disk Snapshots"
            )
        except Exception as e:
            logger.exception(
                "inventory.azure_snapshots_scan_failed",
                region=region,
                error=str(e)
            )

        return all_snapshots

    def _calculate_snapshot_optimization(
        self,
        snap,
        age_days: int,
        disk_size_gb: int,
        is_orphaned: bool,
        snapshot_count_for_disk: int,
        incremental: bool,
        monthly_cost: float
    ) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate Disk Snapshot optimization opportunities.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: Snapshot trs ancien (>365 jours) (CRITICAL - 90 score)
        if age_days > 365:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete old snapshot (>1 year old)",
                "details": f"Snapshot is {age_days} days old ({disk_size_gb}GB). Consider deleting if no longer needed. Saving ${monthly_cost}/month.",
                "alternatives": [
                    {"name": "Delete snapshot", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "critical",
            })

        # Scenario 2: Snapshot orphelin (disque source supprim) (HIGH - 75 score)
        elif is_orphaned:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete orphaned snapshot (source disk deleted)",
                "details": f"Source disk no longer exists. Snapshot is orphaned ({disk_size_gb}GB). Delete to save ${monthly_cost}/month.",
                "alternatives": [
                    {"name": "Delete orphaned snapshot", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "high",
            })

        # Scenario 3: Snapshots multiples du mme disque (>10) (MEDIUM - 50 score)
        elif snapshot_count_for_disk > 10:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Estimate savings: delete oldest 50% of snapshots
            potential_savings = monthly_cost * 0.5
            recommendations.append({
                "action": f"Reduce number of snapshots ({snapshot_count_for_disk} snapshots)",
                "details": f"Source disk has {snapshot_count_for_disk} snapshots. Consider retention policy to delete old snapshots. Save ~${potential_savings}/month.",
                "alternatives": [
                    {"name": "Delete oldest 50% of snapshots", "cost": round(monthly_cost * 0.5, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "medium",
            })

        # Scenario 4: Snapshot non incrmentiel (LOW - 30 score)
        elif not incremental and disk_size_gb > 128:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            # Savings: incremental snapshots are ~80% cheaper
            potential_savings = monthly_cost * 0.8
            recommendations.append({
                "action": "Switch to incremental snapshots",
                "details": f"Using full snapshot ({disk_size_gb}GB). Incremental snapshots could save ~${potential_savings}/month (80% reduction).",
                "alternatives": [
                    {"name": "Use incremental snapshots", "cost": round(monthly_cost * 0.2, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_nat_gateways(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure NAT Gateways for cost intelligence.

        Detection criteria:
        - Pas de subnet attach (CRITICAL - 90 score)
        - Pas d'IP publiques configures (HIGH - 75 score)
        - Trs faible utilisation (<100GB/mois) (MEDIUM - 50 score)
        - Multiple NAT Gateways dans mme VNet (LOW - 30 score)

        Args:
            region: Azure region to scan (e.g., 'eastus')

        Returns:
            List of all NAT Gateways with optimization recommendations
        """
        logger.info("inventory.scan_azure_nat_gateways_start", region=region)
        all_nat_gateways: list[AllCloudResourceData] = []

        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.network import NetworkManagementClient

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            network_client = NetworkManagementClient(credential, self.subscription_id)
            nat_gateways = list(network_client.nat_gateways.list_all())

            logger.info(
                "inventory.azure_nat_gateways_fetched",
                region=region,
                total_nat_gateways=len(nat_gateways)
            )

            # Get all VNets to count NAT Gateways per VNet
            vnets = list(network_client.virtual_networks.list_all())
            nat_per_vnet: dict[str, int] = {}

            for nat in nat_gateways:
                # Filter by region
                if nat.location != region:
                    continue

                # Filter by resource group scope
                if not self.provider._is_resource_in_scope(nat.id):
                    continue

                # NAT Gateway properties
                has_subnets = nat.subnets and len(nat.subnets) > 0
                subnet_count = len(nat.subnets) if nat.subnets else 0

                has_public_ips = nat.public_ip_addresses and len(nat.public_ip_addresses) > 0
                public_ip_count = len(nat.public_ip_addresses) if nat.public_ip_addresses else 0

                # Estimate outbound data usage (requires Azure Monitor - hardcoded for now)
                estimated_gb_outbound = 500.0  # Placeholder - would need Monitor API

                # Count NAT Gateways in same VNet (for optimization scenario)
                vnet_id = None
                if nat.subnets and len(nat.subnets) > 0:
                    # Extract VNet ID from subnet ID
                    subnet_id = nat.subnets[0].id
                    vnet_id = "/".join(subnet_id.split("/")[:9]) if "/" in subnet_id else None

                if vnet_id:
                    if vnet_id not in nat_per_vnet:
                        nat_per_vnet[vnet_id] = 0
                    nat_per_vnet[vnet_id] += 1

                nat_count_in_vnet = nat_per_vnet.get(vnet_id, 1) if vnet_id else 1

                # Pricing calculation (Azure NAT Gateway pricing - US East 2025)
                # Gateway cost: $32.85/month
                # Outbound data: $0.045/GB
                gateway_cost = 32.85
                outbound_cost = estimated_gb_outbound * 0.045

                monthly_cost = gateway_cost + outbound_cost

                # Calculate optimization
                (is_optimizable, optimization_score, optimization_priority,
                 potential_savings, recommendations) = self._calculate_nat_optimization(
                    nat, has_subnets, subnet_count, has_public_ips,
                    public_ip_count, estimated_gb_outbound, nat_count_in_vnet,
                    monthly_cost, gateway_cost
                )

                # Build resource metadata
                resource_metadata = {
                    "subnet_count": subnet_count,
                    "public_ip_count": public_ip_count,
                    "estimated_gb_outbound": estimated_gb_outbound,
                    "nat_count_in_vnet": nat_count_in_vnet,
                    "vnet_id": vnet_id,
                    "provisioning_state": nat.provisioning_state,
                }

                resource = AllCloudResourceData(
                    resource_type="azure_nat_gateway",
                    resource_id=nat.id,
                    resource_name=nat.name,
                    region=region,
                    estimated_monthly_cost=round(monthly_cost, 2),
                    currency="USD",
                    is_optimizable=is_optimizable,
                    optimization_priority=optimization_priority,
                    optimization_score=optimization_score,
                    potential_monthly_savings=round(potential_savings, 2),
                    optimization_recommendations=recommendations,
                    resource_metadata=resource_metadata,
                    created_at_cloud=nat.etag,  # Use etag as proxy
                    last_used_at=None,  # No last-used timestamp available
                    status="active",
                )

                all_nat_gateways.append(resource)

            logger.info(
                "inventory.azure_nat_gateways_scanned",
                region=region,
                total_scanned=len(all_nat_gateways),
                optimizable=sum(1 for r in all_nat_gateways if r.is_optimizable)
            )

        except ImportError:
            logger.error(
                "inventory.azure_sdk_missing",
                region=region,
                sdk="azure-mgmt-network",
                message="Install azure-mgmt-network to scan NAT Gateways"
            )
        except Exception as e:
            logger.exception(
                "inventory.azure_nat_gateways_scan_failed",
                region=region,
                error=str(e)
            )

        return all_nat_gateways

    def _calculate_nat_optimization(
        self,
        nat,
        has_subnets: bool,
        subnet_count: int,
        has_public_ips: bool,
        public_ip_count: int,
        estimated_gb_outbound: float,
        nat_count_in_vnet: int,
        monthly_cost: float,
        gateway_cost: float
    ) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate NAT Gateway optimization opportunities.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: Pas de subnet attach (CRITICAL - 90 score)
        if not has_subnets or subnet_count == 0:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete NAT Gateway without subnets",
                "details": f"NAT Gateway has no subnets attached. Not routing any traffic. Delete to save ${monthly_cost}/month.",
                "alternatives": [
                    {"name": "Delete NAT Gateway", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "critical",
            })

        # Scenario 2: Pas d'IP publiques configures (HIGH - 75 score)
        elif not has_public_ips or public_ip_count == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete NAT Gateway without public IPs",
                "details": f"NAT Gateway has no public IPs configured. Cannot provide outbound connectivity. Delete to save ${monthly_cost}/month.",
                "alternatives": [
                    {"name": "Delete NAT Gateway", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "high",
            })

        # Scenario 3: Trs faible utilisation (<100GB/mois) (MEDIUM - 50 score)
        elif estimated_gb_outbound < 100:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Savings: gateway cost only (keep minimal data cost)
            potential_savings = gateway_cost
            recommendations.append({
                "action": "Consider using Public IP instead of NAT Gateway",
                "details": f"Very low outbound data ({estimated_gb_outbound}GB/month). Public IP on VM may be more cost-effective. Save ${potential_savings}/month.",
                "alternatives": [
                    {"name": "Use Public IP instead", "cost": round(estimated_gb_outbound * 0.005, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "medium",
            })

        # Scenario 4: Multiple NAT Gateways dans mme VNet (LOW - 30 score)
        elif nat_count_in_vnet > 1:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            # Savings: consolidate to 1 NAT Gateway (save 1 gateway cost)
            potential_savings = gateway_cost
            recommendations.append({
                "action": f"Consolidate NAT Gateways in VNet ({nat_count_in_vnet} gateways)",
                "details": f"VNet has {nat_count_in_vnet} NAT Gateways. Consider consolidating to reduce costs. Save ${potential_savings}/month per gateway.",
                "alternatives": [
                    {"name": "Consolidate to 1 NAT Gateway", "cost": round(monthly_cost - potential_savings, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_azure_sql_databases(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure SQL Databases for cost intelligence.

        Detection criteria:
        - Base de donnes paused/stopped (CRITICAL - 90 score)
        - Aucune connexion 30 derniers jours (HIGH - 75 score)
        - DTU trs faible (<5% utilization) (HIGH - 70 score)
        - Premium tier en dev/test (HIGH - 65 score)
        - Geo-replication non ncessaire (MEDIUM - 50 score)

        Args:
            region: Azure region to scan (e.g., 'eastus')

        Returns:
            List of all Azure SQL Databases with optimization recommendations
        """
        logger.info("inventory.scan_azure_sql_databases_start", region=region)
        all_sql_databases: list[AllCloudResourceData] = []

        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.sql import SqlManagementClient

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            sql_client = SqlManagementClient(credential, self.subscription_id)

            # Get all SQL servers first
            sql_servers = list(sql_client.servers.list())

            logger.info(
                "inventory.azure_sql_servers_fetched",
                region=region,
                total_sql_servers=len(sql_servers)
            )

            for server in sql_servers:
                # Filter by region
                if server.location != region:
                    continue

                # Filter by resource group scope
                if not self.provider._is_resource_in_scope(server.id):
                    continue

                # Get resource group from server ID
                resource_group = server.id.split("/")[4]

                # Get all databases in this server
                try:
                    databases = list(sql_client.databases.list_by_server(
                        resource_group_name=resource_group,
                        server_name=server.name
                    ))

                    for db in databases:
                        # Skip 'master' database
                        if db.name.lower() == "master":
                            continue

                        # Database properties
                        status = db.status if db.status else "Online"
                        is_paused = status.lower() in ["paused", "stopped"]

                        sku_name = db.sku.name if db.sku else "Basic"
                        sku_tier = db.sku.tier if db.sku else "Basic"

                        # Capacity (DTU or vCores)
                        capacity = db.sku.capacity if db.sku and db.sku.capacity else 5

                        # Geo-replication
                        has_geo_replication = False  # Would need to check replication links

                        # Estimate DTU utilization (requires Azure Monitor - hardcoded for now)
                        dtu_utilization_percent = 25.0  # Placeholder - would need Monitor API

                        # Days since last connection (requires diagnostic logs - hardcoded)
                        days_since_last_connection = 15  # Placeholder - would need Monitor API

                        # Pricing calculation (Azure SQL Database pricing - US East 2025)
                        # Prices vary widely by tier and size
                        pricing_map = {
                            "Basic": 5.00,           # Basic: $5/month
                            "Standard_S0": 15.00,    # Standard S0: $15/month
                            "Standard_S1": 30.00,    # Standard S1: $30/month
                            "Standard_S2": 75.00,    # Standard S2: $75/month
                            "Standard_S3": 150.00,   # Standard S3: $150/month
                            "Standard_S4": 300.00,   # Standard S4: $300/month
                            "Premium_P1": 465.00,    # Premium P1: $465/month
                            "Premium_P2": 930.00,    # Premium P2: $930/month
                            "Premium_P4": 1860.00,   # Premium P4: $1860/month
                            "Premium_P6": 3720.00,   # Premium P6: $3720/month
                            "Premium_P11": 7000.00,  # Premium P11: $7000/month
                            "Premium_P15": 14000.00, # Premium P15: $14000/month
                        }

                        # Construct SKU key
                        sku_key = f"{sku_tier}_{sku_name}" if sku_tier != "Basic" else sku_tier
                        monthly_cost = pricing_map.get(sku_key, pricing_map.get(sku_tier, 15.00))

                        # Calculate optimization
                        (is_optimizable, optimization_score, optimization_priority,
                         potential_savings, recommendations) = self._calculate_sqldb_optimization(
                            db, status, is_paused, sku_tier, dtu_utilization_percent,
                            days_since_last_connection, has_geo_replication, monthly_cost
                        )

                        # Build resource metadata
                        resource_metadata = {
                            "server_name": server.name,
                            "sku_name": sku_name,
                            "sku_tier": sku_tier,
                            "capacity": capacity,
                            "status": status,
                            "dtu_utilization_percent": dtu_utilization_percent,
                            "days_since_last_connection": days_since_last_connection,
                            "has_geo_replication": has_geo_replication,
                        }

                        resource = AllCloudResourceData(
                            resource_type="azure_sql_database",
                            resource_id=db.id,
                            resource_name=db.name,
                            region=region,
                            estimated_monthly_cost=round(monthly_cost, 2),
                            currency="USD",
                            is_optimizable=is_optimizable,
                            optimization_priority=optimization_priority,
                            optimization_score=optimization_score,
                            potential_monthly_savings=round(potential_savings, 2),
                            optimization_recommendations=recommendations,
                            resource_metadata=resource_metadata,
                            created_at_cloud=db.creation_date,
                            last_used_at=None,  # No last-used timestamp available
                            status="active",
                        )

                        all_sql_databases.append(resource)

                except Exception as db_error:
                    logger.error(
                        "inventory.azure_sql_databases_list_failed",
                        server=server.name,
                        error=str(db_error)
                    )

            logger.info(
                "inventory.azure_sql_databases_scanned",
                region=region,
                total_scanned=len(all_sql_databases),
                optimizable=sum(1 for r in all_sql_databases if r.is_optimizable)
            )

        except ImportError:
            logger.error(
                "inventory.azure_sdk_missing",
                region=region,
                sdk="azure-mgmt-sql",
                message="Install azure-mgmt-sql to scan Azure SQL Databases"
            )
        except Exception as e:
            logger.exception(
                "inventory.azure_sql_databases_scan_failed",
                region=region,
                error=str(e)
            )

        return all_sql_databases

    def _calculate_sqldb_optimization(
        self,
        db,
        status: str,
        is_paused: bool,
        sku_tier: str,
        dtu_utilization_percent: float,
        days_since_last_connection: int,
        has_geo_replication: bool,
        monthly_cost: float
    ) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate Azure SQL Database optimization opportunities.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: Base de donnes paused/stopped (CRITICAL - 90 score)
        if is_paused:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete or resume paused database",
                "details": f"Database is '{status}'. Delete if no longer needed, or resume if required. Save ${monthly_cost}/month.",
                "alternatives": [
                    {"name": "Delete database", "cost": 0, "savings": monthly_cost},
                    {"name": "Resume database", "cost": monthly_cost, "savings": 0},
                ],
                "priority": "critical",
            })

        # Scenario 2: Aucune connexion 30 derniers jours (HIGH - 75 score)
        elif days_since_last_connection > 30:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete unused database (no connections for 30+ days)",
                "details": f"Database has no connections for {days_since_last_connection} days. Delete if no longer needed. Save ${monthly_cost}/month.",
                "alternatives": [
                    {"name": "Delete database", "cost": 0, "savings": monthly_cost},
                    {"name": "Pause database", "cost": round(monthly_cost * 0.1, 2), "savings": round(monthly_cost * 0.9, 2)},
                ],
                "priority": "high",
            })

        # Scenario 3: DTU trs faible (<5% utilization) (HIGH - 70 score)
        elif dtu_utilization_percent < 5.0:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Savings: Downgrade to lower tier (estimate 50% reduction)
            potential_savings = monthly_cost * 0.5
            recommendations.append({
                "action": "Downgrade database tier (very low DTU utilization)",
                "details": f"DTU utilization is only {dtu_utilization_percent}%. Consider downgrading to lower tier. Save ~${potential_savings}/month.",
                "alternatives": [
                    {"name": "Downgrade to lower tier", "cost": round(monthly_cost * 0.5, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "high",
            })

        # Scenario 4: Premium tier en dev/test (HIGH - 65 score)
        elif sku_tier == "Premium" and monthly_cost > 1000:
            is_optimizable = True
            optimization_score = 65
            priority = "high"
            # Savings: Premium  Standard (70% reduction)
            potential_savings = monthly_cost * 0.7
            recommendations.append({
                "action": "Downgrade from Premium to Standard tier",
                "details": f"Using Premium tier (${monthly_cost}/mo). If dev/test environment, Standard tier is sufficient. Save ${potential_savings}/month.",
                "alternatives": [
                    {"name": "Downgrade to Standard tier", "cost": round(monthly_cost * 0.3, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "high",
            })

        # Scenario 5: Geo-replication non ncessaire (MEDIUM - 50 score)
        elif has_geo_replication:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Savings: Geo-replication adds ~100% cost (double the database)
            potential_savings = monthly_cost * 0.5
            recommendations.append({
                "action": "Remove geo-replication if not required",
                "details": f"Database has geo-replication enabled. If not required for DR, remove to save ~${potential_savings}/month (50% reduction).",
                "alternatives": [
                    {"name": "Remove geo-replication", "cost": round(monthly_cost * 0.5, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "medium",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_aks_clusters(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure AKS (Kubernetes) Clusters for cost intelligence.

        Detection criteria:
        - Cluster stopped/deallocated (CRITICAL - 90 score)
        - No node pools configured (HIGH - 75 score)
        - Node pools overprovisioned (>50% idle) (HIGH - 70 score)
        - Premium tier in dev/test (MEDIUM - 50 score)
        - Auto-scaler disabled on production (LOW - 30 score)

        Args:
            region: Azure region to scan (e.g., 'eastus')

        Returns:
            List of all AKS Clusters with optimization recommendations
        """
        logger.info("inventory.scan_azure_aks_start", region=region)
        all_aks_clusters: list[AllCloudResourceData] = []

        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.containerservice import ContainerServiceClient

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            aks_client = ContainerServiceClient(credential, self.subscription_id)
            clusters = list(aks_client.managed_clusters.list())

            logger.info(
                "inventory.azure_aks_fetched",
                region=region,
                total_clusters=len(clusters)
            )

            for cluster in clusters:
                # Filter by region
                if cluster.location != region:
                    continue

                # Filter by resource group scope
                if not self.provider._is_resource_in_scope(cluster.id):
                    continue

                # Cluster properties
                provisioning_state = cluster.provisioning_state if cluster.provisioning_state else "Unknown"
                power_state = cluster.power_state.code if cluster.power_state else "Running"
                is_stopped = power_state.lower() in ["stopped", "deallocated"]

                # Node pools analysis
                agent_pools = cluster.agent_pool_profiles if cluster.agent_pool_profiles else []
                has_node_pools = len(agent_pools) > 0
                total_nodes = sum(pool.count if pool.count else 0 for pool in agent_pools)

                # Auto-scaler analysis
                has_autoscaler = any(
                    pool.enable_auto_scaling for pool in agent_pools if pool.enable_auto_scaling
                )

                # Tier analysis
                sku_tier = cluster.sku.tier if cluster.sku else "Free"

                # Pricing calculation (Azure AKS pricing - US East 2025)
                # Cluster management fee (Standard/Premium tier)
                cluster_fee = 0.0
                if sku_tier == "Standard":
                    cluster_fee = 73.00  # $0.10/h * 730h
                elif sku_tier == "Premium":
                    cluster_fee = 511.00  # $0.70/h * 730h

                # Node pools cost (estimate based on Standard_DS2_v2: $0.096/h)
                node_cost_per_hour = 0.096  # Average for Standard_DS2_v2
                node_pool_cost = total_nodes * node_cost_per_hour * 730

                monthly_cost = cluster_fee + node_pool_cost

                # Calculate optimization
                (is_optimizable, optimization_score, optimization_priority,
                 potential_savings, recommendations) = self._calculate_aks_optimization(
                    cluster, is_stopped, has_node_pools, total_nodes,
                    has_autoscaler, sku_tier, monthly_cost, cluster_fee
                )

                # Build resource metadata
                resource_metadata = {
                    "sku_tier": sku_tier,
                    "provisioning_state": provisioning_state,
                    "power_state": power_state,
                    "node_pool_count": len(agent_pools),
                    "total_nodes": total_nodes,
                    "has_autoscaler": has_autoscaler,
                    "kubernetes_version": cluster.kubernetes_version,
                }

                resource = AllCloudResourceData(
                    resource_type="azure_aks_cluster",
                    resource_id=cluster.id,
                    resource_name=cluster.name,
                    region=region,
                    estimated_monthly_cost=round(monthly_cost, 2),
                    currency="USD",
                    is_optimizable=is_optimizable,
                    optimization_priority=optimization_priority,
                    optimization_score=optimization_score,
                    potential_monthly_savings=round(potential_savings, 2),
                    optimization_recommendations=recommendations,
                    resource_metadata=resource_metadata,
                    created_at_cloud=cluster.provisioning_state,  # Use provisioning state as proxy
                    last_used_at=None,  # No last-used timestamp available
                    status="active",
                )

                all_aks_clusters.append(resource)

            logger.info(
                "inventory.azure_aks_scanned",
                region=region,
                total_scanned=len(all_aks_clusters),
                optimizable=sum(1 for r in all_aks_clusters if r.is_optimizable)
            )

        except ImportError:
            logger.error(
                "inventory.azure_sdk_missing",
                region=region,
                sdk="azure-mgmt-containerservice",
                message="Install azure-mgmt-containerservice to scan AKS Clusters"
            )
        except Exception as e:
            logger.exception(
                "inventory.azure_aks_scan_failed",
                region=region,
                error=str(e)
            )

        return all_aks_clusters

    def _calculate_aks_optimization(
        self,
        cluster,
        is_stopped: bool,
        has_node_pools: bool,
        total_nodes: int,
        has_autoscaler: bool,
        sku_tier: str,
        monthly_cost: float,
        cluster_fee: float
    ) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate AKS Cluster optimization opportunities.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: Cluster stopped/deallocated (CRITICAL - 90 score)
        if is_stopped:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete stopped AKS cluster",
                "details": f"AKS cluster is stopped. You're still paying ${monthly_cost}/month for cluster fee + stopped nodes. Delete if no longer needed.",
                "alternatives": [
                    {"name": "Delete cluster", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "critical",
            })

        # Scenario 2: No node pools configured (HIGH - 75 score)
        elif not has_node_pools or total_nodes == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete AKS cluster without nodes",
                "details": f"AKS cluster has no node pools or nodes. Paying ${monthly_cost}/month for empty cluster. Delete if not needed.",
                "alternatives": [
                    {"name": "Delete cluster", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "high",
            })

        # Scenario 3: Node pools overprovisioned (>50% idle) (HIGH - 70 score)
        # Note: Would need Azure Monitor metrics to detect idle nodes
        elif total_nodes > 5:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Estimate 30% of nodes are idle
            potential_savings = monthly_cost * 0.3
            recommendations.append({
                "action": "Review node pool sizing (potential idle nodes)",
                "details": f"AKS cluster has {total_nodes} nodes. Review actual usage to identify idle nodes. Estimated savings: ${potential_savings}/month.",
                "alternatives": [
                    {"name": "Reduce node count by 30%", "cost": round(monthly_cost * 0.7, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "high",
            })

        # Scenario 4: Premium tier in dev/test (MEDIUM - 50 score)
        elif sku_tier == "Premium" and cluster_fee > 400:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Savings: Premium  Standard ($511  $73)
            potential_savings = cluster_fee - 73.00
            recommendations.append({
                "action": "Downgrade from Premium to Standard tier",
                "details": f"Using Premium tier (${cluster_fee}/mo cluster fee). If dev/test, Standard tier ($73/mo) is sufficient. Save ${potential_savings}/month.",
                "alternatives": [
                    {"name": "Downgrade to Standard tier", "cost": round(monthly_cost - potential_savings, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "medium",
            })

        # Scenario 5: Auto-scaler disabled on production (LOW - 30 score)
        elif not has_autoscaler and total_nodes > 2:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            # Estimate 15% savings with auto-scaler
            potential_savings = monthly_cost * 0.15
            recommendations.append({
                "action": "Enable cluster auto-scaler",
                "details": f"Auto-scaler is disabled. Enable auto-scaling to automatically adjust node count based on demand. Estimated savings: ${potential_savings}/month (15%).",
                "alternatives": [
                    {"name": "Enable auto-scaler", "cost": round(monthly_cost * 0.85, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_function_apps(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Function Apps for cost intelligence.

        Detection criteria:
        - Function app stopped (CRITICAL - 90 score)
        - Zero executions 30 derniers jours (HIGH - 75 score)
        - Premium plan underutilized (<1000 exec/day) (HIGH - 65 score)
        - Dedicated plan when Consumption sufficient (MEDIUM - 50 score)
        - Always On enabled on Consumption (LOW - 30 score)

        Args:
            region: Azure region to scan (e.g., 'eastus')

        Returns:
            List of all Function Apps with optimization recommendations
        """
        logger.info("inventory.scan_azure_functions_start", region=region)
        all_function_apps: list[AllCloudResourceData] = []

        try:
            from azure.identity import ClientSecretCredential
            from azure.mgmt.web import WebSiteManagementClient

            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )

            web_client = WebSiteManagementClient(credential, self.subscription_id)

            # List all web apps (includes Function Apps)
            all_sites = list(web_client.web_apps.list())

            # Filter to only Function Apps
            function_apps = [site for site in all_sites if site.kind and "functionapp" in site.kind.lower()]

            logger.info(
                "inventory.azure_functions_fetched",
                region=region,
                total_function_apps=len(function_apps)
            )

            for func_app in function_apps:
                # Filter by region
                if func_app.location != region:
                    continue

                # Filter by resource group scope
                if not self.provider._is_resource_in_scope(func_app.id):
                    continue

                # Function App properties
                state = func_app.state if func_app.state else "Unknown"
                is_stopped = state.lower() == "stopped"

                # Get hosting plan type
                # Extract resource group from func_app.id
                resource_group = func_app.id.split("/")[4]

                # Get app service plan
                plan_type = "Consumption"  # Default
                plan_sku = "Y1"  # Default for Consumption

                if func_app.server_farm_id:
                    try:
                        plan_id_parts = func_app.server_farm_id.split("/")
                        plan_name = plan_id_parts[-1]
                        plan_rg = plan_id_parts[4]

                        plan = web_client.app_service_plans.get(plan_rg, plan_name)
                        plan_sku = plan.sku.name if plan.sku else "Y1"

                        # Determine plan type
                        if plan_sku.startswith("Y"):
                            plan_type = "Consumption"
                        elif plan_sku.startswith("EP"):
                            plan_type = "Premium"
                        else:
                            plan_type = "Dedicated"
                    except Exception:
                        pass

                # Estimate executions per day (requires Azure Monitor - hardcoded)
                daily_executions = 500  # Placeholder

                # Always On setting (only valid for Premium/Dedicated)
                always_on = func_app.site_config.always_on if func_app.site_config else False

                # Pricing calculation (Azure Functions pricing - US East 2025)
                monthly_cost = 0.0

                if plan_type == "Consumption":
                    # Consumption: $0.20 per million executions
                    monthly_executions = daily_executions * 30
                    execution_cost = (monthly_executions / 1_000_000) * 0.20
                    # GB-seconds: assume 128MB avg, 100ms avg duration
                    gb_seconds = (monthly_executions * 0.1) * (128 / 1024)
                    memory_cost = gb_seconds * 0.000016
                    monthly_cost = execution_cost + memory_cost
                elif plan_type == "Premium":
                    # Premium plans
                    premium_pricing = {
                        "EP1": 169.00,  # 1 vCPU, 3.5GB
                        "EP2": 338.00,  # 2 vCPU, 7GB
                        "EP3": 676.00,  # 4 vCPU, 14GB
                    }
                    monthly_cost = premium_pricing.get(plan_sku, 169.00)
                else:
                    # Dedicated (App Service Plan)
                    # Pricing varies widely, estimate average
                    monthly_cost = 100.00  # Conservative estimate

                # Calculate optimization
                (is_optimizable, optimization_score, optimization_priority,
                 potential_savings, recommendations) = self._calculate_function_optimization(
                    func_app, is_stopped, plan_type, plan_sku, daily_executions,
                    always_on, monthly_cost
                )

                # Build resource metadata
                resource_metadata = {
                    "state": state,
                    "plan_type": plan_type,
                    "plan_sku": plan_sku,
                    "daily_executions": daily_executions,
                    "always_on": always_on,
                    "runtime": func_app.kind,
                }

                resource = AllCloudResourceData(
                    resource_type="azure_function_app",
                    resource_id=func_app.id,
                    resource_name=func_app.name,
                    region=region,
                    estimated_monthly_cost=round(monthly_cost, 2),
                    currency="USD",
                    is_optimizable=is_optimizable,
                    optimization_priority=optimization_priority,
                    optimization_score=optimization_score,
                    potential_monthly_savings=round(potential_savings, 2),
                    optimization_recommendations=recommendations,
                    resource_metadata=resource_metadata,
                    created_at_cloud=func_app.type,  # Use type as proxy
                    last_used_at=None,
                    status="active",
                )

                all_function_apps.append(resource)

            logger.info(
                "inventory.azure_functions_scanned",
                region=region,
                total_scanned=len(all_function_apps),
                optimizable=sum(1 for r in all_function_apps if r.is_optimizable)
            )

        except ImportError:
            logger.error(
                "inventory.azure_sdk_missing",
                region=region,
                sdk="azure-mgmt-web",
                message="Install azure-mgmt-web to scan Function Apps"
            )
        except Exception as e:
            logger.exception(
                "inventory.azure_functions_scan_failed",
                region=region,
                error=str(e)
            )

        return all_function_apps

    def _calculate_function_optimization(
        self,
        func_app,
        is_stopped: bool,
        plan_type: str,
        plan_sku: str,
        daily_executions: int,
        always_on: bool,
        monthly_cost: float
    ) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate Function App optimization opportunities.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: Function app stopped (CRITICAL - 90 score)
        if is_stopped:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete stopped Function App",
                "details": f"Function App is stopped. Delete if no longer needed. Save ${monthly_cost}/month.",
                "alternatives": [
                    {"name": "Delete Function App", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "critical",
            })

        # Scenario 2: Zero executions 30 days (HIGH - 75 score)
        elif daily_executions < 10:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.append({
                "action": "Delete unused Function App (no executions)",
                "details": f"Function App has very few executions ({daily_executions}/day). Delete if not needed. Save ${monthly_cost}/month.",
                "alternatives": [
                    {"name": "Delete Function App", "cost": 0, "savings": monthly_cost},
                ],
                "priority": "high",
            })

        # Scenario 3: Premium plan underutilized (<1000 exec/day) (HIGH - 65 score)
        elif plan_type == "Premium" and daily_executions < 1000:
            is_optimizable = True
            optimization_score = 65
            priority = "high"
            # Savings: Premium  Consumption (estimate $165/mo)
            consumption_cost = (daily_executions * 30 / 1_000_000) * 0.20
            potential_savings = monthly_cost - consumption_cost
            recommendations.append({
                "action": "Downgrade from Premium to Consumption plan",
                "details": f"Using Premium plan (${monthly_cost}/mo) but only {daily_executions} executions/day. Consumption plan would cost ${consumption_cost:.2f}/month. Save ${potential_savings:.2f}/month.",
                "alternatives": [
                    {"name": "Switch to Consumption plan", "cost": round(consumption_cost, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "high",
            })

        # Scenario 4: Dedicated plan when Consumption sufficient (MEDIUM - 50 score)
        elif plan_type == "Dedicated" and daily_executions < 5000:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Savings: Dedicated  Consumption
            consumption_cost = (daily_executions * 30 / 1_000_000) * 0.20
            potential_savings = monthly_cost - consumption_cost
            recommendations.append({
                "action": "Switch from Dedicated to Consumption plan",
                "details": f"Using Dedicated plan (${monthly_cost}/mo) with {daily_executions} executions/day. Consumption plan sufficient. Save ${potential_savings:.2f}/month.",
                "alternatives": [
                    {"name": "Switch to Consumption plan", "cost": round(consumption_cost, 2), "savings": round(potential_savings, 2)},
                ],
                "priority": "medium",
            })

        # Scenario 5: Always On enabled on Consumption (LOW - 30 score)
        elif plan_type == "Consumption" and always_on:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            # Always On not supported on Consumption, but if somehow enabled
            potential_savings = monthly_cost * 0.1
            recommendations.append({
                "action": "Disable Always On (not supported on Consumption)",
                "details": "Always On is enabled but not beneficial on Consumption plan. Disable to avoid cold starts being masked.",
                "alternatives": [
                    {"name": "Disable Always On", "cost": round(monthly_cost, 2), "savings": 0},
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_cosmos_dbs(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Cosmos DB accounts for cost intelligence.

        Detection criteria:
        - Database account paused/offline (CRITICAL - 90 score)
        - Zero requests 30 derniers jours (HIGH - 75 score)
        - Provisioned throughput >> actual usage (>50% idle) (HIGH - 70 score)
        - Multi-region replication without need (MEDIUM - 50 score)
        - Serverless would be cheaper based on usage (LOW - 30 score)
        """
        try:
            from azure.mgmt.cosmosdb import CosmosDBManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-cosmosdb not installed")
            return []

        resources = []
        self.logger.info(f"Scanning Cosmos DB accounts in region: {region}")

        try:
            cosmos_client = CosmosDBManagementClient(
                credential=self.credential,
                subscription_id=self.subscription_id
            )

            # List all Cosmos DB accounts
            async for account in cosmos_client.database_accounts.list():
                try:
                    # Filter by region if specified
                    if region.lower() != "all" and account.location.lower() != region.lower():
                        continue

                    # Get resource group from account ID
                    resource_group = account.id.split("/")[4]

                    # Calculate optimization
                    is_optimizable, score, priority, savings, recommendations = (
                        self._calculate_cosmos_optimization(account)
                    )

                    # Get database count
                    databases = []
                    try:
                        # Get SQL databases
                        sql_dbs = list(cosmos_client.sql_resources.list_sql_databases(
                            resource_group_name=resource_group,
                            account_name=account.name
                        ))
                        databases.extend(sql_dbs)
                    except:
                        pass

                    try:
                        # Get Table API databases
                        tables = list(cosmos_client.table_resources.list_tables(
                            resource_group_name=resource_group,
                            account_name=account.name
                        ))
                        databases.extend(tables)
                    except:
                        pass

                    # Pricing (Azure US East 2025)
                    # Serverless: ~$0.25/1M RUs + $0.25/GB storage
                    # Provisioned: $0.008/RU/hour ($5.84/100 RU/mo)
                    # Multi-region adds 2x-3x cost
                    pricing_info = {
                        "serverless_base": 25.0,  # Typical small workload
                        "provisioned_100ru": 5.84,
                        "provisioned_400ru": 23.36,
                        "provisioned_1000ru": 58.40,
                        "provisioned_10000ru": 584.00,
                        "multi_region_multiplier": 2.0,
                    }

                    # Get provisioning state and capabilities
                    provisioning_state = getattr(account, 'provisioning_state', 'Unknown')
                    capabilities = getattr(account, 'capabilities', [])
                    capability_names = [cap.name for cap in capabilities] if capabilities else []

                    # Detect account type
                    is_serverless = 'EnableServerless' in capability_names
                    is_multi_region = len(account.locations) > 1 if hasattr(account, 'locations') else False

                    # Detect API type via capabilities
                    is_gremlin = 'EnableGremlin' in capability_names
                    is_mongodb = 'EnableMongo' in capability_names
                    account_kind = getattr(account, 'kind', 'GlobalDocumentDB')

                    # Set account kind based on API
                    if account_kind == 'GlobalDocumentDB':
                        if is_gremlin:
                            account_kind = 'Gremlin'
                        elif is_mongodb:
                            account_kind = 'MongoDB'

                    # Estimate monthly cost based on configuration
                    if is_serverless:
                        base_cost = pricing_info["serverless_base"]
                    else:
                        # Assume 400 RU/s for small, 1000 for medium
                        base_cost = pricing_info["provisioned_400ru"]

                    if is_multi_region:
                        base_cost *= pricing_info["multi_region_multiplier"]

                    # Determine resource type based on API
                    if is_gremlin:
                        resource_type = "azure_cosmos_db_gremlin"
                    elif is_mongodb:
                        resource_type = "azure_cosmos_db_mongodb"
                    else:
                        resource_type = "azure_cosmos_db"

                    resources.append(AllCloudResourceData(
                        resource_id=account.id,
                        resource_type=resource_type,
                        resource_name=account.name or f"Unnamed Cosmos DB ({account_kind})",
                        region=account.location,
                        estimated_monthly_cost=base_cost,
                        currency="USD",
                        resource_metadata={
                            "account_id": account.id,
                            "resource_group": resource_group,
                            "provisioning_state": provisioning_state,
                            "kind": getattr(account, 'kind', 'GlobalDocumentDB'),
                            "database_account_offer_type": getattr(account, 'database_account_offer_type', 'Standard'),
                            "consistency_policy": str(getattr(account, 'default_consistency_level', 'Session')),
                            "is_serverless": is_serverless,
                            "is_multi_region": is_multi_region,
                            "locations": [loc.location_name for loc in account.locations] if hasattr(account, 'locations') else [],
                            "database_count": len(databases),
                            "capabilities": capability_names,
                            "tags": dict(account.tags) if account.tags else {},
                        },
                        is_optimizable=is_optimizable,
                        optimization_score=score,
                        optimization_priority=priority,
                        potential_monthly_savings=savings,
                        optimization_recommendations=recommendations,
                        last_used_at=None,
                        created_at_cloud=None,
                    ))

                except Exception as e:
                    self.logger.error(f"Error processing Cosmos DB account {getattr(account, 'name', 'unknown')}: {str(e)}")
                    continue

            self.logger.info(f"Found {len(resources)} Cosmos DB accounts in region {region}")
            return resources

        except Exception as e:
            self.logger.error(f"Error scanning Cosmos DB accounts: {str(e)}")
            return []

    def _calculate_cosmos_optimization(self, account) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate optimization potential for Cosmos DB account.

        Returns:
            (is_optimizable, score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Get account properties
        provisioning_state = getattr(account, 'provisioning_state', 'Unknown')
        capabilities = getattr(account, 'capabilities', [])
        capability_names = [cap.name for cap in capabilities] if capabilities else []
        is_serverless = 'EnableServerless' in capability_names
        is_multi_region = len(account.locations) > 1 if hasattr(account, 'locations') else False
        location_count = len(account.locations) if hasattr(account, 'locations') else 1

        # Scenario 1: Database account paused/offline (CRITICAL - 90)
        if provisioning_state.lower() in ['deleting', 'failed', 'canceled']:
            is_optimizable = True
            optimization_score = max(optimization_score, 90)
            priority = "critical"
            # Estimate savings: Full cost if stopped/failed
            estimated_cost = 100.0 if not is_serverless else 25.0
            if is_multi_region:
                estimated_cost *= 2.0
            potential_savings = max(potential_savings, estimated_cost)

            recommendations.append({
                "title": "Database Account Non Fonctionnel",
                "description": f"Ce compte Cosmos DB est dans l'tat '{provisioning_state}'. Il peut gnrer des cots inutiles s'il n'est pas utilis.",
                "estimated_savings": round(estimated_cost, 2),
                "actions": [
                    "Vrifier l'tat du compte et corriger les problmes",
                    "Supprimer le compte s'il n'est plus ncessaire",
                    "Restaurer depuis une sauvegarde si donnes importantes"
                ],
                "priority": "critical",
            })

        # Scenario 2: Zero requests last 30 days (HIGH - 75)
        # Note: We can't get actual metrics without Azure Monitor, so this is a placeholder
        # In production, you'd check actual request metrics

        # Scenario 3: Provisioned throughput >> actual usage (HIGH - 70)
        if not is_serverless:  # Only applicable to provisioned throughput
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            if priority not in ["critical"]:
                priority = "high"

            # Estimate 50% savings by right-sizing
            estimated_cost = 58.40  # Assume 1000 RU/s
            if is_multi_region:
                estimated_cost *= 2.0
            savings = estimated_cost * 0.5
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Dbit Provisionn Potentiellement Surdimensionn",
                "description": "Ce compte Cosmos DB utilise le mode provisionn. Vrifiez si le dbit configur correspond  l'utilisation relle.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Analyser les mtriques d'utilisation des RU/s dans Azure Monitor",
                    "Rduire le dbit provisionn si utilisation <50%",
                    "Activer l'auto-scaling pour adapter automatiquement",
                    "Considrer le mode Serverless si utilisation sporadique"
                ],
                "priority": "high",
            })

        # Scenario 4: Multi-region without need (MEDIUM - 50)
        if is_multi_region and location_count > 2:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            if priority not in ["critical", "high"]:
                priority = "medium"

            # Multi-region doubles cost, removing extra regions saves significant
            base_cost = 100.0 if not is_serverless else 25.0
            # Savings from removing extra regions (keep 1-2 regions max)
            savings = base_cost * (location_count - 2) * 0.8
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Rplication Multi-Rgion Excessive",
                "description": f"Ce compte Cosmos DB est rpliqu dans {location_count} rgions. Chaque rgion ajoute des cots significatifs.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "valuer la ncessit de chaque rgion de rplication",
                    "Garder uniquement les rgions essentielles (1-2 max)",
                    "Supprimer les rgions secondaires non utilises",
                    f"conomies potentielles: ~{int(savings)}$/mois par rgion supprime"
                ],
                "priority": "medium",
            })

        # Scenario 5: Serverless would be cheaper (LOW - 30)
        if not is_serverless:
            # Serverless is better for sporadic/low-volume workloads
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            if priority not in ["critical", "high", "medium"]:
                priority = "low"

            # Estimate savings if switching to serverless
            provisioned_cost = 58.40  # Assume 1000 RU/s
            serverless_cost = 25.0
            savings = max(0, provisioned_cost - serverless_cost)
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Mode Serverless Potentiellement Plus conomique",
                "description": "Ce compte utilise le dbit provisionn. Le mode Serverless peut tre plus rentable pour les workloads sporadiques.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Analyser le pattern d'utilisation (sporadique vs constant)",
                    "valuer le cot Serverless vs Provisionn pour votre charge",
                    "Crer un nouveau compte Serverless et migrer si pertinent",
                    "Note: Serverless idal pour <1M RU/s par jour"
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_container_apps(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Container Apps for cost intelligence.

        Detection criteria:
        - Container app stopped/deprovisioned (CRITICAL - 90 score)
        - Zero requests 30 derniers jours (HIGH - 75 score)
        - Replicas overprovisioned (>50% idle capacity) (HIGH - 70 score)
        - Consumption plan when Dedicated sufficient (MEDIUM - 50 score)
        - Auto-scaling disabled on production (LOW - 30 score)
        """
        try:
            from azure.mgmt.appcontainers import ContainerAppsAPIClient
        except ImportError:
            self.logger.error("azure-mgmt-appcontainers not installed")
            return []

        resources = []
        self.logger.info(f"Scanning Container Apps in region: {region}")

        try:
            container_client = ContainerAppsAPIClient(
                credential=self.credential,
                subscription_id=self.subscription_id
            )

            # List all Container Apps
            async for app in container_client.container_apps.list_by_subscription():
                try:
                    # Filter by region if specified
                    if region.lower() != "all" and app.location.lower() != region.lower():
                        continue

                    # Get resource group from app ID
                    resource_group = app.id.split("/")[4]

                    # Calculate optimization
                    is_optimizable, score, priority, savings, recommendations = (
                        self._calculate_container_app_optimization(app)
                    )

                    # Pricing (Azure US East 2025)
                    # Consumption: $0.000012/vCPU-second + $0.000003/GiB-second
                    # Dedicated: $72/month per vCPU + $18/month per GiB
                    # Typical small app: 0.25 vCPU, 0.5 GiB = ~$50/mo consumption
                    pricing_info = {
                        "consumption_vcpu_second": 0.000012,
                        "consumption_gb_second": 0.000003,
                        "dedicated_vcpu_month": 72.0,
                        "dedicated_gb_month": 18.0,
                    }

                    # Get configuration
                    provisioning_state = getattr(app, 'provisioning_state', 'Unknown')
                    configuration = getattr(app, 'configuration', None)
                    template = getattr(app, 'template', None)

                    # Get scale settings
                    min_replicas = 0
                    max_replicas = 1
                    if template and hasattr(template, 'scale'):
                        scale = template.scale
                        min_replicas = getattr(scale, 'min_replicas', 0)
                        max_replicas = getattr(scale, 'max_replicas', 1)

                    # Get container resources
                    containers = []
                    total_vcpu = 0.25  # Default
                    total_memory_gb = 0.5  # Default
                    if template and hasattr(template, 'containers'):
                        containers = template.containers
                        for container in containers:
                            resources_config = getattr(container, 'resources', None)
                            if resources_config:
                                cpu = getattr(resources_config, 'cpu', 0.25)
                                memory = getattr(resources_config, 'memory', '0.5Gi')
                                # Parse memory (e.g., "0.5Gi" -> 0.5)
                                try:
                                    mem_value = float(memory.replace('Gi', '').replace('G', ''))
                                except:
                                    mem_value = 0.5
                                total_vcpu += cpu
                                total_memory_gb += mem_value

                    # Get environment type (consumption vs dedicated)
                    managed_environment_id = getattr(app, 'managed_environment_id', '')
                    is_consumption = 'consumption' in managed_environment_id.lower() if managed_environment_id else True

                    # Estimate monthly cost (assume running 24/7)
                    if is_consumption:
                        # Consumption: per second pricing
                        seconds_per_month = 730 * 3600  # 730 hours
                        vcpu_cost = pricing_info["consumption_vcpu_second"] * total_vcpu * seconds_per_month
                        memory_cost = pricing_info["consumption_gb_second"] * total_memory_gb * seconds_per_month
                        base_cost = vcpu_cost + memory_cost
                    else:
                        # Dedicated: monthly pricing
                        base_cost = (pricing_info["dedicated_vcpu_month"] * total_vcpu +
                                   pricing_info["dedicated_gb_month"] * total_memory_gb)

                    # Multiply by average replicas (assume avg = (min + max) / 2)
                    avg_replicas = max(1, (min_replicas + max_replicas) / 2)
                    estimated_cost = base_cost * avg_replicas

                    resources.append(AllCloudResourceData(
                        resource_id=app.id,
                        resource_type="azure_container_app",
                        resource_name=app.name or "Unnamed Container App",
                        region=app.location,
                        estimated_monthly_cost=round(estimated_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "app_id": app.id,
                            "resource_group": resource_group,
                            "provisioning_state": provisioning_state,
                            "managed_environment_id": managed_environment_id,
                            "is_consumption": is_consumption,
                            "min_replicas": min_replicas,
                            "max_replicas": max_replicas,
                            "total_vcpu": total_vcpu,
                            "total_memory_gb": total_memory_gb,
                            "container_count": len(containers),
                            "ingress_enabled": configuration and hasattr(configuration, 'ingress') and configuration.ingress is not None,
                            "tags": dict(app.tags) if app.tags else {},
                        },
                        is_optimizable=is_optimizable,
                        optimization_score=score,
                        optimization_priority=priority,
                        potential_monthly_savings=savings,
                        optimization_recommendations=recommendations,
                        last_used_at=None,
                        created_at_cloud=None,
                    ))

                except Exception as e:
                    self.logger.error(f"Error processing Container App {getattr(app, 'name', 'unknown')}: {str(e)}")
                    continue

            self.logger.info(f"Found {len(resources)} Container Apps in region {region}")
            return resources

        except Exception as e:
            self.logger.error(f"Error scanning Container Apps: {str(e)}")
            return []

    def _calculate_container_app_optimization(self, app) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate optimization potential for Container App.

        Returns:
            (is_optimizable, score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Get app properties
        provisioning_state = getattr(app, 'provisioning_state', 'Unknown')
        template = getattr(app, 'template', None)
        managed_environment_id = getattr(app, 'managed_environment_id', '')
        is_consumption = 'consumption' in managed_environment_id.lower() if managed_environment_id else True

        # Get scale settings
        min_replicas = 0
        max_replicas = 1
        if template and hasattr(template, 'scale'):
            scale = template.scale
            min_replicas = getattr(scale, 'min_replicas', 0)
            max_replicas = getattr(scale, 'max_replicas', 1)

        # Get resources
        total_vcpu = 0.25
        total_memory_gb = 0.5
        if template and hasattr(template, 'containers'):
            containers = template.containers
            for container in containers:
                resources_config = getattr(container, 'resources', None)
                if resources_config:
                    cpu = getattr(resources_config, 'cpu', 0.25)
                    memory = getattr(resources_config, 'memory', '0.5Gi')
                    try:
                        mem_value = float(memory.replace('Gi', '').replace('G', ''))
                    except:
                        mem_value = 0.5
                    total_vcpu += cpu
                    total_memory_gb += mem_value

        # Estimate monthly cost
        if is_consumption:
            seconds_per_month = 730 * 3600
            base_cost = (0.000012 * total_vcpu * seconds_per_month +
                        0.000003 * total_memory_gb * seconds_per_month)
        else:
            base_cost = 72.0 * total_vcpu + 18.0 * total_memory_gb

        avg_replicas = max(1, (min_replicas + max_replicas) / 2)
        monthly_cost = base_cost * avg_replicas

        # Scenario 1: Container app stopped/deprovisioned (CRITICAL - 90)
        if provisioning_state.lower() in ['deprovisioning', 'failed', 'canceled', 'deleting']:
            is_optimizable = True
            optimization_score = max(optimization_score, 90)
            priority = "critical"
            potential_savings = max(potential_savings, monthly_cost)

            recommendations.append({
                "title": "Container App Non Fonctionnelle",
                "description": f"Cette Container App est dans l'tat '{provisioning_state}'. Elle gnre des cots inutiles.",
                "estimated_savings": round(monthly_cost, 2),
                "actions": [
                    "Vrifier l'tat de l'application et corriger les problmes",
                    "Supprimer l'application si elle n'est plus ncessaire",
                    "Redployer l'application si elle est encore utilise"
                ],
                "priority": "critical",
            })

        # Scenario 2: Zero requests last 30 days (HIGH - 75)
        # Note: We can't get actual metrics without Azure Monitor
        # In production, check ingress metrics

        # Scenario 3: Replicas overprovisioned (HIGH - 70)
        if max_replicas > 3 and min_replicas > 1:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            if priority not in ["critical"]:
                priority = "high"

            # Savings from reducing replicas by 50%
            savings = monthly_cost * 0.5
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Rplicas Potentiellement Surdimensionns",
                "description": f"Cette app est configure avec {min_replicas}-{max_replicas} rplicas. Vrifiez si c'est ncessaire.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Analyser les mtriques de charge CPU/mmoire dans Azure Monitor",
                    f"Rduire min_replicas de {min_replicas}  1 si charge faible",
                    f"Rduire max_replicas de {max_replicas}  3 si pic de charge modr",
                    "Activer l'auto-scaling pour adapter dynamiquement"
                ],
                "priority": "high",
            })

        # Scenario 4: Consumption when Dedicated sufficient (MEDIUM - 50)
        # This is the opposite of typical cloud optimization (dedicated is usually more expensive)
        # Only applicable if usage is very high and predictable
        # Skip this scenario as consumption is typically better

        # Scenario 5: Auto-scaling disabled (LOW - 30)
        if min_replicas == max_replicas and min_replicas > 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            if priority not in ["critical", "high", "medium"]:
                priority = "low"

            # Savings from enabling auto-scaling (assume 20% reduction)
            savings = monthly_cost * 0.2
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Auto-Scaling Dsactiv",
                "description": f"Cette app a un nombre fixe de rplicas ({min_replicas}). L'auto-scaling permettrait d'adapter la capacit.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Activer l'auto-scaling pour adapter automatiquement",
                    "Configurer min_replicas=1 et max_replicas=5 pour commencer",
                    "Dfinir des rgles de scaling bases sur CPU/HTTP requests",
                    "conomies potentielles: ~20% en adaptant aux heures creuses"
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_virtual_desktops(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Virtual Desktop (AVD) host pools for cost intelligence.

        Detection criteria:
        - Host pool stopped/deallocated (CRITICAL - 90 score)
        - Zero active sessions 30 derniers jours (HIGH - 75 score)
        - Session hosts overprovisioned (>50% idle capacity) (HIGH - 70 score)
        - Pooled when Personal sufficient (MEDIUM - 50 score)
        - No auto-scaling configured (LOW - 30 score)
        """
        try:
            from azure.mgmt.desktopvirtualization import DesktopVirtualizationMgmtClient
        except ImportError:
            self.logger.error("azure-mgmt-desktopvirtualization not installed")
            return []

        resources = []
        self.logger.info(f"Scanning Virtual Desktop host pools in region: {region}")

        try:
            vd_client = DesktopVirtualizationMgmtClient(
                credential=self.credential,
                subscription_id=self.subscription_id
            )

            # List all host pools
            async for host_pool in vd_client.host_pools.list():
                try:
                    # Filter by region if specified
                    if region.lower() != "all" and host_pool.location.lower() != region.lower():
                        continue

                    # Get resource group from host pool ID
                    resource_group = host_pool.id.split("/")[4]

                    # Get session hosts count
                    session_hosts = []
                    try:
                        session_hosts = list(vd_client.session_hosts.list(
                            resource_group_name=resource_group,
                            host_pool_name=host_pool.name
                        ))
                    except:
                        pass

                    # Calculate optimization
                    is_optimizable, score, priority, savings, recommendations = (
                        self._calculate_virtual_desktop_optimization(host_pool, session_hosts)
                    )

                    # Pricing (Azure US East 2025)
                    # Session host: D2s_v3 (2 vCPU, 8 GB) = $0.096/h = $70/mo
                    # Storage: Premium SSD $0.135/GB/mo
                    # Typical host pool: 2-10 session hosts = $140-$700/mo
                    pricing_info = {
                        "session_host_hourly": 0.096,  # D2s_v3
                        "session_host_monthly": 70.08,
                        "storage_gb_monthly": 0.135,
                    }

                    # Estimate monthly cost
                    session_host_count = len(session_hosts)
                    base_cost = pricing_info["session_host_monthly"] * max(1, session_host_count)

                    # Add storage estimate (assume 128 GB per session host)
                    storage_gb = 128 * max(1, session_host_count)
                    storage_cost = pricing_info["storage_gb_monthly"] * storage_gb

                    estimated_cost = base_cost + storage_cost

                    # Get host pool properties
                    load_balancer_type = getattr(host_pool, 'load_balancer_type', 'BreadthFirst')
                    max_session_limit = getattr(host_pool, 'max_session_limit', 10)
                    host_pool_type = getattr(host_pool, 'host_pool_type', 'Pooled')

                    resources.append(AllCloudResourceData(
                        resource_id=host_pool.id,
                        resource_type="azure_virtual_desktop",
                        resource_name=host_pool.name or "Unnamed Host Pool",
                        region=host_pool.location,
                        estimated_monthly_cost=round(estimated_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "host_pool_id": host_pool.id,
                            "resource_group": resource_group,
                            "host_pool_type": host_pool_type,
                            "load_balancer_type": load_balancer_type,
                            "max_session_limit": max_session_limit,
                            "session_host_count": session_host_count,
                            "storage_gb": storage_gb,
                            "tags": dict(host_pool.tags) if host_pool.tags else {},
                        },
                        is_optimizable=is_optimizable,
                        optimization_score=score,
                        optimization_priority=priority,
                        potential_monthly_savings=savings,
                        optimization_recommendations=recommendations,
                        last_used_at=None,
                        created_at_cloud=None,
                    ))

                except Exception as e:
                    self.logger.error(f"Error processing Virtual Desktop host pool {getattr(host_pool, 'name', 'unknown')}: {str(e)}")
                    continue

            self.logger.info(f"Found {len(resources)} Virtual Desktop host pools in region {region}")
            return resources

        except Exception as e:
            self.logger.error(f"Error scanning Virtual Desktop host pools: {str(e)}")
            return []

    def _calculate_virtual_desktop_optimization(self, host_pool, session_hosts: list) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate optimization potential for Virtual Desktop host pool.

        Returns:
            (is_optimizable, score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Get host pool properties
        host_pool_type = getattr(host_pool, 'host_pool_type', 'Pooled')
        max_session_limit = getattr(host_pool, 'max_session_limit', 10)
        session_host_count = len(session_hosts)

        # Estimate monthly cost
        session_host_monthly = 70.08  # D2s_v3
        storage_monthly = 0.135 * 128  # 128 GB per host
        monthly_cost = (session_host_monthly + storage_monthly) * max(1, session_host_count)

        # Count active vs stopped session hosts
        active_hosts = 0
        stopped_hosts = 0
        for host in session_hosts:
            status = getattr(host, 'status', 'Unknown')
            if status.lower() in ['available', 'running']:
                active_hosts += 1
            elif status.lower() in ['stopped', 'deallocated', 'unavailable']:
                stopped_hosts += 1

        # Scenario 1: Host pool stopped/deallocated (CRITICAL - 90)
        if session_host_count > 0 and stopped_hosts == session_host_count:
            is_optimizable = True
            optimization_score = max(optimization_score, 90)
            priority = "critical"
            potential_savings = max(potential_savings, monthly_cost)

            recommendations.append({
                "title": "Host Pool Entirement Arrt",
                "description": f"Tous les {session_host_count} session hosts sont arrts. Le host pool gnre des cots de stockage inutiles.",
                "estimated_savings": round(monthly_cost * 0.9, 2),  # Can save 90% (storage remains)
                "actions": [
                    "Dmarrer les session hosts si le host pool est encore utilis",
                    "Supprimer le host pool s'il n'est plus ncessaire",
                    "Configurer auto-start/stop pour optimiser les cots"
                ],
                "priority": "critical",
            })

        # Scenario 2: Zero active sessions 30 days (HIGH - 75)
        # Note: We can't get actual session metrics without Azure Monitor
        # In production, check active session count from monitoring

        # Scenario 3: Session hosts overprovisioned (HIGH - 70)
        if session_host_count > 5 and host_pool_type == 'Pooled':
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            if priority not in ["critical"]:
                priority = "high"

            # Assume 50% of hosts can be removed
            savings = monthly_cost * 0.5
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Session Hosts Potentiellement Surdimensionns",
                "description": f"Ce host pool a {session_host_count} session hosts. Vrifiez si tous sont ncessaires.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Analyser les mtriques d'utilisation dans Azure Monitor",
                    f"Rduire de {session_host_count}  {session_host_count // 2} hosts si charge faible",
                    "Activer l'auto-scaling pour adapter automatiquement",
                    "Considrer le scaling bas sur les heures de bureau"
                ],
                "priority": "high",
            })

        # Scenario 4: Pooled when Personal sufficient (MEDIUM - 50)
        if host_pool_type == 'Pooled' and session_host_count <= 2:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            if priority not in ["critical", "high"]:
                priority = "medium"

            # Personal can be cheaper for small user count
            savings = monthly_cost * 0.2
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Type de Host Pool Potentiellement Inadapt",
                "description": f"Host pool 'Pooled' avec seulement {session_host_count} hosts. Un host pool 'Personal' peut tre plus adapt.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "valuer le nombre d'utilisateurs et leur pattern d'utilisation",
                    "Considrer un host pool 'Personal' si <10 utilisateurs",
                    "Personal offre une exprience plus consistente pour petits groupes",
                    "conomies potentielles: ~20% avec Personal pour usage lger"
                ],
                "priority": "medium",
            })

        # Scenario 5: No auto-scaling configured (LOW - 30)
        # Note: Auto-scaling is configured separately, we can't detect it from host pool properties
        # In production, check if scaling plan exists
        if session_host_count > 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            if priority not in ["critical", "high", "medium"]:
                priority = "low"

            # Savings from auto-scaling (assume 30% reduction during off-hours)
            savings = monthly_cost * 0.3
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Auto-Scaling Non Configur",
                "description": "Ce host pool peut bnficier d'un scaling plan pour adapter la capacit automatiquement.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Crer un scaling plan pour ce host pool",
                    "Configurer scaling bas sur les heures de bureau (8h-18h)",
                    "Rduire automatiquement les hosts pendant les week-ends",
                    "conomies potentielles: ~30% avec auto-scaling optimis"
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_hdinsight_clusters(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure HDInsight Spark/Hadoop clusters for cost intelligence.

        Detection criteria:
        - Cluster stopped/failed (CRITICAL - 90 score)
        - Zero jobs 30 derniers jours (HIGH - 75 score)
        - Worker nodes underutilized (<30% CPU) (HIGH - 70 score)
        - Running 24/7 for batch workloads (MEDIUM - 50 score)
        - No auto-scaling enabled (LOW - 30 score)
        """
        try:
            from azure.mgmt.hdinsight import HDInsightManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-hdinsight not installed")
            return []

        resources = []
        self.logger.info(f"Scanning HDInsight clusters in region: {region}")

        try:
            hdi_client = HDInsightManagementClient(
                credential=self.credential,
                subscription_id=self.subscription_id
            )

            # List all clusters
            async for cluster in hdi_client.clusters.list():
                try:
                    # Filter by region if specified
                    if region.lower() != "all" and cluster.location.lower() != region.lower():
                        continue

                    # Get resource group from cluster ID
                    resource_group = cluster.id.split("/")[4]

                    # Calculate optimization
                    is_optimizable, score, priority, savings, recommendations = (
                        self._calculate_hdinsight_optimization(cluster)
                    )

                    # Pricing (Azure US East 2025)
                    # Head node: D3 v2 (4 vCPU, 14 GB) = $0.21/h = $153/mo x 2 = $307/mo
                    # Worker node: D3 v2 = $0.21/h = $153/mo per node
                    # Storage: Standard $0.05/GB/mo
                    # Typical cluster: 2 head + 4 workers = $922/mo
                    pricing_info = {
                        "head_node_hourly": 0.21,  # D3 v2
                        "head_node_monthly": 153.30,
                        "worker_node_hourly": 0.21,
                        "worker_node_monthly": 153.30,
                        "storage_gb_monthly": 0.05,
                    }

                    # Get cluster configuration
                    cluster_state = getattr(cluster.properties, 'cluster_state', 'Unknown')
                    cluster_version = getattr(cluster.properties, 'cluster_version', 'Unknown')
                    tier = getattr(cluster.properties, 'tier', 'Standard')

                    # Get node counts
                    head_node_count = 2  # Always 2 for HDInsight
                    worker_node_count = 0

                    compute_profile = getattr(cluster.properties, 'compute_profile', None)
                    if compute_profile and hasattr(compute_profile, 'roles'):
                        for role in compute_profile.roles:
                            if role.name == 'workernode':
                                target_instance_count = getattr(role, 'target_instance_count', 0)
                                worker_node_count = target_instance_count

                    # Estimate monthly cost
                    head_cost = pricing_info["head_node_monthly"] * head_node_count
                    worker_cost = pricing_info["worker_node_monthly"] * worker_node_count

                    # Add storage estimate (assume 256 GB per worker node)
                    storage_gb = 256 * max(1, worker_node_count)
                    storage_cost = pricing_info["storage_gb_monthly"] * storage_gb

                    estimated_cost = head_cost + worker_cost + storage_cost

                    # Get cluster type
                    cluster_definition = getattr(cluster.properties, 'cluster_definition', None)
                    kind = 'Hadoop'
                    if cluster_definition and hasattr(cluster_definition, 'kind'):
                        kind = cluster_definition.kind

                    # Determine resource type based on cluster kind
                    # Kafka clusters are specialized streaming platforms, treat them separately
                    is_kafka = kind.lower() == 'kafka'
                    resource_type = "azure_hdinsight_kafka" if is_kafka else "azure_hdinsight_cluster"

                    resources.append(AllCloudResourceData(
                        resource_id=cluster.id,
                        resource_type=resource_type,
                        resource_name=cluster.name or f"Unnamed HDInsight {kind} Cluster",
                        region=cluster.location,
                        estimated_monthly_cost=round(estimated_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "cluster_id": cluster.id,
                            "resource_group": resource_group,
                            "cluster_state": cluster_state,
                            "cluster_version": cluster_version,
                            "tier": tier,
                            "kind": kind,
                            "head_node_count": head_node_count,
                            "worker_node_count": worker_node_count,
                            "storage_gb": storage_gb,
                            "tags": dict(cluster.tags) if cluster.tags else {},
                        },
                        is_optimizable=is_optimizable,
                        optimization_score=score,
                        optimization_priority=priority,
                        potential_monthly_savings=savings,
                        optimization_recommendations=recommendations,
                        last_used_at=None,
                        created_at_cloud=None,
                    ))

                except Exception as e:
                    self.logger.error(f"Error processing HDInsight cluster {getattr(cluster, 'name', 'unknown')}: {str(e)}")
                    continue

            self.logger.info(f"Found {len(resources)} HDInsight clusters in region {region}")
            return resources

        except Exception as e:
            self.logger.error(f"Error scanning HDInsight clusters: {str(e)}")
            return []

    def _calculate_hdinsight_optimization(self, cluster) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate optimization potential for HDInsight cluster.

        Returns:
            (is_optimizable, score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Get cluster properties
        cluster_state = getattr(cluster.properties, 'cluster_state', 'Unknown')
        tier = getattr(cluster.properties, 'tier', 'Standard')

        # Get node counts
        head_node_count = 2
        worker_node_count = 0

        compute_profile = getattr(cluster.properties, 'compute_profile', None)
        if compute_profile and hasattr(compute_profile, 'roles'):
            for role in compute_profile.roles:
                if role.name == 'workernode':
                    worker_node_count = getattr(role, 'target_instance_count', 0)

        # Estimate monthly cost
        head_monthly = 153.30 * head_node_count
        worker_monthly = 153.30 * worker_node_count
        storage_monthly = 0.05 * 256 * max(1, worker_node_count)
        monthly_cost = head_monthly + worker_monthly + storage_monthly

        # Scenario 1: Cluster stopped/failed (CRITICAL - 90)
        if cluster_state.lower() in ['error', 'deleting', 'deleted']:
            is_optimizable = True
            optimization_score = max(optimization_score, 90)
            priority = "critical"
            potential_savings = max(potential_savings, monthly_cost)

            recommendations.append({
                "title": "Cluster en tat d'Erreur",
                "description": f"Ce cluster HDInsight est dans l'tat '{cluster_state}'. Il gnre des cots inutiles.",
                "estimated_savings": round(monthly_cost, 2),
                "actions": [
                    "Vrifier les logs pour identifier le problme",
                    "Supprimer le cluster s'il ne peut pas tre rpar",
                    "Restaurer depuis une configuration sauvegarde si ncessaire"
                ],
                "priority": "critical",
            })

        # Scenario 2: Zero jobs 30 days (HIGH - 75)
        # Note: We can't get actual job metrics without Azure Monitor
        # In production, check job submission history

        # Scenario 3: Worker nodes underutilized (HIGH - 70)
        if worker_node_count > 6:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            if priority not in ["critical"]:
                priority = "high"

            # Assume 50% of worker nodes can be removed
            savings = worker_monthly * 0.5
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Worker Nodes Potentiellement Surdimensionns",
                "description": f"Ce cluster a {worker_node_count} worker nodes. Vrifiez si tous sont ncessaires.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Analyser les mtriques CPU/mmoire dans Azure Monitor",
                    f"Rduire de {worker_node_count}  {worker_node_count // 2} workers si charge <30%",
                    "Activer l'auto-scaling pour adapter automatiquement",
                    "Considrer le scaling bas sur la charge de travail"
                ],
                "priority": "high",
            })

        # Scenario 4: Running 24/7 for batch workloads (MEDIUM - 50)
        if worker_node_count > 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            if priority not in ["critical", "high"]:
                priority = "medium"

            # Assume cluster runs 24/7 but only needed 8h/day
            savings = monthly_cost * 0.67  # Save 16h/day
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Cluster Running 24/7 pour Batch",
                "description": "Ce cluster tourne en permanence. Les workloads batch peuvent tre scheduls.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Identifier si les jobs sont batch ou streaming",
                    "Arrter/dmarrer le cluster selon le schedule des jobs",
                    "Utiliser Azure Data Factory pour orchestrer les pipelines",
                    "conomies potentielles: ~67% en arrtant 16h/jour"
                ],
                "priority": "medium",
            })

        # Scenario 5: No auto-scaling enabled (LOW - 30)
        if worker_node_count > 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            if priority not in ["critical", "high", "medium"]:
                priority = "low"

            # Savings from auto-scaling (assume 25% reduction)
            savings = worker_monthly * 0.25
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Auto-Scaling Non Activ",
                "description": "Ce cluster peut bnficier de l'auto-scaling pour adapter la capacit automatiquement.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Activer l'auto-scaling dans les paramtres du cluster",
                    "Configurer min/max workers selon la charge",
                    "Dfinir des mtriques de scaling (CPU, mmoire, pending tasks)",
                    "conomies potentielles: ~25% avec auto-scaling optimis"
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_ml_compute_instances(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure ML Compute Instances for cost intelligence.

        Detection criteria:
        - Instance stopped but billing (CRITICAL - 90 score)
        - Zero activity 30 derniers jours (HIGH - 75 score)
        - Running but no notebooks active (HIGH - 70 score)
        - GPU instance for CPU workload (MEDIUM - 50 score)
        - No auto-shutdown configured (LOW - 30 score)
        """
        try:
            from azure.mgmt.machinelearningservices import AzureMachineLearningWorkspaces
        except ImportError:
            self.logger.error("azure-mgmt-machinelearningservices not installed")
            return []

        resources = []
        self.logger.info(f"Scanning ML Compute Instances in region: {region}")

        try:
            ml_client = AzureMachineLearningWorkspaces(
                credential=self.credential,
                subscription_id=self.subscription_id
            )

            # List all workspaces first
            async for workspace in ml_client.workspaces.list_by_subscription():
                try:
                    # Filter by region if specified
                    if region.lower() != "all" and workspace.location.lower() != region.lower():
                        continue

                    # Get resource group from workspace ID
                    resource_group = workspace.id.split("/")[4]

                    # List compute instances in this workspace
                    try:
                        compute_instances = list(ml_client.compute.list(
                            resource_group_name=resource_group,
                            workspace_name=workspace.name
                        ))
                    except:
                        continue

                    for compute in compute_instances:
                        try:
                            # Only process ComputeInstance type (not AML clusters)
                            compute_type = getattr(compute.properties, 'compute_type', 'Unknown')
                            if compute_type != 'ComputeInstance':
                                continue

                            # Calculate optimization
                            is_optimizable, score, priority, savings, recommendations = (
                                self._calculate_ml_compute_optimization(compute)
                            )

                            # Pricing (Azure US East 2025)
                            # Standard_DS3_v2 (4 vCPU, 14 GB): $0.21/h = $153/mo
                            # Standard_NC6 (6 vCPU, 56 GB, 1 GPU): $0.90/h = $657/mo
                            # Standard_NC24 (24 vCPU, 224 GB, 4 GPU): $3.60/h = $2628/mo
                            pricing_map = {
                                "Standard_DS3_v2": 153.30,
                                "Standard_DS4_v2": 306.60,
                                "Standard_NC6": 657.00,
                                "Standard_NC12": 1314.00,
                                "Standard_NC24": 2628.00,
                            }

                            # Get VM size
                            vm_size = 'Standard_DS3_v2'
                            compute_properties = getattr(compute.properties, 'properties', None)
                            if compute_properties and hasattr(compute_properties, 'vm_size'):
                                vm_size = compute_properties.vm_size

                            # Estimate monthly cost
                            estimated_cost = pricing_map.get(vm_size, 153.30)

                            # Get instance state
                            provisioning_state = getattr(compute.properties, 'provisioning_state', 'Unknown')
                            state_dict = {}
                            if compute_properties:
                                state = getattr(compute_properties, 'state', 'Unknown')
                                state_dict['state'] = state

                            # Get auto-shutdown settings
                            idle_time_before_shutdown = None
                            if compute_properties and hasattr(compute_properties, 'idle_time_before_shutdown'):
                                idle_time_before_shutdown = compute_properties.idle_time_before_shutdown

                            # Detect if GPU instance
                            is_gpu = 'NC' in vm_size or 'ND' in vm_size or 'NV' in vm_size

                            resources.append(AllCloudResourceData(
                                resource_id=compute.id,
                                resource_type="azure_ml_compute",
                                resource_name=compute.name or "Unnamed ML Compute",
                                region=workspace.location,
                                estimated_monthly_cost=round(estimated_cost, 2),
                                currency="USD",
                                resource_metadata={
                                    "compute_id": compute.id,
                                    "resource_group": resource_group,
                                    "workspace_name": workspace.name,
                                    "provisioning_state": provisioning_state,
                                    "vm_size": vm_size,
                                    "is_gpu": is_gpu,
                                    "idle_time_before_shutdown": idle_time_before_shutdown,
                                    "state": state_dict.get('state', 'Unknown'),
                                    "tags": dict(compute.tags) if compute.tags else {},
                                },
                                is_optimizable=is_optimizable,
                                optimization_score=score,
                                optimization_priority=priority,
                                potential_monthly_savings=savings,
                                optimization_recommendations=recommendations,
                                last_used_at=None,
                                created_at_cloud=None,
                            ))

                        except Exception as e:
                            self.logger.error(f"Error processing ML compute {getattr(compute, 'name', 'unknown')}: {str(e)}")
                            continue

                except Exception as e:
                    self.logger.error(f"Error processing ML workspace {getattr(workspace, 'name', 'unknown')}: {str(e)}")
                    continue

            self.logger.info(f"Found {len(resources)} ML Compute Instances in region {region}")
            return resources

        except Exception as e:
            self.logger.error(f"Error scanning ML Compute Instances: {str(e)}")
            return []

    def _calculate_ml_compute_optimization(self, compute) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate optimization potential for ML Compute Instance.

        Returns:
            (is_optimizable, score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Get compute properties
        provisioning_state = getattr(compute.properties, 'provisioning_state', 'Unknown')
        compute_properties = getattr(compute.properties, 'properties', None)

        vm_size = 'Standard_DS3_v2'
        state = 'Unknown'
        idle_time_before_shutdown = None

        if compute_properties:
            vm_size = getattr(compute_properties, 'vm_size', 'Standard_DS3_v2')
            state = getattr(compute_properties, 'state', 'Unknown')
            idle_time_before_shutdown = getattr(compute_properties, 'idle_time_before_shutdown', None)

        # Pricing map
        pricing_map = {
            "Standard_DS3_v2": 153.30,
            "Standard_DS4_v2": 306.60,
            "Standard_NC6": 657.00,
            "Standard_NC12": 1314.00,
            "Standard_NC24": 2628.00,
        }

        monthly_cost = pricing_map.get(vm_size, 153.30)
        is_gpu = 'NC' in vm_size or 'ND' in vm_size or 'NV' in vm_size

        # Scenario 1: Instance stopped but billing (CRITICAL - 90)
        if provisioning_state.lower() in ['failed', 'deleting', 'deleted']:
            is_optimizable = True
            optimization_score = max(optimization_score, 90)
            priority = "critical"
            potential_savings = max(potential_savings, monthly_cost)

            recommendations.append({
                "title": "Instance en tat d'Erreur",
                "description": f"Cette instance ML est dans l'tat '{provisioning_state}'. Elle gnre des cots inutiles.",
                "estimated_savings": round(monthly_cost, 2),
                "actions": [
                    "Vrifier les logs pour identifier le problme",
                    "Supprimer l'instance si elle ne peut pas tre rpare",
                    "Recrer l'instance si elle est encore ncessaire"
                ],
                "priority": "critical",
            })

        # Scenario 2: Zero activity 30 days (HIGH - 75)
        # Note: We can't get actual usage metrics without Azure Monitor
        # In production, check notebook execution history

        # Scenario 3: Running but no notebooks active (HIGH - 70)
        if state.lower() == 'running':
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            if priority not in ["critical"]:
                priority = "high"

            # Assume instance runs but unused 50% of time
            savings = monthly_cost * 0.5
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Instance Running Sans Activit",
                "description": "Cette instance ML est en cours d'excution. Vrifiez si des notebooks sont actifs.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Vrifier les notebooks actifs dans le workspace",
                    "Arrter l'instance si aucune activit",
                    "Configurer auto-shutdown pour arrter automatiquement",
                    "Utiliser des compute clusters pour workloads batch"
                ],
                "priority": "high",
            })

        # Scenario 4: GPU instance for CPU workload (MEDIUM - 50)
        if is_gpu:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            if priority not in ["critical", "high"]:
                priority = "medium"

            # Savings from switching to CPU instance
            cpu_cost = 153.30  # Standard_DS3_v2
            savings = max(0, monthly_cost - cpu_cost)
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Instance GPU pour Workload CPU",
                "description": f"Instance GPU ({vm_size}) cote {int(monthly_cost)}$/mois. Vrifiez si GPU est ncessaire.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Vrifier si vos notebooks utilisent rellement le GPU",
                    "Passer  Standard_DS3_v2 (CPU) si GPU non utilis",
                    "conomies potentielles: ~{int(savings)}$/mois",
                    "Garder GPU uniquement pour deep learning/training"
                ],
                "priority": "medium",
            })

        # Scenario 5: No auto-shutdown configured (LOW - 30)
        if not idle_time_before_shutdown:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            if priority not in ["critical", "high", "medium"]:
                priority = "low"

            # Savings from auto-shutdown (assume 40% reduction)
            savings = monthly_cost * 0.4
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Auto-Shutdown Non Configur",
                "description": "Cette instance n'a pas d'auto-shutdown. Elle peut tourner inutilement.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Configurer auto-shutdown aprs 30 min d'inactivit",
                    "Paramtrer dans Compute > Settings > Auto-shutdown",
                    "conomies potentielles: ~40% avec auto-shutdown optimis",
                    "Instance redmarre rapidement quand ncessaire"
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_app_services(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure App Services (Web Apps) for cost intelligence.

        IMPORTANT: Excludes Function Apps (already scanned separately).

        Detection criteria:
        - App stopped (CRITICAL - 90 score)
        - Zero requests 30 derniers jours (HIGH - 75 score)
        - Premium/Isolated for dev/test (HIGH - 70 score)
        - Over-provisioned tier vs usage (MEDIUM - 50 score)
        - Always On when not needed (LOW - 30 score)
        """
        try:
            from azure.mgmt.web import WebSiteManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-web not installed")
            return []

        resources = []
        self.logger.info(f"Scanning App Services in region: {region}")

        try:
            web_client = WebSiteManagementClient(
                credential=self.credential,
                subscription_id=self.subscription_id
            )

            # List all web apps
            async for site in web_client.web_apps.list():
                try:
                    # IMPORTANT: Filter OUT Function Apps (already scanned in scan_function_apps)
                    if site.kind and "functionapp" in site.kind.lower():
                        continue

                    # Filter by region if specified
                    if region.lower() != "all" and site.location.lower() != region.lower():
                        continue

                    # Get resource group from site ID
                    resource_group = site.id.split("/")[4]

                    # Calculate optimization
                    is_optimizable, score, priority, savings, recommendations = (
                        self._calculate_app_service_optimization(site)
                    )

                    # Pricing (Azure US East 2025)
                    # Free: $0/mo
                    # Shared: $0/mo (dev/test only)
                    # Basic B1: $13/mo
                    # Standard S1: $73/mo
                    # Premium P1v2: $81/mo
                    # Premium P1v3: $124/mo
                    # Isolated I1: $214/mo
                    pricing_map = {
                        "Free": 0.0,
                        "Shared": 0.0,
                        "Basic": 13.14,
                        "Standard": 73.00,
                        "Premium": 81.03,
                        "PremiumV2": 81.03,
                        "PremiumV3": 124.10,
                        "Isolated": 214.00,
                    }

                    # Get App Service Plan
                    server_farm_id = getattr(site, 'server_farm_id', '')
                    plan_tier = 'Standard'
                    plan_name = 'S1'

                    if server_farm_id:
                        # Extract plan resource group and name from server_farm_id
                        parts = server_farm_id.split('/')
                        if len(parts) >= 9:
                            plan_resource_group = parts[4]
                            plan_name_from_id = parts[8]

                            try:
                                plan = web_client.app_service_plans.get(
                                    resource_group_name=plan_resource_group,
                                    name=plan_name_from_id
                                )
                                if plan and hasattr(plan, 'sku'):
                                    sku = plan.sku
                                    plan_tier = getattr(sku, 'tier', 'Standard')
                                    plan_name = getattr(sku, 'name', 'S1')
                            except:
                                pass

                    # Estimate monthly cost
                    estimated_cost = pricing_map.get(plan_tier, 73.00)

                    # Get app state
                    state = getattr(site, 'state', 'Unknown')
                    enabled = getattr(site, 'enabled', True)

                    # Get site config
                    always_on = False
                    try:
                        site_config = web_client.web_apps.get_configuration(
                            resource_group_name=resource_group,
                            name=site.name
                        )
                        always_on = getattr(site_config, 'always_on', False)
                    except:
                        pass

                    resources.append(AllCloudResourceData(
                        resource_id=site.id,
                        resource_type="azure_app_service",
                        resource_name=site.name or "Unnamed App Service",
                        region=site.location,
                        estimated_monthly_cost=round(estimated_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "app_id": site.id,
                            "resource_group": resource_group,
                            "state": state,
                            "enabled": enabled,
                            "kind": site.kind or "app",
                            "plan_tier": plan_tier,
                            "plan_name": plan_name,
                            "always_on": always_on,
                            "default_host_name": getattr(site, 'default_host_name', ''),
                            "tags": dict(site.tags) if site.tags else {},
                        },
                        is_optimizable=is_optimizable,
                        optimization_score=score,
                        optimization_priority=priority,
                        potential_monthly_savings=savings,
                        optimization_recommendations=recommendations,
                        last_used_at=None,
                        created_at_cloud=None,
                    ))

                except Exception as e:
                    self.logger.error(f"Error processing App Service {getattr(site, 'name', 'unknown')}: {str(e)}")
                    continue

            self.logger.info(f"Found {len(resources)} App Services in region {region}")
            return resources

        except Exception as e:
            self.logger.error(f"Error scanning App Services: {str(e)}")
            return []

    def _calculate_app_service_optimization(self, site) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate optimization potential for App Service.

        Returns:
            (is_optimizable, score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Get site properties
        state = getattr(site, 'state', 'Unknown')
        enabled = getattr(site, 'enabled', True)

        # Get plan info from server_farm_id
        server_farm_id = getattr(site, 'server_farm_id', '')
        plan_tier = 'Standard'

        # Pricing map
        pricing_map = {
            "Free": 0.0,
            "Shared": 0.0,
            "Basic": 13.14,
            "Standard": 73.00,
            "Premium": 81.03,
            "PremiumV2": 81.03,
            "PremiumV3": 124.10,
            "Isolated": 214.00,
        }

        # Try to extract tier from tags or default to Standard
        tags = dict(site.tags) if site.tags else {}
        if 'tier' in tags:
            plan_tier = tags['tier']

        monthly_cost = pricing_map.get(plan_tier, 73.00)

        # Scenario 1: App stopped (CRITICAL - 90)
        if state.lower() == 'stopped' or not enabled:
            is_optimizable = True
            optimization_score = max(optimization_score, 90)
            priority = "critical"
            potential_savings = max(potential_savings, monthly_cost)

            recommendations.append({
                "title": "App Service Arrte",
                "description": f"Cette app est dans l'tat '{state}'. Elle gnre des cots inutiles.",
                "estimated_savings": round(monthly_cost, 2),
                "actions": [
                    "Dmarrer l'app si elle est encore ncessaire",
                    "Supprimer l'app et le plan si plus utilis",
                    "Sauvegarder la configuration avant suppression"
                ],
                "priority": "critical",
            })

        # Scenario 2: Zero requests 30 days (HIGH - 75)
        # Note: We can't get actual request metrics without Azure Monitor
        # In production, check Application Insights metrics

        # Scenario 3: Premium/Isolated for dev/test (HIGH - 70)
        if plan_tier in ['Premium', 'PremiumV2', 'PremiumV3', 'Isolated']:
            # Check if dev/test based on name or tags
            name_lower = site.name.lower() if site.name else ''
            is_dev_test = any(keyword in name_lower for keyword in ['dev', 'test', 'staging', 'qa'])

            if is_dev_test or 'environment' in tags and tags['environment'].lower() in ['dev', 'test', 'staging']:
                is_optimizable = True
                optimization_score = max(optimization_score, 70)
                if priority not in ["critical"]:
                    priority = "high"

                # Savings from downgrading to Basic or Standard
                basic_cost = 13.14
                savings = max(0, monthly_cost - basic_cost)
                potential_savings = max(potential_savings, savings)

                recommendations.append({
                    "title": "Tier Premium pour Environnement Dev/Test",
                    "description": f"App {plan_tier} ({int(monthly_cost)}$/mo) pour dev/test. Basic suffit.",
                    "estimated_savings": round(savings, 2),
                    "actions": [
                        "Passer au tier Basic B1 ($13/mo) pour dev/test",
                        "Garder Premium uniquement pour production",
                        f"conomies potentielles: ~{int(savings)}$/mois",
                        "Performances largement suffisantes pour dev/test"
                    ],
                    "priority": "high",
                })

        # Scenario 4: Over-provisioned tier vs usage (MEDIUM - 50)
        if plan_tier in ['Standard', 'Premium', 'PremiumV2', 'PremiumV3', 'Isolated']:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            if priority not in ["critical", "high"]:
                priority = "medium"

            # Assume can downgrade one tier
            basic_cost = 13.14
            savings = (monthly_cost - basic_cost) * 0.5
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Tier Potentiellement Surdimensionn",
                "description": f"App sur {plan_tier}. Vrifiez si ce tier est ncessaire.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Analyser les mtriques CPU/mmoire dans Azure Monitor",
                    "Passer  un tier infrieur si utilisation <50%",
                    "Tester avec Basic ou Standard si charge faible",
                    "conomies potentielles: ~50% en descendant d'un tier"
                ],
                "priority": "medium",
            })

        # Scenario 5: Always On when not needed (LOW - 30)
        # Note: Always On adds ~10% to cost for keeping instance warm
        # We can't detect it without getting the site config, which we already tried above
        # Skip this scenario as it's low priority and complex to detect

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_redis_caches(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Cache for Redis instances for cost intelligence.

        Detection criteria:
        - Cache stopped/failed (CRITICAL - 90 score)
        - Zero connections 30 derniers jours (HIGH - 75 score)
        - Low cache hit rate <50% (HIGH - 70 score)
        - Premium tier for dev/test (MEDIUM - 50 score)
        - No persistence configured on Premium (LOW - 30 score)
        """
        try:
            from azure.mgmt.redis import RedisManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-redis not installed")
            return []

        resources = []
        self.logger.info(f"Scanning Redis caches in region: {region}")

        try:
            redis_client = RedisManagementClient(
                credential=self.credential,
                subscription_id=self.subscription_id
            )

            # List all Redis caches
            async for cache in redis_client.redis.list():
                try:
                    # Filter by region if specified
                    if region.lower() != "all" and cache.location.lower() != region.lower():
                        continue

                    # Get resource group from cache ID
                    resource_group = cache.id.split("/")[4]

                    # Calculate optimization
                    is_optimizable, score, priority, savings, recommendations = (
                        self._calculate_redis_optimization(cache)
                    )

                    # Pricing (Azure US East 2025)
                    # Basic C0 (250 MB): $16/mo
                    # Basic C1 (1 GB): $55/mo
                    # Standard C0 (250 MB): $32/mo (with replication)
                    # Standard C2 (2.5 GB): $123/mo
                    # Premium P1 (6 GB): $255/mo
                    # Premium P4 (26 GB): $1020/mo
                    pricing_map = {
                        "Basic_C0": 16.24,
                        "Basic_C1": 55.48,
                        "Basic_C2": 110.96,
                        "Standard_C0": 32.48,
                        "Standard_C1": 110.96,
                        "Standard_C2": 123.13,
                        "Standard_C3": 246.26,
                        "Standard_C4": 492.52,
                        "Premium_P1": 255.50,
                        "Premium_P2": 511.00,
                        "Premium_P3": 1022.00,
                        "Premium_P4": 1022.00,
                    }

                    # Get SKU info
                    sku = cache.sku
                    sku_name = getattr(sku, 'name', 'Standard')
                    sku_family = getattr(sku, 'family', 'C')
                    sku_capacity = getattr(sku, 'capacity', 0)

                    # Build pricing key
                    pricing_key = f"{sku_name}_{sku_family}{sku_capacity}"
                    estimated_cost = pricing_map.get(pricing_key, 110.96)  # Default to Standard C1

                    # Get cache properties
                    provisioning_state = getattr(cache, 'provisioning_state', 'Unknown')
                    redis_version = getattr(cache, 'redis_version', 'Unknown')
                    enable_non_ssl_port = getattr(cache, 'enable_non_ssl_port', False)

                    # Get persistence settings (Premium only)
                    redis_configuration = getattr(cache, 'redis_configuration', None)
                    persistence_enabled = False
                    if redis_configuration:
                        rdb_backup_enabled = getattr(redis_configuration, 'rdb_backup_enabled', None)
                        if rdb_backup_enabled:
                            persistence_enabled = True

                    # Get port and hostname
                    port = getattr(cache, 'port', 6379)
                    ssl_port = getattr(cache, 'ssl_port', 6380)
                    host_name = getattr(cache, 'host_name', '')

                    resources.append(AllCloudResourceData(
                        resource_id=cache.id,
                        resource_type="azure_redis_cache",
                        resource_name=cache.name or "Unnamed Redis Cache",
                        region=cache.location,
                        estimated_monthly_cost=round(estimated_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "cache_id": cache.id,
                            "resource_group": resource_group,
                            "provisioning_state": provisioning_state,
                            "sku_name": sku_name,
                            "sku_family": sku_family,
                            "sku_capacity": sku_capacity,
                            "redis_version": redis_version,
                            "enable_non_ssl_port": enable_non_ssl_port,
                            "persistence_enabled": persistence_enabled,
                            "port": port,
                            "ssl_port": ssl_port,
                            "host_name": host_name,
                            "tags": dict(cache.tags) if cache.tags else {},
                        },
                        is_optimizable=is_optimizable,
                        optimization_score=score,
                        optimization_priority=priority,
                        potential_monthly_savings=savings,
                        optimization_recommendations=recommendations,
                        last_used_at=None,
                        created_at_cloud=None,
                    ))

                except Exception as e:
                    self.logger.error(f"Error processing Redis cache {getattr(cache, 'name', 'unknown')}: {str(e)}")
                    continue

            self.logger.info(f"Found {len(resources)} Redis caches in region {region}")
            return resources

        except Exception as e:
            self.logger.error(f"Error scanning Redis caches: {str(e)}")
            return []

    def _calculate_redis_optimization(self, cache) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate optimization potential for Redis cache.

        Returns:
            (is_optimizable, score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Get cache properties
        provisioning_state = getattr(cache, 'provisioning_state', 'Unknown')
        sku = cache.sku
        sku_name = getattr(sku, 'name', 'Standard')
        sku_family = getattr(sku, 'family', 'C')
        sku_capacity = getattr(sku, 'capacity', 0)

        # Pricing map
        pricing_map = {
            "Basic_C0": 16.24,
            "Basic_C1": 55.48,
            "Standard_C0": 32.48,
            "Standard_C1": 110.96,
            "Standard_C2": 123.13,
            "Premium_P1": 255.50,
            "Premium_P2": 511.00,
            "Premium_P3": 1022.00,
            "Premium_P4": 1022.00,
        }

        pricing_key = f"{sku_name}_{sku_family}{sku_capacity}"
        monthly_cost = pricing_map.get(pricing_key, 110.96)

        # Get persistence settings
        redis_configuration = getattr(cache, 'redis_configuration', None)
        persistence_enabled = False
        if redis_configuration:
            rdb_backup_enabled = getattr(redis_configuration, 'rdb_backup_enabled', None)
            if rdb_backup_enabled:
                persistence_enabled = True

        # Scenario 1: Cache stopped/failed (CRITICAL - 90)
        if provisioning_state.lower() in ['failed', 'deleting', 'deleted', 'disabled']:
            is_optimizable = True
            optimization_score = max(optimization_score, 90)
            priority = "critical"
            potential_savings = max(potential_savings, monthly_cost)

            recommendations.append({
                "title": "Redis Cache en tat d'Erreur",
                "description": f"Ce cache Redis est dans l'tat '{provisioning_state}'. Il gnre des cots inutiles.",
                "estimated_savings": round(monthly_cost, 2),
                "actions": [
                    "Vrifier les logs pour identifier le problme",
                    "Supprimer le cache s'il ne peut pas tre rpar",
                    "Recrer le cache si encore ncessaire"
                ],
                "priority": "critical",
            })

        # Scenario 2: Zero connections 30 days (HIGH - 75)
        # Note: We can't get actual connection metrics without Azure Monitor
        # In production, check connection count from monitoring

        # Scenario 3: Low cache hit rate <50% (HIGH - 70)
        # Note: We can't get cache hit rate without Azure Monitor
        # In production, check cache hit/miss ratio

        # Scenario 4: Premium tier for dev/test (MEDIUM - 50)
        if sku_name == 'Premium':
            # Check if dev/test based on name or tags
            name_lower = cache.name.lower() if cache.name else ''
            tags = dict(cache.tags) if cache.tags else {}
            is_dev_test = any(keyword in name_lower for keyword in ['dev', 'test', 'staging', 'qa'])

            if is_dev_test or 'environment' in tags and tags['environment'].lower() in ['dev', 'test', 'staging']:
                is_optimizable = True
                optimization_score = max(optimization_score, 50)
                if priority not in ["critical", "high"]:
                    priority = "medium"

                # Savings from downgrading to Standard
                standard_cost = 110.96  # Standard C1
                savings = max(0, monthly_cost - standard_cost)
                potential_savings = max(potential_savings, savings)

                recommendations.append({
                    "title": "Tier Premium pour Environnement Dev/Test",
                    "description": f"Cache Premium ({int(monthly_cost)}$/mo) pour dev/test. Standard suffit.",
                    "estimated_savings": round(savings, 2),
                    "actions": [
                        "Passer au tier Standard pour dev/test",
                        "Garder Premium uniquement pour production",
                        f"conomies potentielles: ~{int(savings)}$/mois",
                        "Standard offre performances suffisantes pour dev/test"
                    ],
                    "priority": "medium",
                })

        # Scenario 5: No persistence on Premium (LOW - 30)
        if sku_name == 'Premium' and not persistence_enabled:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            if priority not in ["critical", "high", "medium"]:
                priority = "low"

            # Savings: none directly, but best practice recommendation
            savings = 0
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Persistence Non Configure sur Premium",
                "description": "Cache Premium sans persistence. Risque de perte de donnes en cas de redmarrage.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Activer RDB persistence dans les paramtres",
                    "Configurer backup automatique quotidien/hebdomadaire",
                    "Protger contre la perte de donnes",
                    "Cot de persistence: ~5% du prix du cache"
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_event_hubs(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Event Hubs namespaces for cost intelligence.

        Detection criteria:
        - Namespace inactive/failed (CRITICAL - 90 score)
        - Zero incoming messages 30 derniers jours (HIGH - 75 score)
        - Throughput units overprovisioned (HIGH - 70 score)
        - Premium for low-volume workload (MEDIUM - 50 score)
        - Auto-inflate disabled (LOW - 30 score)
        """
        try:
            from azure.mgmt.eventhub import EventHubManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-eventhub not installed")
            return []

        resources = []
        self.logger.info(f"Scanning Event Hubs namespaces in region: {region}")

        try:
            eh_client = EventHubManagementClient(
                credential=self.credential,
                subscription_id=self.subscription_id
            )

            # List all Event Hub namespaces
            async for namespace in eh_client.namespaces.list():
                try:
                    # Filter by region if specified
                    if region.lower() != "all" and namespace.location.lower() != region.lower():
                        continue

                    # Get resource group from namespace ID
                    resource_group = namespace.id.split("/")[4]

                    # Get Event Hubs count
                    event_hubs_count = 0
                    try:
                        event_hubs = list(eh_client.event_hubs.list_by_namespace(
                            resource_group_name=resource_group,
                            namespace_name=namespace.name
                        ))
                        event_hubs_count = len(event_hubs)
                    except:
                        pass

                    # Calculate optimization
                    is_optimizable, score, priority, savings, recommendations = (
                        self._calculate_event_hub_optimization(namespace, event_hubs_count)
                    )

                    # Pricing (Azure US East 2025)
                    # Basic: $11.36/mo (1 throughput unit, 1M events)
                    # Standard: $22.72/mo per throughput unit (1M events, 1 day retention)
                    # Premium: $673/mo per processing unit (100M events, 7 days retention)
                    # Dedicated: Custom pricing (1 CU = ~$8000/mo)
                    pricing_map = {
                        "Basic": 11.36,
                        "Standard": 22.72,  # per TU
                        "Premium": 673.00,  # per PU
                    }

                    # Get SKU info
                    sku = namespace.sku
                    sku_name = getattr(sku, 'name', 'Standard')
                    sku_tier = getattr(sku, 'tier', 'Standard')
                    sku_capacity = getattr(sku, 'capacity', 1)

                    # Estimate monthly cost
                    base_price = pricing_map.get(sku_name, 22.72)
                    estimated_cost = base_price * sku_capacity

                    # Get namespace properties
                    provisioning_state = getattr(namespace, 'provisioning_state', 'Unknown')
                    status = getattr(namespace, 'status', 'Unknown')
                    is_auto_inflate_enabled = getattr(namespace, 'is_auto_inflate_enabled', False)
                    maximum_throughput_units = getattr(namespace, 'maximum_throughput_units', 0)

                    # Get Kafka and zone redundancy settings
                    kafka_enabled = getattr(namespace, 'kafka_enabled', False)
                    zone_redundant = getattr(namespace, 'zone_redundant', False)

                    resources.append(AllCloudResourceData(
                        resource_id=namespace.id,
                        resource_type="azure_event_hub",
                        resource_name=namespace.name or "Unnamed Event Hub Namespace",
                        region=namespace.location,
                        estimated_monthly_cost=round(estimated_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "namespace_id": namespace.id,
                            "resource_group": resource_group,
                            "provisioning_state": provisioning_state,
                            "status": status,
                            "sku_name": sku_name,
                            "sku_tier": sku_tier,
                            "sku_capacity": sku_capacity,
                            "is_auto_inflate_enabled": is_auto_inflate_enabled,
                            "maximum_throughput_units": maximum_throughput_units,
                            "kafka_enabled": kafka_enabled,
                            "zone_redundant": zone_redundant,
                            "event_hubs_count": event_hubs_count,
                            "tags": dict(namespace.tags) if namespace.tags else {},
                        },
                        is_optimizable=is_optimizable,
                        optimization_score=score,
                        optimization_priority=priority,
                        potential_monthly_savings=savings,
                        optimization_recommendations=recommendations,
                        last_used_at=None,
                        created_at_cloud=None,
                    ))

                except Exception as e:
                    self.logger.error(f"Error processing Event Hub namespace {getattr(namespace, 'name', 'unknown')}: {str(e)}")
                    continue

            self.logger.info(f"Found {len(resources)} Event Hub namespaces in region {region}")
            return resources

        except Exception as e:
            self.logger.error(f"Error scanning Event Hub namespaces: {str(e)}")
            return []

    def _calculate_event_hub_optimization(self, namespace, event_hubs_count: int) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate optimization potential for Event Hub namespace.

        Returns:
            (is_optimizable, score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Get namespace properties
        provisioning_state = getattr(namespace, 'provisioning_state', 'Unknown')
        status = getattr(namespace, 'status', 'Unknown')
        sku = namespace.sku
        sku_name = getattr(sku, 'name', 'Standard')
        sku_capacity = getattr(sku, 'capacity', 1)
        is_auto_inflate_enabled = getattr(namespace, 'is_auto_inflate_enabled', False)

        # Pricing map
        pricing_map = {
            "Basic": 11.36,
            "Standard": 22.72,
            "Premium": 673.00,
        }

        base_price = pricing_map.get(sku_name, 22.72)
        monthly_cost = base_price * sku_capacity

        # Scenario 1: Namespace inactive/failed (CRITICAL - 90)
        if provisioning_state.lower() in ['failed', 'deleting', 'deleted'] or status.lower() in ['disabled', 'restoring', 'unknown']:
            is_optimizable = True
            optimization_score = max(optimization_score, 90)
            priority = "critical"
            potential_savings = max(potential_savings, monthly_cost)

            recommendations.append({
                "title": "Event Hub Namespace Non Fonctionnel",
                "description": f"Ce namespace est dans l'tat '{provisioning_state}/{status}'. Il gnre des cots inutiles.",
                "estimated_savings": round(monthly_cost, 2),
                "actions": [
                    "Vrifier les logs pour identifier le problme",
                    "Supprimer le namespace s'il ne peut pas tre rpar",
                    "Recrer le namespace si encore ncessaire"
                ],
                "priority": "critical",
            })

        # Scenario 2: Zero incoming messages 30 days (HIGH - 75)
        # Note: We can't get actual message metrics without Azure Monitor
        # In production, check incoming messages/bytes from monitoring

        # Scenario 3: Throughput units overprovisioned (HIGH - 70)
        if sku_name == 'Standard' and sku_capacity > 3:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            if priority not in ["critical"]:
                priority = "high"

            # Assume can reduce by 50%
            savings = monthly_cost * 0.5
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Throughput Units Potentiellement Surdimensionns",
                "description": f"Ce namespace a {sku_capacity} throughput units. Vrifiez si tous sont ncessaires.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Analyser les mtriques de throughput dans Azure Monitor",
                    f"Rduire de {sku_capacity}  {sku_capacity // 2} TUs si charge <50%",
                    "Activer auto-inflate pour adapter automatiquement",
                    "conomies potentielles: ~50% en rduisant les TUs"
                ],
                "priority": "high",
            })

        # Scenario 4: Premium for low-volume (MEDIUM - 50)
        if sku_name == 'Premium' and event_hubs_count <= 2:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            if priority not in ["critical", "high"]:
                priority = "medium"

            # Savings from downgrading to Standard
            standard_cost = 22.72 * 2  # 2 TUs
            savings = max(0, monthly_cost - standard_cost)
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Tier Premium pour Faible Volume",
                "description": f"Namespace Premium ({int(monthly_cost)}$/mo) avec seulement {event_hubs_count} Event Hubs.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Passer au tier Standard si volume <100M events/mois",
                    "Garder Premium uniquement pour haute performance",
                    f"conomies potentielles: ~{int(savings)}$/mois",
                    "Standard suffit pour la plupart des workloads"
                ],
                "priority": "medium",
            })

        # Scenario 5: Auto-inflate disabled (LOW - 30)
        if sku_name == 'Standard' and not is_auto_inflate_enabled and sku_capacity < 10:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            if priority not in ["critical", "high", "medium"]:
                priority = "low"

            # Savings from auto-inflate (reduce base capacity, scale on demand)
            savings = monthly_cost * 0.2
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Auto-Inflate Non Activ",
                "description": "L'auto-inflate permet d'adapter automatiquement la capacit selon la charge.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Activer auto-inflate dans les paramtres du namespace",
                    "Configurer max throughput units (ex: 10-20)",
                    "Rduire la capacit de base et laisser auto-scale",
                    "conomies potentielles: ~20% en adaptant  la charge"
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_netapp_files(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure NetApp Files capacity pools for cost intelligence.

        Detection criteria:
        - Pool/volume not mounted (CRITICAL - 90 score)
        - Zero IOPS 30 derniers jours (HIGH - 75 score)
        - Ultra tier for standard workload (HIGH - 70 score)
        - Overprovisioned capacity >50% unused (MEDIUM - 50 score)
        - No snapshots configured (LOW - 30 score)
        """
        try:
            from azure.mgmt.netappfiles import NetAppManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-netappfiles not installed")
            return []

        resources = []
        self.logger.info(f"Scanning NetApp Files capacity pools in region: {region}")

        try:
            netapp_client = NetAppManagementClient(
                credential=self.credential,
                subscription_id=self.subscription_id
            )

            # List all NetApp accounts
            async for account in netapp_client.accounts.list():
                try:
                    # Filter by region if specified
                    if region.lower() != "all" and account.location.lower() != region.lower():
                        continue

                    # Get resource group from account ID
                    resource_group = account.id.split("/")[4]

                    # List capacity pools in this account
                    try:
                        pools = list(netapp_client.pools.list(
                            resource_group_name=resource_group,
                            account_name=account.name
                        ))
                    except:
                        pools = []

                    for pool in pools:
                        try:
                            # Get volumes in this pool
                            volumes_count = 0
                            try:
                                volumes = list(netapp_client.volumes.list(
                                    resource_group_name=resource_group,
                                    account_name=account.name,
                                    pool_name=pool.name
                                ))
                                volumes_count = len(volumes)
                            except:
                                pass

                            # Calculate optimization
                            is_optimizable, score, priority, savings, recommendations = (
                                self._calculate_netapp_optimization(pool, volumes_count)
                            )

                            # Pricing (Azure US East 2025)
                            # Standard: $0.000202/GB/hour = $147.50/TB/mo
                            # Premium: $0.000403/GB/hour = $294.19/TB/mo
                            # Ultra: $0.000538/GB/hour = $392.54/TB/mo
                            pricing_map = {
                                "Standard": 0.000202,  # per GB/hour
                                "Premium": 0.000403,
                                "Ultra": 0.000538,
                            }

                            # Get pool properties
                            service_level = getattr(pool, 'service_level', 'Standard')
                            size_bytes = getattr(pool, 'size', 0)
                            size_gb = size_bytes / (1024 ** 3) if size_bytes else 0

                            # Calculate monthly cost (730 hours per month)
                            hourly_rate = pricing_map.get(service_level, 0.000202)
                            estimated_cost = size_gb * hourly_rate * 730

                            # Get pool state
                            provisioning_state = getattr(pool, 'provisioning_state', 'Unknown')

                            # Get QoS type
                            qos_type = getattr(pool, 'qos_type', 'Auto')

                            resources.append(AllCloudResourceData(
                                resource_id=pool.id,
                                resource_type="azure_netapp_files",
                                resource_name=f"{account.name}/{pool.name}",
                                region=account.location,
                                estimated_monthly_cost=round(estimated_cost, 2),
                                currency="USD",
                                resource_metadata={
                                    "pool_id": pool.id,
                                    "account_name": account.name,
                                    "pool_name": pool.name,
                                    "resource_group": resource_group,
                                    "provisioning_state": provisioning_state,
                                    "service_level": service_level,
                                    "size_gb": round(size_gb, 2),
                                    "size_tb": round(size_gb / 1024, 2),
                                    "qos_type": qos_type,
                                    "volumes_count": volumes_count,
                                    "tags": dict(pool.tags) if pool.tags else {},
                                },
                                is_optimizable=is_optimizable,
                                optimization_score=score,
                                optimization_priority=priority,
                                potential_monthly_savings=savings,
                                optimization_recommendations=recommendations,
                                last_used_at=None,
                                created_at_cloud=None,
                            ))

                        except Exception as e:
                            self.logger.error(f"Error processing NetApp pool {getattr(pool, 'name', 'unknown')}: {str(e)}")
                            continue

                except Exception as e:
                    self.logger.error(f"Error processing NetApp account {getattr(account, 'name', 'unknown')}: {str(e)}")
                    continue

            self.logger.info(f"Found {len(resources)} NetApp Files capacity pools in region {region}")
            return resources

        except Exception as e:
            self.logger.error(f"Error scanning NetApp Files: {str(e)}")
            return []

    def _calculate_netapp_optimization(self, pool, volumes_count: int) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate optimization potential for NetApp Files capacity pool.

        Returns:
            (is_optimizable, score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Get pool properties
        provisioning_state = getattr(pool, 'provisioning_state', 'Unknown')
        service_level = getattr(pool, 'service_level', 'Standard')
        size_bytes = getattr(pool, 'size', 0)
        size_gb = size_bytes / (1024 ** 3) if size_bytes else 0
        size_tb = size_gb / 1024

        # Pricing map (per GB/hour)
        pricing_map = {
            "Standard": 0.000202,
            "Premium": 0.000403,
            "Ultra": 0.000538,
        }

        hourly_rate = pricing_map.get(service_level, 0.000202)
        monthly_cost = size_gb * hourly_rate * 730

        # Scenario 1: Pool not mounted / no volumes (CRITICAL - 90)
        if volumes_count == 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 90)
            priority = "critical"
            potential_savings = max(potential_savings, monthly_cost)

            recommendations.append({
                "title": "Capacity Pool Sans Volumes",
                "description": f"Ce pool NetApp ({size_tb:.2f} TB) n'a aucun volume. Il gnre des cots inutiles.",
                "estimated_savings": round(monthly_cost, 2),
                "actions": [
                    "Crer des volumes si le pool est encore ncessaire",
                    "Supprimer le pool s'il n'est plus utilis",
                    f"conomies potentielles: {int(monthly_cost)}$/mois"
                ],
                "priority": "critical",
            })

        # Scenario 2: Zero IOPS 30 days (HIGH - 75)
        # Note: We can't get actual IOPS metrics without Azure Monitor
        # In production, check IOPS/throughput from monitoring

        # Scenario 3: Ultra tier for standard workload (HIGH - 70)
        if service_level == 'Ultra':
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            if priority not in ["critical"]:
                priority = "high"

            # Savings from downgrading to Premium or Standard
            premium_cost = size_gb * 0.000403 * 730
            savings = max(0, monthly_cost - premium_cost)
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Tier Ultra pour Workload Standard",
                "description": f"Pool Ultra ({int(monthly_cost)}$/mo pour {size_tb:.2f} TB). Vrifiez si Ultra est ncessaire.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Analyser les besoins IOPS/throughput rels",
                    "Passer  Premium si <64MB/s ou <4000 IOPS/TB",
                    "Passer  Standard si <16MB/s ou <1000 IOPS/TB",
                    f"conomies potentielles: ~{int(savings)}$/mois avec Premium"
                ],
                "priority": "high",
            })

        # Scenario 4: Overprovisioned capacity (MEDIUM - 50)
        if size_tb > 4:
            # Assume pool is overprovisioned if >4TB
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            if priority not in ["critical", "high"]:
                priority = "medium"

            # Assume 50% overprovisioned
            savings = monthly_cost * 0.5
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Capacit Potentiellement Surdimensionne",
                "description": f"Pool de {size_tb:.2f} TB. Vrifiez l'utilisation relle des volumes.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Analyser l'utilisation actuelle des volumes",
                    "Rduire la taille du pool si usage <50%",
                    "Taille minimum: 4 TB par pool",
                    "conomies potentielles: ~50% en rduisant la capacit"
                ],
                "priority": "medium",
            })

        # Scenario 5: No snapshots configured (LOW - 30)
        # Note: We can't easily detect snapshot policies without additional API calls
        # This is a best practice recommendation
        if volumes_count > 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            if priority not in ["critical", "high", "medium"]:
                priority = "low"

            # No direct savings, but best practice
            savings = 0
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Snapshots Non Configurs",
                "description": "Aucune politique de snapshot dtecte. Risque de perte de donnes.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Configurer une snapshot policy sur les volumes",
                    "Schedule recommand: hourly, daily, weekly",
                    "Snapshots consomment de la capacit du pool",
                    "Cot des snapshots: inclus dans la capacit du pool"
                ],
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_cognitive_search(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Cognitive Search services for cost intelligence.

        Detection criteria:
        - Service failed/stopped (CRITICAL - 90 score)
        - Zero queries 30 derniers jours (HIGH - 75 score)
        - Overprovisioned replicas (HIGH - 70 score)
        - Standard tier for low query volume (MEDIUM - 50 score)
        - No indexing activity 90 days (LOW - 30 score)

        Args:
            region: Azure region

        Returns:
            List of Azure Cognitive Search services with optimization analysis
        """
        resources = []

        try:
            from azure.mgmt.search import SearchManagementClient

            search_client = SearchManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing map per tier per month (approximate)
            pricing_map = {
                "free": 0.0,
                "basic": 75.0,  # Basic tier
                "standard": 250.0,  # Standard S1
                "standard2": 1000.0,  # Standard S2
                "standard3": 2400.0,  # Standard S3
                "storage_optimized_l1": 1500.0,  # Storage Optimized L1
                "storage_optimized_l2": 3000.0,  # Storage Optimized L2
            }

            # List all Search services
            search_services = list(search_client.services.list_by_subscription())
            logger.info(
                "inventory.azure.cognitive_search.found",
                region=region,
                count=len(search_services),
            )

            for service in search_services:
                try:
                    # Filter by region
                    if service.location != region:
                        continue

                    service_name = service.name or "Unknown"
                    resource_group = service.id.split("/")[4] if service.id else "Unknown"

                    # Get SKU tier
                    sku_name = service.sku.name.lower() if service.sku and service.sku.name else "unknown"

                    # Get replica and partition count
                    replica_count = getattr(service, "replica_count", 1)
                    partition_count = getattr(service, "partition_count", 1)

                    # Get provisioning state
                    provisioning_state = getattr(service, "provisioning_state", "Unknown")
                    status = getattr(service, "status", "Unknown")

                    # Get search units (replicas  partitions)
                    search_units = replica_count * partition_count

                    # Calculate monthly cost
                    base_cost = pricing_map.get(sku_name, 250.0)
                    monthly_cost = base_cost * search_units

                    # TODO: Get actual metrics from Azure Monitor (queries, indexing operations)
                    # For MVP, use placeholder metrics
                    query_count_30d = 0  # Placeholder
                    indexing_operations_90d = 0  # Placeholder

                    # Calculate optimization potential
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_cognitive_search_optimization(
                        provisioning_state=provisioning_state,
                        status=status,
                        sku_name=sku_name,
                        replica_count=replica_count,
                        partition_count=partition_count,
                        query_count_30d=query_count_30d,
                        indexing_operations_90d=indexing_operations_90d,
                        monthly_cost=monthly_cost,
                    )

                    # Build metadata
                    metadata = {
                        "service_name": service_name,
                        "resource_group": resource_group,
                        "sku": sku_name,
                        "replica_count": replica_count,
                        "partition_count": partition_count,
                        "search_units": search_units,
                        "provisioning_state": provisioning_state,
                        "status": status,
                        "public_network_access": getattr(service, "public_network_access", "Unknown"),
                        "hosting_mode": getattr(service, "hosting_mode", "default"),
                        "optimization_details": recommendations,
                    }

                    resource = AllCloudResourceData(
                        resource_type="azure_cognitive_search",
                        resource_id=service.id or f"search-{service_name}",
                        resource_name=service_name,
                        region=region,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata=metadata,
                        is_optimizable=is_optimizable,
                        optimization_priority=priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations.get("actions", []),
                    )

                    resources.append(resource)
                    logger.info(
                        "inventory.azure.cognitive_search.processed",
                        service_name=service_name,
                        sku=sku_name,
                        cost=monthly_cost,
                        optimizable=is_optimizable,
                    )

                except Exception as e:
                    logger.error(
                        "inventory.azure.cognitive_search.error",
                        service=service.name if hasattr(service, "name") else "Unknown",
                        error=str(e),
                    )
                    continue

        except Exception as e:
            logger.error("inventory.azure.cognitive_search.scan_error", region=region, error=str(e))

        return resources

    def _calculate_cognitive_search_optimization(
        self,
        provisioning_state: str,
        status: str,
        sku_name: str,
        replica_count: int,
        partition_count: int,
        query_count_30d: int,
        indexing_operations_90d: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """Calculate optimization potential for Cognitive Search service."""
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = {"actions": [], "estimated_savings": 0.0, "priority": "low"}

        # CRITICAL (90 score): Service failed or stopped
        if provisioning_state.lower() in ["failed", "deleting"] or status.lower() in [
            "degraded",
            "disabled",
        ]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"Service en tat '{provisioning_state}' - investigate et rparez ou supprimez",
                    "Service non oprationnel - cot 100% vitable",
                    "Action: Vrifier les logs Azure, rparer ou supprimer le service",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "critical",
            })

        # HIGH (75 score): Zero queries in 30 days
        elif query_count_30d == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    "Aucune requte dtecte depuis 30 jours",
                    "Service potentiellement inutilis - considrer la suppression",
                    "Action: Vrifier si le service est encore ncessaire",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # HIGH (70 score): Overprovisioned replicas (>1 replica, low query volume)
        elif replica_count > 1 and query_count_30d < 10000:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Savings = cost of extra replicas
            savings = monthly_cost * ((replica_count - 1) / (replica_count * partition_count))
            potential_savings = savings
            recommendations.update({
                "actions": [
                    f"Replicas surprovisionns: {replica_count} replicas pour faible volume",
                    f"Volume de requtes: {query_count_30d} sur 30 jours",
                    "Action: Rduire  1 replica pour conomiser",
                    f"conomies estimes: ${savings:.2f}/mois",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # MEDIUM (50 score): Standard tier for low query volume
        elif sku_name in ["standard", "standard2", "standard3"] and query_count_30d < 1000:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Savings = downgrade to Basic
            savings = monthly_cost - 75.0  # Basic tier cost
            potential_savings = max(savings, 0.0)
            recommendations.update({
                "actions": [
                    f"Tier Standard pour faible volume: {query_count_30d} queries/30j",
                    f"Cot actuel: ${monthly_cost:.2f}/mois ({sku_name})",
                    "Action: Downgrade vers Basic tier ($75/mois)",
                    f"conomies estimes: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "medium",
            })

        # LOW (30 score): No indexing activity in 90 days
        elif indexing_operations_90d == 0 and monthly_cost > 0:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            savings = monthly_cost * 0.2  # 20% potential savings
            potential_savings = savings
            recommendations.update({
                "actions": [
                    "Aucune opration d'indexation depuis 90 jours",
                    "Index potentiellement obsoltes ou statiques",
                    "Action: Vrifier si le service est encore utilis",
                    f"conomies potentielles: ${savings:.2f}/mois si supprim",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_api_management(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure API Management services for cost intelligence.

        Detection criteria:
        - Service failed/stopped (CRITICAL - 90 score)
        - Zero API calls 30 derniers jours (HIGH - 75 score)
        - Premium/Standard tier for low volume (HIGH - 70 score)
        - Multiple gateways underutilized (MEDIUM - 50 score)
        - Developer tier in production (LOW - 30 score)

        Args:
            region: Azure region

        Returns:
            List of Azure API Management services with optimization analysis
        """
        resources = []

        try:
            from azure.mgmt.apimanagement import ApiManagementClient

            apim_client = ApiManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing map per tier per month (approximate)
            pricing_map = {
                "consumption": 0.0,  # Pay per call: $0.04/10K calls
                "developer": 50.0,  # Developer tier (no SLA)
                "basic": 150.0,  # Basic tier
                "standard": 700.0,  # Standard tier
                "premium": 2795.0,  # Premium tier (per unit)
            }

            # List all API Management services
            apim_services = list(apim_client.api_management_service.list_by_subscription())
            logger.info(
                "inventory.azure.api_management.found",
                region=region,
                count=len(apim_services),
            )

            for service in apim_services:
                try:
                    # Filter by region
                    if service.location != region:
                        continue

                    service_name = service.name or "Unknown"
                    resource_group = service.id.split("/")[4] if service.id else "Unknown"

                    # Get SKU tier
                    sku_name = service.sku.name.lower() if service.sku and service.sku.name else "unknown"
                    sku_capacity = service.sku.capacity if service.sku and service.sku.capacity else 1

                    # Get provisioning state
                    provisioning_state = getattr(service, "provisioning_state", "Unknown")

                    # Get gateway count (self-hosted gateways)
                    gateway_count = 0  # TODO: Query self-hosted gateways via API

                    # Calculate monthly cost
                    base_cost = pricing_map.get(sku_name, 700.0)
                    if sku_name == "consumption":
                        # Consumption tier: $0.04 per 10K calls (estimate 100K calls/month)
                        monthly_cost = (100_000 / 10_000) * 0.04
                    else:
                        monthly_cost = base_cost * sku_capacity

                    # TODO: Get actual metrics from Azure Monitor (API calls, request count)
                    # For MVP, use placeholder metrics
                    api_calls_30d = 0  # Placeholder
                    request_count_30d = 0  # Placeholder

                    # Calculate optimization potential
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_api_management_optimization(
                        provisioning_state=provisioning_state,
                        sku_name=sku_name,
                        sku_capacity=sku_capacity,
                        api_calls_30d=api_calls_30d,
                        request_count_30d=request_count_30d,
                        gateway_count=gateway_count,
                        monthly_cost=monthly_cost,
                    )

                    # Build metadata
                    metadata = {
                        "service_name": service_name,
                        "resource_group": resource_group,
                        "sku": sku_name,
                        "sku_capacity": sku_capacity,
                        "provisioning_state": provisioning_state,
                        "publisher_email": getattr(service, "publisher_email", "Unknown"),
                        "publisher_name": getattr(service, "publisher_name", "Unknown"),
                        "gateway_url": getattr(service, "gateway_url", "Unknown"),
                        "portal_url": getattr(service, "portal_url", "Unknown"),
                        "optimization_details": recommendations,
                    }

                    resource = AllCloudResourceData(
                        resource_type="azure_api_management",
                        resource_id=service.id or f"apim-{service_name}",
                        resource_name=service_name,
                        region=region,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata=metadata,
                        is_optimizable=is_optimizable,
                        optimization_priority=priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations.get("actions", []),
                    )

                    resources.append(resource)
                    logger.info(
                        "inventory.azure.api_management.processed",
                        service_name=service_name,
                        sku=sku_name,
                        cost=monthly_cost,
                        optimizable=is_optimizable,
                    )

                except Exception as e:
                    logger.error(
                        "inventory.azure.api_management.error",
                        service=service.name if hasattr(service, "name") else "Unknown",
                        error=str(e),
                    )
                    continue

        except Exception as e:
            logger.error("inventory.azure.api_management.scan_error", region=region, error=str(e))

        return resources

    def _calculate_api_management_optimization(
        self,
        provisioning_state: str,
        sku_name: str,
        sku_capacity: int,
        api_calls_30d: int,
        request_count_30d: int,
        gateway_count: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """Calculate optimization potential for API Management service."""
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = {"actions": [], "estimated_savings": 0.0, "priority": "low"}

        # CRITICAL (90 score): Service failed or stopped
        if provisioning_state.lower() in ["failed", "deleting", "deleted"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"Service en tat '{provisioning_state}' - investigate et rparez ou supprimez",
                    "Service non oprationnel - cot 100% vitable",
                    "Action: Vrifier les logs Azure, rparer ou supprimer le service",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "critical",
            })

        # HIGH (75 score): Zero API calls in 30 days
        elif api_calls_30d == 0 and request_count_30d == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    "Aucun appel API dtect depuis 30 jours",
                    "Service potentiellement inutilis - considrer la suppression",
                    "Action: Vrifier si le service est encore ncessaire",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # HIGH (70 score): Premium/Standard tier for low volume (<10K calls/day)
        elif sku_name in ["premium", "standard"] and api_calls_30d < 300000:  # <10K/day
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Savings = downgrade to Basic or Consumption
            if sku_name == "premium":
                savings = monthly_cost - 150.0  # Downgrade to Basic
            else:  # standard
                savings = monthly_cost - 150.0  # Downgrade to Basic
            potential_savings = max(savings, 0.0)
            recommendations.update({
                "actions": [
                    f"Tier {sku_name.capitalize()} pour faible volume: {api_calls_30d} calls/30j",
                    f"Cot actuel: ${monthly_cost:.2f}/mois",
                    "Action: Downgrade vers Basic ($150/mois) ou Consumption (pay-per-call)",
                    f"conomies estimes: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # MEDIUM (50 score): Multiple gateways underutilized
        elif gateway_count > 1 and api_calls_30d < 100000:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            savings = monthly_cost * 0.3  # 30% potential savings
            potential_savings = savings
            recommendations.update({
                "actions": [
                    f"Multiples gateways ({gateway_count}) pour faible volume",
                    f"Volume: {api_calls_30d} calls/30 jours",
                    "Action: Consolider vers un seul gateway",
                    f"conomies estimes: ${savings:.2f}/mois",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "medium",
            })

        # LOW (30 score): Developer tier in production (no SLA)
        elif sku_name == "developer" and monthly_cost > 0:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            # No direct savings, but best practice
            savings = 0
            potential_savings = savings
            recommendations.update({
                "actions": [
                    "Developer tier utilis (pas de SLA)",
                    "Tier Developer destin au dveloppement, pas  la production",
                    "Action: Upgrade vers Basic ou Standard pour SLA",
                    "Note: Upgrade cote plus, mais garantit SLA et stabilit",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_cdn(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure CDN profiles and endpoints for cost intelligence.

        Detection criteria:
        - Endpoint stopped/disabled (CRITICAL - 90 score)
        - Zero bandwidth 30 derniers jours (HIGH - 75 score)
        - Zero requests 30 derniers jours (HIGH - 70 score)
        - Premium tier for low traffic (MEDIUM - 50 score)
        - No caching rules configured (LOW - 30 score)

        Args:
            region: Azure region

        Returns:
            List of Azure CDN profiles with optimization analysis
        """
        resources = []

        try:
            from azure.mgmt.cdn import CdnManagementClient

            cdn_client = CdnManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing per GB (approximate)
            pricing_per_gb = {
                "standard_microsoft": 0.087,  # Standard Microsoft
                "standard_akamai": 0.087,  # Standard Akamai
                "standard_verizon": 0.087,  # Standard Verizon
                "premium_verizon": 0.20,  # Premium Verizon
            }

            # List all CDN profiles
            cdn_profiles = list(cdn_client.profiles.list())
            logger.info(
                "inventory.azure.cdn.found",
                region=region,
                count=len(cdn_profiles),
            )

            for profile in cdn_profiles:
                try:
                    # Filter by region
                    if profile.location != region:
                        continue

                    profile_name = profile.name or "Unknown"
                    resource_group = profile.id.split("/")[4] if profile.id else "Unknown"

                    # Get SKU tier
                    sku_name = profile.sku.name.lower() if profile.sku and profile.sku.name else "unknown"

                    # Get provisioning state
                    provisioning_state = getattr(profile, "provisioning_state", "Unknown")

                    # Count endpoints in this profile
                    endpoint_count = 0
                    total_bandwidth_gb = 0
                    total_requests = 0
                    has_caching_rules = False

                    try:
                        endpoints = list(cdn_client.endpoints.list_by_profile(resource_group, profile_name))
                        endpoint_count = len(endpoints)

                        for endpoint in endpoints:
                            # Check if endpoint has caching rules
                            delivery_policy = getattr(endpoint, "delivery_policy", None)
                            if delivery_policy:
                                has_caching_rules = True

                            # TODO: Get actual metrics from Azure Monitor (bandwidth, requests)
                            # For MVP, use placeholder metrics
                            # total_bandwidth_gb += endpoint bandwidth (30 days)
                            # total_requests += endpoint requests (30 days)

                    except Exception as e:
                        logger.warning("inventory.azure.cdn.endpoints_error", profile=profile_name, error=str(e))

                    # Calculate monthly cost estimate
                    # CDN is pay-as-you-go: bandwidth + requests
                    # Estimate: $0.087/GB + $0.0075/10K requests
                    # For MVP, estimate 100 GB/month if endpoints exist
                    estimated_bandwidth_gb = 100 if endpoint_count > 0 else 0
                    bandwidth_cost = estimated_bandwidth_gb * pricing_per_gb.get(sku_name, 0.087)
                    requests_cost = (100_000 / 10_000) * 0.0075  # Estimate 100K requests
                    monthly_cost = bandwidth_cost + requests_cost

                    # Calculate optimization potential
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_cdn_optimization(
                        provisioning_state=provisioning_state,
                        sku_name=sku_name,
                        endpoint_count=endpoint_count,
                        bandwidth_gb_30d=total_bandwidth_gb,
                        requests_30d=total_requests,
                        has_caching_rules=has_caching_rules,
                        monthly_cost=monthly_cost,
                    )

                    # Build metadata
                    metadata = {
                        "profile_name": profile_name,
                        "resource_group": resource_group,
                        "sku": sku_name,
                        "endpoint_count": endpoint_count,
                        "provisioning_state": provisioning_state,
                        "resource_state": getattr(profile, "resource_state", "Unknown"),
                        "has_caching_rules": has_caching_rules,
                        "optimization_details": recommendations,
                    }

                    resource = AllCloudResourceData(
                        resource_type="azure_cdn",
                        resource_id=profile.id or f"cdn-{profile_name}",
                        resource_name=profile_name,
                        region=region,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata=metadata,
                        is_optimizable=is_optimizable,
                        optimization_priority=priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations.get("actions", []),
                    )

                    resources.append(resource)
                    logger.info(
                        "inventory.azure.cdn.processed",
                        profile_name=profile_name,
                        sku=sku_name,
                        endpoints=endpoint_count,
                        cost=monthly_cost,
                        optimizable=is_optimizable,
                    )

                except Exception as e:
                    logger.error(
                        "inventory.azure.cdn.error",
                        profile=profile.name if hasattr(profile, "name") else "Unknown",
                        error=str(e),
                    )
                    continue

        except Exception as e:
            logger.error("inventory.azure.cdn.scan_error", region=region, error=str(e))

        return resources

    def _calculate_cdn_optimization(
        self,
        provisioning_state: str,
        sku_name: str,
        endpoint_count: int,
        bandwidth_gb_30d: float,
        requests_30d: int,
        has_caching_rules: bool,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """Calculate optimization potential for CDN profile."""
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = {"actions": [], "estimated_savings": 0.0, "priority": "low"}

        # CRITICAL (90 score): Profile failed or deleting
        if provisioning_state.lower() in ["failed", "deleting", "deleted"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"Profile en tat '{provisioning_state}' - investigate et rparez ou supprimez",
                    "Profile non oprationnel - cot 100% vitable",
                    "Action: Vrifier les logs Azure, rparer ou supprimer le profile",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "critical",
            })

        # HIGH (75 score): Zero bandwidth in 30 days
        elif bandwidth_gb_30d == 0 and endpoint_count > 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    "Aucune bande passante utilise depuis 30 jours",
                    f"{endpoint_count} endpoint(s) configur(s) mais non utilis(s)",
                    "Action: Supprimer les endpoints inutiliss ou le profile",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # HIGH (70 score): Zero requests in 30 days
        elif requests_30d == 0 and endpoint_count > 0:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    "Aucune requte dtecte depuis 30 jours",
                    f"{endpoint_count} endpoint(s) configur(s) mais non sollicit(s)",
                    "Action: Vrifier si les endpoints sont encore ncessaires",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # MEDIUM (50 score): Premium tier for low traffic (<100GB/month)
        elif "premium" in sku_name and bandwidth_gb_30d < 100:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Savings = downgrade to Standard
            # Premium: $0.20/GB, Standard: $0.087/GB
            savings = bandwidth_gb_30d * (0.20 - 0.087)
            potential_savings = max(savings, 0.0)
            recommendations.update({
                "actions": [
                    f"Premium tier pour faible traffic: {bandwidth_gb_30d:.1f} GB/30j",
                    "Cot Premium: $0.20/GB vs Standard: $0.087/GB",
                    "Action: Downgrade vers Standard Microsoft ou Verizon",
                    f"conomies estimes: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "medium",
            })

        # LOW (30 score): No caching rules configured
        elif not has_caching_rules and endpoint_count > 0:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            # No direct savings, but best practice
            savings = 0
            potential_savings = savings
            recommendations.update({
                "actions": [
                    "Aucune rgle de caching configure",
                    "Rgles de caching optimisent performance et rduisent cots",
                    "Action: Configurer des caching rules (TTL, query string caching)",
                    "Note: conomies indirectes via rduction de bandwidth origin",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_container_instances(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Container Instances for cost intelligence.

        Detection criteria:
        - Container stopped/failed (CRITICAL - 90 score)
        - Zero CPU usage 30 derniers jours (HIGH - 75 score)
        - High cost per container (HIGH - 70 score)
        - Long-running containers (MEDIUM - 50 score)
        - No resource limits configured (LOW - 30 score)

        Args:
            region: Azure region

        Returns:
            List of Azure Container Instances with optimization analysis
        """
        resources = []

        try:
            from azure.mgmt.containerinstance import ContainerInstanceManagementClient

            aci_client = ContainerInstanceManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing per hour (approximate)
            vcpu_price_per_hour = 0.0000125  # ~$0.0000125/vCPU-second
            memory_gb_price_per_hour = 0.0000014  # ~$0.0000014/GB-second

            # List all Container Groups
            container_groups = list(aci_client.container_groups.list())
            logger.info(
                "inventory.azure.container_instances.found",
                region=region,
                count=len(container_groups),
            )

            for group in container_groups:
                try:
                    # Filter by region
                    if group.location != region:
                        continue

                    group_name = group.name or "Unknown"
                    resource_group = group.id.split("/")[4] if group.id else "Unknown"

                    # Get container group properties
                    provisioning_state = getattr(group, "provisioning_state", "Unknown")
                    instance_view_state = getattr(group, "instance_view", None)
                    state = "Unknown"
                    if instance_view_state and hasattr(instance_view_state, "state"):
                        state = instance_view_state.state

                    # Get resource requests (vCPU + memory)
                    total_vcpu = 0.0
                    total_memory_gb = 0.0
                    container_count = 0
                    has_resource_limits = False

                    if group.containers:
                        for container in group.containers:
                            container_count += 1
                            if hasattr(container, "resources") and container.resources:
                                requests = container.resources.requests
                                if requests:
                                    total_vcpu += getattr(requests, "cpu", 0.0)
                                    total_memory_gb += getattr(requests, "memory_in_gb", 0.0)

                                limits = getattr(container.resources, "limits", None)
                                if limits:
                                    has_resource_limits = True

                    # Calculate monthly cost (assuming 730 hours/month)
                    hours_per_month = 730
                    vcpu_cost = total_vcpu * vcpu_price_per_hour * hours_per_month * 3600  # Convert to seconds
                    memory_cost = total_memory_gb * memory_gb_price_per_hour * hours_per_month * 3600
                    monthly_cost = vcpu_cost + memory_cost

                    # Get uptime (creation time)
                    uptime_days = 0
                    if hasattr(group, "instance_view") and group.instance_view:
                        if hasattr(group.instance_view, "events") and group.instance_view.events:
                            # Calculate uptime from first event
                            uptime_days = 30  # Placeholder

                    # TODO: Get actual metrics from Azure Monitor (CPU usage, uptime)
                    # For MVP, use placeholder metrics
                    cpu_usage_percent = 0  # Placeholder

                    # Calculate optimization potential
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_container_instance_optimization(
                        provisioning_state=provisioning_state,
                        state=state,
                        total_vcpu=total_vcpu,
                        total_memory_gb=total_memory_gb,
                        cpu_usage_percent=cpu_usage_percent,
                        uptime_days=uptime_days,
                        has_resource_limits=has_resource_limits,
                        monthly_cost=monthly_cost,
                    )

                    # Build metadata
                    metadata = {
                        "container_group_name": group_name,
                        "resource_group": resource_group,
                        "provisioning_state": provisioning_state,
                        "state": state,
                        "container_count": container_count,
                        "total_vcpu": total_vcpu,
                        "total_memory_gb": total_memory_gb,
                        "os_type": getattr(group, "os_type", "Unknown"),
                        "restart_policy": getattr(group, "restart_policy", "Always"),
                        "has_resource_limits": has_resource_limits,
                        "optimization_details": recommendations,
                    }

                    resource = AllCloudResourceData(
                        resource_type="azure_container_instance",
                        resource_id=group.id or f"aci-{group_name}",
                        resource_name=group_name,
                        region=region,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata=metadata,
                        is_optimizable=is_optimizable,
                        optimization_priority=priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations.get("actions", []),
                    )

                    resources.append(resource)
                    logger.info(
                        "inventory.azure.container_instances.processed",
                        group_name=group_name,
                        vcpu=total_vcpu,
                        memory_gb=total_memory_gb,
                        cost=monthly_cost,
                        optimizable=is_optimizable,
                    )

                except Exception as e:
                    logger.error(
                        "inventory.azure.container_instances.error",
                        group=group.name if hasattr(group, "name") else "Unknown",
                        error=str(e),
                    )
                    continue

        except Exception as e:
            logger.error("inventory.azure.container_instances.scan_error", region=region, error=str(e))

        return resources

    def _calculate_container_instance_optimization(
        self,
        provisioning_state: str,
        state: str,
        total_vcpu: float,
        total_memory_gb: float,
        cpu_usage_percent: float,
        uptime_days: int,
        has_resource_limits: bool,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """Calculate optimization potential for Container Instance."""
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = {"actions": [], "estimated_savings": 0.0, "priority": "low"}

        # CRITICAL (90 score): Container stopped or failed
        if provisioning_state.lower() in ["failed", "deleting"] or state.lower() in ["stopped", "terminated"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"Container en tat '{state}' (provisioning: {provisioning_state})",
                    "Container non oprationnel - cot 100% vitable",
                    "Action: Vrifier les logs, rparer ou supprimer le container group",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "critical",
            })

        # HIGH (75 score): Zero CPU usage in 30 days
        elif cpu_usage_percent == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    "Aucune utilisation CPU dtecte depuis 30 jours",
                    "Container potentiellement inutilis ou en idle permanent",
                    "Action: Vrifier si le container est encore ncessaire",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # HIGH (70 score): High cost per container (>$100/mo)
        elif monthly_cost > 100:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            savings = monthly_cost * 0.5  # 50% potential savings via AKS migration
            potential_savings = savings
            recommendations.update({
                "actions": [
                    f"Cot lev pour Container Instance: ${monthly_cost:.2f}/mois",
                    f"Ressources: {total_vcpu} vCPUs, {total_memory_gb} GB RAM",
                    "Action: Migrer vers Azure Kubernetes Service (AKS) pour conomiser",
                    f"conomies estimes: ${savings:.2f}/mois (50% via AKS)",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # MEDIUM (50 score): Long-running containers (>30 days)
        elif uptime_days > 30:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            savings = monthly_cost * 0.3  # 30% potential savings
            potential_savings = savings
            recommendations.update({
                "actions": [
                    f"Container long-running: {uptime_days} jours d'uptime",
                    "Containers persistants cotent plus cher sur ACI",
                    "Action: Migrer vers AKS ou App Service pour workloads persistants",
                    f"conomies estimes: ${savings:.2f}/mois",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "medium",
            })

        # LOW (30 score): No resource limits configured
        elif not has_resource_limits:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            savings = 0
            potential_savings = savings
            recommendations.update({
                "actions": [
                    "Aucune limite de ressources configure",
                    "Resource limits vitent les dpassements de cots",
                    "Action: Configurer CPU/memory limits pour contrler les cots",
                    "Note: Meilleure pratique pour la gestion des cots",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_logic_apps(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Logic Apps workflows for cost intelligence.

        Detection criteria:
        - Workflow disabled/failed (CRITICAL - 90 score)
        - Zero workflow runs 30 derniers jours (HIGH - 75 score)
        - High execution failure rate (HIGH - 70 score)
        - Consumption plan for high volume (MEDIUM - 50 score)
        - No error handling configured (LOW - 30 score)

        Args:
            region: Azure region

        Returns:
            List of Azure Logic Apps workflows with optimization analysis
        """
        resources = []

        try:
            from azure.mgmt.logic import LogicManagementClient

            logic_client = LogicManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing (approximate)
            action_price_consumption = 0.000025  # After 4000 free actions/month
            connector_price_standard = 0.000125  # Standard connector per call
            vcpu_price_standard = 0.192  # Standard plan per vCPU-hour
            memory_price_standard = 0.0137  # Standard plan per GB-hour

            # List all workflows (we need to list by resource group)
            # For simplicity, we'll query all resource groups
            from azure.mgmt.resource import ResourceManagementClient

            resource_client = ResourceManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            all_workflows = []
            resource_groups = list(resource_client.resource_groups.list())

            for rg in resource_groups:
                try:
                    rg_name = rg.name
                    workflows = list(logic_client.workflows.list_by_resource_group(rg_name))
                    all_workflows.extend([(rg_name, wf) for wf in workflows])
                except Exception as e:
                    logger.warning("inventory.azure.logic_apps.rg_error", rg=rg.name, error=str(e))
                    continue

            logger.info(
                "inventory.azure.logic_apps.found",
                region=region,
                count=len(all_workflows),
            )

            for rg_name, workflow in all_workflows:
                try:
                    # Filter by region
                    if workflow.location != region:
                        continue

                    workflow_name = workflow.name or "Unknown"

                    # Get workflow properties
                    state = getattr(workflow, "state", "Unknown")  # Enabled/Disabled
                    provisioning_state = getattr(workflow, "provisioning_state", "Unknown")

                    # Get workflow definition (to check error handling)
                    has_error_handling = False
                    if hasattr(workflow, "definition") and workflow.definition:
                        definition_str = str(workflow.definition)
                        if "runAfter" in definition_str or "catch" in definition_str.lower():
                            has_error_handling = True

                    # Get integration account (Standard vs Consumption)
                    integration_account = getattr(workflow, "integration_account", None)
                    is_standard = integration_account is not None

                    # TODO: Get actual metrics from Azure Monitor (runs, failures, executions)
                    # For MVP, use placeholder metrics
                    total_runs_30d = 0  # Placeholder
                    failed_runs_30d = 0  # Placeholder
                    failure_rate = 0.0  # Placeholder
                    total_actions_30d = 0  # Placeholder (for Consumption pricing)

                    # Calculate monthly cost estimate
                    if is_standard:
                        # Standard plan: estimate vCPU + memory cost
                        # Assume 1 vCPU + 1.75 GB memory for small workflow
                        monthly_cost = (vcpu_price_standard * 730) + (memory_price_standard * 1.75 * 730)
                    else:
                        # Consumption plan: estimate based on actions
                        # Assume 10K actions/month if workflow exists
                        estimated_actions = 10_000
                        monthly_cost = max(0, (estimated_actions - 4000)) * action_price_consumption
                        monthly_cost += estimated_actions * connector_price_standard * 0.5  # 50% connector calls

                    # Calculate optimization potential
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_logic_app_optimization(
                        state=state,
                        provisioning_state=provisioning_state,
                        total_runs_30d=total_runs_30d,
                        failed_runs_30d=failed_runs_30d,
                        failure_rate=failure_rate,
                        total_actions_30d=total_actions_30d,
                        is_standard=is_standard,
                        has_error_handling=has_error_handling,
                        monthly_cost=monthly_cost,
                    )

                    # Build metadata
                    metadata = {
                        "workflow_name": workflow_name,
                        "resource_group": rg_name,
                        "state": state,
                        "provisioning_state": provisioning_state,
                        "plan_type": "Standard" if is_standard else "Consumption",
                        "has_error_handling": has_error_handling,
                        "sku": getattr(workflow.sku, "name", "Unknown") if hasattr(workflow, "sku") else "Unknown",
                        "optimization_details": recommendations,
                    }

                    resource = AllCloudResourceData(
                        resource_type="azure_logic_app",
                        resource_id=workflow.id or f"logic-{workflow_name}",
                        resource_name=workflow_name,
                        region=region,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata=metadata,
                        is_optimizable=is_optimizable,
                        optimization_priority=priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations.get("actions", []),
                    )

                    resources.append(resource)
                    logger.info(
                        "inventory.azure.logic_apps.processed",
                        workflow_name=workflow_name,
                        state=state,
                        plan_type="Standard" if is_standard else "Consumption",
                        cost=monthly_cost,
                        optimizable=is_optimizable,
                    )

                except Exception as e:
                    logger.error(
                        "inventory.azure.logic_apps.error",
                        workflow=workflow.name if hasattr(workflow, "name") else "Unknown",
                        error=str(e),
                    )
                    continue

        except Exception as e:
            logger.error("inventory.azure.logic_apps.scan_error", region=region, error=str(e))

        return resources

    def _calculate_logic_app_optimization(
        self,
        state: str,
        provisioning_state: str,
        total_runs_30d: int,
        failed_runs_30d: int,
        failure_rate: float,
        total_actions_30d: int,
        is_standard: bool,
        has_error_handling: bool,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """Calculate optimization potential for Logic App workflow."""
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = {"actions": [], "estimated_savings": 0.0, "priority": "low"}

        # CRITICAL (90 score): Workflow disabled or failed
        if state.lower() == "disabled" or provisioning_state.lower() in ["failed", "deleting"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"Workflow en tat '{state}' (provisioning: {provisioning_state})",
                    "Workflow non oprationnel - cot 100% vitable",
                    "Action: Vrifier les logs, rparer ou supprimer le workflow",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "critical",
            })

        # HIGH (75 score): Zero workflow runs in 30 days
        elif total_runs_30d == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    "Aucune excution dtecte depuis 30 jours",
                    "Workflow potentiellement inutilis",
                    "Action: Vrifier si le workflow est encore ncessaire",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # HIGH (70 score): High execution failure rate (>50%)
        elif failure_rate > 0.5:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            savings = monthly_cost * 0.5  # 50% potential savings by fixing errors
            potential_savings = savings
            recommendations.update({
                "actions": [
                    f"Taux d'chec lev: {failure_rate*100:.1f}% des excutions",
                    f"checs: {failed_runs_30d} sur {total_runs_30d} excutions",
                    "Action: Investiguer et corriger les erreurs du workflow",
                    f"conomies estimes: ${savings:.2f}/mois en vitant les re-runs",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # MEDIUM (50 score): Consumption plan for high volume (>1M actions/mo)
        elif not is_standard and total_actions_30d > 1_000_000:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Standard plan would be cheaper for high volume
            standard_cost = (0.192 * 730) + (0.0137 * 1.75 * 730)  # 1 vCPU + 1.75GB
            savings = monthly_cost - standard_cost
            potential_savings = max(savings, 0.0)
            recommendations.update({
                "actions": [
                    f"Consumption plan pour haut volume: {total_actions_30d:,} actions/30j",
                    f"Cot actuel: ${monthly_cost:.2f}/mois",
                    "Action: Migrer vers Standard plan pour conomiser",
                    f"conomies estimes: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "medium",
            })

        # LOW (30 score): No error handling configured
        elif not has_error_handling:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            savings = 0
            potential_savings = savings
            recommendations.update({
                "actions": [
                    "Aucun error handling configur dans le workflow",
                    "Error handling vite les checs coteux et les re-runs",
                    "Action: Ajouter try-catch ou runAfter avec conditions",
                    "Note: Meilleure pratique pour la fiabilit",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_log_analytics(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Log Analytics Workspaces for cost intelligence.

        Detection criteria:
        - Workspace not used/failed (CRITICAL - 90 score)
        - Zero data ingestion 30 derniers jours (HIGH - 75 score)
        - High retention cost (HIGH - 70 score)
        - Pay-as-you-go for high volume (MEDIUM - 50 score)
        - No data retention policy configured (LOW - 30 score)

        Args:
            region: Azure region

        Returns:
            List of Azure Log Analytics Workspaces with optimization analysis
        """
        resources = []

        try:
            from azure.mgmt.loganalytics import LogAnalyticsManagementClient

            log_client = LogAnalyticsManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing per GB (approximate)
            ingestion_price_payg = 2.30  # Pay-as-you-go per GB
            retention_price_per_gb = 0.10  # Per GB per month after 31 days
            commitment_100gb_price = 230.0  # Commitment tier 100GB/day (30% discount)

            # List all workspaces
            workspaces = list(log_client.workspaces.list())
            logger.info(
                "inventory.azure.log_analytics.found",
                region=region,
                count=len(workspaces),
            )

            for workspace in workspaces:
                try:
                    # Filter by region
                    if workspace.location != region:
                        continue

                    workspace_name = workspace.name or "Unknown"
                    resource_group = workspace.id.split("/")[4] if workspace.id else "Unknown"

                    # Get workspace properties
                    provisioning_state = getattr(workspace, "provisioning_state", "Unknown")

                    # Get SKU (pricing tier)
                    sku_name = "Unknown"
                    if hasattr(workspace, "sku") and workspace.sku:
                        sku_name = workspace.sku.name if hasattr(workspace.sku, "name") else "Unknown"

                    # Get retention days
                    retention_days = getattr(workspace, "retention_in_days", 30)
                    has_retention_policy = retention_days > 0

                    # Get daily quota (commitment tier indicator)
                    daily_quota_gb = getattr(workspace, "daily_quota_gb", -1)
                    is_commitment_tier = daily_quota_gb > 0

                    # TODO: Get actual metrics from Azure Monitor (data ingestion, queries)
                    # For MVP, use placeholder metrics
                    total_ingestion_gb_30d = 0  # Placeholder
                    daily_ingestion_gb = 0  # Placeholder
                    query_count_30d = 0  # Placeholder

                    # Calculate monthly cost estimate
                    if is_commitment_tier and daily_quota_gb >= 100:
                        # Commitment tier: 100GB/day = $230/day
                        monthly_cost = commitment_100gb_price * 30
                    else:
                        # Pay-as-you-go: estimate 10GB/day ingestion
                        estimated_daily_ingestion = 10
                        ingestion_cost = estimated_daily_ingestion * 30 * ingestion_price_payg

                        # Retention cost (beyond 31 days free)
                        if retention_days > 31:
                            retention_gb = estimated_daily_ingestion * retention_days
                            retention_cost = retention_gb * retention_price_per_gb
                        else:
                            retention_cost = 0

                        monthly_cost = ingestion_cost + retention_cost

                    # Calculate optimization potential
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_log_analytics_optimization(
                        provisioning_state=provisioning_state,
                        sku_name=sku_name,
                        retention_days=retention_days,
                        has_retention_policy=has_retention_policy,
                        total_ingestion_gb_30d=total_ingestion_gb_30d,
                        daily_ingestion_gb=daily_ingestion_gb,
                        query_count_30d=query_count_30d,
                        is_commitment_tier=is_commitment_tier,
                        monthly_cost=monthly_cost,
                    )

                    # Build metadata
                    metadata = {
                        "workspace_name": workspace_name,
                        "resource_group": resource_group,
                        "provisioning_state": provisioning_state,
                        "sku": sku_name,
                        "retention_days": retention_days,
                        "has_retention_policy": has_retention_policy,
                        "daily_quota_gb": daily_quota_gb,
                        "is_commitment_tier": is_commitment_tier,
                        "public_network_access": getattr(workspace, "public_network_access_for_ingestion", "Unknown"),
                        "optimization_details": recommendations,
                    }

                    resource = AllCloudResourceData(
                        resource_type="azure_log_analytics",
                        resource_id=workspace.id or f"log-{workspace_name}",
                        resource_name=workspace_name,
                        region=region,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata=metadata,
                        is_optimizable=is_optimizable,
                        optimization_priority=priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations.get("actions", []),
                    )

                    resources.append(resource)
                    logger.info(
                        "inventory.azure.log_analytics.processed",
                        workspace_name=workspace_name,
                        sku=sku_name,
                        retention_days=retention_days,
                        cost=monthly_cost,
                        optimizable=is_optimizable,
                    )

                except Exception as e:
                    logger.error(
                        "inventory.azure.log_analytics.error",
                        workspace=workspace.name if hasattr(workspace, "name") else "Unknown",
                        error=str(e),
                    )
                    continue

        except Exception as e:
            logger.error("inventory.azure.log_analytics.scan_error", region=region, error=str(e))

        return resources

    def _calculate_log_analytics_optimization(
        self,
        provisioning_state: str,
        sku_name: str,
        retention_days: int,
        has_retention_policy: bool,
        total_ingestion_gb_30d: float,
        daily_ingestion_gb: float,
        query_count_30d: int,
        is_commitment_tier: bool,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """Calculate optimization potential for Log Analytics Workspace."""
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = {"actions": [], "estimated_savings": 0.0, "priority": "low"}

        # CRITICAL (90 score): Workspace not used or failed
        if provisioning_state.lower() in ["failed", "deleting", "deleted"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"Workspace en tat '{provisioning_state}'",
                    "Workspace non oprationnel - cot 100% vitable",
                    "Action: Vrifier les logs, rparer ou supprimer le workspace",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "critical",
            })

        # HIGH (75 score): Zero data ingestion in 30 days
        elif total_ingestion_gb_30d == 0 and query_count_30d == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    "Aucune ingestion de donnes depuis 30 jours",
                    "Workspace potentiellement inutilis",
                    "Action: Vrifier si le workspace est encore ncessaire",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # HIGH (70 score): High retention cost (>90 days for non-critical data)
        elif retention_days > 90:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Savings = reduce retention from 90+ to 31 days
            savings_per_gb = (retention_days - 31) * 0.10
            estimated_gb = daily_ingestion_gb * retention_days if daily_ingestion_gb > 0 else 300  # 10GB/day * 30 days
            savings = estimated_gb * (savings_per_gb / retention_days)  # Proportional savings
            potential_savings = max(savings, monthly_cost * 0.3)  # At least 30% savings
            recommendations.update({
                "actions": [
                    f"Rtention leve: {retention_days} jours",
                    "Rtention longue cote cher pour donnes non-critiques",
                    "Action: Rduire la rtention  31 jours (gratuit) ou archiver vers blob storage",
                    f"conomies estimes: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # MEDIUM (50 score): Pay-as-you-go for high volume (>100GB/day)
        elif not is_commitment_tier and daily_ingestion_gb > 100:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Commitment tier saves 30%
            current_monthly = daily_ingestion_gb * 30 * 2.30
            commitment_monthly = 230 * 30  # 100GB/day commitment
            savings = current_monthly - commitment_monthly
            potential_savings = max(savings, 0.0)
            recommendations.update({
                "actions": [
                    f"Pay-as-you-go pour haut volume: {daily_ingestion_gb:.1f} GB/jour",
                    f"Cot actuel: ${current_monthly:.2f}/mois",
                    "Action: Migrer vers Commitment Tier (100GB/day) pour conomiser 30%",
                    f"conomies estimes: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "medium",
            })

        # LOW (30 score): No data retention policy configured
        elif not has_retention_policy or retention_days == 0:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            savings = 0
            potential_savings = savings
            recommendations.update({
                "actions": [
                    "Aucune politique de rtention configure",
                    "Rtention par dfaut peut entraner cots non contrls",
                    "Action: Configurer une retention policy adapte aux besoins",
                    "Note: Meilleure pratique pour gestion des cots",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_backup_vaults(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure Backup Vaults for cost intelligence."""
        resources = []

        try:
            from azure.mgmt.recoveryservices import RecoveryServicesClient

            client = RecoveryServicesClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing per protected instance (varies by redundancy and tier)
            pricing_map = {
                "LRS_Standard": 5.0,  # Locally redundant standard
                "LRS_Archive": 2.5,  # Archive tier
                "ZRS_Standard": 6.25,  # Zone redundant
                "GRS_Standard": 10.0,  # Geo redundant standard
                "GRS_Archive": 5.0,  # Geo redundant archive
            }

            # List all Recovery Services Vaults
            vaults = client.vaults.list_by_subscription_id()

            for vault in vaults:
                try:
                    # Extract basic info
                    vault_id = vault.id or "unknown"
                    vault_name = vault.name or "unknown"
                    vault_location = vault.location or region
                    provisioning_state = (
                        vault.properties.provisioning_state
                        if vault.properties
                        else "Unknown"
                    )

                    # Get redundancy settings
                    redundancy = "LRS"
                    if vault.sku and vault.sku.name:
                        redundancy = vault.sku.name  # Standard, RS0 (GRS)

                    # Estimate protected items (requires backup client)
                    protected_items_count = 0
                    backup_jobs_30d = 0
                    backup_policies_count = 0
                    last_backup_time = None

                    try:
                        from azure.mgmt.recoveryservicesbackup import (
                            RecoveryServicesBackupClient,
                        )

                        backup_client = RecoveryServicesBackupClient(
                            credential=self.credential,
                            subscription_id=self.subscription_id,
                        )

                        # Get resource group from vault ID
                        resource_group = vault_id.split("/")[4] if "/" in vault_id else ""

                        # Count protected items
                        try:
                            protected_items = (
                                backup_client.backup_protected_items.list(
                                    vault_name=vault_name, resource_group_name=resource_group
                                )
                            )
                            protected_items_count = sum(1 for _ in protected_items)
                        except Exception:
                            pass

                        # Count backup policies
                        try:
                            policies = backup_client.backup_policies.list(
                                vault_name=vault_name, resource_group_name=resource_group
                            )
                            backup_policies_count = sum(1 for _ in policies)
                        except Exception:
                            pass

                        # Count recent backup jobs
                        try:
                            from datetime import datetime, timedelta

                            start_time = datetime.utcnow() - timedelta(days=30)
                            jobs = backup_client.backup_jobs.list(
                                vault_name=vault_name,
                                resource_group_name=resource_group,
                                filter=f"startTime eq '{start_time.isoformat()}Z'",
                            )
                            backup_jobs_30d = sum(1 for _ in jobs)
                        except Exception:
                            pass

                    except Exception as e:
                        logger.debug(
                            "backup_vault_metrics_error",
                            vault_name=vault_name,
                            error=str(e),
                        )

                    # Calculate monthly cost
                    redundancy_key = f"{redundancy}_Standard"
                    price_per_instance = pricing_map.get(redundancy_key, 5.0)
                    monthly_cost = protected_items_count * price_per_instance

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_backup_vault_optimization(
                        provisioning_state=provisioning_state,
                        protected_items_count=protected_items_count,
                        backup_jobs_30d=backup_jobs_30d,
                        backup_policies_count=backup_policies_count,
                        redundancy=redundancy,
                        monthly_cost=monthly_cost,
                    )

                    # Build metadata
                    metadata = {
                        "vault_id": vault_id,
                        "vault_name": vault_name,
                        "location": vault_location,
                        "provisioning_state": provisioning_state,
                        "redundancy": redundancy,
                        "protected_items": protected_items_count,
                        "backup_jobs_30d": backup_jobs_30d,
                        "backup_policies": backup_policies_count,
                        "price_per_instance": round(price_per_instance, 2),
                        "tags": dict(vault.tags) if vault.tags else {},
                    }

                    # Create resource data
                    resource_data = AllCloudResourceData(
                        resource_id=vault_id,
                        resource_name=vault_name,
                        resource_type="azure_backup_vault",
                        region=vault_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata=metadata,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_priority=priority,
                        potential_monthly_savings=round(potential_savings, 2),
                        optimization_recommendations=recommendations,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        "backup_vault_scan_error",
                        vault_name=vault.name if vault.name else "unknown",
                        error=str(e),
                    )
                    continue

        except Exception as e:
            logger.error("scan_backup_vaults_error", region=region, error=str(e))

        logger.info(
            "scan_backup_vaults_complete",
            region=region,
            total_vaults=len(resources),
        )
        return resources

    def _calculate_backup_vault_optimization(
        self,
        provisioning_state: str,
        protected_items_count: int,
        backup_jobs_30d: int,
        backup_policies_count: int,
        redundancy: str,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """Calculate optimization potential for Backup Vault."""
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = {"actions": [], "estimated_savings": 0.0, "priority": "low"}

        # CRITICAL (90 score): Vault in failed state
        if provisioning_state.lower() in ["failed", "deleting", "deleted"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"Vault en tat '{provisioning_state}'",
                    "Vault non oprationnel - cot 100% vitable",
                    "Action: Vrifier les logs, rparer ou supprimer le vault",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "critical",
            })

        # HIGH (75 score): Zero protected items
        elif protected_items_count == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    "Aucun lment protg dans le vault",
                    "Vault vide - cot 100% vitable",
                    "Action: Supprimer le vault ou commencer  l'utiliser",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # HIGH (70 score): No backup jobs in 30 days
        elif backup_jobs_30d == 0 and protected_items_count > 0:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"{protected_items_count} lments protgs mais aucun backup en 30 jours",
                    "Sauvegardes potentiellement non fonctionnelles",
                    "Action: Vrifier la configuration des policies ou supprimer les items",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # MEDIUM (50 score): GRS redundancy for non-critical data
        elif redundancy in ["RS0", "GeoRedundant"] and protected_items_count > 0:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Savings = switch from GRS ($10) to LRS ($5) per instance
            savings_per_instance = 5.0
            potential_savings = protected_items_count * savings_per_instance
            recommendations.update({
                "actions": [
                    f"Redondance gographique (GRS) pour {protected_items_count} items",
                    f"GRS cote 2x plus cher que LRS (${10.0} vs ${5.0}/instance)",
                    "Action: Migrer vers LRS si la redondance gographique n'est pas critique",
                    f"conomies estimes: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "medium",
            })

        # LOW (30 score): No backup policies configured
        elif backup_policies_count == 0 and protected_items_count > 0:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            savings = 0
            potential_savings = savings
            recommendations.update({
                "actions": [
                    f"{protected_items_count} items protgs sans backup policy",
                    "Absence de policies peut indiquer une configuration incomplte",
                    "Action: Configurer des backup policies appropries",
                    "Note: Meilleure pratique pour gestion des sauvegardes",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_data_factory_pipelines(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure Data Factory instances for cost intelligence."""
        resources = []

        try:
            from azure.mgmt.datafactory import DataFactoryManagementClient

            client = DataFactoryManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing: $0.005 per activity run (orchestration), $1 per vCore-hour (data flow)
            price_per_activity_run = 0.005
            price_per_vcore_hour = 1.0

            # List all Data Factories
            factories = client.factories.list()

            for factory in factories:
                try:
                    # Extract basic info
                    factory_id = factory.id or "unknown"
                    factory_name = factory.name or "unknown"
                    factory_location = factory.location or region
                    provisioning_state = (
                        factory.provisioning_state if factory.provisioning_state else "Unknown"
                    )

                    # Get resource group from factory ID
                    resource_group = factory_id.split("/")[4] if "/" in factory_id else ""

                    # Count pipelines, triggers, and runs
                    pipeline_count = 0
                    trigger_count = 0
                    pipeline_runs_30d = 0
                    failed_runs_30d = 0
                    total_activity_runs = 0

                    try:
                        # Count pipelines
                        pipelines = client.pipelines.list_by_factory(
                            resource_group_name=resource_group, factory_name=factory_name
                        )
                        pipeline_count = sum(1 for _ in pipelines)

                        # Count triggers
                        triggers = client.triggers.list_by_factory(
                            resource_group_name=resource_group, factory_name=factory_name
                        )
                        trigger_count = sum(1 for _ in triggers)

                        # Get pipeline runs from last 30 days
                        try:
                            from datetime import datetime, timedelta

                            end_time = datetime.utcnow()
                            start_time = end_time - timedelta(days=30)

                            # Query pipeline runs
                            filter_params = {
                                "lastUpdatedAfter": start_time,
                                "lastUpdatedBefore": end_time,
                            }

                            pipeline_runs = client.pipeline_runs.query_by_factory(
                                resource_group_name=resource_group,
                                factory_name=factory_name,
                                filter_parameters=filter_params,
                            )

                            for run in pipeline_runs.value if pipeline_runs.value else []:
                                pipeline_runs_30d += 1
                                if run.status in ["Failed", "Cancelled"]:
                                    failed_runs_30d += 1

                            # Estimate activity runs (average 5 activities per pipeline run)
                            total_activity_runs = pipeline_runs_30d * 5

                        except Exception:
                            pass

                    except Exception as e:
                        logger.debug(
                            "data_factory_metrics_error",
                            factory_name=factory_name,
                            error=str(e),
                        )

                    # Calculate monthly cost
                    # Base on activity runs only (data flows require separate analysis)
                    monthly_activity_cost = total_activity_runs * price_per_activity_run
                    monthly_cost = monthly_activity_cost

                    # Calculate optimization
                    failure_rate = (
                        (failed_runs_30d / pipeline_runs_30d * 100)
                        if pipeline_runs_30d > 0
                        else 0
                    )

                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_data_factory_optimization(
                        provisioning_state=provisioning_state,
                        pipeline_count=pipeline_count,
                        trigger_count=trigger_count,
                        pipeline_runs_30d=pipeline_runs_30d,
                        failed_runs_30d=failed_runs_30d,
                        failure_rate=failure_rate,
                        monthly_cost=monthly_cost,
                    )

                    # Build metadata
                    metadata = {
                        "factory_id": factory_id,
                        "factory_name": factory_name,
                        "location": factory_location,
                        "provisioning_state": provisioning_state,
                        "pipeline_count": pipeline_count,
                        "trigger_count": trigger_count,
                        "pipeline_runs_30d": pipeline_runs_30d,
                        "failed_runs_30d": failed_runs_30d,
                        "failure_rate_pct": round(failure_rate, 2),
                        "total_activity_runs": total_activity_runs,
                        "price_per_activity": price_per_activity_run,
                        "tags": dict(factory.tags) if factory.tags else {},
                    }

                    # Create resource data
                    resource_data = AllCloudResourceData(
                        resource_id=factory_id,
                        resource_name=factory_name,
                        resource_type="azure_data_factory_pipeline",
                        region=factory_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata=metadata,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_priority=priority,
                        potential_monthly_savings=round(potential_savings, 2),
                        optimization_recommendations=recommendations,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        "data_factory_scan_error",
                        factory_name=factory.name if factory.name else "unknown",
                        error=str(e),
                    )
                    continue

        except Exception as e:
            logger.error("scan_data_factory_pipelines_error", region=region, error=str(e))

        logger.info(
            "scan_data_factory_pipelines_complete",
            region=region,
            total_factories=len(resources),
        )
        return resources

    def _calculate_data_factory_optimization(
        self,
        provisioning_state: str,
        pipeline_count: int,
        trigger_count: int,
        pipeline_runs_30d: int,
        failed_runs_30d: int,
        failure_rate: float,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """Calculate optimization potential for Data Factory."""
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = {"actions": [], "estimated_savings": 0.0, "priority": "low"}

        # CRITICAL (90 score): Data Factory in failed state
        if provisioning_state.lower() in ["failed", "deleting", "deleted"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"Data Factory en tat '{provisioning_state}'",
                    "Data Factory non oprationnel - cot 100% vitable",
                    "Action: Vrifier les logs, rparer ou supprimer la Data Factory",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "critical",
            })

        # HIGH (75 score): Zero pipeline runs in 30 days
        elif pipeline_runs_30d == 0 and pipeline_count > 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"{pipeline_count} pipelines mais aucun run en 30 jours",
                    "Data Factory potentiellement inutilise",
                    "Action: Supprimer la Data Factory ou activer les pipelines",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # HIGH (70 score): High failure rate >50%
        elif failure_rate > 50 and pipeline_runs_30d > 0:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Assume failures waste 50% of costs
            potential_savings = monthly_cost * 0.5
            recommendations.update({
                "actions": [
                    f"Taux d'chec lev: {failure_rate:.1f}% ({failed_runs_30d}/{pipeline_runs_30d} runs)",
                    "checs frquents gaspillent des ressources",
                    "Action: Dbugger les pipelines, amliorer la gestion d'erreurs",
                    f"conomies estimes: ${potential_savings:.2f}/mois (50% de rduction d'checs)",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # MEDIUM (50 score): Many pipelines with no recent runs
        elif pipeline_count >= 5 and pipeline_runs_30d == 0:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Assume we can delete half the unused pipelines
            potential_savings = monthly_cost * 0.5 if monthly_cost > 0 else 0
            recommendations.update({
                "actions": [
                    f"{pipeline_count} pipelines inactifs",
                    "Pipelines inutiliss crent de la complexit et du risque",
                    "Action: Nettoyer les pipelines obsoltes",
                    f"conomies estimes: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "medium",
            })

        # LOW (30 score): No triggers configured
        elif trigger_count == 0 and pipeline_count > 0:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            savings = 0
            potential_savings = savings
            recommendations.update({
                "actions": [
                    f"{pipeline_count} pipelines sans triggers",
                    "Absence de triggers peut indiquer configuration manuelle",
                    "Action: Configurer des triggers pour automation",
                    "Note: Meilleure pratique pour orchestration automatise",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_synapse_serverless_sql(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure Synapse Analytics workspaces for serverless SQL pool cost intelligence."""
        resources = []

        try:
            from azure.mgmt.synapse import SynapseManagementClient

            client = SynapseManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing: $5 per TB of data processed
            price_per_tb = 5.0

            # List all Synapse workspaces
            workspaces = client.workspaces.list()

            for workspace in workspaces:
                try:
                    # Extract basic info
                    workspace_id = workspace.id or "unknown"
                    workspace_name = workspace.name or "unknown"
                    workspace_location = workspace.location or region
                    provisioning_state = workspace.provisioning_state if workspace.provisioning_state else "Unknown"

                    # Serverless SQL pool is built-in to every workspace
                    # Endpoint format: {workspace_name}-ondemand.sql.azuresynapse.net
                    serverless_endpoint = f"{workspace_name}-ondemand.sql.azuresynapse.net" if workspace.connectivity_endpoints else "unknown"

                    # Check if workspace has serverless SQL endpoint active
                    has_serverless_sql = False
                    if workspace.connectivity_endpoints:
                        if "sqlOnDemand" in workspace.connectivity_endpoints:
                            has_serverless_sql = True
                            serverless_endpoint = workspace.connectivity_endpoints.get("sqlOnDemand", serverless_endpoint)

                    # Estimate monthly data processed (would require actual query metrics)
                    # For now, we'll use placeholder values and flag for investigation
                    estimated_tb_per_month = 0.0  # Would need actual metrics from Azure Monitor
                    monthly_cost = estimated_tb_per_month * price_per_tb

                    # Get resource group from workspace ID
                    resource_group = workspace_id.split("/")[4] if "/" in workspace_id else ""

                    # Try to get SQL pools count
                    sql_pools_count = 0
                    try:
                        sql_pools = client.sql_pools.list_by_workspace(
                            resource_group_name=resource_group,
                            workspace_name=workspace_name
                        )
                        sql_pools_count = sum(1 for _ in sql_pools)
                    except Exception:
                        pass

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_synapse_serverless_optimization(
                        provisioning_state=provisioning_state,
                        has_serverless_sql=has_serverless_sql,
                        sql_pools_count=sql_pools_count,
                        estimated_tb_per_month=estimated_tb_per_month,
                        monthly_cost=monthly_cost,
                    )

                    # Build metadata
                    metadata = {
                        "workspace_id": workspace_id,
                        "workspace_name": workspace_name,
                        "location": workspace_location,
                        "provisioning_state": provisioning_state,
                        "has_serverless_sql": has_serverless_sql,
                        "serverless_endpoint": serverless_endpoint,
                        "sql_pools_count": sql_pools_count,
                        "estimated_tb_per_month": round(estimated_tb_per_month, 2),
                        "price_per_tb": price_per_tb,
                        "tags": dict(workspace.tags) if workspace.tags else {},
                    }

                    # Create resource data
                    resource_data = AllCloudResourceData(
                        resource_id=workspace_id,
                        resource_name=workspace_name,
                        resource_type="azure_synapse_serverless_sql",
                        region=workspace_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata=metadata,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_priority=priority,
                        potential_monthly_savings=round(potential_savings, 2),
                        optimization_recommendations=recommendations,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        "synapse_workspace_scan_error",
                        workspace_name=workspace.name if workspace.name else "unknown",
                        error=str(e),
                    )
                    continue

        except Exception as e:
            logger.error("scan_synapse_serverless_sql_error", region=region, error=str(e))

        logger.info(
            "scan_synapse_serverless_sql_complete",
            region=region,
            total_workspaces=len(resources),
        )
        return resources

    def _calculate_synapse_serverless_optimization(
        self,
        provisioning_state: str,
        has_serverless_sql: bool,
        sql_pools_count: int,
        estimated_tb_per_month: float,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """Calculate optimization potential for Synapse Serverless SQL."""
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = {"actions": [], "estimated_savings": 0.0, "priority": "low"}

        # CRITICAL (90 score): Workspace in failed state
        if provisioning_state.lower() in ["failed", "deleting", "deleted"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"Workspace Synapse en tat '{provisioning_state}'",
                    "Workspace non oprationnel - cot potentiellement vitable",
                    "Action: Vrifier les logs, rparer ou supprimer le workspace",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "critical",
            })

        # HIGH (75 score): Workspace actif sans serverless SQL pool configur
        elif not has_serverless_sql and sql_pools_count == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = 0.0  # No cost if not using it, but workspace overhead
            recommendations.update({
                "actions": [
                    "Workspace Synapse sans serverless SQL ni dedicated pools",
                    "Workspace potentiellement inutilis",
                    "Action: Supprimer le workspace ou commencer  l'utiliser",
                    "Note: conomies indirectes (complexit, maintenance)",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # HIGH (70 score): High data processing cost (>$500/month)
        elif monthly_cost > 500:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Suggest optimization strategies (partitioning, caching, etc.)
            potential_savings = monthly_cost * 0.3  # 30% savings through optimization
            recommendations.update({
                "actions": [
                    f"Cot lev de data processing: ${monthly_cost:.2f}/mois ({estimated_tb_per_month:.2f} TB)",
                    "Usage intensif de serverless SQL peut tre optimis",
                    "Action: Analyser les queries, implmenter caching, partitioning, ou considrer dedicated SQL pool",
                    f"conomies estimes: ${potential_savings:.2f}/mois (30% rduction)",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # MEDIUM (50 score): Workspace inactif mais serverless SQL enabled
        elif has_serverless_sql and estimated_tb_per_month == 0.0:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            potential_savings = 0.0  # Serverless has no idle cost, but investigate
            recommendations.update({
                "actions": [
                    "Serverless SQL endpoint actif mais aucune donne processed en 30 jours",
                    "Workspace potentiellement inutilis ou mtriques non disponibles",
                    "Action: Vrifier l'usage rel via Azure Monitor ou supprimer si inutilis",
                    "Note: Serverless SQL n'a pas de cot idle (pay-per-query uniquement)",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "medium",
            })

        # LOW (30 score): Pas de quotas/budgets configurs
        elif has_serverless_sql and monthly_cost == 0.0:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            savings = 0
            potential_savings = savings
            recommendations.update({
                "actions": [
                    "Serverless SQL actif sans quotas/budgets configurs",
                    "Absence de limites peut entraner cots non contrls",
                    "Action: Configurer des cost controls via Azure Cost Management",
                    "Note: Meilleure pratique pour gestion des cots",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_storage_sftp(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure Storage Accounts with SFTP enabled for cost intelligence."""
        resources = []

        try:
            from azure.mgmt.storage import StorageManagementClient

            client = StorageManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing: $0.30/hour ($220/month) + storage costs
            sftp_hourly_cost = 0.30
            sftp_monthly_cost = sftp_hourly_cost * 24 * 30  # ~$216/month

            # List all storage accounts
            storage_accounts = client.storage_accounts.list()

            for account in storage_accounts:
                try:
                    # Check if SFTP is enabled
                    if not account.is_sftp_enabled:
                        continue  # Skip accounts without SFTP

                    # Extract basic info
                    account_id = account.id or "unknown"
                    account_name = account.name or "unknown"
                    account_location = account.location or region
                    provisioning_state = safe_get_value(account.provisioning_state, "Unknown")

                    # Get resource group from account ID
                    resource_group = account_id.split("/")[4] if "/" in account_id else ""

                    # Get account properties
                    sku_name = safe_get_value(account.sku.name if account.sku else None, "Unknown")
                    account_kind = safe_get_value(account.kind, "Unknown")

                    # Estimate storage usage (would need actual metrics)
                    storage_used_gb = 0.0  # Would need Azure Monitor metrics
                    storage_cost = 0.0  # Depends on tier, redundancy, etc.

                    # Total monthly cost: SFTP fee + storage
                    monthly_cost = sftp_monthly_cost + storage_cost

                    # Try to estimate transaction count (would need metrics)
                    transaction_count_30d = 0  # Would need actual metrics

                    # Calculate days since SFTP enabled (if creation time available)
                    days_sftp_enabled = 30  # Placeholder
                    if account.creation_time:
                        from datetime import datetime
                        delta = datetime.utcnow() - account.creation_time
                        days_sftp_enabled = delta.days

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_storage_sftp_optimization(
                        provisioning_state=provisioning_state,
                        days_sftp_enabled=days_sftp_enabled,
                        transaction_count_30d=transaction_count_30d,
                        sftp_monthly_cost=sftp_monthly_cost,
                        monthly_cost=monthly_cost,
                    )

                    # Build metadata
                    metadata = {
                        "account_id": account_id,
                        "account_name": account_name,
                        "location": account_location,
                        "provisioning_state": provisioning_state,
                        "is_sftp_enabled": True,
                        "sku": sku_name,
                        "kind": account_kind,
                        "days_sftp_enabled": days_sftp_enabled,
                        "transaction_count_30d": transaction_count_30d,
                        "storage_used_gb": round(storage_used_gb, 2),
                        "sftp_hourly_cost": sftp_hourly_cost,
                        "sftp_monthly_cost": round(sftp_monthly_cost, 2),
                        "tags": dict(account.tags) if account.tags else {},
                    }

                    # Create resource data
                    resource_data = AllCloudResourceData(
                        resource_id=account_id,
                        resource_name=account_name,
                        resource_type="azure_storage_sftp",
                        region=account_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata=metadata,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_priority=priority,
                        potential_monthly_savings=round(potential_savings, 2),
                        optimization_recommendations=recommendations,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        "storage_sftp_scan_error",
                        account_name=account.name if account.name else "unknown",
                        error=str(e),
                    )
                    continue

        except Exception as e:
            logger.error("scan_storage_sftp_error", region=region, error=str(e))

        logger.info(
            "scan_storage_sftp_complete",
            region=region,
            total_sftp_accounts=len(resources),
        )
        return resources

    def _calculate_storage_sftp_optimization(
        self,
        provisioning_state: str,
        days_sftp_enabled: int,
        transaction_count_30d: int,
        sftp_monthly_cost: float,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """Calculate optimization potential for Storage Account with SFTP."""
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = {"actions": [], "estimated_savings": 0.0, "priority": "low"}

        # CRITICAL (90 score): Storage account failed but SFTP still enabled
        if provisioning_state.lower() in ["failed", "deleting", "deleted"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = sftp_monthly_cost
            recommendations.update({
                "actions": [
                    f"Storage Account en tat '{provisioning_state}' mais SFTP enabled",
                    f"SFTP cote ${sftp_monthly_cost:.2f}/mois mme si account failed",
                    "Action: Dsactiver SFTP ou supprimer le storage account",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "critical",
            })

        # HIGH (75 score): SFTP enabled >30 days sans transactions
        elif days_sftp_enabled >= 30 and transaction_count_30d == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = sftp_monthly_cost
            recommendations.update({
                "actions": [
                    f"SFTP enabled depuis {days_sftp_enabled} jours sans aucune transaction",
                    f"SFTP inutilis gaspille ${sftp_monthly_cost:.2f}/mois",
                    "Action: Dsactiver SFTP (peut ractiver au besoin sans perte de config)",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # HIGH (70 score): SFTP enabled avec trs peu de transactions
        elif transaction_count_30d > 0 and transaction_count_30d < 100:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = sftp_monthly_cost
            recommendations.update({
                "actions": [
                    f"SFTP enabled avec seulement {transaction_count_30d} transactions en 30 jours",
                    f"Usage trs faible pour ${sftp_monthly_cost:.2f}/mois",
                    "Action: Dsactiver SFTP et enable on-demand, ou utiliser alternative (Azure Files)",
                    f"conomies estimes: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # MEDIUM (50 score): SFTP enabled en permanence pour usage occasionnel
        elif transaction_count_30d >= 100 and transaction_count_30d < 1000:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Suggest enable/disable strategy instead of always-on
            potential_savings = sftp_monthly_cost * 0.5  # 50% savings with on-demand
            recommendations.update({
                "actions": [
                    f"SFTP enabled 24/7 avec {transaction_count_30d} transactions/mois",
                    f"Usage modr pour ${sftp_monthly_cost:.2f}/mois always-on",
                    "Action: Enable SFTP uniquement quand ncessaire (disable aprs usage)",
                    f"conomies estimes: ${potential_savings:.2f}/mois (50% rduction)",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "medium",
            })

        # LOW (30 score): Pas de monitoring des connexions SFTP
        elif transaction_count_30d == 0:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            savings = 0
            potential_savings = savings
            recommendations.update({
                "actions": [
                    "SFTP enabled sans monitoring des connexions configur",
                    "Mtriques non disponibles pour analyser l'usage rel",
                    "Action: Configurer Azure Monitor pour tracking SFTP usage",
                    "Note: Meilleure pratique pour gestion des cots",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_ad_domain_services(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure AD Domain Services (Microsoft Entra Domain Services) for cost intelligence."""
        resources = []

        try:
            from azure.mgmt.resource import ResourceManagementClient

            resource_client = ResourceManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing per SKU per month
            pricing_map = {
                "Standard": 109,  # Estimated (not found in search results)
                "Enterprise": 292,
                "Premium": 1168,
            }

            # Query for Microsoft.AAD/domainServices resources
            filter_query = "resourceType eq 'Microsoft.AAD/domainServices'"
            domain_services = resource_client.resources.list(filter=filter_query)

            for domain_service in domain_services:
                try:
                    # Extract basic info
                    domain_id = domain_service.id or "unknown"
                    domain_name = domain_service.name or "unknown"
                    domain_location = domain_service.location or region

                    # Get resource properties
                    properties = domain_service.properties if domain_service.properties else {}

                    # Extract SKU
                    sku = properties.get("sku", "Unknown")
                    if isinstance(sku, dict):
                        sku = sku.get("name", "Unknown")

                    # Get provisioning state
                    provisioning_state = properties.get("provisioningState", "Unknown")

                    # Get health status
                    health_monitors = properties.get("healthMonitors", [])
                    health_alerts = properties.get("healthAlerts", [])
                    has_health_alerts = len(health_alerts) > 0 if health_alerts else False

                    # Get sync status
                    sync_scope = properties.get("syncScope", "Unknown")
                    sync_owner = properties.get("syncOwner", "Unknown")

                    # Get deployment configuration
                    replica_sets = properties.get("replicaSets", [])
                    replica_count = len(replica_sets) if replica_sets else 0

                    # Calculate monthly cost based on SKU
                    monthly_cost = pricing_map.get(sku, 109)  # Default to Standard if unknown

                    # Estimate VMs joined to domain (would need actual metrics)
                    vms_joined = 0  # Would need Azure Monitor or LDAP queries
                    auth_requests_per_hour = 0  # Would need metrics

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_ad_domain_services_optimization(
                        provisioning_state=provisioning_state,
                        sku=sku,
                        has_health_alerts=has_health_alerts,
                        vms_joined=vms_joined,
                        auth_requests_per_hour=auth_requests_per_hour,
                        monthly_cost=monthly_cost,
                    )

                    # Build metadata
                    metadata = {
                        "domain_id": domain_id,
                        "domain_name": domain_name,
                        "location": domain_location,
                        "provisioning_state": provisioning_state,
                        "sku": sku,
                        "sync_scope": sync_scope,
                        "sync_owner": sync_owner,
                        "replica_count": replica_count,
                        "has_health_alerts": has_health_alerts,
                        "health_alerts_count": len(health_alerts) if health_alerts else 0,
                        "vms_joined": vms_joined,
                        "auth_requests_per_hour": auth_requests_per_hour,
                        "tags": dict(domain_service.tags) if domain_service.tags else {},
                    }

                    # Create resource data
                    resource_data = AllCloudResourceData(
                        resource_id=domain_id,
                        resource_name=domain_name,
                        resource_type="azure_ad_domain_services",
                        region=domain_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata=metadata,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_priority=priority,
                        potential_monthly_savings=round(potential_savings, 2),
                        optimization_recommendations=recommendations,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        "ad_domain_services_scan_error",
                        domain_name=domain_service.name if domain_service.name else "unknown",
                        error=str(e),
                    )
                    continue

        except Exception as e:
            logger.error("scan_ad_domain_services_error", region=region, error=str(e))

        logger.info(
            "scan_ad_domain_services_complete",
            region=region,
            total_domain_services=len(resources),
        )
        return resources

    def _calculate_ad_domain_services_optimization(
        self,
        provisioning_state: str,
        sku: str,
        has_health_alerts: bool,
        vms_joined: int,
        auth_requests_per_hour: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """Calculate optimization potential for Azure AD Domain Services."""
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = {"actions": [], "estimated_savings": 0.0, "priority": "low"}

        # CRITICAL (90 score): Domain Services not running or failed
        if provisioning_state.lower() in ["failed", "deleting", "deleted", "notrunning"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"Azure AD Domain Services en tat '{provisioning_state}'",
                    f"Service non oprationnel - cot ${monthly_cost:.2f}/mois vitable",
                    "Action: Vrifier les logs, rparer ou supprimer le domain service",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "critical",
            })

        # HIGH (75 score): Premium SKU ($1168/mois) pour <10K auth/hour
        elif sku == "Premium" and auth_requests_per_hour < 10000:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            # Downgrade from Premium ($1168) to Enterprise ($292)
            potential_savings = 1168 - 292
            recommendations.update({
                "actions": [
                    f"SKU Premium (${1168}/mois) surdimensionn pour {auth_requests_per_hour} auth/hour",
                    "Premium supporte 10K-70K auth/hour, usage actuel plus faible",
                    "Action: Downgrade vers Enterprise SKU pour conomiser 75%",
                    f"conomies estimes: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # HIGH (70 score): Domain Services non utilis (0 VMs joines)
        elif vms_joined == 0:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"Azure AD Domain Services actif mais aucune VM joine au domaine",
                    f"Service inutilis - gaspille ${monthly_cost:.2f}/mois",
                    "Action: Supprimer le domain service ou commencer  l'utiliser",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # MEDIUM (50 score): Enterprise SKU pour <3K auth/hour
        elif sku == "Enterprise" and auth_requests_per_hour < 3000:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Downgrade from Enterprise ($292) to Standard ($109)
            potential_savings = 292 - 109
            recommendations.update({
                "actions": [
                    f"SKU Enterprise (${292}/mois) pour {auth_requests_per_hour} auth/hour",
                    "Enterprise supporte 3K-10K auth/hour, Standard suffit pour <3K",
                    "Action: Downgrade vers Standard SKU pour conomiser 63%",
                    f"conomies estimes: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "medium",
            })

        # LOW (30 score): Health alerts non rsolus
        elif has_health_alerts:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            savings = 0
            potential_savings = savings
            recommendations.update({
                "actions": [
                    "Azure AD Domain Services a des health alerts non rsolus",
                    "Alerts peuvent indiquer problmes de performance ou scurit",
                    "Action: Rsoudre les health alerts via Azure Portal",
                    "Note: Meilleure pratique pour fiabilit du service",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_service_bus_premium(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure Service Bus Premium namespaces for cost intelligence."""
        resources = []

        try:
            from azure.mgmt.servicebus import ServiceBusManagementClient

            client = ServiceBusManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing: ~$670/month per messaging unit (flat rate)
            price_per_messaging_unit_monthly = 670.0

            # List all Service Bus namespaces
            namespaces = client.namespaces.list()

            for namespace in namespaces:
                try:
                    # Filter only Premium tier namespaces
                    if not namespace.sku or namespace.sku.name.lower() != "premium":
                        continue  # Skip non-Premium namespaces

                    # Extract basic info
                    namespace_id = namespace.id or "unknown"
                    namespace_name = namespace.name or "unknown"
                    namespace_location = namespace.location or region
                    provisioning_state = namespace.provisioning_state if namespace.provisioning_state else "Unknown"

                    # Get messaging units (capacity)
                    messaging_units = namespace.sku.capacity if namespace.sku else 1

                    # Calculate monthly cost
                    monthly_cost = messaging_units * price_per_messaging_unit_monthly

                    # Get resource group from namespace ID
                    resource_group = namespace_id.split("/")[4] if "/" in namespace_id else ""

                    # Count queues and topics
                    queues_count = 0
                    topics_count = 0
                    try:
                        queues = client.queues.list_by_namespace(
                            resource_group_name=resource_group,
                            namespace_name=namespace_name
                        )
                        queues_count = sum(1 for _ in queues)
                    except Exception:
                        pass

                    try:
                        topics = client.topics.list_by_namespace(
                            resource_group_name=resource_group,
                            namespace_name=namespace_name
                        )
                        topics_count = sum(1 for _ in topics)
                    except Exception:
                        pass

                    # Check if geo-disaster recovery is configured
                    has_geo_dr = False
                    try:
                        disaster_recovery_configs = client.disaster_recovery_configs.list(
                            resource_group_name=resource_group,
                            namespace_name=namespace_name
                        )
                        has_geo_dr = sum(1 for _ in disaster_recovery_configs) > 0
                    except Exception:
                        pass

                    # Estimate throughput usage (would need metrics)
                    estimated_throughput_percent = 0  # Would need Azure Monitor metrics

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_service_bus_premium_optimization(
                        provisioning_state=provisioning_state,
                        messaging_units=messaging_units,
                        queues_count=queues_count,
                        topics_count=topics_count,
                        has_geo_dr=has_geo_dr,
                        estimated_throughput_percent=estimated_throughput_percent,
                        monthly_cost=monthly_cost,
                    )

                    # Build metadata
                    metadata = {
                        "namespace_id": namespace_id,
                        "namespace_name": namespace_name,
                        "location": namespace_location,
                        "provisioning_state": provisioning_state,
                        "sku": "Premium",
                        "messaging_units": messaging_units,
                        "queues_count": queues_count,
                        "topics_count": topics_count,
                        "has_geo_dr": has_geo_dr,
                        "estimated_throughput_percent": estimated_throughput_percent,
                        "price_per_unit_monthly": price_per_messaging_unit_monthly,
                        "tags": dict(namespace.tags) if namespace.tags else {},
                    }

                    # Create resource data
                    resource_data = AllCloudResourceData(
                        resource_id=namespace_id,
                        resource_name=namespace_name,
                        resource_type="azure_service_bus_premium",
                        region=namespace_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata=metadata,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_priority=priority,
                        potential_monthly_savings=round(potential_savings, 2),
                        optimization_recommendations=recommendations,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        "service_bus_premium_scan_error",
                        namespace_name=namespace.name if namespace.name else "unknown",
                        error=str(e),
                    )
                    continue

        except Exception as e:
            logger.error("scan_service_bus_premium_error", region=region, error=str(e))

        logger.info(
            "scan_service_bus_premium_complete",
            region=region,
            total_premium_namespaces=len(resources),
        )
        return resources

    def _calculate_service_bus_premium_optimization(
        self,
        provisioning_state: str,
        messaging_units: int,
        queues_count: int,
        topics_count: int,
        has_geo_dr: bool,
        estimated_throughput_percent: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """Calculate optimization potential for Service Bus Premium."""
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = {"actions": [], "estimated_savings": 0.0, "priority": "low"}

        # CRITICAL (90 score): Namespace in failed state
        if provisioning_state.lower() in ["failed", "deleting", "deleted"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"Service Bus Premium namespace en tat '{provisioning_state}'",
                    f"Namespace non oprationnel - cot ${monthly_cost:.2f}/mois vitable",
                    "Action: Vrifier les logs, rparer ou supprimer le namespace",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "critical",
            })

        # HIGH (75 score): Premium tier avec 0 queues/topics
        elif queues_count == 0 and topics_count == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"Premium namespace vide (0 queues, 0 topics) - cot ${monthly_cost:.2f}/mois",
                    f"{messaging_units} messaging unit(s) inutilises",
                    "Action: Supprimer le namespace ou crer des queues/topics",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # HIGH (70 score): Premium tier avec trs faible dbit
        elif estimated_throughput_percent > 0 and estimated_throughput_percent < 10:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = monthly_cost * 0.5  # Suggest downgrade or Standard tier
            recommendations.update({
                "actions": [
                    f"Faible utilisation: {estimated_throughput_percent}% du dbit Premium",
                    f"Premium tier cote ${monthly_cost:.2f}/mois pour usage minimal",
                    "Action: Considrer Standard tier ou rduire messaging units",
                    f"conomies estimes: ${potential_savings:.2f}/mois (50% rduction)",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # MEDIUM (50 score): Premium tier surdimensionn
        elif messaging_units >= 4:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Suggest reducing from 4+ units to 2 units
            units_to_reduce = messaging_units - 2
            potential_savings = units_to_reduce * 670
            recommendations.update({
                "actions": [
                    f"Surdimensionnement potentiel: {messaging_units} messaging units",
                    f"Cot actuel: ${monthly_cost:.2f}/mois",
                    "Action: Analyser le dbit rel et rduire messaging units si possible",
                    f"conomies estimes: ${potential_savings:.2f}/mois (rduction  2 units)",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "medium",
            })

        # LOW (30 score): Premium sans geo-disaster recovery
        elif not has_geo_dr and messaging_units >= 2:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            savings = 0
            potential_savings = savings
            recommendations.update({
                "actions": [
                    "Premium tier sans geo-disaster recovery configur",
                    "Premium offre geo-DR mais non utilis",
                    "Action: Configurer geo-DR ou considrer Standard tier",
                    "Note: Meilleure pratique pour haute disponibilit",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_iot_hub(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure IoT Hubs for cost intelligence."""
        resources = []

        try:
            from azure.mgmt.iothub import IotHubClient

            client = IotHubClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing map (monthly)
            pricing_map = {
                "F1": 0.0,  # Free tier (8K messages/day)
                "B1": 10.0,  # Basic tier
                "B2": 50.0,
                "B3": 500.0,
                "S1": 25.0,  # Standard tier (400K messages/day)
                "S2": 250.0,  # 6M messages/day
                "S3": 2500.0,  # 300M messages/day
            }

            # List all IoT Hubs
            iot_hubs = client.iot_hub_resource.list_by_subscription()

            for hub in iot_hubs:
                try:
                    # Extract basic info
                    hub_id = hub.id or "unknown"
                    hub_name = hub.name or "unknown"
                    hub_location = hub.location or region
                    provisioning_state = hub.properties.provisioning_state if hub.properties else "Unknown"

                    # Get SKU info
                    sku_name = hub.sku.name if hub.sku else "Unknown"
                    sku_tier = hub.sku.tier if hub.sku else "Unknown"
                    sku_capacity = hub.sku.capacity if hub.sku else 1  # Number of units

                    # Calculate monthly cost
                    price_per_unit = pricing_map.get(sku_name, 25.0)
                    monthly_cost = price_per_unit * sku_capacity

                    # Get resource group from hub ID
                    resource_group = hub_id.split("/")[4] if "/" in hub_id else ""

                    # Get device statistics
                    device_count = 0
                    try:
                        stats = client.iot_hub_resource.get_stats(
                            resource_group_name=resource_group,
                            resource_name=hub_name
                        )
                        device_count = stats.total_device_count if stats.total_device_count else 0
                    except Exception:
                        pass

                    # Estimate message quota usage (would need metrics)
                    daily_message_quota = 0
                    if sku_name == "F1":
                        daily_message_quota = 8000
                    elif sku_name in ["B1", "S1"]:
                        daily_message_quota = 400000 * sku_capacity
                    elif sku_name == "S2":
                        daily_message_quota = 6000000 * sku_capacity
                    elif sku_name == "S3":
                        daily_message_quota = 300000000 * sku_capacity

                    # Estimate usage (would need actual metrics)
                    estimated_daily_messages = 0  # Would need Azure Monitor metrics
                    usage_percent = 0
                    if daily_message_quota > 0 and estimated_daily_messages > 0:
                        usage_percent = (estimated_daily_messages / daily_message_quota) * 100

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_iot_hub_optimization(
                        provisioning_state=provisioning_state,
                        sku_name=sku_name,
                        sku_tier=sku_tier,
                        sku_capacity=sku_capacity,
                        device_count=device_count,
                        usage_percent=usage_percent,
                        monthly_cost=monthly_cost,
                    )

                    # Build metadata
                    metadata = {
                        "hub_id": hub_id,
                        "hub_name": hub_name,
                        "location": hub_location,
                        "provisioning_state": provisioning_state,
                        "sku_name": sku_name,
                        "sku_tier": sku_tier,
                        "sku_capacity": sku_capacity,
                        "device_count": device_count,
                        "daily_message_quota": daily_message_quota,
                        "usage_percent": round(usage_percent, 2),
                        "price_per_unit": price_per_unit,
                        "tags": dict(hub.tags) if hub.tags else {},
                    }

                    # Create resource data
                    resource_data = AllCloudResourceData(
                        resource_id=hub_id,
                        resource_name=hub_name,
                        resource_type="azure_iot_hub",
                        region=hub_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata=metadata,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_priority=priority,
                        potential_monthly_savings=round(potential_savings, 2),
                        optimization_recommendations=recommendations,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        "iot_hub_scan_error",
                        hub_name=hub.name if hub.name else "unknown",
                        error=str(e),
                    )
                    continue

        except Exception as e:
            logger.error("scan_iot_hub_error", region=region, error=str(e))

        logger.info(
            "scan_iot_hub_complete",
            region=region,
            total_iot_hubs=len(resources),
        )
        return resources

    def _calculate_iot_hub_optimization(
        self,
        provisioning_state: str,
        sku_name: str,
        sku_tier: str,
        sku_capacity: int,
        device_count: int,
        usage_percent: float,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """Calculate optimization potential for IoT Hub."""
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = {"actions": [], "estimated_savings": 0.0, "priority": "low"}

        # CRITICAL (90 score): IoT Hub in failed state
        if provisioning_state.lower() in ["failed", "deleting", "deleted"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"IoT Hub en tat '{provisioning_state}'",
                    f"Hub non oprationnel - cot ${monthly_cost:.2f}/mois vitable",
                    "Action: Vrifier les logs, rparer ou supprimer l'IoT Hub",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "critical",
            })

        # HIGH (75 score): Standard tier avec 0 devices
        elif sku_tier.lower() == "standard" and device_count == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.update({
                "actions": [
                    f"IoT Hub Standard ({sku_name}) avec 0 devices enregistrs",
                    f"Hub inutilis - cot ${monthly_cost:.2f}/mois",
                    "Action: Supprimer le hub ou commencer  enregistrer des devices",
                    f"conomies potentielles: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # HIGH (70 score): Standard tier avec usage <10%
        elif sku_tier.lower() == "standard" and usage_percent > 0 and usage_percent < 10:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Suggest downgrade to Basic or Free tier
            potential_savings = monthly_cost * 0.6  # 60% savings with Basic
            recommendations.update({
                "actions": [
                    f"Faible utilisation: {usage_percent:.1f}% du quota messages",
                    f"Standard tier ({sku_name}) cote ${monthly_cost:.2f}/mois pour usage minimal",
                    "Action: Downgrade vers Basic tier ou rduire capacity",
                    f"conomies estimes: ${potential_savings:.2f}/mois (60% rduction)",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "high",
            })

        # MEDIUM (50 score): S2/S3 tier surdimensionn
        elif sku_name in ["S2", "S3"] and device_count < 1000:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Suggest downgrade from S2/S3 to S1
            current_cost = monthly_cost
            s1_cost = 25.0 * sku_capacity
            potential_savings = current_cost - s1_cost
            recommendations.update({
                "actions": [
                    f"Tier {sku_name} surdimensionn pour {device_count} devices",
                    f"Cot actuel: ${current_cost:.2f}/mois",
                    "Action: Downgrade vers S1 tier pour conomiser",
                    f"conomies estimes: ${potential_savings:.2f}/mois",
                ],
                "estimated_savings": round(potential_savings, 2),
                "priority": "medium",
            })

        # LOW (30 score): Pas de monitoring configur
        elif usage_percent == 0 and device_count > 0:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            savings = 0
            potential_savings = savings
            recommendations.update({
                "actions": [
                    f"IoT Hub avec {device_count} devices mais metrics non disponibles",
                    "Monitoring non configur pour analyser l'usage rel",
                    "Action: Configurer Azure Monitor pour tracking messages",
                    "Note: Meilleure pratique pour gestion des cots",
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_stream_analytics(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure Stream Analytics jobs for cost intelligence."""
        resources = []

        try:
            from azure.mgmt.streamanalytics import StreamAnalyticsManagementClient

            client = StreamAnalyticsManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing: $0.11 per streaming unit-hour (V2 pricing)
            # Monthly cost = $0.11 * 24h * 30 days * streaming_units = $79.20 per SU
            price_per_su_monthly = 0.11 * 24 * 30

            # List all Stream Analytics jobs
            jobs = client.streaming_jobs.list()

            for job in jobs:
                try:
                    # Extract metadata
                    job_name = job.name
                    job_id = job.id
                    resource_group = job_id.split("/")[4] if len(job_id.split("/")) > 4 else "unknown"
                    job_location = job.location or region
                    job_state = job.job_state or "Unknown"

                    # Get transformation (streaming units)
                    streaming_units = 0
                    if job.transformation and hasattr(job.transformation, "streaming_units"):
                        streaming_units = job.transformation.streaming_units or 0

                    # Count inputs and outputs
                    inputs_count = 0
                    outputs_count = 0
                    try:
                        inputs_list = list(client.inputs.list_by_streaming_job(
                            resource_group_name=resource_group,
                            job_name=job_name
                        ))
                        inputs_count = len(inputs_list)
                    except Exception:
                        pass

                    try:
                        outputs_list = list(client.outputs.list_by_streaming_job(
                            resource_group_name=resource_group,
                            job_name=job_name
                        ))
                        outputs_count = len(outputs_list)
                    except Exception:
                        pass

                    # Get diagnostic settings (monitoring)
                    has_diagnostics = False
                    try:
                        from azure.mgmt.monitor import MonitorManagementClient
                        monitor_client = MonitorManagementClient(
                            credential=self.credential, subscription_id=self.subscription_id
                        )
                        diag_settings = list(monitor_client.diagnostic_settings.list(resource_uri=job_id))
                        has_diagnostics = len(diag_settings) > 0
                    except Exception:
                        pass

                    # Calculate monthly cost
                    monthly_cost = price_per_su_monthly * streaming_units

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations_data,
                    ) = self._calculate_stream_analytics_optimization(
                        job_state=job_state,
                        streaming_units=streaming_units,
                        inputs_count=inputs_count,
                        outputs_count=outputs_count,
                        has_diagnostics=has_diagnostics,
                        monthly_cost=monthly_cost,
                    )

                    # Build AllCloudResourceData
                    resource_data = AllCloudResourceData(
                        resource_id=job_id,
                        resource_name=job_name,
                        resource_type="azure_stream_analytics",
                        region=job_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "job_state": job_state,
                            "streaming_units": streaming_units,
                            "inputs_count": inputs_count,
                            "outputs_count": outputs_count,
                            "has_diagnostics": has_diagnostics,
                            "resource_group": resource_group,
                            "sku": job.sku.name if job.sku else "Unknown",
                        },
                        last_used_at=None,
                        created_at_cloud=job.created_date if hasattr(job, "created_date") and job.created_date else None,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_recommendations=recommendations_data,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        f"Error scanning Stream Analytics job {job.name}: {str(e)}"
                    )
                    continue

        except Exception as e:
            logger.error(f"Error listing Stream Analytics jobs: {str(e)}")

        return resources

    def _calculate_stream_analytics_optimization(
        self,
        job_state: str,
        streaming_units: int,
        inputs_count: int,
        outputs_count: int,
        has_diagnostics: bool,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """
        Calculate Stream Analytics optimization based on 5 scenarios.

        Scenarios:
        1. CRITICAL (90): Job failed
        2. HIGH (75): Job stopped >30 days (inferred from state)
        3. HIGH (70): Job running but 0 inputs/outputs
        4. MEDIUM (50): Oversized streaming units (>6 SU)
        5. LOW (30): No diagnostic monitoring configured
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = {"scenarios": []}

        # Scenario 1: CRITICAL - Job failed
        if job_state.lower() in ["failed", "degraded"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            savings = monthly_cost
            recommendations["scenarios"].append({
                "scenario": "Job failed or degraded",
                "description": (
                    f"Le job Stream Analytics est en tat '{job_state}'. "
                    f"Cot gaspill: ${monthly_cost:.2f}/mois. "
                    f"Actions recommandes: (1) Vrifier les logs d'erreur, "
                    f"(2) Corriger la configuration des inputs/outputs, "
                    f"(3) Arrter le job si non corrigeable pour viter les cots."
                ),
                "actions": [
                    f"Vrifier les logs d'erreur du job '{job_state}'",
                    "Corriger la configuration des inputs/outputs",
                    "Arrter le job si non rparable",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "critical",
            })

        # Scenario 2: HIGH - Job stopped >30 days (inferred from stopped state)
        elif job_state.lower() == "stopped":
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority != "critical" else priority
            savings = monthly_cost
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Job arrt depuis longtemps",
                "description": (
                    f"Le job Stream Analytics est arrt. "
                    f"Si non utilis depuis >30 jours, supprimer pour conomiser ${monthly_cost:.2f}/mois. "
                    f"Actions recommandes: (1) Vrifier la dernire utilisation, "
                    f"(2) Supprimer si obsolte, "
                    f"(3) Redmarrer si ncessaire."
                ),
                "actions": [
                    "Vrifier la dernire date d'utilisation du job",
                    "Supprimer le job si obsolte (>30 jours)",
                    "Redmarrer si encore ncessaire",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 3: HIGH - Job running but 0 inputs/outputs
        elif job_state.lower() == "running" and (inputs_count == 0 or outputs_count == 0):
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority not in ["critical"] else priority
            savings = monthly_cost
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Job actif sans inputs/outputs configurs",
                "description": (
                    f"Le job Stream Analytics tourne avec {inputs_count} inputs et {outputs_count} outputs. "
                    f"Job inutilisable sans inputs/outputs. Cot: ${monthly_cost:.2f}/mois. "
                    f"Actions recommandes: (1) Configurer les inputs/outputs, "
                    f"(2) Arrter le job si configuration impossible."
                ),
                "actions": [
                    f"Configurer les inputs ({inputs_count}) et outputs ({outputs_count})",
                    "Tester le job avec des donnes relles",
                    "Arrter le job si non utilis",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 4: MEDIUM - Oversized streaming units (>6 SU)
        elif streaming_units > 6:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority not in ["critical", "high"] else priority
            # Estimate 50% reduction possible
            target_su = max(3, streaming_units // 2)
            savings = monthly_cost * 0.5
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Streaming Units surdimensionns",
                "description": (
                    f"Le job utilise {streaming_units} SU (Streaming Units). "
                    f"Cot actuel: ${monthly_cost:.2f}/mois. "
                    f"Analyser le throughput rel pour rduire  ~{target_su} SU. "
                    f"Actions recommandes: (1) Analyser les mtriques de throughput, "
                    f"(2) Rduire les SU progressivement, "
                    f"(3) conomiser jusqu' ${savings:.2f}/mois."
                ),
                "actions": [
                    f"Analyser les mtriques de throughput actuelles ({streaming_units} SU)",
                    f"Rduire progressivement  ~{target_su} SU",
                    "Monitorer les performances aprs rduction",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "medium",
            })

        # Scenario 5: LOW - No diagnostic monitoring configured
        elif not has_diagnostics:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            savings = monthly_cost * 0.1  # Visibility helps optimize
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Pas de monitoring configur",
                "description": (
                    f"Le job Stream Analytics n'a pas de diagnostic monitoring activ. "
                    f"Sans mtriques, impossible d'optimiser le throughput et les cots. "
                    f"Actions recommandes: (1) Activer Diagnostic Settings, "
                    f"(2) Envoyer logs vers Log Analytics, "
                    f"(3) Crer des alertes sur les mtriques cls."
                ),
                "actions": [
                    "Activer Diagnostic Settings pour le job",
                    "Envoyer logs vers Log Analytics Workspace",
                    "Crer des alertes sur mtriques cls (errors, throughput)",
                    f"Note: Meilleure visibilit pour optimiser (conomie ~${savings:.2f}/mois)"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_ai_document_intelligence(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure Document Intelligence (Form Recognizer) endpoints for cost intelligence."""
        resources = []

        try:
            from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient

            client = CognitiveServicesManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing map (monthly estimates based on usage)
            # F0: Free tier (500 pages/month)
            # S0: Standard tier $1.50 per 1K pages
            pricing_map = {
                "F0": 0.0,  # Free tier
                "S0": 150.0,  # Estimate: 100K pages/month = $150
            }

            # List all Cognitive Services accounts
            accounts = client.accounts.list()

            for account in accounts:
                try:
                    # Filter only FormRecognizer/Document Intelligence accounts
                    if not account.kind or account.kind.lower() != "formrecognizer":
                        continue

                    # Extract metadata
                    account_name = account.name
                    account_id = account.id
                    resource_group = account_id.split("/")[4] if len(account_id.split("/")) > 4 else "unknown"
                    account_location = account.location or region
                    sku_name = account.sku.name if account.sku else "Unknown"
                    provisioning_state = account.properties.provisioning_state if hasattr(account, "properties") and hasattr(account.properties, "provisioning_state") else "Unknown"

                    # Check if endpoint is accessible
                    endpoint_accessible = False
                    if account.properties and hasattr(account.properties, "endpoint") and account.properties.endpoint:
                        endpoint_accessible = True

                    # Get diagnostic settings (monitoring)
                    has_diagnostics = False
                    try:
                        from azure.mgmt.monitor import MonitorManagementClient
                        monitor_client = MonitorManagementClient(
                            credential=self.credential, subscription_id=self.subscription_id
                        )
                        diag_settings = list(monitor_client.diagnostic_settings.list(resource_uri=account_id))
                        has_diagnostics = len(diag_settings) > 0
                    except Exception:
                        pass

                    # Check if private endpoint is configured
                    has_private_endpoint = False
                    if account.properties and hasattr(account.properties, "private_endpoint_connections"):
                        connections = account.properties.private_endpoint_connections or []
                        has_private_endpoint = len(connections) > 0

                    # Get estimated monthly cost
                    monthly_cost = pricing_map.get(sku_name, 150.0)

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations_data,
                    ) = self._calculate_document_intelligence_optimization(
                        sku_name=sku_name,
                        provisioning_state=provisioning_state,
                        endpoint_accessible=endpoint_accessible,
                        has_diagnostics=has_diagnostics,
                        has_private_endpoint=has_private_endpoint,
                        monthly_cost=monthly_cost,
                    )

                    # Build AllCloudResourceData
                    resource_data = AllCloudResourceData(
                        resource_id=account_id,
                        resource_name=account_name,
                        resource_type="azure_document_intelligence",
                        region=account_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "sku": sku_name,
                            "provisioning_state": provisioning_state,
                            "endpoint_accessible": endpoint_accessible,
                            "has_diagnostics": has_diagnostics,
                            "has_private_endpoint": has_private_endpoint,
                            "resource_group": resource_group,
                            "kind": account.kind,
                        },
                        last_used_at=None,
                        created_at_cloud=None,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_recommendations=recommendations_data,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        f"Error scanning Document Intelligence account {account.name}: {str(e)}"
                    )
                    continue

        except Exception as e:
            logger.error(f"Error listing Document Intelligence accounts: {str(e)}")

        return resources

    def _calculate_document_intelligence_optimization(
        self,
        sku_name: str,
        provisioning_state: str,
        endpoint_accessible: bool,
        has_diagnostics: bool,
        has_private_endpoint: bool,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """
        Calculate Document Intelligence optimization based on 5 scenarios.

        Scenarios:
        1. CRITICAL (90): Account failed provisioning
        2. HIGH (75): S0 tier with inaccessible endpoint
        3. HIGH (70): S0 tier without any usage metrics
        4. MEDIUM (50): S0 tier without private endpoint (security)
        5. LOW (30): No diagnostic monitoring configured
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = {"scenarios": []}

        # Scenario 1: CRITICAL - Account failed provisioning
        if provisioning_state.lower() in ["failed", "deleting"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            savings = monthly_cost
            recommendations["scenarios"].append({
                "scenario": "Account en tat d'chec",
                "description": (
                    f"Le compte Document Intelligence est en tat '{provisioning_state}'. "
                    f"Cot gaspill: ${monthly_cost:.2f}/mois. "
                    f"Actions recommandes: (1) Vrifier les logs d'erreur, "
                    f"(2) Recrer le compte si ncessaire, "
                    f"(3) Supprimer si non utilis."
                ),
                "actions": [
                    f"Vrifier les logs d'erreur pour '{provisioning_state}'",
                    "Recrer le compte si ncessaire",
                    "Supprimer le compte si non utilis",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "critical",
            })

        # Scenario 2: HIGH - S0 tier with inaccessible endpoint
        elif sku_name == "S0" and not endpoint_accessible:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority != "critical" else priority
            savings = monthly_cost
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Endpoint inaccessible en tier payant",
                "description": (
                    f"Le compte Document Intelligence S0 a un endpoint inaccessible. "
                    f"Cot: ${monthly_cost:.2f}/mois sans utilisation possible. "
                    f"Actions recommandes: (1) Vrifier la configuration rseau, "
                    f"(2) Corriger les rgles de pare-feu, "
                    f"(3) Downgrade vers F0 ou supprimer si non utilis."
                ),
                "actions": [
                    "Vrifier la configuration rseau et endpoint",
                    "Corriger les rgles de pare-feu/NSG",
                    "Downgrade vers F0 (gratuit) si non utilis",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 3: HIGH - S0 tier without usage monitoring
        elif sku_name == "S0" and not has_diagnostics:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority not in ["critical"] else priority
            # Assume 50% of cost is waste without monitoring
            savings = monthly_cost * 0.5
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Tier payant sans monitoring d'usage",
                "description": (
                    f"Le compte Document Intelligence S0 n'a pas de monitoring configur. "
                    f"Cot actuel: ${monthly_cost:.2f}/mois. "
                    f"Impossible de vrifier si le tier S0 est justifi sans mtriques. "
                    f"Actions recommandes: (1) Activer diagnostic settings, "
                    f"(2) Analyser l'usage rel, "
                    f"(3) Downgrade vers F0 si usage <500 pages/mois."
                ),
                "actions": [
                    "Activer Diagnostic Settings pour tracking usage",
                    "Analyser le volume de pages traites par mois",
                    "Downgrade vers F0 si usage <500 pages/mois",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 4: MEDIUM - S0 tier without private endpoint (security)
        elif sku_name == "S0" and not has_private_endpoint:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority not in ["critical", "high"] else priority
            # Estimate 10% cost for security improvement
            savings = monthly_cost * 0.1
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Pas de Private Endpoint configur",
                "description": (
                    f"Le compte Document Intelligence S0 expose un endpoint public. "
                    f"Cot actuel: ${monthly_cost:.2f}/mois. "
                    f"Risque de scurit pour donnes sensibles (documents). "
                    f"Actions recommandes: (1) Configurer Private Endpoint, "
                    f"(2) Restreindre l'accs rseau, "
                    f"(3) Activer firewall rules."
                ),
                "actions": [
                    "Configurer Azure Private Endpoint",
                    "Restreindre accs rseau (VNet only)",
                    "Activer firewall rules et IP filtering",
                    f"Note: Meilleure scurit pour donnes sensibles"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "medium",
            })

        # Scenario 5: LOW - No diagnostic monitoring configured (F0 tier)
        elif sku_name == "F0" and not has_diagnostics:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            savings = 0  # Free tier, but monitoring helps prevent overages
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Pas de monitoring configur (Free tier)",
                "description": (
                    f"Le compte Document Intelligence F0 (gratuit) n'a pas de monitoring. "
                    f"Sans mtriques, risque de dpasser la limite gratuite (500 pages/mois). "
                    f"Actions recommandes: (1) Activer Diagnostic Settings, "
                    f"(2) Crer des alertes sur quota usage, "
                    f"(3) Monitorer pour viter charges inattendues."
                ),
                "actions": [
                    "Activer Diagnostic Settings",
                    "Crer alertes sur quota usage (500 pages/mois)",
                    "Monitorer pour viter dpassement et charges",
                    "Note: Prvention de cots inattendus"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_computer_vision(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure Computer Vision accounts for cost intelligence."""
        resources = []

        try:
            from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient

            client = CognitiveServicesManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing map (monthly estimates based on usage)
            # F0: Free tier (5K transactions/month)
            # S1: Standard tier ~$1 per 1K transactions
            pricing_map = {
                "F0": 0.0,  # Free tier
                "S1": 150.0,  # Estimate: 150K transactions/month = $150
                "S0": 150.0,  # Legacy tier, same as S1
            }

            # List all Cognitive Services accounts
            accounts = client.accounts.list()

            for account in accounts:
                try:
                    # Filter only ComputerVision accounts
                    if not account.kind or account.kind.lower() != "computervision":
                        continue

                    # Extract metadata
                    account_name = account.name
                    account_id = account.id
                    resource_group = account_id.split("/")[4] if len(account_id.split("/")) > 4 else "unknown"
                    account_location = account.location or region
                    sku_name = account.sku.name if account.sku else "Unknown"
                    provisioning_state = account.properties.provisioning_state if hasattr(account, "properties") and hasattr(account.properties, "provisioning_state") else "Unknown"

                    # Check if endpoint is accessible
                    endpoint_accessible = False
                    if account.properties and hasattr(account.properties, "endpoint") and account.properties.endpoint:
                        endpoint_accessible = True

                    # Get diagnostic settings (monitoring)
                    has_diagnostics = False
                    try:
                        from azure.mgmt.monitor import MonitorManagementClient
                        monitor_client = MonitorManagementClient(
                            credential=self.credential, subscription_id=self.subscription_id
                        )
                        diag_settings = list(monitor_client.diagnostic_settings.list(resource_uri=account_id))
                        has_diagnostics = len(diag_settings) > 0
                    except Exception:
                        pass

                    # Check if private endpoint is configured
                    has_private_endpoint = False
                    if account.properties and hasattr(account.properties, "private_endpoint_connections"):
                        connections = account.properties.private_endpoint_connections or []
                        has_private_endpoint = len(connections) > 0

                    # Get estimated monthly cost
                    monthly_cost = pricing_map.get(sku_name, 150.0)

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations_data,
                    ) = self._calculate_computer_vision_optimization(
                        sku_name=sku_name,
                        provisioning_state=provisioning_state,
                        endpoint_accessible=endpoint_accessible,
                        has_diagnostics=has_diagnostics,
                        has_private_endpoint=has_private_endpoint,
                        monthly_cost=monthly_cost,
                    )

                    # Build AllCloudResourceData
                    resource_data = AllCloudResourceData(
                        resource_id=account_id,
                        resource_name=account_name,
                        resource_type="azure_computer_vision",
                        region=account_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "sku": sku_name,
                            "provisioning_state": provisioning_state,
                            "endpoint_accessible": endpoint_accessible,
                            "has_diagnostics": has_diagnostics,
                            "has_private_endpoint": has_private_endpoint,
                            "resource_group": resource_group,
                            "kind": account.kind,
                        },
                        last_used_at=None,
                        created_at_cloud=None,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_recommendations=recommendations_data,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        f"Error scanning Computer Vision account {account.name}: {str(e)}"
                    )
                    continue

        except Exception as e:
            logger.error(f"Error listing Computer Vision accounts: {str(e)}")

        return resources

    def _calculate_computer_vision_optimization(
        self,
        sku_name: str,
        provisioning_state: str,
        endpoint_accessible: bool,
        has_diagnostics: bool,
        has_private_endpoint: bool,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """
        Calculate Computer Vision optimization based on 5 scenarios.

        Scenarios:
        1. CRITICAL (90): Account failed provisioning
        2. HIGH (75): S1 tier avec endpoint inaccessible
        3. HIGH (70): S1 tier sans usage metrics/monitoring
        4. MEDIUM (50): S1 tier sans private endpoint (scurit images)
        5. LOW (30): F0 tier sans monitoring (risque dpassement quota)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = {"scenarios": []}

        # Scenario 1: CRITICAL - Account failed provisioning
        if provisioning_state.lower() in ["failed", "deleting"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            savings = monthly_cost
            recommendations["scenarios"].append({
                "scenario": "Account en tat d'chec",
                "description": (
                    f"Le compte Computer Vision est en tat '{provisioning_state}'. "
                    f"Cot gaspill: ${monthly_cost:.2f}/mois. "
                    f"Actions recommandes: (1) Vrifier les logs d'erreur, "
                    f"(2) Recrer le compte si ncessaire, "
                    f"(3) Supprimer si non utilis."
                ),
                "actions": [
                    f"Vrifier les logs d'erreur pour '{provisioning_state}'",
                    "Recrer le compte si ncessaire",
                    "Supprimer le compte si non utilis",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "critical",
            })

        # Scenario 2: HIGH - S1 tier avec endpoint inaccessible
        elif sku_name in ["S1", "S0"] and not endpoint_accessible:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority != "critical" else priority
            savings = monthly_cost
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Endpoint inaccessible en tier payant",
                "description": (
                    f"Le compte Computer Vision {sku_name} a un endpoint inaccessible. "
                    f"Cot: ${monthly_cost:.2f}/mois sans utilisation possible. "
                    f"Actions recommandes: (1) Vrifier la configuration rseau, "
                    f"(2) Corriger les rgles de pare-feu, "
                    f"(3) Downgrade vers F0 ou supprimer si non utilis."
                ),
                "actions": [
                    "Vrifier la configuration rseau et endpoint",
                    "Corriger les rgles de pare-feu/NSG",
                    "Downgrade vers F0 (gratuit) si non utilis",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 3: HIGH - S1 tier sans usage metrics/monitoring
        elif sku_name in ["S1", "S0"] and not has_diagnostics:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority not in ["critical"] else priority
            # Assume 50% of cost is waste without monitoring
            savings = monthly_cost * 0.5
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Tier payant sans monitoring d'usage",
                "description": (
                    f"Le compte Computer Vision {sku_name} n'a pas de monitoring configur. "
                    f"Cot actuel: ${monthly_cost:.2f}/mois. "
                    f"Impossible de vrifier si le tier {sku_name} est justifi sans mtriques. "
                    f"Actions recommandes: (1) Activer diagnostic settings, "
                    f"(2) Analyser l'usage rel (transactions/mois), "
                    f"(3) Downgrade vers F0 si usage <5K transactions/mois."
                ),
                "actions": [
                    "Activer Diagnostic Settings pour tracking usage",
                    "Analyser le volume de transactions par mois",
                    "Downgrade vers F0 si usage <5K transactions/mois",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 4: MEDIUM - S1 tier sans private endpoint (scurit images)
        elif sku_name in ["S1", "S0"] and not has_private_endpoint:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority not in ["critical", "high"] else priority
            # Estimate 10% cost for security improvement
            savings = monthly_cost * 0.1
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Pas de Private Endpoint configur",
                "description": (
                    f"Le compte Computer Vision {sku_name} expose un endpoint public. "
                    f"Cot actuel: ${monthly_cost:.2f}/mois. "
                    f"Risque de scurit pour images sensibles (OCR, analyse visuelle). "
                    f"Actions recommandes: (1) Configurer Private Endpoint, "
                    f"(2) Restreindre l'accs rseau, "
                    f"(3) Activer firewall rules."
                ),
                "actions": [
                    "Configurer Azure Private Endpoint",
                    "Restreindre accs rseau (VNet only)",
                    "Activer firewall rules et IP filtering",
                    "Note: Meilleure scurit pour images sensibles"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "medium",
            })

        # Scenario 5: LOW - F0 tier sans monitoring (risque dpassement quota)
        elif sku_name == "F0" and not has_diagnostics:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            savings = 0  # Free tier, but monitoring helps prevent overages
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Pas de monitoring configur (Free tier)",
                "description": (
                    f"Le compte Computer Vision F0 (gratuit) n'a pas de monitoring. "
                    f"Sans mtriques, risque de dpasser la limite gratuite (5K transactions/mois). "
                    f"Actions recommandes: (1) Activer Diagnostic Settings, "
                    f"(2) Crer des alertes sur quota usage, "
                    f"(3) Monitorer pour viter charges inattendues."
                ),
                "actions": [
                    "Activer Diagnostic Settings",
                    "Crer alertes sur quota usage (5K transactions/mois)",
                    "Monitorer pour viter dpassement et charges",
                    "Note: Prvention de cots inattendus"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_face_api(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure Face API accounts for cost intelligence."""
        resources = []

        try:
            from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient

            client = CognitiveServicesManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing map (monthly estimates based on usage)
            # F0: Free tier (30K transactions/month)
            # S0: Standard tier $0.40-$1 per 1K transactions
            pricing_map = {
                "F0": 0.0,  # Free tier
                "S0": 150.0,  # Estimate: 150K transactions/month = $150
            }

            # List all Cognitive Services accounts
            accounts = client.accounts.list()

            for account in accounts:
                try:
                    # Filter only Face accounts
                    if not account.kind or account.kind.lower() != "face":
                        continue

                    # Extract metadata
                    account_name = account.name
                    account_id = account.id
                    resource_group = account_id.split("/")[4] if len(account_id.split("/")) > 4 else "unknown"
                    account_location = account.location or region
                    sku_name = account.sku.name if account.sku else "Unknown"
                    provisioning_state = account.properties.provisioning_state if hasattr(account, "properties") and hasattr(account.properties, "provisioning_state") else "Unknown"

                    # Check if endpoint is accessible
                    endpoint_accessible = False
                    if account.properties and hasattr(account.properties, "endpoint") and account.properties.endpoint:
                        endpoint_accessible = True

                    # Get diagnostic settings (monitoring)
                    has_diagnostics = False
                    try:
                        from azure.mgmt.monitor import MonitorManagementClient
                        monitor_client = MonitorManagementClient(
                            credential=self.credential, subscription_id=self.subscription_id
                        )
                        diag_settings = list(monitor_client.diagnostic_settings.list(resource_uri=account_id))
                        has_diagnostics = len(diag_settings) > 0
                    except Exception:
                        pass

                    # Check if private endpoint is configured
                    has_private_endpoint = False
                    if account.properties and hasattr(account.properties, "private_endpoint_connections"):
                        connections = account.properties.private_endpoint_connections or []
                        has_private_endpoint = len(connections) > 0

                    # Get estimated monthly cost
                    monthly_cost = pricing_map.get(sku_name, 150.0)

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations_data,
                    ) = self._calculate_face_api_optimization(
                        sku_name=sku_name,
                        provisioning_state=provisioning_state,
                        endpoint_accessible=endpoint_accessible,
                        has_diagnostics=has_diagnostics,
                        has_private_endpoint=has_private_endpoint,
                        monthly_cost=monthly_cost,
                    )

                    # Build AllCloudResourceData
                    resource_data = AllCloudResourceData(
                        resource_id=account_id,
                        resource_name=account_name,
                        resource_type="azure_face_api",
                        region=account_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "sku": sku_name,
                            "provisioning_state": provisioning_state,
                            "endpoint_accessible": endpoint_accessible,
                            "has_diagnostics": has_diagnostics,
                            "has_private_endpoint": has_private_endpoint,
                            "resource_group": resource_group,
                            "kind": account.kind,
                        },
                        last_used_at=None,
                        created_at_cloud=None,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_recommendations=recommendations_data,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        f"Error scanning Face API account {account.name}: {str(e)}"
                    )
                    continue

        except Exception as e:
            logger.error(f"Error listing Face API accounts: {str(e)}")

        return resources

    def _calculate_face_api_optimization(
        self,
        sku_name: str,
        provisioning_state: str,
        endpoint_accessible: bool,
        has_diagnostics: bool,
        has_private_endpoint: bool,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """
        Calculate Face API optimization based on 5 scenarios.

        Scenarios:
        1. CRITICAL (90): Account failed provisioning
        2. HIGH (75): S0 tier avec endpoint inaccessible
        3. HIGH (70): S0 tier sans usage metrics
        4. MEDIUM (50): S0 tier sans private endpoint (donnes biomtriques sensibles)
        5. LOW (30): F0 tier sans monitoring quota
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = {"scenarios": []}

        # Scenario 1: CRITICAL - Account failed provisioning
        if provisioning_state.lower() in ["failed", "deleting"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            savings = monthly_cost
            recommendations["scenarios"].append({
                "scenario": "Account en tat d'chec",
                "description": (
                    f"Le compte Face API est en tat '{provisioning_state}'. "
                    f"Cot gaspill: ${monthly_cost:.2f}/mois. "
                    f"Actions recommandes: (1) Vrifier les logs d'erreur, "
                    f"(2) Recrer le compte si ncessaire, "
                    f"(3) Supprimer si non utilis."
                ),
                "actions": [
                    f"Vrifier les logs d'erreur pour '{provisioning_state}'",
                    "Recrer le compte si ncessaire",
                    "Supprimer le compte si non utilis",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "critical",
            })

        # Scenario 2: HIGH - S0 tier avec endpoint inaccessible
        elif sku_name == "S0" and not endpoint_accessible:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority != "critical" else priority
            savings = monthly_cost
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Endpoint inaccessible en tier payant",
                "description": (
                    f"Le compte Face API S0 a un endpoint inaccessible. "
                    f"Cot: ${monthly_cost:.2f}/mois sans utilisation possible. "
                    f"Actions recommandes: (1) Vrifier la configuration rseau, "
                    f"(2) Corriger les rgles de pare-feu, "
                    f"(3) Downgrade vers F0 ou supprimer si non utilis."
                ),
                "actions": [
                    "Vrifier la configuration rseau et endpoint",
                    "Corriger les rgles de pare-feu/NSG",
                    "Downgrade vers F0 (gratuit) si non utilis",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 3: HIGH - S0 tier sans usage metrics
        elif sku_name == "S0" and not has_diagnostics:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority not in ["critical"] else priority
            # Assume 50% of cost is waste without monitoring
            savings = monthly_cost * 0.5
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Tier payant sans monitoring d'usage",
                "description": (
                    f"Le compte Face API S0 n'a pas de monitoring configur. "
                    f"Cot actuel: ${monthly_cost:.2f}/mois. "
                    f"Impossible de vrifier si le tier S0 est justifi sans mtriques. "
                    f"Actions recommandes: (1) Activer diagnostic settings, "
                    f"(2) Analyser l'usage rel (face detection/verification), "
                    f"(3) Downgrade vers F0 si usage <30K transactions/mois."
                ),
                "actions": [
                    "Activer Diagnostic Settings pour tracking usage",
                    "Analyser le volume de dtections faciales par mois",
                    "Downgrade vers F0 si usage <30K transactions/mois",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 4: MEDIUM - S0 tier sans private endpoint (donnes biomtriques)
        elif sku_name == "S0" and not has_private_endpoint:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority not in ["critical", "high"] else priority
            # Estimate 15% cost for security improvement (biometric data is critical)
            savings = monthly_cost * 0.15
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Pas de Private Endpoint pour donnes biomtriques",
                "description": (
                    f"Le compte Face API S0 expose un endpoint public. "
                    f"Cot actuel: ${monthly_cost:.2f}/mois. "
                    f"RISQUE CRITIQUE: Donnes biomtriques sensibles (RGPD/GDPR). "
                    f"Actions recommandes: (1) Configurer Private Endpoint URGENT, "
                    f"(2) Restreindre l'accs rseau, "
                    f"(3) Activer firewall rules."
                ),
                "actions": [
                    "URGENT: Configurer Azure Private Endpoint",
                    "Restreindre accs rseau (VNet only)",
                    "Activer firewall rules et IP filtering",
                    "Note: Conformit RGPD pour donnes biomtriques"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "medium",
            })

        # Scenario 5: LOW - F0 tier sans monitoring quota
        elif sku_name == "F0" and not has_diagnostics:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            savings = 0  # Free tier, but monitoring helps prevent overages
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Pas de monitoring configur (Free tier)",
                "description": (
                    f"Le compte Face API F0 (gratuit) n'a pas de monitoring. "
                    f"Sans mtriques, risque de dpasser la limite gratuite (30K transactions/mois). "
                    f"Actions recommandes: (1) Activer Diagnostic Settings, "
                    f"(2) Crer des alertes sur quota usage, "
                    f"(3) Monitorer pour viter charges inattendues."
                ),
                "actions": [
                    "Activer Diagnostic Settings",
                    "Crer alertes sur quota usage (30K transactions/mois)",
                    "Monitorer pour viter dpassement et charges",
                    "Note: Prvention de cots inattendus"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_text_analytics(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure Text Analytics (Language Service) accounts for cost intelligence."""
        resources = []

        try:
            from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient

            client = CognitiveServicesManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing map (monthly estimates based on usage)
            # F0: Free tier (5K text records/month)
            # S: Standard tier ~$2 per 1K text records
            pricing_map = {
                "F0": 0.0,  # Free tier
                "S": 200.0,  # Estimate: 100K text records/month = $200
                "S0": 200.0,  # Legacy tier, same as S
            }

            # List all Cognitive Services accounts
            accounts = client.accounts.list()

            for account in accounts:
                try:
                    # Filter only TextAnalytics accounts
                    if not account.kind or account.kind.lower() != "textanalytics":
                        continue

                    # Extract metadata
                    account_name = account.name
                    account_id = account.id
                    resource_group = account_id.split("/")[4] if len(account_id.split("/")) > 4 else "unknown"
                    account_location = account.location or region
                    sku_name = account.sku.name if account.sku else "Unknown"
                    provisioning_state = account.properties.provisioning_state if hasattr(account, "properties") and hasattr(account.properties, "provisioning_state") else "Unknown"

                    # Check if endpoint is accessible
                    endpoint_accessible = False
                    if account.properties and hasattr(account.properties, "endpoint") and account.properties.endpoint:
                        endpoint_accessible = True

                    # Get diagnostic settings (monitoring)
                    has_diagnostics = False
                    try:
                        from azure.mgmt.monitor import MonitorManagementClient
                        monitor_client = MonitorManagementClient(
                            credential=self.credential, subscription_id=self.subscription_id
                        )
                        diag_settings = list(monitor_client.diagnostic_settings.list(resource_uri=account_id))
                        has_diagnostics = len(diag_settings) > 0
                    except Exception:
                        pass

                    # Check if private endpoint is configured
                    has_private_endpoint = False
                    if account.properties and hasattr(account.properties, "private_endpoint_connections"):
                        connections = account.properties.private_endpoint_connections or []
                        has_private_endpoint = len(connections) > 0

                    # Get estimated monthly cost
                    monthly_cost = pricing_map.get(sku_name, 200.0)

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations_data,
                    ) = self._calculate_text_analytics_optimization(
                        sku_name=sku_name,
                        provisioning_state=provisioning_state,
                        endpoint_accessible=endpoint_accessible,
                        has_diagnostics=has_diagnostics,
                        has_private_endpoint=has_private_endpoint,
                        monthly_cost=monthly_cost,
                    )

                    # Build AllCloudResourceData
                    resource_data = AllCloudResourceData(
                        resource_id=account_id,
                        resource_name=account_name,
                        resource_type="azure_text_analytics",
                        region=account_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "sku": sku_name,
                            "provisioning_state": provisioning_state,
                            "endpoint_accessible": endpoint_accessible,
                            "has_diagnostics": has_diagnostics,
                            "has_private_endpoint": has_private_endpoint,
                            "resource_group": resource_group,
                            "kind": account.kind,
                        },
                        last_used_at=None,
                        created_at_cloud=None,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_recommendations=recommendations_data,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        f"Error scanning Text Analytics account {account.name}: {str(e)}"
                    )
                    continue

        except Exception as e:
            logger.error(f"Error listing Text Analytics accounts: {str(e)}")

        return resources

    def _calculate_text_analytics_optimization(
        self,
        sku_name: str,
        provisioning_state: str,
        endpoint_accessible: bool,
        has_diagnostics: bool,
        has_private_endpoint: bool,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """
        Calculate Text Analytics optimization based on 5 scenarios.

        Scenarios:
        1. CRITICAL (90): Account failed provisioning
        2. HIGH (75): S tier avec endpoint inaccessible
        3. HIGH (70): S tier sans usage metrics
        4. MEDIUM (50): S tier sans private endpoint
        5. LOW (30): F0 tier sans monitoring
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = {"scenarios": []}

        # Scenario 1: CRITICAL - Account failed provisioning
        if provisioning_state.lower() in ["failed", "deleting"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            savings = monthly_cost
            recommendations["scenarios"].append({
                "scenario": "Account en tat d'chec",
                "description": (
                    f"Le compte Text Analytics est en tat '{provisioning_state}'. "
                    f"Cot gaspill: ${monthly_cost:.2f}/mois. "
                    f"Actions recommandes: (1) Vrifier les logs d'erreur, "
                    f"(2) Recrer le compte si ncessaire, "
                    f"(3) Supprimer si non utilis."
                ),
                "actions": [
                    f"Vrifier les logs d'erreur pour '{provisioning_state}'",
                    "Recrer le compte si ncessaire",
                    "Supprimer le compte si non utilis",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "critical",
            })

        # Scenario 2: HIGH - S tier avec endpoint inaccessible
        elif sku_name in ["S", "S0"] and not endpoint_accessible:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority != "critical" else priority
            savings = monthly_cost
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Endpoint inaccessible en tier payant",
                "description": (
                    f"Le compte Text Analytics {sku_name} a un endpoint inaccessible. "
                    f"Cot: ${monthly_cost:.2f}/mois sans utilisation possible. "
                    f"Actions recommandes: (1) Vrifier la configuration rseau, "
                    f"(2) Corriger les rgles de pare-feu, "
                    f"(3) Downgrade vers F0 ou supprimer si non utilis."
                ),
                "actions": [
                    "Vrifier la configuration rseau et endpoint",
                    "Corriger les rgles de pare-feu/NSG",
                    "Downgrade vers F0 (gratuit) si non utilis",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 3: HIGH - S tier sans usage metrics
        elif sku_name in ["S", "S0"] and not has_diagnostics:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority not in ["critical"] else priority
            # Assume 50% of cost is waste without monitoring
            savings = monthly_cost * 0.5
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Tier payant sans monitoring d'usage",
                "description": (
                    f"Le compte Text Analytics {sku_name} n'a pas de monitoring configur. "
                    f"Cot actuel: ${monthly_cost:.2f}/mois. "
                    f"Impossible de vrifier si le tier {sku_name} est justifi sans mtriques. "
                    f"Actions recommandes: (1) Activer diagnostic settings, "
                    f"(2) Analyser l'usage rel (text records/mois), "
                    f"(3) Downgrade vers F0 si usage <5K text records/mois."
                ),
                "actions": [
                    "Activer Diagnostic Settings pour tracking usage",
                    "Analyser le volume de text records par mois",
                    "Downgrade vers F0 si usage <5K text records/mois",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 4: MEDIUM - S tier sans private endpoint
        elif sku_name in ["S", "S0"] and not has_private_endpoint:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority not in ["critical", "high"] else priority
            # Estimate 10% cost for security improvement
            savings = monthly_cost * 0.1
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Pas de Private Endpoint configur",
                "description": (
                    f"Le compte Text Analytics {sku_name} expose un endpoint public. "
                    f"Cot actuel: ${monthly_cost:.2f}/mois. "
                    f"Risque de scurit pour donnes textuelles sensibles (NER, PII). "
                    f"Actions recommandes: (1) Configurer Private Endpoint, "
                    f"(2) Restreindre l'accs rseau, "
                    f"(3) Activer firewall rules."
                ),
                "actions": [
                    "Configurer Azure Private Endpoint",
                    "Restreindre accs rseau (VNet only)",
                    "Activer firewall rules et IP filtering",
                    "Note: Meilleure scurit pour donnes textuelles sensibles"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "medium",
            })

        # Scenario 5: LOW - F0 tier sans monitoring
        elif sku_name == "F0" and not has_diagnostics:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            savings = 0  # Free tier, but monitoring helps prevent overages
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Pas de monitoring configur (Free tier)",
                "description": (
                    f"Le compte Text Analytics F0 (gratuit) n'a pas de monitoring. "
                    f"Sans mtriques, risque de dpasser la limite gratuite (5K text records/mois). "
                    f"Actions recommandes: (1) Activer Diagnostic Settings, "
                    f"(2) Crer des alertes sur quota usage, "
                    f"(3) Monitorer pour viter charges inattendues."
                ),
                "actions": [
                    "Activer Diagnostic Settings",
                    "Crer alertes sur quota usage (5K text records/mois)",
                    "Monitorer pour viter dpassement et charges",
                    "Note: Prvention de cots inattendus"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_speech_services(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure Speech Services accounts for cost intelligence."""
        resources = []

        try:
            from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient

            client = CognitiveServicesManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing map (monthly estimates based on usage)
            # F0: Free tier (5 audio hours/month STT + 0.5M chars/month TTS)
            # S0: Standard tier $1/hour STT + $16 per million chars TTS Neural
            pricing_map = {
                "F0": 0.0,  # Free tier
                "S0": 200.0,  # Estimate: 100 hours STT + 100K chars TTS = $200
            }

            # List all Cognitive Services accounts
            accounts = client.accounts.list()

            for account in accounts:
                try:
                    # Filter only SpeechServices accounts
                    if not account.kind or account.kind.lower() != "speechservices":
                        continue

                    # Extract metadata
                    account_name = account.name
                    account_id = account.id
                    resource_group = account_id.split("/")[4] if len(account_id.split("/")) > 4 else "unknown"
                    account_location = account.location or region
                    sku_name = account.sku.name if account.sku else "Unknown"
                    provisioning_state = account.properties.provisioning_state if hasattr(account, "properties") and hasattr(account.properties, "provisioning_state") else "Unknown"

                    # Check if endpoint is accessible
                    endpoint_accessible = False
                    if account.properties and hasattr(account.properties, "endpoint") and account.properties.endpoint:
                        endpoint_accessible = True

                    # Get diagnostic settings (monitoring)
                    has_diagnostics = False
                    try:
                        from azure.mgmt.monitor import MonitorManagementClient
                        monitor_client = MonitorManagementClient(
                            credential=self.credential, subscription_id=self.subscription_id
                        )
                        diag_settings = list(monitor_client.diagnostic_settings.list(resource_uri=account_id))
                        has_diagnostics = len(diag_settings) > 0
                    except Exception:
                        pass

                    # Check if private endpoint is configured
                    has_private_endpoint = False
                    if account.properties and hasattr(account.properties, "private_endpoint_connections"):
                        connections = account.properties.private_endpoint_connections or []
                        has_private_endpoint = len(connections) > 0

                    # Get estimated monthly cost
                    monthly_cost = pricing_map.get(sku_name, 200.0)

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations_data,
                    ) = self._calculate_speech_services_optimization(
                        sku_name=sku_name,
                        provisioning_state=provisioning_state,
                        endpoint_accessible=endpoint_accessible,
                        has_diagnostics=has_diagnostics,
                        has_private_endpoint=has_private_endpoint,
                        monthly_cost=monthly_cost,
                    )

                    # Build AllCloudResourceData
                    resource_data = AllCloudResourceData(
                        resource_id=account_id,
                        resource_name=account_name,
                        resource_type="azure_speech_services",
                        region=account_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "sku": sku_name,
                            "provisioning_state": provisioning_state,
                            "endpoint_accessible": endpoint_accessible,
                            "has_diagnostics": has_diagnostics,
                            "has_private_endpoint": has_private_endpoint,
                            "resource_group": resource_group,
                            "kind": account.kind,
                        },
                        last_used_at=None,
                        created_at_cloud=None,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_recommendations=recommendations_data,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        f"Error scanning Speech Services account {account.name}: {str(e)}"
                    )
                    continue

        except Exception as e:
            logger.error(f"Error listing Speech Services accounts: {str(e)}")

        return resources

    def _calculate_speech_services_optimization(
        self,
        sku_name: str,
        provisioning_state: str,
        endpoint_accessible: bool,
        has_diagnostics: bool,
        has_private_endpoint: bool,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """
        Calculate Speech Services optimization based on 5 scenarios.

        Scenarios:
        1. CRITICAL (90): Account failed provisioning
        2. HIGH (75): S0 tier avec endpoint inaccessible
        3. HIGH (70): S0 tier sans usage metrics
        4. MEDIUM (50): S0 tier sans private endpoint (audio sensible)
        5. LOW (30): F0 tier sans monitoring quota
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = {"scenarios": []}

        # Scenario 1: CRITICAL - Account failed provisioning
        if provisioning_state.lower() in ["failed", "deleting"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            savings = monthly_cost
            recommendations["scenarios"].append({
                "scenario": "Account en tat d'chec",
                "description": (
                    f"Le compte Speech Services est en tat '{provisioning_state}'. "
                    f"Cot gaspill: ${monthly_cost:.2f}/mois. "
                    f"Actions recommandes: (1) Vrifier les logs d'erreur, "
                    f"(2) Recrer le compte si ncessaire, "
                    f"(3) Supprimer si non utilis."
                ),
                "actions": [
                    f"Vrifier les logs d'erreur pour '{provisioning_state}'",
                    "Recrer le compte si ncessaire",
                    "Supprimer le compte si non utilis",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "critical",
            })

        # Scenario 2: HIGH - S0 tier avec endpoint inaccessible
        elif sku_name == "S0" and not endpoint_accessible:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority != "critical" else priority
            savings = monthly_cost
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Endpoint inaccessible en tier payant",
                "description": (
                    f"Le compte Speech Services S0 a un endpoint inaccessible. "
                    f"Cot: ${monthly_cost:.2f}/mois sans utilisation possible. "
                    f"Actions recommandes: (1) Vrifier la configuration rseau, "
                    f"(2) Corriger les rgles de pare-feu, "
                    f"(3) Downgrade vers F0 ou supprimer si non utilis."
                ),
                "actions": [
                    "Vrifier la configuration rseau et endpoint",
                    "Corriger les rgles de pare-feu/NSG",
                    "Downgrade vers F0 (gratuit) si non utilis",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 3: HIGH - S0 tier sans usage metrics
        elif sku_name == "S0" and not has_diagnostics:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority not in ["critical"] else priority
            # Assume 50% of cost is waste without monitoring
            savings = monthly_cost * 0.5
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Tier payant sans monitoring d'usage",
                "description": (
                    f"Le compte Speech Services S0 n'a pas de monitoring configur. "
                    f"Cot actuel: ${monthly_cost:.2f}/mois. "
                    f"Impossible de vrifier si le tier S0 est justifi sans mtriques. "
                    f"Actions recommandes: (1) Activer diagnostic settings, "
                    f"(2) Analyser l'usage rel (STT hours + TTS chars), "
                    f"(3) Downgrade vers F0 si usage <5 hours STT + <0.5M chars TTS/mois."
                ),
                "actions": [
                    "Activer Diagnostic Settings pour tracking usage",
                    "Analyser le volume STT (hours) et TTS (chars) par mois",
                    "Downgrade vers F0 si faible usage",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 4: MEDIUM - S0 tier sans private endpoint (audio sensible)
        elif sku_name == "S0" and not has_private_endpoint:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority not in ["critical", "high"] else priority
            # Estimate 10% cost for security improvement
            savings = monthly_cost * 0.1
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Pas de Private Endpoint pour audio sensible",
                "description": (
                    f"Le compte Speech Services S0 expose un endpoint public. "
                    f"Cot actuel: ${monthly_cost:.2f}/mois. "
                    f"Risque de scurit pour donnes audio sensibles (voix, conversations). "
                    f"Actions recommandes: (1) Configurer Private Endpoint, "
                    f"(2) Restreindre l'accs rseau, "
                    f"(3) Activer firewall rules."
                ),
                "actions": [
                    "Configurer Azure Private Endpoint",
                    "Restreindre accs rseau (VNet only)",
                    "Activer firewall rules et IP filtering",
                    "Note: Meilleure scurit pour donnes audio sensibles"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "medium",
            })

        # Scenario 5: LOW - F0 tier sans monitoring quota
        elif sku_name == "F0" and not has_diagnostics:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            savings = 0  # Free tier, but monitoring helps prevent overages
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Pas de monitoring configur (Free tier)",
                "description": (
                    f"Le compte Speech Services F0 (gratuit) n'a pas de monitoring. "
                    f"Sans mtriques, risque de dpasser limites gratuites (5 hours STT + 0.5M chars TTS/mois). "
                    f"Actions recommandes: (1) Activer Diagnostic Settings, "
                    f"(2) Crer des alertes sur quota usage, "
                    f"(3) Monitorer pour viter charges inattendues."
                ),
                "actions": [
                    "Activer Diagnostic Settings",
                    "Crer alertes sur quota usage (STT + TTS)",
                    "Monitorer pour viter dpassement et charges",
                    "Note: Prvention de cots inattendus"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_bot_service(self, region: str) -> list[AllCloudResourceData]:
        """Scan ALL Azure Bot Service resources for cost intelligence."""
        resources = []

        try:
            from azure.mgmt.botservice import AzureBotService

            client = AzureBotService(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing: Standard Channels FREE, Premium Channels $0.50 per 1K messages after 10K free
            # Estimate: $50/month for 110K Premium messages

            # List all bot resources
            bots = client.bots.list()

            for bot in bots:
                try:
                    # Extract metadata
                    bot_name = bot.name
                    bot_id = bot.id
                    resource_group = bot_id.split("/")[4] if len(bot_id.split("/")) > 4 else "unknown"
                    bot_location = bot.location or region
                    bot_kind = bot.kind if hasattr(bot, "kind") and bot.kind else "Unknown"

                    # Get bot properties
                    provisioning_state = bot.properties.provisioning_state if hasattr(bot.properties, "provisioning_state") else "Unknown"
                    endpoint = bot.properties.endpoint if hasattr(bot.properties, "endpoint") and bot.properties.endpoint else None

                    # Check if bot has Application Insights configured
                    has_app_insights = False
                    if hasattr(bot.properties, "developer_app_insights_key") and bot.properties.developer_app_insights_key:
                        has_app_insights = True

                    # Count configured channels
                    channels_count = 0
                    try:
                        channels_list = list(client.channels.list_by_resource_group(
                            resource_group_name=resource_group,
                            resource_name=bot_name
                        ))
                        channels_count = len(channels_list)
                    except Exception:
                        pass

                    # Estimate monthly cost based on Premium channels
                    # Assumption: if bot exists, estimate ~$50/month for Premium usage
                    monthly_cost = 50.0 if channels_count > 0 else 0.0

                    # Calculate optimization
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        potential_savings,
                        recommendations_data,
                    ) = self._calculate_bot_service_optimization(
                        bot_kind=bot_kind,
                        provisioning_state=provisioning_state,
                        channels_count=channels_count,
                        has_app_insights=has_app_insights,
                        endpoint=endpoint,
                        monthly_cost=monthly_cost,
                    )

                    # Build AllCloudResourceData
                    resource_data = AllCloudResourceData(
                        resource_id=bot_id,
                        resource_name=bot_name,
                        resource_type="azure_bot_service",
                        region=bot_location,
                        estimated_monthly_cost=round(monthly_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "kind": bot_kind,
                            "provisioning_state": provisioning_state,
                            "channels_count": channels_count,
                            "has_app_insights": has_app_insights,
                            "endpoint": endpoint or "Not configured",
                            "resource_group": resource_group,
                        },
                        last_used_at=None,
                        created_at_cloud=None,
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_recommendations=recommendations_data,
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        f"Error scanning Bot Service {bot.name}: {str(e)}"
                    )
                    continue

        except Exception as e:
            logger.error(f"Error listing Bot Service resources: {str(e)}")

        return resources

    def _calculate_bot_service_optimization(
        self,
        bot_kind: str,
        provisioning_state: str,
        channels_count: int,
        has_app_insights: bool,
        endpoint: str | None,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, dict]:
        """
        Calculate Bot Service optimization based on 5 scenarios.

        Scenarios:
        1. CRITICAL (90): Bot resource failed/deleting
        2. HIGH (75): Bot avec channels configurs mais aucun endpoint
        3. HIGH (70): Bot sans channels configurs
        4. MEDIUM (50): Bot sans monitoring/Application Insights
        5. LOW (30): Bot sans backup ou disaster recovery
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = {"scenarios": []}

        # Scenario 1: CRITICAL - Bot failed/deleting
        if provisioning_state.lower() in ["failed", "deleting"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            savings = monthly_cost
            recommendations["scenarios"].append({
                "scenario": "Bot en tat d'chec",
                "description": (
                    f"Le Bot Service est en tat '{provisioning_state}'. "
                    f"Cot potentiel gaspill: ${monthly_cost:.2f}/mois. "
                    f"Actions recommandes: (1) Vrifier les logs d'erreur, "
                    f"(2) Recrer le bot si ncessaire, "
                    f"(3) Supprimer si non utilis."
                ),
                "actions": [
                    f"Vrifier les logs d'erreur pour '{provisioning_state}'",
                    "Recrer le bot si ncessaire",
                    "Supprimer le bot si obsolte",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "critical",
            })

        # Scenario 2: HIGH - Bot avec channels mais sans endpoint
        elif channels_count > 0 and not endpoint:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority != "critical" else priority
            savings = monthly_cost
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Bot avec channels mais sans endpoint",
                "description": (
                    f"Le Bot a {channels_count} channel(s) configur(s) mais aucun endpoint. "
                    f"Cot: ${monthly_cost:.2f}/mois sans possibilit de rpondre aux messages. "
                    f"Actions recommandes: (1) Configurer l'endpoint du bot, "
                    f"(2) Dployer le code du bot, "
                    f"(3) Supprimer les channels si bot non utilis."
                ),
                "actions": [
                    "Configurer l'endpoint du bot (messaging endpoint)",
                    "Dployer le code du bot sur Azure App Service/Functions",
                    f"Supprimer les {channels_count} channel(s) si non utilis",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 3: HIGH - Bot sans channels configurs
        elif channels_count == 0 and monthly_cost == 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority not in ["critical"] else priority
            # No cost but waste of resource
            savings = 0
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Bot sans channels configurs",
                "description": (
                    f"Le Bot Service n'a aucun channel configur. "
                    f"Bot inutilisable sans channels (Teams, Slack, Web Chat, etc.). "
                    f"Actions recommandes: (1) Configurer au moins un channel, "
                    f"(2) Tester le bot, "
                    f"(3) Supprimer si projet abandonn."
                ),
                "actions": [
                    "Configurer au moins un channel (Teams, Web Chat, Slack...)",
                    "Tester le bot avec le channel configur",
                    "Supprimer le bot si projet abandonn",
                    "Note: Aucun cot actuel mais ressource gaspille"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 4: MEDIUM - Bot sans monitoring/Application Insights
        elif not has_app_insights and channels_count > 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority not in ["critical", "high"] else priority
            # Estimate 10% improvement with monitoring
            savings = monthly_cost * 0.1
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Pas de monitoring Application Insights",
                "description": (
                    f"Le Bot Service n'a pas Application Insights configur. "
                    f"Cot actuel: ${monthly_cost:.2f}/mois. "
                    f"Sans tlmtrie, impossible d'analyser conversations et optimiser. "
                    f"Actions recommandes: (1) Configurer Application Insights, "
                    f"(2) Analyser mtriques (messages, errors, latency), "
                    f"(3) Optimiser le bot bas sur usage rel."
                ),
                "actions": [
                    "Configurer Application Insights pour le bot",
                    "Analyser mtriques (messages count, errors, latency)",
                    "Optimiser le bot bas sur usage rel",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "medium",
            })

        # Scenario 5: LOW - Pas de backup/disaster recovery
        elif channels_count > 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            savings = 0  # Best practice, no direct savings
            potential_savings = max(potential_savings, savings)
            recommendations["scenarios"].append({
                "scenario": "Pas de stratgie de backup configure",
                "description": (
                    f"Le Bot Service n'a pas de stratgie de backup/disaster recovery. "
                    f"En cas de panne, risque de perte de service et impact business. "
                    f"Actions recommandes: (1) Configurer multi-region deployment, "
                    f"(2) Sauvegarder configuration et code du bot, "
                    f"(3) Tester disaster recovery plan."
                ),
                "actions": [
                    "Configurer multi-region deployment pour HA",
                    "Sauvegarder configuration bot (ARM template/Terraform)",
                    "Tester disaster recovery plan",
                    "Note: Meilleure pratique pour continuit de service"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_application_insights(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Application Insights resources for cost intelligence.

        Application Insights = Monitoring/observability service for applications (telemetry, logs, metrics).

        Pricing: Pay-as-you-go $2.30-$2.76/GB ingested (5 GB free/month) OR Commitment Tiers (15-36% discount).
        Typical cost: $50-500/month depending on data volume (10-200 GB/month).
        """
        resources = []

        try:
            from azure.mgmt.applicationinsights import ApplicationInsightsManagementClient

            client = ApplicationInsightsManagementClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing map (monthly estimates based on typical data ingestion)
            # Pay-as-you-go: $2.50/GB average
            # Estimate assumes workspace-based pricing
            pricing_map = {
                "pay_as_you_go_10gb": 25.0,  # 10 GB/month * $2.50 (5 GB free included)
                "pay_as_you_go_50gb": 125.0,  # 50 GB/month * $2.50
                "pay_as_you_go_100gb": 250.0,  # 100 GB/month * $2.50
                "pay_as_you_go_200gb": 500.0,  # 200 GB/month * $2.50
                "commitment_tier_100gb": 200.0,  # 100 GB/day commitment (~20% discount)
            }

            # List all Application Insights components
            components = client.components.list()

            for component in components:
                try:
                    # Extract metadata
                    component_name = component.name
                    resource_group = component.id.split("/")[4] if "/" in component.id else "unknown"
                    location = component.location if hasattr(component, "location") else region
                    provisioning_state = component.provisioning_state if hasattr(component, "provisioning_state") else "Unknown"
                    application_type = component.application_type if hasattr(component, "application_type") else "other"

                    # Check if workspace-based or classic
                    is_workspace_based = False
                    workspace_resource_id = None
                    if hasattr(component, "workspace_resource_id") and component.workspace_resource_id:
                        is_workspace_based = True
                        workspace_resource_id = component.workspace_resource_id

                    # Get ingestion settings
                    daily_cap_gb = None
                    retention_days = 90  # Default
                    if hasattr(component, "ingestion_mode"):
                        ingestion_mode = component.ingestion_mode
                    else:
                        ingestion_mode = "LogAnalytics" if is_workspace_based else "ApplicationInsights"

                    # Try to get daily cap (if available)
                    try:
                        billing = client.component_current_billing_features.get(
                            resource_group_name=resource_group,
                            resource_name=component_name
                        )
                        if hasattr(billing, "current_billing_features"):
                            # Daily cap in GB
                            if "data_volume_cap" in billing.current_billing_features:
                                daily_cap_data = billing.current_billing_features.get("data_volume_cap")
                                if daily_cap_data and hasattr(daily_cap_data, "cap"):
                                    daily_cap_gb = daily_cap_data.cap
                    except Exception:
                        pass  # Daily cap not available

                    # Try to get retention period
                    try:
                        if hasattr(component, "retention_in_days"):
                            retention_days = component.retention_in_days
                    except Exception:
                        pass

                    # Estimate monthly cost (we don't have actual ingestion data via SDK easily)
                    # Default: assume 50 GB/month for typical app
                    estimated_monthly_cost = pricing_map["pay_as_you_go_50gb"]

                    # Calculate optimization opportunities
                    (
                        is_optimizable,
                        optimization_score,
                        optimization_priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_app_insights_optimization(
                        provisioning_state=provisioning_state,
                        is_workspace_based=is_workspace_based,
                        daily_cap_gb=daily_cap_gb,
                        retention_days=retention_days,
                        estimated_monthly_cost=estimated_monthly_cost,
                    )

                    # Build resource metadata
                    resource_metadata = {
                        "component_name": component_name,
                        "resource_group": resource_group,
                        "location": location,
                        "provisioning_state": provisioning_state,
                        "application_type": application_type,
                        "is_workspace_based": is_workspace_based,
                        "workspace_resource_id": workspace_resource_id,
                        "ingestion_mode": ingestion_mode,
                        "daily_cap_gb": daily_cap_gb,
                        "retention_days": retention_days,
                        "instrumentation_key": component.instrumentation_key if hasattr(component, "instrumentation_key") else None,
                        "connection_string": component.connection_string if hasattr(component, "connection_string") else None,
                    }

                    # Extract tags
                    tags = component.tags if hasattr(component, "tags") and component.tags else {}

                    # Create resource data
                    resource_data = AllCloudResourceData(
                        resource_type="azure_application_insights",
                        resource_id=component.id,
                        resource_name=component_name,
                        region=location,
                        estimated_monthly_cost=estimated_monthly_cost,
                        currency="USD",
                        utilization_status="unknown",  # Would need Azure Monitor metrics
                        is_optimizable=is_optimizable,
                        optimization_priority=optimization_priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations,
                        resource_metadata=resource_metadata,
                        tags=tags,
                        resource_status=provisioning_state,
                        created_at_cloud=None,  # Not available in SDK
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        "azure.application_insights.scan_component_failed",
                        component_id=getattr(component, "id", "unknown"),
                        error=str(e),
                    )
                    continue

            logger.info(
                "azure.application_insights.scan_complete",
                region=region,
                total_components=len(resources),
                optimizable=sum(1 for r in resources if r.is_optimizable),
            )

        except Exception as e:
            logger.error("azure.application_insights.scan_failed", region=region, error=str(e))

        return resources

    def _calculate_app_insights_optimization(
        self,
        provisioning_state: str,
        is_workspace_based: bool,
        daily_cap_gb: float | None,
        retention_days: int,
        estimated_monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate optimization opportunities for Application Insights.

        5 scenarios:
        1. CRITICAL (90): Failed/Deleted state
        2. HIGH (75): No data ingestion 30+ days (unused service)
        3. HIGH (70): Excessive ingestion >100 GB/month without Commitment Tier
        4. MEDIUM (50): Retention >90 days without business need
        5. LOW (30): No Daily Cap configured (budget risk)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Failed or Deleted state
        if provisioning_state.lower() in ["failed", "deleted", "deleting"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            savings = estimated_monthly_cost  # Full cost if removed
            potential_savings += savings

            recommendations.append({
                "scenario": "Application Insights en tat Failed/Deleted",
                "details": (
                    f"Cet Application Insights est en tat '{provisioning_state}' et ne fonctionne plus. "
                    f"Cot mensuel actuel: ${estimated_monthly_cost:.2f}. "
                    f"Actions recommandes: (1) Recrer le composant si ncessaire, "
                    f"(2) Supprimer compltement si obsolte, "
                    f"(3) Vrifier pourquoi le provisioning a chou."
                ),
                "actions": [
                    "Vrifier les logs de provisioning pour cause d'chec",
                    "Recrer Application Insights avec configuration correcte",
                    "OU supprimer dfinitivement si obsolte",
                    "Vrifier quotas et limites subscription Azure"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "critical",
            })

        # Scenario 2: HIGH - No data ingestion for 30+ days (unused service)
        # Note: We can't easily check ingestion volume via SDK, so this is commented for future
        # elif last_ingestion_days >= 30:
        #     is_optimizable = True
        #     optimization_score = 75
        #     priority = "high"
        #     savings = estimated_monthly_cost * 0.9  # 90% savings if deleted

        # Scenario 3: HIGH - Excessive ingestion >100 GB/month without Commitment Tier
        elif estimated_monthly_cost > 250 and not is_workspace_based:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            savings = estimated_monthly_cost * 0.20  # 20% savings with Commitment Tier
            potential_savings += savings

            recommendations.append({
                "scenario": "Ingestion volumineuse sans Commitment Tier",
                "details": (
                    f"Application Insights ingre >100 GB/mois (cot: ${estimated_monthly_cost:.2f}/mois). "
                    f"Utiliser un Commitment Tier peut rduire les cots de 15-36%. "
                    f"conomie potentielle: ${savings:.2f}/mois (20% estim). "
                    f"Actions recommandes: (1) Migrer vers workspace-based avec Commitment Tier, "
                    f"(2) Analyser volume ingestion rel, (3) Optimiser sampling rate si ncessaire."
                ),
                "actions": [
                    "Analyser volume ingestion mensuel exact (Azure Portal)",
                    "Migrer vers workspace-based Application Insights",
                    "Configurer Commitment Tier adapt (100 GB/day, 200 GB/day...)",
                    "Ajuster sampling rate pour rduire volume si pertinent"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 4: MEDIUM - Retention >90 days without business need
        elif retention_days > 90:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Retention cost ~$0.10/GB/month beyond 31 days free
            # Assume 50 GB/month ingestion  50 GB stored
            # Extra 60 days retention (90-30 free)  ~$3/month savings if reduced to 30 days
            savings = 5.0  # Conservative estimate
            potential_savings += savings

            recommendations.append({
                "scenario": f"Rtention excessive ({retention_days} jours)",
                "details": (
                    f"Application Insights retient les donnes pendant {retention_days} jours. "
                    f"Au-del de 31 jours gratuits, la rtention cote ~$0.10/GB/mois. "
                    f"Si pas de besoin mtier, rduire  30-60 jours. "
                    f"conomie potentielle: ${savings:.2f}/mois. "
                    f"Actions recommandes: (1) Vrifier exigences rglementaires/mtier, "
                    f"(2) Rduire rtention  30-60 jours si possible, "
                    f"(3) Exporter donnes anciennes vers stockage froid si archivage ncessaire."
                ),
                "actions": [
                    f"Vrifier si rtention {retention_days} jours est requise (compliance/mtier)",
                    "Rduire rtention  30-60 jours si pas de contrainte",
                    "Configurer export continu vers Azure Storage (archivage long-terme)",
                    "Note: 31 premiers jours gratuits, puis ~$0.10/GB/mois"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "medium",
            })

        # Scenario 5: LOW - No Daily Cap configured (budget risk)
        elif daily_cap_gb is None and estimated_monthly_cost > 50:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            savings = 0.0  # No direct savings, but prevents overage

            recommendations.append({
                "scenario": "Pas de Daily Cap configur (risque dpassement budget)",
                "details": (
                    f"Application Insights n'a pas de Daily Cap configur. "
                    f"Risque: ingestion excessive imprvue  facture inattendue. "
                    f"Cot mensuel actuel: ${estimated_monthly_cost:.2f}. "
                    f"Actions recommandes: (1) Configurer Daily Cap adapt au budget, "
                    f"(2) Configurer alertes dpassement quota, "
                    f"(3) Surveiller ingestion quotidienne."
                ),
                "actions": [
                    "Configurer Daily Cap (ex: 5 GB/day pour app moyenne)",
                    "Activer alertes dpassement quota (Azure Monitor)",
                    "Surveiller ingestion quotidienne (Azure Portal  Usage and estimated costs)",
                    "Note: Prvient factures imprvues mais ne rduit pas cot directement"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_managed_devops_pools(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Managed DevOps Pools for cost intelligence.

        Managed DevOps Pools = Managed infrastructure for Azure DevOps pipeline agents.

        Pricing: 1st parallel job FREE, then $15/month per additional parallel job.
        Typical cost: $15-150/month depending on number of agents.
        """
        resources = []

        try:
            from azure.mgmt.devopsinfrastructure import DevOpsInfrastructureMgmtClient

            client = DevOpsInfrastructureMgmtClient(
                credential=self.credential, subscription_id=self.subscription_id
            )

            # Pricing: $15 per parallel job (first job free)
            price_per_agent = 15.0

            # List all Managed DevOps Pools
            pools = client.pools.list_by_subscription()

            for pool in pools:
                try:
                    # Extract metadata
                    pool_name = pool.name
                    resource_group = pool.id.split("/")[4] if "/" in pool.id else "unknown"
                    location = pool.location if hasattr(pool, "location") else region
                    provisioning_state = pool.properties.provisioning_state if hasattr(pool.properties, "provisioning_state") else "Unknown"

                    # Get pool properties
                    max_agents = 0
                    agent_profile = None
                    organization_profile = None

                    if hasattr(pool.properties, "maximum_concurrency"):
                        max_agents = pool.properties.maximum_concurrency

                    if hasattr(pool.properties, "agent_profile"):
                        agent_profile = pool.properties.agent_profile
                        # Agent profile contains info about VM SKU, images, etc.

                    if hasattr(pool.properties, "dev_ops_organization_profile"):
                        organization_profile = pool.properties.dev_ops_organization_profile
                        # Contains Azure DevOps organization info

                    # Estimate monthly cost
                    # First agent free, then $15 per additional agent
                    if max_agents <= 1:
                        estimated_monthly_cost = 0.0  # First agent free
                    else:
                        estimated_monthly_cost = (max_agents - 1) * price_per_agent

                    # Calculate optimization opportunities
                    (
                        is_optimizable,
                        optimization_score,
                        optimization_priority,
                        potential_savings,
                        recommendations,
                    ) = self._calculate_devops_pools_optimization(
                        provisioning_state=provisioning_state,
                        max_agents=max_agents,
                        agent_profile=agent_profile,
                        estimated_monthly_cost=estimated_monthly_cost,
                    )

                    # Build resource metadata
                    resource_metadata = {
                        "pool_name": pool_name,
                        "resource_group": resource_group,
                        "location": location,
                        "provisioning_state": provisioning_state,
                        "maximum_concurrency": max_agents,
                        "agent_profile": str(agent_profile) if agent_profile else None,
                        "organization_profile": str(organization_profile) if organization_profile else None,
                        "fabric_profile": str(pool.properties.fabric_profile) if hasattr(pool.properties, "fabric_profile") else None,
                    }

                    # Extract tags
                    tags = pool.tags if hasattr(pool, "tags") and pool.tags else {}

                    # Create resource data
                    resource_data = AllCloudResourceData(
                        resource_type="azure_managed_devops_pools",
                        resource_id=pool.id,
                        resource_name=pool_name,
                        region=location,
                        estimated_monthly_cost=estimated_monthly_cost,
                        currency="USD",
                        utilization_status="unknown",  # Would need pipeline run metrics
                        is_optimizable=is_optimizable,
                        optimization_priority=optimization_priority,
                        optimization_score=optimization_score,
                        potential_monthly_savings=potential_savings,
                        optimization_recommendations=recommendations,
                        resource_metadata=resource_metadata,
                        tags=tags,
                        resource_status=provisioning_state,
                        created_at_cloud=None,  # Not available in SDK
                    )

                    resources.append(resource_data)

                except Exception as e:
                    logger.error(
                        "azure.managed_devops_pools.scan_pool_failed",
                        pool_id=getattr(pool, "id", "unknown"),
                        error=str(e),
                    )
                    continue

            logger.info(
                "azure.managed_devops_pools.scan_complete",
                region=region,
                total_pools=len(resources),
                optimizable=sum(1 for r in resources if r.is_optimizable),
            )

        except Exception as e:
            logger.error("azure.managed_devops_pools.scan_failed", region=region, error=str(e))

        return resources

    def _calculate_devops_pools_optimization(
        self,
        provisioning_state: str,
        max_agents: int,
        agent_profile: any,
        estimated_monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate optimization opportunities for Managed DevOps Pools.

        5 scenarios:
        1. CRITICAL (90): Failed/Deleted state
        2. HIGH (75): Pool sans agents depuis 30+ jours (unused)
        3. HIGH (70): Agents idle >80% du temps (over-provisioned)
        4. MEDIUM (50): Pool Dev/Test avec agents premium (Standard suffisant)
        5. LOW (30): Multiple pools consolidables (economy of scale)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Failed or Deleted state
        if provisioning_state.lower() in ["failed", "deleted", "deleting"]:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            savings = estimated_monthly_cost  # Full cost if removed
            potential_savings += savings

            recommendations.append({
                "scenario": "Managed DevOps Pool en tat Failed/Deleted",
                "details": (
                    f"Ce pool DevOps est en tat '{provisioning_state}' et ne fonctionne plus. "
                    f"Cot mensuel actuel: ${estimated_monthly_cost:.2f}. "
                    f"Actions recommandes: (1) Recrer le pool si ncessaire, "
                    f"(2) Supprimer compltement si obsolte, "
                    f"(3) Vrifier pourquoi le provisioning a chou."
                ),
                "actions": [
                    "Vrifier les logs de provisioning pour cause d'chec",
                    "Recrer Managed DevOps Pool avec configuration correcte",
                    "OU supprimer dfinitivement si obsolte",
                    "Vrifier quotas et limites subscription Azure"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "critical",
            })

        # Scenario 2: HIGH - Pool sans agents (never used or abandoned)
        elif max_agents == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            savings = 15.0  # At least one agent's worth
            potential_savings += savings

            recommendations.append({
                "scenario": "Pool DevOps sans agents configurs",
                "details": (
                    f"Le pool '{provisioning_state}' n'a aucun agent configur (max_agents=0). "
                    f"Ce pool ne peut excuter aucun pipeline et gnre des frais fixes. "
                    f"conomie potentielle: ${savings:.2f}/mois si supprim. "
                    f"Actions recommandes: (1) Supprimer le pool si inutilis, "
                    f"(2) OU configurer des agents si besoin futur, "
                    f"(3) Vrifier pipelines Azure DevOps associs."
                ),
                "actions": [
                    "Vrifier si le pool est rfrenc dans des pipelines Azure DevOps",
                    "Supprimer le pool si jamais utilis",
                    "OU configurer agents si besoin pipeline identifi",
                    "Nettoyer pools obsoltes (organisation)"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 3: HIGH - Agents idle >80% (over-provisioned)
        # Note: We can't check actual usage via SDK easily, so this scenario uses agent count heuristic
        elif max_agents > 5:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Assume 30% of agents could be removed
            reducible_agents = int(max_agents * 0.3)
            savings = reducible_agents * 15.0
            potential_savings += savings

            recommendations.append({
                "scenario": f"Pool sur-dimensionn ({max_agents} agents)",
                "details": (
                    f"Le pool a {max_agents} agents configurs. "
                    f"Pour la plupart des organisations, 3-5 agents suffisent. "
                    f"Si agents idle >50% du temps, rduire la capacit. "
                    f"conomie potentielle: ${savings:.2f}/mois en rduisant de ~30%. "
                    f"Actions recommandes: (1) Analyser utilisation relle des agents, "
                    f"(2) Rduire maximum_concurrency si idle, "
                    f"(3) Utiliser autoscaling si pics de charge ponctuels."
                ),
                "actions": [
                    "Analyser utilisation agents via Azure DevOps Analytics",
                    "Identifier taux d'idle (cible: <50% idle)",
                    f"Rduire maximum_concurrency de {max_agents}  {max_agents - reducible_agents}",
                    "Configurer autoscaling pour pics de charge"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "high",
            })

        # Scenario 4: MEDIUM - Dev/Test pool avec agents premium (Standard suffisant)
        # Check if agent_profile suggests premium SKU (heuristic: if profile mentions "Standard_D" or higher)
        elif agent_profile and "Standard_D" in str(agent_profile) and max_agents >= 2:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Assume 20% savings by downgrading to Basic SKU
            savings = estimated_monthly_cost * 0.20
            potential_savings += savings

            recommendations.append({
                "scenario": "Pool Dev/Test avec VM SKU premium",
                "details": (
                    f"Le pool utilise des VM SKU premium (ex: Standard_D series) pour {max_agents} agents. "
                    f"Pour environnements Dev/Test, des SKU Standard ou Basic suffisent souvent. "
                    f"conomie potentielle: ${savings:.2f}/mois (20% estim). "
                    f"Actions recommandes: (1) valuer besoins rels CPU/RAM, "
                    f"(2) Downgrader vers SKU moins cher si pertinent, "
                    f"(3) Rserver SKU premium pour production uniquement."
                ),
                "actions": [
                    "Analyser utilisation CPU/RAM des agents (Azure Monitor)",
                    "Downgrader vers VM SKU Basic/Standard si <50% utilisation",
                    "Rserver SKU premium pour pipelines production critiques",
                    "Estimer conomies: ~20-30% avec SKU infrieur"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "medium",
            })

        # Scenario 5: LOW - Multiple pools consolidables (note: this requires global view, so heuristic)
        elif max_agents == 1 and estimated_monthly_cost == 0:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            savings = 0.0  # No direct savings, but organizational cleanup

            recommendations.append({
                "scenario": "Pool avec 1 agent (gratuit) - consolidation possible",
                "details": (
                    f"Ce pool utilise 1 agent (gratuit). "
                    f"Si l'organisation a plusieurs pools similaires, la consolidation peut simplifier gestion. "
                    f"conomie potentielle: ${savings:.2f}/mois (mais gains organisationnels). "
                    f"Actions recommandes: (1) Auditer tous les pools de l'organisation, "
                    f"(2) Consolider pools similaires, "
                    f"(3) Standardiser configuration agents."
                ),
                "actions": [
                    "Auditer tous Managed DevOps Pools de l'organisation",
                    "Identifier pools redondants (mme projet/quipe)",
                    "Consolider en pools partags (conomie chelle)",
                    "Note: Gains organisationnels > conomies directes"
                ],
                "estimated_savings": round(savings, 2),
                "priority": "low",
            })

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_private_endpoints(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Private Endpoints for cost intelligence.

        Private Endpoints enable private connectivity to Azure services.
        Pricing: ~$7.30/month per Private Endpoint (Standard) + data processing charges
        Typical cost: $7-15/month depending on data transfer volume

        Detection criteria:
        - Private Endpoint failed/deleting (CRITICAL - 90 score)
        - Not connected to any resource (orphan) (HIGH - 75 score)
        - Connected to deallocated/stopped resource (HIGH - 70 score)
        - Redundant endpoints for same resource (MEDIUM - 50 score)
        - Using Premium without justification (LOW - 30 score)
        """
        try:
            from azure.mgmt.network import NetworkManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-network not installed")
            return []

        resources = []
        self.logger.info(f"Scanning Private Endpoints in region: {region}")

        try:
            network_client = NetworkManagementClient(
                credential=self.credential,
                subscription_id=self.subscription_id
            )

            # List all Private Endpoints
            async for endpoint in network_client.private_endpoints.list_all():
                try:
                    # Filter by region if specified
                    if region.lower() != "all" and endpoint.location.lower() != region.lower():
                        continue

                    # Get resource group from endpoint ID
                    resource_group = endpoint.id.split("/")[4]

                    # Get connection state
                    connection_state = "unknown"
                    connected_resource_id = None
                    connected_resource_count = 0

                    if hasattr(endpoint, 'private_link_service_connections'):
                        connections = endpoint.private_link_service_connections or []
                        connected_resource_count = len(connections)
                        if connections:
                            first_conn = connections[0]
                            if hasattr(first_conn, 'private_link_service_connection_state'):
                                conn_state = first_conn.private_link_service_connection_state
                                if conn_state:
                                    connection_state = getattr(conn_state, 'status', 'unknown')
                            if hasattr(first_conn, 'private_link_service_id'):
                                connected_resource_id = first_conn.private_link_service_id

                    # Get provisioning state
                    provisioning_state = getattr(endpoint, 'provisioning_state', 'Unknown')

                    # Calculate optimization
                    is_optimizable, score, priority, savings, recommendations = (
                        self._calculate_private_endpoint_optimization(
                            provisioning_state,
                            connection_state,
                            connected_resource_count,
                            connected_resource_id
                        )
                    )

                    # Pricing (Azure US East 2025)
                    # Private Endpoint: $7.30/month (flat rate)
                    # Data processing: $0.01/GB inbound + $0.01/GB outbound
                    # Typical: $7-15/month depending on traffic
                    base_monthly_cost = 7.30

                    # Estimate data processing cost (assume 100 GB/month average)
                    estimated_data_gb = 100
                    data_processing_cost = estimated_data_gb * 0.02  # $0.01 in + $0.01 out

                    estimated_cost = base_monthly_cost + data_processing_cost

                    resources.append(AllCloudResourceData(
                        resource_id=endpoint.id,
                        resource_type="azure_private_endpoint",
                        resource_name=endpoint.name or "Unnamed Private Endpoint",
                        region=endpoint.location,
                        estimated_monthly_cost=round(estimated_cost, 2),
                        currency="USD",
                        resource_metadata={
                            "endpoint_id": endpoint.id,
                            "resource_group": resource_group,
                            "provisioning_state": provisioning_state,
                            "connection_state": connection_state,
                            "connected_resource_id": connected_resource_id,
                            "connected_resource_count": connected_resource_count,
                            "subnet_id": endpoint.subnet.id if endpoint.subnet else None,
                            "tags": dict(endpoint.tags) if endpoint.tags else {},
                        },
                        is_optimizable=is_optimizable,
                        optimization_score=score,
                        optimization_priority=priority,
                        potential_monthly_savings=savings,
                        optimization_recommendations=recommendations,
                        last_used_at=None,
                        created_at_cloud=None,
                    ))

                except Exception as e:
                    self.logger.error(f"Error processing Private Endpoint {getattr(endpoint, 'name', 'unknown')}: {str(e)}")
                    continue

            self.logger.info(f"Found {len(resources)} Private Endpoints in region {region}")
            return resources

        except Exception as e:
            self.logger.error(f"Error scanning Private Endpoints: {str(e)}")
            return []

    def _calculate_private_endpoint_optimization(
        self,
        provisioning_state: str,
        connection_state: str,
        connected_resource_count: int,
        connected_resource_id: str | None,
    ) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate optimization potential for Private Endpoint.

        Returns:
            (is_optimizable, score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        base_cost = 9.30  # $7.30 base + ~$2 data processing

        # Scenario 1: Private Endpoint failed/deleting (CRITICAL - 90)
        if provisioning_state.lower() in ['failed', 'deleting', 'deleted']:
            is_optimizable = True
            optimization_score = max(optimization_score, 90)
            priority = "critical"
            potential_savings = max(potential_savings, base_cost)

            recommendations.append({
                "title": "Private Endpoint Non Fonctionnel",
                "description": f"Ce Private Endpoint est dans l'tat '{provisioning_state}'. Il gnre des cots inutiles.",
                "estimated_savings": round(base_cost, 2),
                "actions": [
                    "Vrifier les logs pour identifier le problme",
                    "Supprimer le Private Endpoint s'il ne peut pas tre rpar",
                    "Recrer le Private Endpoint si encore ncessaire"
                ],
                "priority": "critical",
            })

        # Scenario 2: Not connected to any resource (orphan) (HIGH - 75)
        if connected_resource_count == 0 or connection_state.lower() in ['rejected', 'disconnected']:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            if priority not in ["critical"]:
                priority = "high"
            potential_savings = max(potential_savings, base_cost)

            recommendations.append({
                "title": "Private Endpoint Non Connect (Orphelin)",
                "description": "Ce Private Endpoint n'est connect  aucune ressource. Il gnre des cots inutiles de $7-9/mois.",
                "estimated_savings": round(base_cost, 2),
                "actions": [
                    "Vrifier si le Private Endpoint est encore ncessaire",
                    "Supprimer si non utilis",
                    "Connecter  une ressource si oubli de configuration",
                    f"conomie: ${base_cost}/mois"
                ],
                "priority": "high",
            })

        # Scenario 3: Connected to deallocated/stopped resource (HIGH - 70)
        # Note: We can't easily check if connected resource is stopped without querying each resource type
        # This would require additional API calls per endpoint
        # In production, you'd check the connected resource status

        # Scenario 4: Redundant endpoints for same resource (MEDIUM - 50)
        if connected_resource_count > 1:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            if priority not in ["critical", "high"]:
                priority = "medium"

            # Assume can eliminate half of redundant connections
            savings = base_cost * 0.5
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Endpoints Redondants Potentiels",
                "description": f"Ce Private Endpoint a {connected_resource_count} connexions. Vrifiez si toutes sont ncessaires.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    f"Analyser les {connected_resource_count} connexions Private Link",
                    "Identifier si plusieurs endpoints pointent vers mme ressource",
                    "Consolider en un seul endpoint si possible",
                    "Note: Chaque endpoint cote $7-9/mois"
                ],
                "priority": "medium",
            })

        # Scenario 5: Using Premium without justification (LOW - 30)
        # Note: Private Endpoints don't have SKUs (Standard/Premium)
        # This scenario doesn't apply to Private Endpoints
        # Leaving as placeholder for consistency

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_ml_endpoints(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure ML Online Endpoints for cost intelligence.

        ML Endpoints are deployed models for real-time inference.
        Pricing: $0.50-$10/hour selon instance SKU ($360-$7200/mois) + inference costs
        Typical cost: $500-3000/month for production endpoints

        Detection criteria:
        - Endpoint failed/unhealthy (CRITICAL - 90 score)
        - Zero inference requests 30+ days (HIGH - 75 score)
        - Overprovisioned compute (traffic < 30% capacity) (HIGH - 70 score)
        - Premium SKU for low-traffic endpoint (MEDIUM - 50 score)
        - No auto-scaling configured (LOW - 30 score)
        """
        try:
            from azure.ai.ml import MLClient
        except ImportError:
            self.logger.error("azure-ai-ml not installed")
            return []

        resources = []
        self.logger.info(f"Scanning ML Endpoints in region: {region}")

        try:
            ml_client = MLClient(
                credential=self.credential,
                subscription_id=self.subscription_id
            )

            # List all ML workspaces first
            from azure.mgmt.machinelearningservices import AzureMachineLearningWorkspaces
            ml_mgmt_client = AzureMachineLearningWorkspaces(
                credential=self.credential,
                subscription_id=self.subscription_id
            )

            for workspace in ml_mgmt_client.workspaces.list():
                try:
                    # Filter by region
                    if region.lower() != "all" and workspace.location.lower() != region.lower():
                        continue

                    # Get resource group
                    resource_group = workspace.id.split("/")[4]

                    # Create MLClient for this specific workspace
                    workspace_ml_client = MLClient(
                        credential=self.credential,
                        subscription_id=self.subscription_id,
                        resource_group_name=resource_group,
                        workspace_name=workspace.name
                    )

                    # List all online endpoints in this workspace
                    endpoints = workspace_ml_client.online_endpoints.list()

                    for endpoint in endpoints:
                        try:
                            # Get endpoint details
                            endpoint_name = endpoint.name
                            provisioning_state = getattr(endpoint, 'provisioning_state', 'Unknown')

                            # Get deployments for this endpoint
                            deployments = list(workspace_ml_client.online_deployments.list(endpoint_name=endpoint_name))
                            deployment_count = len(deployments)

                            # Calculate total instance count and estimate cost
                            total_instances = 0
                            estimated_hourly_cost = 0.0

                            for deployment in deployments:
                                instance_count = getattr(deployment, 'instance_count', 0)
                                total_instances += instance_count

                                # Estimate cost based on instance type
                                # Simplified pricing: Standard_DS2_v2 = $0.50/hour, Premium = $10/hour
                                instance_type = getattr(deployment, 'instance_type', 'Standard_DS2_v2')
                                if 'Premium' in instance_type or 'GPU' in instance_type:
                                    estimated_hourly_cost += instance_count * 10.0
                                elif 'Standard_DS3' in instance_type or 'Standard_F' in instance_type:
                                    estimated_hourly_cost += instance_count * 2.0
                                else:
                                    estimated_hourly_cost += instance_count * 0.50

                            estimated_monthly_cost = estimated_hourly_cost * 730  # hours/month

                            # Calculate optimization
                            is_optimizable, score, priority, savings, recommendations = (
                                self._calculate_ml_endpoint_optimization(
                                    provisioning_state,
                                    deployment_count,
                                    total_instances,
                                    estimated_monthly_cost
                                )
                            )

                            resources.append(AllCloudResourceData(
                                resource_id=f"{workspace.id}/onlineEndpoints/{endpoint_name}",
                                resource_type="azure_ml_endpoint",
                                resource_name=endpoint_name or "Unnamed ML Endpoint",
                                region=workspace.location,
                                estimated_monthly_cost=round(estimated_monthly_cost, 2),
                                currency="USD",
                                resource_metadata={
                                    "endpoint_name": endpoint_name,
                                    "workspace_name": workspace.name,
                                    "resource_group": resource_group,
                                    "provisioning_state": provisioning_state,
                                    "deployment_count": deployment_count,
                                    "total_instances": total_instances,
                                    "estimated_hourly_cost": round(estimated_hourly_cost, 2),
                                },
                                is_optimizable=is_optimizable,
                                optimization_score=score,
                                optimization_priority=priority,
                                potential_monthly_savings=savings,
                                optimization_recommendations=recommendations,
                                last_used_at=None,
                                created_at_cloud=None,
                            ))

                        except Exception as e:
                            self.logger.error(f"Error processing ML endpoint {getattr(endpoint, 'name', 'unknown')}: {str(e)}")
                            continue

                except Exception as e:
                    self.logger.error(f"Error processing workspace {getattr(workspace, 'name', 'unknown')}: {str(e)}")
                    continue

            self.logger.info(f"Found {len(resources)} ML Endpoints in region {region}")
            return resources

        except Exception as e:
            self.logger.error(f"Error scanning ML Endpoints: {str(e)}")
            return []

    def _calculate_ml_endpoint_optimization(
        self,
        provisioning_state: str,
        deployment_count: int,
        total_instances: int,
        estimated_monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict]]:
        """
        Calculate optimization potential for ML Endpoint.

        Returns:
            (is_optimizable, score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: Endpoint failed/unhealthy (CRITICAL - 90)
        if provisioning_state.lower() in ['failed', 'deleting', 'deleted']:
            is_optimizable = True
            optimization_score = max(optimization_score, 90)
            priority = "critical"
            potential_savings = max(potential_savings, estimated_monthly_cost)

            recommendations.append({
                "title": "ML Endpoint Non Fonctionnel",
                "description": f"Cet endpoint est dans l'tat '{provisioning_state}'. Il gnre des cots inutiles.",
                "estimated_savings": round(estimated_monthly_cost, 2),
                "actions": [
                    "Vrifier les logs de dploiement",
                    "Supprimer l'endpoint s'il ne peut pas tre rpar",
                    "Redployer le modle si encore ncessaire"
                ],
                "priority": "critical",
            })

        # Scenario 2: Zero inference requests 30+ days (HIGH - 75)
        # Note: We can't get actual request metrics without Azure Monitor
        # In production, check actual inference request count

        # Scenario 3: Overprovisioned compute (HIGH - 70)
        if total_instances > 3:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            if priority not in ["critical"]:
                priority = "high"

            # Assume can reduce by 50%
            savings = estimated_monthly_cost * 0.5
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "Instances Potentiellement Surdimensionnes",
                "description": f"Cet endpoint a {total_instances} instances. Vrifiez si toutes sont ncessaires.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Analyser les mtriques de trafic dans Azure Monitor",
                    f"Rduire de {total_instances}  {total_instances // 2} instances si charge <30%",
                    "Activer auto-scaling pour adapter automatiquement",
                    f"conomie potentielle: ${savings:.2f}/mois"
                ],
                "priority": "high",
            })

        # Scenario 4: Premium SKU for low-traffic endpoint (MEDIUM - 50)
        if estimated_monthly_cost > 2000 and total_instances <= 2:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            if priority not in ["critical", "high"]:
                priority = "medium"

            # Assume can switch to cheaper SKU (60% savings)
            savings = estimated_monthly_cost * 0.6
            potential_savings = max(potential_savings, savings)

            recommendations.append({
                "title": "SKU Premium pour Trafic Faible",
                "description": f"Cot ${estimated_monthly_cost:.2f}/mois avec peu d'instances suggre Premium SKU inutile.",
                "estimated_savings": round(savings, 2),
                "actions": [
                    "Analyser le trafic rel d'infrence (requtes/jour)",
                    "Passer  Standard_DS2_v2 ou Standard_F2s_v2 si <1000 req/jour",
                    "Utiliser batch inference pour workloads non-temps-rel",
                    f"conomie: ~60% (${savings:.2f}/mois)"
                ],
                "priority": "medium",
            })

        # Scenario 5: No auto-scaling configured (LOW - 30)
        if deployment_count > 0 and total_instances > 1:
            # We can't easily check if auto-scaling is enabled without deployment details
            # This is a placeholder for best practice recommendation
            pass

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_synapse_sql_pools(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Synapse Dedicated SQL Pools for cost intelligence.

        Synapse SQL Pools are massively parallel processing (MPP) data warehouses.
        Pricing: $1.20-$360/hour depending on DWU level (DW100c to DW30000c)
        Typical cost: $900-259,000/month for production data warehouses

        Detection criteria:
        - SQL pool paused >7 days (CRITICAL - 90 score) - Still incurs storage costs
        - Zero queries 30+ days (HIGH - 75 score)
        - Overprovisioned DWUs (query load <30% capacity) (HIGH - 70 score)
        - No auto-pause configured (MEDIUM - 50 score)
        - Gen1 DWU (upgrade to Gen2) (LOW - 30 score)

        Returns:
            List of all Synapse SQL Pools with optimization recommendations
        """
        try:
            from azure.mgmt.synapse import SynapseManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-synapse not installed")
            return []

        resources = []
        self.logger.info(f"Scanning Synapse SQL Pools in region: {region}")

        try:
            # Create Synapse client
            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )
            synapse_client = SynapseManagementClient(credential, self.subscription_id)

            # Iterate through workspaces
            workspaces = synapse_client.workspaces.list()
            workspace_count = 0

            for workspace in workspaces:
                workspace_count += 1
                workspace_name = workspace.name
                workspace_location = workspace.location

                # Filter by region if specified
                if self.regions and workspace_location not in self.regions:
                    continue

                # Filter by resource group if specified
                resource_group = workspace.id.split('/')[4]
                if self.resource_groups and resource_group not in self.resource_groups:
                    continue

                # Get SQL pools in this workspace
                try:
                    sql_pools = synapse_client.sql_pools.list_by_workspace(
                        resource_group_name=resource_group,
                        workspace_name=workspace_name
                    )

                    for pool in sql_pools:
                        pool_name = pool.name
                        pool_status = getattr(pool, 'status', 'Unknown')
                        sku = getattr(pool, 'sku', None)

                        # Get DWU level (e.g., "DW1000c", "DW100c")
                        dw_name = sku.name if sku else "Unknown"
                        tier = sku.tier if sku and hasattr(sku, 'tier') else "Unknown"

                        # Calculate cost based on DWU level
                        monthly_cost = self._estimate_synapse_sql_pool_cost(dw_name, pool_status)

                        # Check optimization opportunities
                        is_optimizable, score, priority, savings, recommendations = \
                            await self._calculate_synapse_sql_pool_optimization(
                                pool, pool_status, dw_name, tier, monthly_cost
                            )

                        # Build metadata
                        metadata = {
                            "workspace_name": workspace_name,
                            "pool_name": pool_name,
                            "status": pool_status,
                            "dw_level": dw_name,
                            "tier": tier,
                            "resource_group": resource_group,
                            "collation": getattr(pool, 'collation', None),
                            "creation_date": getattr(pool, 'creation_date', None),
                            "max_size_bytes": getattr(pool, 'max_size_bytes', None),
                        }

                        # Determine if orphan (paused >90 days = likely abandoned)
                        is_orphan = pool_status.lower() == 'paused' and score >= 90

                        # Create resource record
                        resource = AllCloudResourceData(
                            resource_id=pool.id,
                            resource_name=pool_name,
                            resource_type="azure_synapse_sql_pool",
                            region=workspace_location,
                            estimated_monthly_cost=monthly_cost,
                            currency="USD",
                            resource_metadata=metadata,
                            is_orphan=is_orphan,
                            is_optimizable=is_optimizable and not is_orphan,
                            optimization_score=score if not is_orphan else 0,
                            optimization_priority=priority if not is_orphan else "none",
                            potential_monthly_savings=savings if not is_orphan else 0.0,
                            optimization_recommendations=recommendations if not is_orphan else []
                        )

                        resources.append(resource)
                        self.logger.info(
                            f"Found Synapse SQL Pool: {pool_name} "
                            f"(Status: {pool_status}, DWU: {dw_name}, "
                            f"Cost: ${monthly_cost:.2f}/mo, Optimizable: {is_optimizable})"
                        )

                except Exception as e:
                    self.logger.error(
                        f"Error scanning SQL pools in workspace {workspace_name}: {str(e)}"
                    )
                    continue

            self.logger.info(
                f"Synapse SQL Pool scan complete: {len(resources)} pools found "
                f"across {workspace_count} workspaces"
            )

        except Exception as e:
            self.logger.error(f"Error scanning Synapse SQL Pools: {str(e)}")

        return resources

    def _estimate_synapse_sql_pool_cost(self, dw_name: str, status: str) -> float:
        """
        Estimate monthly cost for Synapse SQL Pool based on DWU level.

        Pricing (Gen2 cDWU - compute optimized):
        - DW100c: $1.20/hour = $876/month
        - DW200c: $2.40/hour = $1,752/month
        - DW500c: $6.00/hour = $4,380/month
        - DW1000c: $12.00/hour = $8,760/month
        - DW2000c: $24.00/hour = $17,520/month
        - DW5000c: $60.00/hour = $43,800/month
        - DW10000c: $120.00/hour = $87,600/month
        - DW15000c: $180.00/hour = $131,400/month
        - DW30000c: $360.00/hour = $262,800/month

        Note: When paused, only storage costs apply (~$120/TB/month)
        """
        hours_per_month = 730  # Average

        # Pricing map for Gen2 cDWU levels
        pricing_map = {
            "DW100c": 1.20,
            "DW200c": 2.40,
            "DW300c": 3.60,
            "DW400c": 4.80,
            "DW500c": 6.00,
            "DW1000c": 12.00,
            "DW1500c": 18.00,
            "DW2000c": 24.00,
            "DW2500c": 30.00,
            "DW3000c": 36.00,
            "DW5000c": 60.00,
            "DW6000c": 72.00,
            "DW7500c": 90.00,
            "DW10000c": 120.00,
            "DW15000c": 180.00,
            "DW30000c": 360.00,
        }

        # Check if paused (storage-only costs)
        if status.lower() == 'paused':
            # Assume 1TB average storage = $120/month
            return 120.0

        # Get hourly rate
        hourly_rate = pricing_map.get(dw_name, 12.00)  # Default to DW1000c

        return hourly_rate * hours_per_month

    async def _calculate_synapse_sql_pool_optimization(
        self,
        pool: Any,
        status: str,
        dw_name: str,
        tier: str,
        monthly_cost: float
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Synapse SQL Pool.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # SCENARIO 1: CRITICAL - SQL pool paused >7 days
        # This is likely abandoned or forgotten - still incurs storage costs
        if status.lower() == 'paused':
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            # Savings = eliminate storage costs by deleting backup and pool
            potential_savings = 120.0  # Storage costs per month
            recommendations.append(
                "CRITICAL: SQL pool has been paused for extended period. "
                "If no longer needed, delete to eliminate storage costs ($120/TB/month)."
            )

        # SCENARIO 2: HIGH - Zero queries 30+ days (placeholder - needs query metrics)
        # In real implementation, would check Synapse analytics/monitoring
        elif status.lower() == 'online':
            # This is a placeholder - would require querying Synapse monitoring/analytics
            # For now, we can't determine query activity without additional API calls
            pass

        # SCENARIO 3: HIGH - Overprovisioned DWUs (placeholder - needs query metrics)
        # In real implementation, would analyze DWU utilization vs query load
        # If DWU usage <30%, could downgrade to smaller tier
        if status.lower() == 'online' and dw_name.startswith('DW'):
            # Extract DWU number (e.g., "DW5000c" -> 5000)
            try:
                dw_number = int(dw_name.replace('DW', '').replace('c', ''))

                # If using very large DWU (>5000c), recommend reviewing necessity
                if dw_number >= 5000:
                    is_optimizable = True
                    optimization_score = max(optimization_score, 70)
                    priority = "high" if priority == "none" else priority

                    # Potential savings: Downgrade from DW5000c to DW2000c
                    current_hourly = monthly_cost / 730
                    potential_hourly = 24.00  # DW2000c
                    if current_hourly > potential_hourly:
                        potential_savings = (current_hourly - potential_hourly) * 730
                        recommendations.append(
                            f"HIGH: Large DWU tier ({dw_name}). Review query patterns - "
                            f"if average load <30%, consider downgrading to save ${potential_savings:.2f}/month."
                        )
            except ValueError:
                pass

        # SCENARIO 4: MEDIUM - No auto-pause configured
        # Synapse SQL Pools support auto-pause to save costs when idle
        # This is a placeholder - would require checking pool settings via API
        if status.lower() == 'online' and not is_optimizable:
            # This is a best practice recommendation
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority == "none" else priority
            recommendations.append(
                "MEDIUM: Consider enabling auto-pause for idle periods. "
                "SQL pool can auto-pause after inactivity to reduce costs."
            )

        # SCENARIO 5: LOW - Gen1 DWU (upgrade to Gen2)
        # Gen1 = "DW100", "DW200", etc. (no 'c' suffix)
        # Gen2 = "DW100c", "DW200c", etc. ('c' suffix = compute optimized)
        if tier == "DW" or (dw_name.startswith('DW') and not dw_name.endswith('c')):
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            recommendations.append(
                "LOW: Using Gen1 DWU tier. Migrate to Gen2 (compute optimized) "
                "for 5x better performance at same cost."
            )

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_vpn_gateways(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure VPN Gateways for cost intelligence.

        VPN Gateways provide site-to-site, point-to-site, and VNet-to-VNet connectivity.
        Pricing: $27-$650/month depending on SKU (Basic, VpnGw1-5, VpnGw1AZ-5AZ)
        Typical cost: $150-400/month for production gateways

        Detection criteria:
        - No active connections 30+ days (CRITICAL - 90 score)
        - Very low data transfer <1GB/month (HIGH - 75 score)
        - Overprovisioned SKU (traffic <30% capacity) (HIGH - 70 score)
        - Point-to-Site only (use Azure Bastion instead) (MEDIUM - 50 score)
        - Legacy Basic SKU (LOW - 30 score)

        Returns:
            List of all VPN Gateways with optimization recommendations
        """
        try:
            from azure.mgmt.network import NetworkManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-network not installed")
            return []

        resources = []
        self.logger.info(f"Scanning VPN Gateways in region: {region}")

        try:
            # Create network client
            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )
            network_client = NetworkManagementClient(credential, self.subscription_id)

            # Get all resource groups
            resource_groups = await self._get_resource_groups()

            for rg in resource_groups:
                rg_name = rg.name

                # Filter by resource group if specified
                if self.resource_groups and rg_name not in self.resource_groups:
                    continue

                try:
                    # List VPN gateways in this resource group
                    vpn_gateways = network_client.virtual_network_gateways.list(rg_name)

                    for gateway in vpn_gateways:
                        gateway_name = gateway.name
                        gateway_location = gateway.location

                        # Filter by region if specified
                        if self.regions and gateway_location not in self.regions:
                            continue

                        # Only process VPN gateways (not ExpressRoute)
                        gateway_type = getattr(gateway, 'gateway_type', 'Unknown')
                        if gateway_type.lower() != 'vpn':
                            continue

                        # Get SKU and configuration
                        sku = getattr(gateway, 'sku', None)
                        sku_name = sku.name if sku else "Unknown"
                        sku_tier = sku.tier if sku and hasattr(sku, 'tier') else "Unknown"

                        # Get VPN type and configuration
                        vpn_type = getattr(gateway, 'vpn_type', 'Unknown')
                        vpn_client_config = getattr(gateway, 'vpn_client_configuration', None)
                        has_p2s = vpn_client_config is not None
                        bgp_settings = getattr(gateway, 'bgp_settings', None)
                        has_bgp = bgp_settings is not None

                        # Get active connections count (placeholder - needs additional API call)
                        # In real implementation, would query network_client.virtual_network_gateway_connections.list()
                        active_connections = 0  # Placeholder

                        # Calculate monthly cost based on SKU
                        monthly_cost = self._estimate_vpn_gateway_cost(sku_name)

                        # Check optimization opportunities
                        is_optimizable, score, priority, savings, recommendations = \
                            await self._calculate_vpn_gateway_optimization(
                                gateway, sku_name, has_p2s, active_connections, monthly_cost
                            )

                        # Build metadata
                        metadata = {
                            "gateway_name": gateway_name,
                            "sku": sku_name,
                            "tier": sku_tier,
                            "vpn_type": vpn_type,
                            "has_point_to_site": has_p2s,
                            "has_bgp": has_bgp,
                            "active_connections": active_connections,
                            "resource_group": rg_name,
                            "provisioning_state": getattr(gateway, 'provisioning_state', 'Unknown'),
                        }

                        # Determine if orphan (no connections for 90+ days = likely abandoned)
                        is_orphan = active_connections == 0 and score >= 90

                        # Create resource record
                        resource = AllCloudResourceData(
                            resource_id=gateway.id,
                            resource_name=gateway_name,
                            resource_type="azure_vpn_gateway",
                            region=gateway_location,
                            estimated_monthly_cost=monthly_cost,
                            currency="USD",
                            resource_metadata=metadata,
                            is_orphan=is_orphan,
                            is_optimizable=is_optimizable and not is_orphan,
                            optimization_score=score if not is_orphan else 0,
                            optimization_priority=priority if not is_orphan else "none",
                            potential_monthly_savings=savings if not is_orphan else 0.0,
                            optimization_recommendations=recommendations if not is_orphan else []
                        )

                        resources.append(resource)
                        self.logger.info(
                            f"Found VPN Gateway: {gateway_name} "
                            f"(SKU: {sku_name}, P2S: {has_p2s}, "
                            f"Cost: ${monthly_cost:.2f}/mo, Optimizable: {is_optimizable})"
                        )

                except Exception as e:
                    self.logger.error(
                        f"Error scanning VPN gateways in resource group {rg_name}: {str(e)}"
                    )
                    continue

            self.logger.info(
                f"VPN Gateway scan complete: {len(resources)} gateways found"
            )

        except Exception as e:
            self.logger.error(f"Error scanning VPN Gateways: {str(e)}")

        return resources

    def _estimate_vpn_gateway_cost(self, sku_name: str) -> float:
        """
        Estimate monthly cost for VPN Gateway based on SKU.

        Pricing (monthly, includes 730 hours):
        - Basic: $27/month (legacy, site-to-site only, no BGP, max 10 tunnels)
        - VpnGw1: $150/month (30 tunnels, 650 Mbps, BGP)
        - VpnGw2: $380/month (30 tunnels, 1 Gbps, BGP)
        - VpnGw3: $410/month (30 tunnels, 1.25 Gbps, BGP)
        - VpnGw4: $580/month (100 tunnels, 5 Gbps, BGP)
        - VpnGw5: $650/month (100 tunnels, 10 Gbps, BGP)
        - VpnGw1AZ-5AZ: Zone-redundant versions (+10% cost)

        Note: Plus data transfer costs ($0.087/GB outbound)
        """
        pricing_map = {
            "Basic": 27.0,
            "VpnGw1": 150.0,
            "VpnGw2": 380.0,
            "VpnGw3": 410.0,
            "VpnGw4": 580.0,
            "VpnGw5": 650.0,
            "VpnGw1AZ": 165.0,  # +10% for zone redundancy
            "VpnGw2AZ": 418.0,
            "VpnGw3AZ": 451.0,
            "VpnGw4AZ": 638.0,
            "VpnGw5AZ": 715.0,
        }

        return pricing_map.get(sku_name, 150.0)  # Default to VpnGw1

    async def _calculate_vpn_gateway_optimization(
        self,
        gateway: Any,
        sku_name: str,
        has_p2s: bool,
        active_connections: int,
        monthly_cost: float
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for VPN Gateway.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # SCENARIO 1: CRITICAL - No active connections 30+ days
        # This is a placeholder - would require querying connection history/metrics
        if active_connections == 0:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost  # Full cost savings by deletion
            recommendations.append(
                f"CRITICAL: VPN Gateway has no active connections. "
                f"If no longer needed, delete to save ${monthly_cost:.2f}/month."
            )

        # SCENARIO 2: HIGH - Very low data transfer <1GB/month (placeholder)
        # In real implementation, would check Azure Monitor metrics for GatewayBandwidth
        # For now, this is a placeholder for future implementation
        elif False:  # Placeholder condition
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority == "none" else priority
            recommendations.append(
                "HIGH: Very low data transfer detected (<1GB/month). "
                "Verify VPN is actively used or consider deletion."
            )

        # SCENARIO 3: HIGH - Overprovisioned SKU (traffic <30% capacity)
        # Placeholder - would require analyzing bandwidth metrics vs SKU capacity
        if sku_name in ["VpnGw4", "VpnGw5", "VpnGw4AZ", "VpnGw5AZ"]:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority == "none" else priority

            # Calculate savings by downgrading to VpnGw1
            potential_savings = monthly_cost - 150.0
            if potential_savings > 0:
                recommendations.append(
                    f"HIGH: High-tier SKU ({sku_name}). Review bandwidth usage - "
                    f"if <30% capacity, downgrade to VpnGw1 to save ${potential_savings:.2f}/month."
                )

        # SCENARIO 4: MEDIUM - Point-to-Site only (use Azure Bastion instead)
        # Azure Bastion provides secure RDP/SSH access without VPN (~$140/month)
        if has_p2s and active_connections == 0 and not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority == "none" else priority
            # Bastion is ~$140/month, VPN Gateway is ~$150/month, so minimal savings
            # But Bastion is simpler and more secure for admin access only
            recommendations.append(
                "MEDIUM: VPN Gateway configured only for Point-to-Site. "
                "Consider Azure Bastion instead for secure admin access (simpler, more secure)."
            )

        # SCENARIO 5: LOW - Legacy Basic SKU
        # Basic SKU lacks BGP, IKEv2, zone redundancy, and modern features
        if sku_name == "Basic" and not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            recommendations.append(
                "LOW: Using legacy Basic SKU. Upgrade to VpnGw1 for BGP support, "
                "IKEv2, better performance, and modern features (+$123/month)."
            )

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_vnet_peerings(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure VNet Peerings for cost intelligence.

        VNet Peering connects two Azure Virtual Networks for private communication.
        Pricing: $0.01/GB intra-region, $0.035-0.05/GB inter-region/global
        Typical cost: $10-100/month depending on traffic volume

        Detection criteria:
        - Peering in Failed/Disconnected state (CRITICAL - 90 score)
        - Peering with 0 traffic 30+ days (HIGH - 75 score)
        - Global peering with very low traffic <1GB/month (HIGH - 70 score)
        - Unidirectional peering (should be bidirectional) (MEDIUM - 50 score)
        - Redundant peerings between same VNets (LOW - 30 score)

        Returns:
            List of all VNet Peerings with optimization recommendations
        """
        try:
            from azure.mgmt.network import NetworkManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-network not installed")
            return []

        resources = []
        self.logger.info(f"Scanning VNet Peerings in region: {region}")

        try:
            # Create network client
            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )
            network_client = NetworkManagementClient(credential, self.subscription_id)

            # Get all resource groups
            resource_groups = await self._get_resource_groups()

            for rg in resource_groups:
                rg_name = rg.name

                # Filter by resource group if specified
                if self.resource_groups and rg_name not in self.resource_groups:
                    continue

                try:
                    # List virtual networks in this resource group
                    vnets = network_client.virtual_networks.list(rg_name)

                    for vnet in vnets:
                        vnet_name = vnet.name
                        vnet_location = vnet.location

                        # Filter by region if specified
                        if self.regions and vnet_location not in self.regions:
                            continue

                        # Get peerings for this VNet
                        peerings = getattr(vnet, 'virtual_network_peerings', [])

                        for peering in peerings:
                            peering_name = peering.name
                            peering_state = getattr(peering, 'peering_state', 'Unknown')
                            provisioning_state = getattr(peering, 'provisioning_state', 'Unknown')

                            # Get remote VNet info
                            remote_vnet = getattr(peering, 'remote_virtual_network', None)
                            remote_vnet_id = remote_vnet.id if remote_vnet else "Unknown"

                            # Determine if global peering (different regions)
                            # Parse remote VNet region from ID or assume same region
                            is_global = False  # Placeholder - would need to query remote VNet

                            # Estimate traffic (placeholder - would need Azure Monitor metrics)
                            monthly_gb_transfer = 0.0  # Placeholder

                            # Calculate cost
                            monthly_cost = self._estimate_vnet_peering_cost(is_global, monthly_gb_transfer)

                            # Check optimization opportunities
                            is_optimizable, score, priority, savings, recommendations = \
                                await self._calculate_vnet_peering_optimization(
                                    peering, peering_state, is_global, monthly_gb_transfer, monthly_cost
                                )

                            # Build metadata
                            metadata = {
                                "vnet_name": vnet_name,
                                "peering_name": peering_name,
                                "peering_state": peering_state,
                                "provisioning_state": provisioning_state,
                                "remote_vnet_id": remote_vnet_id,
                                "is_global": is_global,
                                "allow_virtual_network_access": getattr(peering, 'allow_virtual_network_access', False),
                                "allow_forwarded_traffic": getattr(peering, 'allow_forwarded_traffic', False),
                                "allow_gateway_transit": getattr(peering, 'allow_gateway_transit', False),
                                "use_remote_gateways": getattr(peering, 'use_remote_gateways', False),
                                "resource_group": rg_name,
                            }

                            # Determine if orphan (failed/disconnected state = waste)
                            is_orphan = peering_state in ['Failed', 'Disconnected'] and score >= 90

                            # Create resource record
                            resource = AllCloudResourceData(
                                resource_id=f"{vnet.id}/virtualNetworkPeerings/{peering_name}",
                                resource_name=f"{vnet_name}  {peering_name}",
                                resource_type="azure_vnet_peering",
                                region=vnet_location,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=metadata,
                                is_orphan=is_orphan,
                                is_optimizable=is_optimizable and not is_orphan,
                                optimization_score=score if not is_orphan else 0,
                                optimization_priority=priority if not is_orphan else "none",
                                potential_monthly_savings=savings if not is_orphan else 0.0,
                                optimization_recommendations=recommendations if not is_orphan else []
                            )

                            resources.append(resource)
                            self.logger.info(
                                f"Found VNet Peering: {vnet_name}  {peering_name} "
                                f"(State: {peering_state}, Global: {is_global}, "
                                f"Cost: ${monthly_cost:.2f}/mo, Optimizable: {is_optimizable})"
                            )

                except Exception as e:
                    self.logger.error(
                        f"Error scanning VNet peerings in resource group {rg_name}: {str(e)}"
                    )
                    continue

            self.logger.info(
                f"VNet Peering scan complete: {len(resources)} peerings found"
            )

        except Exception as e:
            self.logger.error(f"Error scanning VNet Peerings: {str(e)}")

        return resources

    def _estimate_vnet_peering_cost(self, is_global: bool, monthly_gb: float) -> float:
        """
        Estimate monthly cost for VNet Peering based on traffic.

        Pricing:
        - Intra-region: $0.01/GB ingress + $0.01/GB egress = $0.02/GB total
        - Inter-region (same geography): $0.035/GB
        - Global peering (cross-geography): $0.05/GB

        Note: Most peerings have low traffic, so base cost is often minimal
        """
        if monthly_gb == 0:
            # Peering itself is free, only data transfer is charged
            # Assume minimum $5/month for minimal traffic
            return 5.0

        if is_global:
            # Global peering
            cost_per_gb = 0.05
        else:
            # Assume intra-region (most common)
            cost_per_gb = 0.02

        return monthly_gb * cost_per_gb

    async def _calculate_vnet_peering_optimization(
        self,
        peering: Any,
        peering_state: str,
        is_global: bool,
        monthly_gb: float,
        monthly_cost: float
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for VNet Peering.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # SCENARIO 1: CRITICAL - Peering in Failed/Disconnected state
        if peering_state in ['Failed', 'Disconnected']:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append(
                f"CRITICAL: VNet Peering is in {peering_state} state. "
                f"Delete if no longer needed to save ${monthly_cost:.2f}/month."
            )

        # SCENARIO 2: HIGH - Peering with 0 traffic 30+ days (placeholder)
        # In real implementation, would check Azure Monitor metrics for BytesTransferred
        elif monthly_gb == 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority == "none" else priority
            potential_savings = monthly_cost
            recommendations.append(
                "HIGH: VNet Peering has no traffic for 30+ days. "
                f"Verify if still needed or delete to save ${monthly_cost:.2f}/month."
            )

        # SCENARIO 3: HIGH - Global peering with very low traffic <1GB/month
        # Global peering is expensive ($0.05/GB), recommend migrating data or using alternative
        if is_global and monthly_gb < 1.0 and monthly_gb > 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority == "none" else priority
            # Savings: Use ExpressRoute or VPN Gateway for low-volume traffic
            potential_savings = monthly_cost * 0.5  # 50% savings estimate
            recommendations.append(
                f"HIGH: Global VNet Peering with very low traffic ({monthly_gb:.2f} GB/month). "
                f"Consider ExpressRoute or VPN Gateway for cost optimization (~50% savings)."
            )

        # SCENARIO 4: MEDIUM - Unidirectional peering (should be bidirectional)
        # This is a best practice check - bidirectional peering ensures full connectivity
        # Placeholder - would require checking if remote VNet has reciprocal peering
        allow_virtual_network_access = getattr(peering, 'allow_virtual_network_access', False)
        if not allow_virtual_network_access and not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority == "none" else priority
            recommendations.append(
                "MEDIUM: VNet Peering may not have reciprocal peering configured. "
                "Ensure bidirectional peering for full connectivity."
            )

        # SCENARIO 5: LOW - Redundant peerings (placeholder)
        # In real implementation, would check if multiple peerings exist between same VNets
        # This is a placeholder for future enhancement
        if False:  # Placeholder condition
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            recommendations.append(
                "LOW: Multiple VNet Peerings detected between same VNets. "
                "Consolidate to single peering to simplify management."
            )

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_front_doors(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Front Door profiles for cost intelligence.

        Front Door is a global CDN + WAF + load balancer service.
        Pricing: Standard $35/mois, Premium $330/mois + data transfer
        Typical cost: $50-500/month

        Detection criteria:
        - Front Door with 0 requests 30+ days (CRITICAL - 90 score)
        - Premium tier with WAF disabled (overpaying) (HIGH - 75 score)
        - Very low traffic <1GB/month (HIGH - 70 score)
        - Endpoints not used or redundant (MEDIUM - 50 score)
        - Classic tier (migration to Standard/Premium) (LOW - 30 score)

        Returns:
            List of all Front Door profiles with optimization recommendations
        """
        try:
            from azure.mgmt.frontdoor import FrontDoorManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-frontdoor not installed")
            return []

        resources = []
        self.logger.info(f"Scanning Front Door profiles (global service)")

        try:
            # Create Front Door client
            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )
            frontdoor_client = FrontDoorManagementClient(credential, self.subscription_id)

            # Get all resource groups
            resource_groups = await self._get_resource_groups()

            for rg in resource_groups:
                rg_name = rg.name

                # Filter by resource group if specified
                if self.resource_groups and rg_name not in self.resource_groups:
                    continue

                try:
                    # List Front Door profiles (new Standard/Premium)
                    # Note: This API may differ - azure-mgmt-frontdoor has multiple versions
                    # Using legacy Front Door list for compatibility
                    front_doors = frontdoor_client.front_doors.list_by_resource_group(rg_name)

                    for fd in front_doors:
                        fd_name = fd.name
                        fd_location = getattr(fd, 'location', 'Global')  # Front Door is global

                        # Get SKU/tier
                        sku = getattr(fd, 'sku', None)
                        sku_name = sku.name if sku and hasattr(sku, 'name') else "Classic"

                        # Get provisioning state
                        provisioning_state = getattr(fd, 'provisioning_state', 'Unknown')
                        resource_state = getattr(fd, 'resource_state', 'Unknown')

                        # Get endpoints
                        frontend_endpoints = getattr(fd, 'frontend_endpoints', [])
                        backend_pools = getattr(fd, 'backend_pools', [])
                        routing_rules = getattr(fd, 'routing_rules', [])

                        # Get WAF policy (if Premium)
                        web_application_firewall_policy_link = getattr(fd, 'web_application_firewall_policy_link', None)
                        has_waf = web_application_firewall_policy_link is not None

                        # Estimate monthly traffic (placeholder - would need Azure Monitor)
                        monthly_requests = 0  # Placeholder
                        monthly_gb_transfer = 0.0  # Placeholder

                        # Calculate cost
                        monthly_cost = self._estimate_front_door_cost(sku_name, monthly_requests)

                        # Check optimization opportunities
                        is_optimizable, score, priority, savings, recommendations = \
                            await self._calculate_front_door_optimization(
                                fd, sku_name, has_waf, monthly_requests, monthly_gb_transfer, monthly_cost
                            )

                        # Build metadata
                        metadata = {
                            "front_door_name": fd_name,
                            "sku": sku_name,
                            "provisioning_state": provisioning_state,
                            "resource_state": resource_state,
                            "has_waf": has_waf,
                            "frontend_endpoints_count": len(frontend_endpoints),
                            "backend_pools_count": len(backend_pools),
                            "routing_rules_count": len(routing_rules),
                            "resource_group": rg_name,
                        }

                        # Determine if orphan (0 requests for 90+ days = waste)
                        is_orphan = monthly_requests == 0 and score >= 90

                        # Create resource record
                        resource = AllCloudResourceData(
                            resource_id=fd.id,
                            resource_name=fd_name,
                            resource_type="azure_front_door",
                            region=fd_location,
                            estimated_monthly_cost=monthly_cost,
                            currency="USD",
                            resource_metadata=metadata,
                            is_orphan=is_orphan,
                            is_optimizable=is_optimizable and not is_orphan,
                            optimization_score=score if not is_orphan else 0,
                            optimization_priority=priority if not is_orphan else "none",
                            potential_monthly_savings=savings if not is_orphan else 0.0,
                            optimization_recommendations=recommendations if not is_orphan else []
                        )

                        resources.append(resource)
                        self.logger.info(
                            f"Found Front Door: {fd_name} "
                            f"(SKU: {sku_name}, WAF: {has_waf}, "
                            f"Endpoints: {len(frontend_endpoints)}, "
                            f"Cost: ${monthly_cost:.2f}/mo, Optimizable: {is_optimizable})"
                        )

                except Exception as e:
                    self.logger.error(
                        f"Error scanning Front Doors in resource group {rg_name}: {str(e)}"
                    )
                    continue

            self.logger.info(
                f"Front Door scan complete: {len(resources)} profiles found"
            )

        except Exception as e:
            self.logger.error(f"Error scanning Front Doors: {str(e)}")

        return resources

    def _estimate_front_door_cost(self, sku: str, monthly_requests: int) -> float:
        """
        Estimate monthly cost for Front Door based on SKU and usage.

        Pricing:
        - Classic: $35/month base + $0.03/GB data transfer + $0.01/10K requests
        - Standard: $35/month base + $0.03/GB data transfer + $0.01/10K requests
        - Premium: $330/month base + $0.04/GB data transfer + WAF ($10/policy + $1/rule)

        For simplicity, using base pricing + minimal traffic assumption
        """
        if sku in ["Premium", "Premium_AzureFrontDoor"]:
            base_cost = 330.0
        elif sku in ["Standard", "Standard_AzureFrontDoor"]:
            base_cost = 35.0
        else:
            # Classic or unknown
            base_cost = 35.0

        # Add estimated data transfer costs (assume 10GB/month average)
        if sku in ["Premium", "Premium_AzureFrontDoor"]:
            data_transfer_cost = 10 * 0.04  # $0.04/GB
        else:
            data_transfer_cost = 10 * 0.03  # $0.03/GB

        return base_cost + data_transfer_cost

    async def _calculate_front_door_optimization(
        self,
        front_door: Any,
        sku: str,
        has_waf: bool,
        monthly_requests: int,
        monthly_gb: float,
        monthly_cost: float
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Front Door.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # SCENARIO 1: CRITICAL - Front Door with 0 requests 30+ days
        if monthly_requests == 0:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append(
                f"CRITICAL: Front Door has no requests for 30+ days. "
                f"Delete if no longer needed to save ${monthly_cost:.2f}/month."
            )

        # SCENARIO 2: HIGH - Premium tier with WAF disabled (overpaying)
        # Premium is $330/month vs Standard $35/month - WAF is the main differentiator
        elif sku in ["Premium", "Premium_AzureFrontDoor"] and not has_waf:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority == "none" else priority
            # Savings: Downgrade to Standard
            potential_savings = 330.0 - 35.0  # $295/month
            recommendations.append(
                f"HIGH: Premium tier without WAF enabled. "
                f"Downgrade to Standard tier to save ${potential_savings:.2f}/month."
            )

        # SCENARIO 3: HIGH - Very low traffic <1GB/month
        # Front Door has high base cost - not cost-effective for very low traffic
        elif monthly_gb < 1.0 and monthly_gb > 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority == "none" else priority
            # Savings: Use Azure CDN Standard instead (~$10/month)
            potential_savings = monthly_cost - 10.0
            if potential_savings > 0:
                recommendations.append(
                    f"HIGH: Very low traffic ({monthly_gb:.2f} GB/month). "
                    f"Consider Azure CDN Standard instead to save ${potential_savings:.2f}/month."
                )

        # SCENARIO 4: MEDIUM - Unused endpoints or redundant rules
        # Check if endpoints are configured but not used
        frontend_endpoints = getattr(front_door, 'frontend_endpoints', [])
        routing_rules = getattr(front_door, 'routing_rules', [])

        if len(frontend_endpoints) > 5 and not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority == "none" else priority
            recommendations.append(
                f"MEDIUM: {len(frontend_endpoints)} frontend endpoints configured. "
                f"Review and remove unused endpoints to simplify configuration."
            )

        # SCENARIO 5: LOW - Classic tier (migration recommended)
        # Classic tier is being deprecated, recommend migration
        if sku == "Classic" and not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            recommendations.append(
                "LOW: Using Classic tier. Migrate to Standard or Premium tier "
                "for better performance, features, and support."
            )

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_container_registries(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Container Registries for cost intelligence.

        Container Registry is a private Docker registry for storing container images.
        Pricing: Basic $5/mois, Standard $20/mois, Premium $50/mois + storage + bandwidth
        Typical cost: $20-200/month

        Detection criteria:
        - Registry inutilis (0 pulls 90+ days) (CRITICAL - 90 score)
        - Premium tier sans geo-replication (HIGH - 75 score)
        - Images obsoltes non nettoyes (>50 untagged) (HIGH - 70 score)
        - Pas de retention policy (MEDIUM - 50 score)
        - Basic tier pour production (LOW - 30 score)

        Returns:
            List of all Container Registries with optimization recommendations
        """
        try:
            from azure.mgmt.containerregistry import ContainerRegistryManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-containerregistry not installed")
            return []

        resources = []
        self.logger.info(f"Scanning Container Registries in region: {region}")

        try:
            # Create Container Registry client
            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )
            acr_client = ContainerRegistryManagementClient(credential, self.subscription_id)

            # Get all resource groups
            resource_groups = await self._get_resource_groups()

            for rg in resource_groups:
                rg_name = rg.name

                # Filter by resource group if specified
                if self.resource_groups and rg_name not in self.resource_groups:
                    continue

                try:
                    # List registries in this resource group
                    registries = acr_client.registries.list_by_resource_group(rg_name)

                    for registry in registries:
                        registry_name = registry.name
                        registry_location = registry.location

                        # Filter by region if specified
                        if self.regions and registry_location not in self.regions:
                            continue

                        # Get SKU
                        sku = getattr(registry, 'sku', None)
                        sku_name = sku.name if sku else "Basic"

                        # Get provisioning state
                        provisioning_state = getattr(registry, 'provisioning_state', 'Unknown')

                        # Get geo-replication status (Premium only)
                        replications = []
                        has_geo_replication = False
                        if sku_name == "Premium":
                            try:
                                replications_list = acr_client.replications.list(rg_name, registry_name)
                                replications = list(replications_list)
                                has_geo_replication = len(replications) > 1  # More than 1 = geo-replicated
                            except Exception:
                                pass

                        # Get storage usage (placeholder - would need Azure Monitor)
                        storage_gb = 10.0  # Placeholder

                        # Calculate cost
                        monthly_cost = self._estimate_container_registry_cost(sku_name, storage_gb, has_geo_replication)

                        # Estimate usage metrics (placeholder - would need Azure Monitor)
                        successful_pulls_30d = 0  # Placeholder
                        successful_pushes_30d = 0  # Placeholder
                        untagged_images_count = 0  # Placeholder

                        # Check optimization opportunities
                        is_optimizable, score, priority, savings, recommendations = \
                            await self._calculate_container_registry_optimization(
                                registry, sku_name, has_geo_replication, successful_pulls_30d,
                                untagged_images_count, monthly_cost
                            )

                        # Build metadata
                        metadata = {
                            "registry_name": registry_name,
                            "sku": sku_name,
                            "provisioning_state": provisioning_state,
                            "admin_user_enabled": getattr(registry, 'admin_user_enabled', False),
                            "has_geo_replication": has_geo_replication,
                            "replication_count": len(replications),
                            "storage_gb": storage_gb,
                            "successful_pulls_30d": successful_pulls_30d,
                            "successful_pushes_30d": successful_pushes_30d,
                            "untagged_images_count": untagged_images_count,
                            "resource_group": rg_name,
                        }

                        # Determine if orphan (0 pulls for 90+ days = waste)
                        is_orphan = successful_pulls_30d == 0 and score >= 90

                        # Create resource record
                        resource = AllCloudResourceData(
                            resource_id=registry.id,
                            resource_name=registry_name,
                            resource_type="azure_container_registry",
                            region=registry_location,
                            estimated_monthly_cost=monthly_cost,
                            currency="USD",
                            resource_metadata=metadata,
                            is_orphan=is_orphan,
                            is_optimizable=is_optimizable and not is_orphan,
                            optimization_score=score if not is_orphan else 0,
                            optimization_priority=priority if not is_orphan else "none",
                            potential_monthly_savings=savings if not is_orphan else 0.0,
                            optimization_recommendations=recommendations if not is_orphan else []
                        )

                        resources.append(resource)
                        self.logger.info(
                            f"Found Container Registry: {registry_name} "
                            f"(SKU: {sku_name}, Geo-replication: {has_geo_replication}, "
                            f"Cost: ${monthly_cost:.2f}/mo, Optimizable: {is_optimizable})"
                        )

                except Exception as e:
                    self.logger.error(
                        f"Error scanning container registries in resource group {rg_name}: {str(e)}"
                    )
                    continue

            self.logger.info(
                f"Container Registry scan complete: {len(resources)} registries found"
            )

        except Exception as e:
            self.logger.error(f"Error scanning Container Registries: {str(e)}")

        return resources

    def _estimate_container_registry_cost(self, sku: str, storage_gb: float, geo_replication: bool) -> float:
        """
        Estimate monthly cost for Container Registry based on SKU and usage.

        Pricing:
        - Basic: $5/month + $0.10/GB storage
        - Standard: $20/month + $0.10/GB storage + webhooks
        - Premium: $50/month + $0.10/GB storage + geo-replication + content trust

        Geo-replication: ~$50/month per additional region (Premium only)
        """
        # Base costs
        base_costs = {
            "Basic": 5.0,
            "Standard": 20.0,
            "Premium": 50.0,
        }

        base_cost = base_costs.get(sku, 20.0)

        # Storage costs ($0.10/GB)
        storage_cost = storage_gb * 0.10

        # Geo-replication cost (Premium only, ~$50 per additional region)
        geo_cost = 0.0
        if sku == "Premium" and geo_replication:
            geo_cost = 50.0  # Assume 1 additional region

        return base_cost + storage_cost + geo_cost

    async def _calculate_container_registry_optimization(
        self,
        registry: Any,
        sku: str,
        has_geo_replication: bool,
        successful_pulls_30d: int,
        untagged_images_count: int,
        monthly_cost: float
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Container Registry.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # SCENARIO 1: CRITICAL - Registry inutilis (0 pulls pendant 90+ jours)
        if successful_pulls_30d == 0:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append(
                f"CRITICAL: Container Registry has no pulls for 90+ days. "
                f"Delete if no longer needed to save ${monthly_cost:.2f}/month."
            )

        # SCENARIO 2: HIGH - Premium tier sans geo-replication (overpaying)
        elif sku == "Premium" and not has_geo_replication:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority == "none" else priority
            # Savings: Downgrade to Standard
            potential_savings = 50.0 - 20.0  # $30/month
            recommendations.append(
                f"HIGH: Premium tier without geo-replication enabled. "
                f"Downgrade to Standard tier to save ${potential_savings:.2f}/month."
            )

        # SCENARIO 3: HIGH - Images obsoltes non nettoyes (>50 untagged images)
        # Untagged images consume storage and indicate poor CI/CD hygiene
        elif untagged_images_count > 50:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority == "none" else priority
            # Savings: Reduce storage costs by cleaning up
            storage_savings = (untagged_images_count / 50) * 5.0  # Estimate $5 per 50 images
            potential_savings = min(storage_savings, monthly_cost * 0.3)  # Max 30% of cost
            recommendations.append(
                f"HIGH: {untagged_images_count} untagged images detected. "
                f"Enable retention policy to auto-delete unused images and save ~${potential_savings:.2f}/month."
            )

        # SCENARIO 4: MEDIUM - Pas de retention policy configure
        # Placeholder - would require checking registry policies via API
        # For now, this is a best practice recommendation
        if not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority == "none" else priority
            recommendations.append(
                "MEDIUM: No retention policy configured. "
                "Enable automatic cleanup of old/untagged images to reduce storage costs."
            )

        # SCENARIO 5: LOW - Basic tier pour production (upgrade recommended)
        # Basic tier lacks webhooks, geo-replication, and advanced security features
        if sku == "Basic" and not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            recommendations.append(
                "LOW: Using Basic tier. Upgrade to Standard for webhooks, "
                "better performance, and production-grade features (+$15/month)."
            )

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_service_bus_topics(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Service Bus Topics for cost intelligence.

        Service Bus Topic is a pub/sub messaging service with multiple subscriptions.
        Pricing: Standard $10/mois + $0.05/million ops, Premium $677/mois (dedicated capacity)
        Typical cost: $15-100/month

        Detection criteria:
        - Topic sans abonnements actifs 30+ days (CRITICAL - 90 score)
        - Premium tier avec faible volume <1M messages/mois (HIGH - 75 score)
        - Messages morts non traits >1000 (HIGH - 70 score)
        - Pas de TTL configur (MEDIUM - 50 score)
        - Auto-delete non configur (LOW - 30 score)

        Returns:
            List of all Service Bus Topics with optimization recommendations
        """
        try:
            from azure.mgmt.servicebus import ServiceBusManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-servicebus not installed")
            return []

        resources = []
        self.logger.info(f"Scanning Service Bus Topics in region: {region}")

        try:
            # Create Service Bus client
            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )
            sb_client = ServiceBusManagementClient(credential, self.subscription_id)

            # Get all resource groups
            resource_groups = await self._get_resource_groups()

            for rg in resource_groups:
                rg_name = rg.name

                # Filter by resource group if specified
                if self.resource_groups and rg_name not in self.resource_groups:
                    continue

                try:
                    # List Service Bus namespaces in this resource group
                    namespaces = sb_client.namespaces.list_by_resource_group(rg_name)

                    for namespace in namespaces:
                        namespace_name = namespace.name
                        namespace_location = namespace.location

                        # Filter by region if specified
                        if self.regions and namespace_location not in self.regions:
                            continue

                        # Get SKU/tier
                        sku = getattr(namespace, 'sku', None)
                        tier = sku.name if sku else "Standard"

                        # List topics in this namespace
                        topics = sb_client.topics.list_by_namespace(rg_name, namespace_name)

                        for topic in topics:
                            topic_name = topic.name
                            status = getattr(topic, 'status', 'Unknown')

                            # Get topic properties
                            max_size_in_mb = getattr(topic, 'max_size_in_megabytes', 0)
                            enable_partitioning = getattr(topic, 'enable_partitioning', False)
                            enable_batched_operations = getattr(topic, 'enable_batched_operations', False)
                            default_message_time_to_live = getattr(topic, 'default_message_time_to_live', None)
                            auto_delete_on_idle = getattr(topic, 'auto_delete_on_idle', None)

                            # Get subscriptions count
                            subscriptions = sb_client.subscriptions.list_by_topic(rg_name, namespace_name, topic_name)
                            subscriptions_list = list(subscriptions)
                            subscription_count = len(subscriptions_list)

                            # Estimate usage metrics (placeholder - would need Azure Monitor)
                            monthly_operations = 0  # Placeholder
                            dead_letter_messages_count = 0  # Placeholder

                            # Calculate cost (shared with namespace, estimate per topic)
                            monthly_cost = self._estimate_service_bus_cost(tier, monthly_operations)

                            # Check optimization opportunities
                            is_optimizable, score, priority, savings, recommendations = \
                                await self._calculate_service_bus_topic_optimization(
                                    topic, tier, subscription_count, monthly_operations,
                                    dead_letter_messages_count, default_message_time_to_live,
                                    auto_delete_on_idle, monthly_cost
                                )

                            # Build metadata
                            metadata = {
                                "namespace_name": namespace_name,
                                "topic_name": topic_name,
                                "tier": tier,
                                "status": status,
                                "subscription_count": subscription_count,
                                "max_size_mb": max_size_in_mb,
                                "enable_partitioning": enable_partitioning,
                                "enable_batched_operations": enable_batched_operations,
                                "has_ttl": default_message_time_to_live is not None,
                                "has_auto_delete": auto_delete_on_idle is not None,
                                "dead_letter_messages_count": dead_letter_messages_count,
                                "resource_group": rg_name,
                            }

                            # Determine if orphan (no subscriptions for 30+ days = waste)
                            is_orphan = subscription_count == 0 and score >= 90

                            # Create resource record
                            resource = AllCloudResourceData(
                                resource_id=topic.id,
                                resource_name=f"{namespace_name}/{topic_name}",
                                resource_type="azure_service_bus_topic",
                                region=namespace_location,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=metadata,
                                is_orphan=is_orphan,
                                is_optimizable=is_optimizable and not is_orphan,
                                optimization_score=score if not is_orphan else 0,
                                optimization_priority=priority if not is_orphan else "none",
                                potential_monthly_savings=savings if not is_orphan else 0.0,
                                optimization_recommendations=recommendations if not is_orphan else []
                            )

                            resources.append(resource)
                            self.logger.info(
                                f"Found Service Bus Topic: {namespace_name}/{topic_name} "
                                f"(Tier: {tier}, Subscriptions: {subscription_count}, "
                                f"Cost: ${monthly_cost:.2f}/mo, Optimizable: {is_optimizable})"
                            )

                except Exception as e:
                    self.logger.error(
                        f"Error scanning Service Bus topics in resource group {rg_name}: {str(e)}"
                    )
                    continue

            self.logger.info(
                f"Service Bus Topic scan complete: {len(resources)} topics found"
            )

        except Exception as e:
            self.logger.error(f"Error scanning Service Bus Topics: {str(e)}")

        return resources

    async def scan_service_bus_queues(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Service Bus Queues for cost intelligence.

        Service Bus Queue is a FIFO messaging service with delivery guarantees.
        Pricing: Standard $10/mois + $0.05/million ops, Premium $677/mois
        Typical cost: $15-100/month

        Detection criteria:
        - Queue inutilise (0 messages 90+ days) (CRITICAL - 90 score)
        - Premium tier avec faible volume <1M messages/mois (HIGH - 75 score)
        - Messages morts (Dead Letter) non traits >1000 (HIGH - 70 score)
        - Duplicate detection non active (MEDIUM - 50 score)
        - Lock duration excessive >5min (LOW - 30 score)

        Returns:
            List of all Service Bus Queues with optimization recommendations
        """
        try:
            from azure.mgmt.servicebus import ServiceBusManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-servicebus not installed")
            return []

        resources = []
        self.logger.info(f"Scanning Service Bus Queues in region: {region}")

        try:
            # Create Service Bus client
            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )
            sb_client = ServiceBusManagementClient(credential, self.subscription_id)

            # Get all resource groups
            resource_groups = await self._get_resource_groups()

            for rg in resource_groups:
                rg_name = rg.name

                # Filter by resource group if specified
                if self.resource_groups and rg_name not in self.resource_groups:
                    continue

                try:
                    # List Service Bus namespaces
                    namespaces = sb_client.namespaces.list_by_resource_group(rg_name)

                    for namespace in namespaces:
                        namespace_name = namespace.name
                        namespace_location = namespace.location

                        # Filter by region if specified
                        if self.regions and namespace_location not in self.regions:
                            continue

                        # Get SKU/tier
                        sku = getattr(namespace, 'sku', None)
                        tier = sku.name if sku else "Standard"

                        # List queues in this namespace
                        queues = sb_client.queues.list_by_namespace(rg_name, namespace_name)

                        for queue in queues:
                            queue_name = queue.name
                            status = getattr(queue, 'status', 'Unknown')

                            # Get queue properties
                            max_size_in_mb = getattr(queue, 'max_size_in_megabytes', 0)
                            enable_partitioning = getattr(queue, 'enable_partitioning', False)
                            requires_duplicate_detection = getattr(queue, 'requires_duplicate_detection', False)
                            lock_duration = getattr(queue, 'lock_duration', None)
                            default_message_time_to_live = getattr(queue, 'default_message_time_to_live', None)
                            auto_delete_on_idle = getattr(queue, 'auto_delete_on_idle', None)

                            # Estimate usage metrics (placeholder - would need Azure Monitor)
                            monthly_operations = 0  # Placeholder
                            active_messages_count = 0  # Placeholder
                            dead_letter_messages_count = 0  # Placeholder

                            # Calculate cost
                            monthly_cost = self._estimate_service_bus_cost(tier, monthly_operations)

                            # Check optimization opportunities
                            is_optimizable, score, priority, savings, recommendations = \
                                await self._calculate_service_bus_queue_optimization(
                                    queue, tier, active_messages_count, monthly_operations,
                                    dead_letter_messages_count, requires_duplicate_detection,
                                    lock_duration, monthly_cost
                                )

                            # Build metadata
                            metadata = {
                                "namespace_name": namespace_name,
                                "queue_name": queue_name,
                                "tier": tier,
                                "status": status,
                                "max_size_mb": max_size_in_mb,
                                "enable_partitioning": enable_partitioning,
                                "requires_duplicate_detection": requires_duplicate_detection,
                                "lock_duration": str(lock_duration) if lock_duration else None,
                                "has_ttl": default_message_time_to_live is not None,
                                "has_auto_delete": auto_delete_on_idle is not None,
                                "active_messages_count": active_messages_count,
                                "dead_letter_messages_count": dead_letter_messages_count,
                                "resource_group": rg_name,
                            }

                            # Determine if orphan (0 active messages for 90+ days = waste)
                            is_orphan = active_messages_count == 0 and score >= 90

                            # Create resource record
                            resource = AllCloudResourceData(
                                resource_id=queue.id,
                                resource_name=f"{namespace_name}/{queue_name}",
                                resource_type="azure_service_bus_queue",
                                region=namespace_location,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=metadata,
                                is_orphan=is_orphan,
                                is_optimizable=is_optimizable and not is_orphan,
                                optimization_score=score if not is_orphan else 0,
                                optimization_priority=priority if not is_orphan else "none",
                                potential_monthly_savings=savings if not is_orphan else 0.0,
                                optimization_recommendations=recommendations if not is_orphan else []
                            )

                            resources.append(resource)
                            self.logger.info(
                                f"Found Service Bus Queue: {namespace_name}/{queue_name} "
                                f"(Tier: {tier}, Active messages: {active_messages_count}, "
                                f"Cost: ${monthly_cost:.2f}/mo, Optimizable: {is_optimizable})"
                            )

                except Exception as e:
                    self.logger.error(
                        f"Error scanning Service Bus queues in resource group {rg_name}: {str(e)}"
                    )
                    continue

            self.logger.info(
                f"Service Bus Queue scan complete: {len(resources)} queues found"
            )

        except Exception as e:
            self.logger.error(f"Error scanning Service Bus Queues: {str(e)}")

        return resources

    def _estimate_service_bus_cost(self, tier: str, monthly_operations: int) -> float:
        """
        Estimate monthly cost for Service Bus based on tier and operations.

        Pricing:
        - Basic: $0.05/million operations (queues only, no topics)
        - Standard: $10/month base + $0.05/million operations
        - Premium: $677/month (1 messaging unit) - dedicated capacity

        For simplicity, using base pricing + minimal operations assumption
        """
        if tier == "Premium":
            return 677.0
        elif tier == "Standard":
            base_cost = 10.0
            # Add operation costs ($0.05 per million)
            operation_cost = (monthly_operations / 1000000) * 0.05
            return base_cost + operation_cost
        else:
            # Basic
            operation_cost = (monthly_operations / 1000000) * 0.05
            return operation_cost

    async def _calculate_service_bus_topic_optimization(
        self,
        topic: Any,
        tier: str,
        subscription_count: int,
        monthly_operations: int,
        dead_letter_messages_count: int,
        default_message_time_to_live: Any,
        auto_delete_on_idle: Any,
        monthly_cost: float
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Service Bus Topic.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # SCENARIO 1: CRITICAL - Topic sans abonnements actifs 30+ jours
        if subscription_count == 0:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append(
                f"CRITICAL: Service Bus Topic has no active subscriptions. "
                f"Delete if no longer needed to save ${monthly_cost:.2f}/month."
            )

        # SCENARIO 2: HIGH - Premium tier avec faible volume <1M messages/mois
        elif tier == "Premium" and monthly_operations < 1000000:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority == "none" else priority
            # Savings: Downgrade to Standard
            potential_savings = 677.0 - 10.0  # $667/month
            recommendations.append(
                f"HIGH: Premium tier with low message volume ({monthly_operations/1000:.0f}K ops/month). "
                f"Downgrade to Standard tier to save ${potential_savings:.2f}/month."
            )

        # SCENARIO 3: HIGH - Messages morts non traits >1000
        elif dead_letter_messages_count > 1000:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority == "none" else priority
            recommendations.append(
                f"HIGH: {dead_letter_messages_count} dead letter messages detected. "
                f"Review and fix application errors causing message failures."
            )

        # SCENARIO 4: MEDIUM - Pas de TTL configur
        if default_message_time_to_live is None and not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority == "none" else priority
            recommendations.append(
                "MEDIUM: No message TTL (Time-To-Live) configured. "
                "Messages may accumulate indefinitely. Set TTL to prevent storage bloat."
            )

        # SCENARIO 5: LOW - Auto-delete non configur
        if auto_delete_on_idle is None and not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            recommendations.append(
                "LOW: Auto-delete on idle not configured. "
                "Enable to automatically clean up unused topics."
            )

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def _calculate_service_bus_queue_optimization(
        self,
        queue: Any,
        tier: str,
        active_messages_count: int,
        monthly_operations: int,
        dead_letter_messages_count: int,
        requires_duplicate_detection: bool,
        lock_duration: Any,
        monthly_cost: float
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Service Bus Queue.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # SCENARIO 1: CRITICAL - Queue inutilise (0 messages 90+ jours)
        if active_messages_count == 0:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append(
                f"CRITICAL: Service Bus Queue has no active messages for 90+ days. "
                f"Delete if no longer needed to save ${monthly_cost:.2f}/month."
            )

        # SCENARIO 2: HIGH - Premium tier avec faible volume
        elif tier == "Premium" and monthly_operations < 1000000:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority == "none" else priority
            potential_savings = 677.0 - 10.0
            recommendations.append(
                f"HIGH: Premium tier with low message volume ({monthly_operations/1000:.0f}K ops/month). "
                f"Downgrade to Standard tier to save ${potential_savings:.2f}/month."
            )

        # SCENARIO 3: HIGH - Messages morts non traits >1000
        elif dead_letter_messages_count > 1000:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority == "none" else priority
            recommendations.append(
                f"HIGH: {dead_letter_messages_count} dead letter messages detected. "
                f"Review and fix application errors causing message failures."
            )

        # SCENARIO 4: MEDIUM - Duplicate detection non active
        if not requires_duplicate_detection and not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority == "none" else priority
            recommendations.append(
                "MEDIUM: Duplicate detection not enabled. "
                "Enable to prevent processing duplicate messages (best practice)."
            )

        # SCENARIO 5: LOW - Lock duration excessive >5 minutes
        # Lock duration controls how long a message is locked for processing
        # Excessive lock duration can cause delays if consumer crashes
        if lock_duration and not is_optimizable:
            # lock_duration is a timedelta - extract minutes
            try:
                lock_minutes = lock_duration.total_seconds() / 60
                if lock_minutes > 5:
                    is_optimizable = True
                    optimization_score = max(optimization_score, 30)
                    priority = "low" if priority == "none" else priority
                    recommendations.append(
                        f"LOW: Lock duration is {lock_minutes:.0f} minutes (>5 min threshold). "
                        f"Reduce to improve message processing throughput."
                    )
            except:
                pass

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_event_grid_subscriptions(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Event Grid Subscriptions for cost intelligence.

        Event Grid Subscription routes events from sources to handlers (webhooks, functions, etc.).
        Pricing: $0.60 per million operations (premier million gratuit/mois)
        Typical cost: $1-20/month

        Detection criteria:
        - Subscription vers endpoint mort/inaccessible (CRITICAL - 90 score)
        - Subscription inactive (0 vnements 90+ days) (HIGH - 75 score)
        - Dead letter destination non configure (HIGH - 70 score)
        - Filtres trop larges (MEDIUM - 50 score)
        - Pas d'Advanced Filtering (LOW - 30 score)

        Returns:
            List of all Event Grid Subscriptions with optimization recommendations
        """
        try:
            from azure.mgmt.eventgrid import EventGridManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-eventgrid not installed")
            return []

        resources = []
        self.logger.info(f"Scanning Event Grid Subscriptions in region: {region}")

        try:
            # Create Event Grid client
            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )
            eventgrid_client = EventGridManagementClient(credential, self.subscription_id)

            # Get all resource groups
            resource_groups = await self._get_resource_groups()

            for rg in resource_groups:
                rg_name = rg.name

                # Filter by resource group if specified
                if self.resource_groups and rg_name not in self.resource_groups:
                    continue

                try:
                    # List Event Grid topics in this resource group
                    topics = eventgrid_client.topics.list_by_resource_group(rg_name)

                    for topic in topics:
                        topic_name = topic.name
                        topic_location = topic.location

                        # Filter by region if specified
                        if self.regions and topic_location not in self.regions:
                            continue

                        # Get subscriptions for this topic
                        try:
                            subscriptions = eventgrid_client.event_subscriptions.list_by_resource(
                                topic.id
                            )

                            for subscription in subscriptions:
                                subscription_name = subscription.name
                                provisioning_state = getattr(subscription, 'provisioning_state', 'Unknown')

                                # Get destination
                                destination = getattr(subscription, 'destination', None)
                                destination_type = type(destination).__name__ if destination else "Unknown"

                                # Get dead letter config
                                dead_letter_destination = getattr(subscription, 'dead_letter_destination', None)
                                has_dead_letter = dead_letter_destination is not None

                                # Get filter
                                filter_obj = getattr(subscription, 'filter', None)
                                has_subject_filter = False
                                has_advanced_filter = False
                                if filter_obj:
                                    has_subject_filter = getattr(filter_obj, 'subject_begins_with', None) is not None
                                    advanced_filters = getattr(filter_obj, 'advanced_filters', [])
                                    has_advanced_filter = len(advanced_filters) > 0

                                # Estimate usage (placeholder - would need Azure Monitor)
                                monthly_operations = 0  # Placeholder
                                delivery_success_rate = 100.0  # Placeholder (0-100%)

                                # Calculate cost
                                monthly_cost = self._estimate_event_grid_cost(monthly_operations)

                                # Check optimization opportunities
                                is_optimizable, score, priority, savings, recommendations = \
                                    await self._calculate_event_grid_subscription_optimization(
                                        subscription, delivery_success_rate, monthly_operations,
                                        has_dead_letter, has_subject_filter, has_advanced_filter, monthly_cost
                                    )

                                # Build metadata
                                metadata = {
                                    "topic_name": topic_name,
                                    "subscription_name": subscription_name,
                                    "destination_type": destination_type,
                                    "provisioning_state": provisioning_state,
                                    "has_dead_letter": has_dead_letter,
                                    "has_subject_filter": has_subject_filter,
                                    "has_advanced_filter": has_advanced_filter,
                                    "delivery_success_rate": delivery_success_rate,
                                    "monthly_operations": monthly_operations,
                                    "resource_group": rg_name,
                                }

                                # Determine if orphan (endpoint mort = waste)
                                is_orphan = delivery_success_rate < 10.0 and score >= 90

                                # Create resource record
                                resource = AllCloudResourceData(
                                    resource_id=subscription.id,
                                    resource_name=f"{topic_name}/{subscription_name}",
                                    resource_type="azure_event_grid_subscription",
                                    region=topic_location,
                                    estimated_monthly_cost=monthly_cost,
                                    currency="USD",
                                    resource_metadata=metadata,
                                    is_orphan=is_orphan,
                                    is_optimizable=is_optimizable and not is_orphan,
                                    optimization_score=score if not is_orphan else 0,
                                    optimization_priority=priority if not is_orphan else "none",
                                    potential_monthly_savings=savings if not is_orphan else 0.0,
                                    optimization_recommendations=recommendations if not is_orphan else []
                                )

                                resources.append(resource)
                                self.logger.info(
                                    f"Found Event Grid Subscription: {topic_name}/{subscription_name} "
                                    f"(Success rate: {delivery_success_rate:.1f}%, Dead letter: {has_dead_letter}, "
                                    f"Cost: ${monthly_cost:.2f}/mo, Optimizable: {is_optimizable})"
                                )

                        except Exception as e:
                            self.logger.error(
                                f"Error scanning subscriptions for topic {topic_name}: {str(e)}"
                            )
                            continue

                except Exception as e:
                    self.logger.error(
                        f"Error scanning Event Grid topics in resource group {rg_name}: {str(e)}"
                    )
                    continue

            self.logger.info(
                f"Event Grid Subscription scan complete: {len(resources)} subscriptions found"
            )

        except Exception as e:
            self.logger.error(f"Error scanning Event Grid Subscriptions: {str(e)}")

        return resources

    def _estimate_event_grid_cost(self, monthly_operations: int) -> float:
        """
        Estimate monthly cost for Event Grid based on operations.

        Pricing:
        - $0.60 per million operations
        - Premier million gratuit chaque mois

        Note: Trs bas cot - gnralement <$10/mois
        """
        # Premier million gratuit
        if monthly_operations <= 1000000:
            return 0.0

        # Au-del du million gratuit
        billable_operations = monthly_operations - 1000000
        cost_per_million = 0.60
        return (billable_operations / 1000000) * cost_per_million

    async def _calculate_event_grid_subscription_optimization(
        self,
        subscription: Any,
        delivery_success_rate: float,
        monthly_operations: int,
        has_dead_letter: bool,
        has_subject_filter: bool,
        has_advanced_filter: bool,
        monthly_cost: float
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Event Grid Subscription.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # SCENARIO 1: CRITICAL - Subscription vers endpoint mort/inaccessible
        # Delivery success rate <10% = endpoint probablement mort
        if delivery_success_rate < 10.0:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append(
                f"CRITICAL: Event Grid Subscription has very low delivery success rate ({delivery_success_rate:.1f}%). "
                f"Check endpoint health or delete if no longer needed to save ${monthly_cost:.2f}/month."
            )

        # SCENARIO 2: HIGH - Subscription inactive (0 vnements depuis 90+ jours)
        elif monthly_operations == 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority == "none" else priority
            potential_savings = monthly_cost
            recommendations.append(
                f"HIGH: Event Grid Subscription has no events for 90+ days. "
                f"Delete if no longer needed to save ${monthly_cost:.2f}/month."
            )

        # SCENARIO 3: HIGH - Dead letter destination non configure
        # Sans dead letter, vnements en chec sont perdus
        elif not has_dead_letter:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority == "none" else priority
            recommendations.append(
                "HIGH: No dead letter destination configured. "
                "Events that fail delivery are lost. Configure dead lettering for reliability."
            )

        # SCENARIO 4: MEDIUM - Filtres trop larges (traite tous vnements)
        # Pas de filtres = traite tous vnements = gaspillage potentiel
        if not has_subject_filter and not has_advanced_filter and not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority == "none" else priority
            recommendations.append(
                "MEDIUM: No event filters configured. "
                "Subscription processes all events. Add filters to reduce unnecessary processing."
            )

        # SCENARIO 5: LOW - Pas d'Advanced Filtering (best practice)
        # Advanced filters permettent filtrage plus granulaire
        if not has_advanced_filter and not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            recommendations.append(
                "LOW: No advanced filters configured. "
                "Use advanced filtering for better event routing optimization."
            )

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_key_vault_secrets(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Key Vault Secrets for cost intelligence.

        Key Vault stores secrets, keys, and certificates securely.
        Pricing: $0.03 per 10K operations + minimal storage
        Typical cost: $5-50/month

        Detection criteria:
        - Secrets expirs ou non accessibles 90+ days (CRITICAL - 90 score)
        - Pas de date d'expiration (HIGH - 75 score)
        - Secrets non rotationns 365+ days (HIGH - 70 score)
        - Soft-delete non activ (MEDIUM - 50 score)
        - Pas de monitoring/alerting (LOW - 30 score)

        Returns:
            List of all Key Vault Secrets with optimization recommendations
        """
        try:
            from azure.mgmt.keyvault import KeyVaultManagementClient
        except ImportError:
            self.logger.error("azure-mgmt-keyvault not installed")
            return []

        resources = []
        self.logger.info(f"Scanning Key Vault Secrets in region: {region}")

        try:
            # Create Key Vault client
            credential = ClientSecretCredential(
                tenant_id=self.tenant_id,
                client_id=self.client_id,
                client_secret=self.client_secret
            )
            kv_client = KeyVaultManagementClient(credential, self.subscription_id)

            # Get all resource groups
            resource_groups = await self._get_resource_groups()

            for rg in resource_groups:
                rg_name = rg.name

                # Filter by resource group if specified
                if self.resource_groups and rg_name not in self.resource_groups:
                    continue

                try:
                    # List Key Vaults in this resource group
                    vaults = kv_client.vaults.list_by_resource_group(rg_name)

                    for vault in vaults:
                        vault_name = vault.name
                        vault_location = vault.location

                        # Filter by region if specified
                        if self.regions and vault_location not in self.regions:
                            continue

                        # Get vault properties
                        sku = getattr(vault.properties, 'sku', None) if hasattr(vault, 'properties') else None
                        sku_name = sku.name if sku else "Standard"
                        soft_delete_enabled = getattr(vault.properties, 'enable_soft_delete', False) if hasattr(vault, 'properties') else False

                        # Estimate metrics (placeholder - would need Azure Monitor / Key Vault SDK)
                        monthly_operations = 1000  # Placeholder
                        secrets_count = 5  # Placeholder
                        secrets_without_expiration = 0  # Placeholder
                        secrets_last_access_90d_ago = 0  # Placeholder
                        secrets_not_rotated_365d = 0  # Placeholder

                        # Calculate cost
                        monthly_cost = self._estimate_key_vault_cost(monthly_operations, secrets_count)

                        # Check optimization opportunities
                        is_optimizable, score, priority, savings, recommendations = \
                            await self._calculate_key_vault_secret_optimization(
                                vault, soft_delete_enabled, secrets_without_expiration,
                                secrets_last_access_90d_ago, secrets_not_rotated_365d, monthly_cost
                            )

                        # Build metadata
                        metadata = {
                            "vault_name": vault_name,
                            "sku": sku_name,
                            "soft_delete_enabled": soft_delete_enabled,
                            "secrets_count": secrets_count,
                            "secrets_without_expiration": secrets_without_expiration,
                            "secrets_last_access_90d_ago": secrets_last_access_90d_ago,
                            "secrets_not_rotated_365d": secrets_not_rotated_365d,
                            "monthly_operations": monthly_operations,
                            "resource_group": rg_name,
                        }

                        # Determine if orphan (secrets non accds = waste)
                        is_orphan = secrets_last_access_90d_ago > 0 and score >= 90

                        # Create resource record
                        resource = AllCloudResourceData(
                            resource_id=vault.id,
                            resource_name=vault_name,
                            resource_type="azure_key_vault_secret",
                            region=vault_location,
                            estimated_monthly_cost=monthly_cost,
                            currency="USD",
                            resource_metadata=metadata,
                            is_orphan=is_orphan,
                            is_optimizable=is_optimizable and not is_orphan,
                            optimization_score=score if not is_orphan else 0,
                            optimization_priority=priority if not is_orphan else "none",
                            potential_monthly_savings=savings if not is_orphan else 0.0,
                            optimization_recommendations=recommendations if not is_orphan else []
                        )

                        resources.append(resource)
                        self.logger.info(
                            f"Found Key Vault: {vault_name} "
                            f"(SKU: {sku_name}, Soft-delete: {soft_delete_enabled}, "
                            f"Secrets: {secrets_count}, Cost: ${monthly_cost:.2f}/mo, Optimizable: {is_optimizable})"
                        )

                except Exception as e:
                    self.logger.error(
                        f"Error scanning Key Vaults in resource group {rg_name}: {str(e)}"
                    )
                    continue

            self.logger.info(
                f"Key Vault Secret scan complete: {len(resources)} vaults found"
            )

        except Exception as e:
            self.logger.error(f"Error scanning Key Vault Secrets: {str(e)}")

        return resources

    def _estimate_key_vault_cost(self, monthly_operations: int, secrets_count: int) -> float:
        """
        Estimate monthly cost for Key Vault based on operations.

        Pricing:
        - $0.03 per 10,000 operations
        - Secrets: Free storage, paid per access
        - HSM-backed secrets (Premium): $1/secret/month

        For simplicity, using operation-based pricing
        """
        cost_per_10k_ops = 0.03
        return (monthly_operations / 10000) * cost_per_10k_ops

    async def _calculate_key_vault_secret_optimization(
        self,
        vault: Any,
        soft_delete_enabled: bool,
        secrets_without_expiration: int,
        secrets_last_access_90d_ago: int,
        secrets_not_rotated_365d: int,
        monthly_cost: float
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Key Vault Secret.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # SCENARIO 1: CRITICAL - Secrets expirs ou non accessibles 90+ days
        if secrets_last_access_90d_ago > 0:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost * 0.5  # Assume 50% cost reduction
            recommendations.append(
                f"CRITICAL: {secrets_last_access_90d_ago} secrets not accessed for 90+ days. "
                f"Review and delete unused secrets to save ~${potential_savings:.2f}/month."
            )

        # SCENARIO 2: HIGH - Pas de date d'expiration configure (risque scurit)
        elif secrets_without_expiration > 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority == "none" else priority
            recommendations.append(
                f"HIGH: {secrets_without_expiration} secrets without expiration date. "
                f"Set expiration dates for automatic rotation and improved security."
            )

        # SCENARIO 3: HIGH - Secrets non rotationns depuis 365+ jours
        elif secrets_not_rotated_365d > 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "high" if priority == "none" else priority
            recommendations.append(
                f"HIGH: {secrets_not_rotated_365d} secrets not rotated for 365+ days. "
                f"Rotate secrets regularly (recommended: every 90 days) for security."
            )

        # SCENARIO 4: MEDIUM - Soft-delete non activ (risque de perte)
        if not soft_delete_enabled and not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 50)
            priority = "medium" if priority == "none" else priority
            recommendations.append(
                "MEDIUM: Soft-delete not enabled. "
                "Enable soft-delete to protect against accidental secret deletion."
            )

        # SCENARIO 5: LOW - Pas de monitoring/alerting configur
        # Placeholder - would require checking diagnostic settings
        if not is_optimizable:
            is_optimizable = True
            optimization_score = max(optimization_score, 30)
            priority = "low" if priority == "none" else priority
            recommendations.append(
                "LOW: Configure monitoring and alerting for Key Vault access. "
                "Enable diagnostic logs to track secret access and detect anomalies."
            )

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_app_configurations(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure App Configuration stores for cost intelligence.

        Pricing (Azure App Configuration):
        - Free tier: $0/mois (1 store, 1000 requests/jour max)
        - Standard tier: $1.20/jour (~$36/mois) + data transfer

        Typical cost: $0-50/month

        Returns:
            List of AllCloudResourceData (is_orphan=False, is_optimizable=True)
        """
        try:
            from azure.mgmt.appconfiguration import AppConfigurationManagementClient
        except ImportError:
            logger.warning("azure-mgmt-appconfiguration not installed, skipping App Configuration scan")
            return []

        resources = []

        try:
            credential = self._get_azure_credential()
            app_config_client = AppConfigurationManagementClient(credential, self.subscription_id)

            # Iterate over resource groups
            for rg_name in self.resource_groups:
                try:
                    # List App Configuration stores in resource group
                    stores = app_config_client.configuration_stores.list_by_resource_group(rg_name)

                    for store in stores:
                        try:
                            # Extract location from store.location
                            store_location = getattr(store, "location", "unknown").lower()

                            # Filter by region if not global
                            if region.lower() != "global" and store_location != region.lower():
                                continue

                            # Extract metadata
                            store_id = store.id
                            store_name = store.name
                            sku_name = getattr(store.sku, "name", "Free").lower()  # "free" or "standard"
                            creation_date = getattr(store, "creation_date", None)
                            public_network_access = getattr(store, "public_network_access", "Enabled")

                            # Point-in-Time Recovery (only available in Standard tier)
                            soft_delete_retention_days = getattr(store, "soft_delete_retention_in_days", 0)
                            has_soft_delete = soft_delete_retention_days > 0

                            # Tags
                            tags = getattr(store, "tags", {}) or {}

                            # Calculate age
                            age_days = 0
                            if creation_date:
                                age_days = (datetime.now(timezone.utc) - creation_date).days

                            # Estimate daily requests (we don't have actual metrics, estimate based on tier)
                            estimated_daily_requests = 0
                            if sku_name == "standard":
                                # Assume Standard tier = high usage (otherwise why pay?)
                                estimated_daily_requests = 50000  # Estimate
                            else:
                                estimated_daily_requests = 500  # Free tier usage

                            # Count configuration keys (requires Azure Resource Graph or direct API call)
                            # For simplicity, we'll estimate based on tier
                            estimated_config_keys = 10 if sku_name == "free" else 50

                            # Estimate monthly cost
                            monthly_cost = self._estimate_app_configuration_cost(
                                sku_name=sku_name,
                                estimated_daily_requests=estimated_daily_requests
                            )

                            # Check optimization opportunities
                            is_optimizable, optimization_score, priority, potential_savings, recommendations = (
                                self._calculate_app_configuration_optimization(
                                    sku_name=sku_name,
                                    age_days=age_days,
                                    estimated_daily_requests=estimated_daily_requests,
                                    has_soft_delete=has_soft_delete,
                                    estimated_config_keys=estimated_config_keys,
                                    monthly_cost=monthly_cost,
                                )
                            )

                            # Build metadata
                            metadata = {
                                "store_id": store_id,
                                "store_name": store_name,
                                "sku": sku_name,
                                "location": store_location,
                                "resource_group": rg_name,
                                "age_days": age_days,
                                "estimated_daily_requests": estimated_daily_requests,
                                "estimated_config_keys": estimated_config_keys,
                                "has_soft_delete": has_soft_delete,
                                "soft_delete_retention_days": soft_delete_retention_days,
                                "public_network_access": public_network_access,
                                "tags": tags,
                            }

                            resource = AllCloudResourceData(
                                resource_id=store_id,
                                resource_type="azure_app_configuration",
                                resource_name=store_name,
                                region=store_location,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=metadata,
                                created_at_cloud=creation_date,
                                is_orphan=False,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                            )

                            resources.append(resource)
                            logger.info(
                                f"Found App Configuration store: {store_name} in {store_location} "
                                f"(SKU: {sku_name}, Optimizable: {is_optimizable}, Score: {optimization_score})"
                            )

                        except Exception as e:
                            logger.error(f"Error processing App Configuration store {getattr(store, 'name', 'unknown')}: {e}")
                            continue

                except Exception as e:
                    logger.error(f"Error listing App Configuration stores in resource group {rg_name}: {e}")
                    continue

        except Exception as e:
            logger.error(f"Error scanning Azure App Configuration stores: {e}")

        logger.info(f"Completed Azure App Configuration scan in region {region}: {len(resources)} stores found")
        return resources

    def _estimate_app_configuration_cost(
        self,
        sku_name: str,
        estimated_daily_requests: int,
    ) -> float:
        """
        Estimate monthly cost for Azure App Configuration.

        Pricing:
        - Free tier: $0 (1 store, 1000 requests/day max)
        - Standard tier: $1.20/day (~$36/month) + data transfer
        """
        if sku_name == "free":
            return 0.0

        # Standard tier
        base_cost_per_day = 1.20
        monthly_cost = base_cost_per_day * 30  # ~$36/month

        # Add data transfer cost (estimate $0.01/GB)
        # Assume 1KB per request, so 1M requests = 1GB
        monthly_requests = estimated_daily_requests * 30
        data_transfer_gb = monthly_requests / 1000000
        data_transfer_cost = data_transfer_gb * 0.01

        return round(monthly_cost + data_transfer_cost, 2)

    def _calculate_app_configuration_optimization(
        self,
        sku_name: str,
        age_days: int,
        estimated_daily_requests: int,
        has_soft_delete: bool,
        estimated_config_keys: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Azure App Configuration.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Standard tier with 0 requests for 30+ days
        if sku_name == "standard" and estimated_daily_requests == 0 and age_days >= 30:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost  # Full cost savings by downgrading
            recommendations.append(
                "Standard tier App Configuration store has 0 requests for 30+ days. "
                "Downgrade to Free tier or delete if unused. "
                f"Potential savings: ${potential_savings:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Standard tier with very low usage (<1K requests/day)
        if sku_name == "standard" and estimated_daily_requests < 1000:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost * 0.9  # 90% savings by downgrading
            recommendations.append(
                f"Standard tier App Configuration store has very low usage ({estimated_daily_requests} requests/day). "
                "Free tier supports 1000 requests/day. "
                f"Downgrade to Free tier to save ${potential_savings:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Point-in-Time Recovery not used (Standard feature waste)
        if sku_name == "standard" and not has_soft_delete:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = monthly_cost * 0.3  # Partial waste
            recommendations.append(
                "Standard tier App Configuration store does not have Point-in-Time Recovery enabled. "
                "Enable soft delete to leverage Standard tier features, or downgrade to Free tier. "
                f"Potential savings: ${potential_savings:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Configuration keys not used for 90+ days
        if age_days >= 90 and estimated_config_keys > 0 and estimated_daily_requests < 100:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            potential_savings = monthly_cost * 0.5 if sku_name == "standard" else 0.0
            recommendations.append(
                f"App Configuration store has {estimated_config_keys} configuration keys but very low usage "
                f"({estimated_daily_requests} requests/day for {age_days} days). "
                "Review and remove unused configuration keys, or delete the store if not needed."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - No feature flags used (underutilizing capabilities)
        if sku_name == "standard" and estimated_config_keys > 0:
            # This is a best practice recommendation, not a cost issue
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = 0.0
            recommendations.append(
                "App Configuration store is not leveraging feature flags. "
                "Consider using feature management capabilities to control feature rollouts dynamically."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_api_managements(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure API Management services for cost intelligence.

        Pricing (Azure API Management):
        - Consumption: $0.035 per 10K calls + $3.50 per GB
        - Developer: $49.79/mois (1M calls inclus)
        - Basic: $147.24/mois (pas de SLA)
        - Standard: $735.48/mois + scale units
        - Premium: $2943.04/mois + scale units + multi-region

        Typical cost: $50-500/month

        Returns:
            List of AllCloudResourceData (is_orphan=False, is_optimizable=True)
        """
        try:
            from azure.mgmt.apimanagement import ApiManagementClient
        except ImportError:
            logger.warning("azure-mgmt-apimanagement not installed, skipping API Management scan")
            return []

        resources = []

        try:
            credential = self._get_azure_credential()
            apim_client = ApiManagementClient(credential, self.subscription_id)

            # Iterate over resource groups
            for rg_name in self.resource_groups:
                try:
                    # List API Management services in resource group
                    services = apim_client.api_management_service.list_by_resource_group(rg_name)

                    for service in services:
                        try:
                            # Extract location
                            service_location = getattr(service, "location", "unknown").lower()

                            # Filter by region if not global
                            if region.lower() != "global" and service_location != region.lower():
                                continue

                            # Extract metadata
                            service_id = service.id
                            service_name = service.name
                            sku_name = getattr(service.sku, "name", "Unknown").lower()  # consumption, developer, basic, standard, premium
                            sku_capacity = getattr(service.sku, "capacity", 1)
                            creation_time = getattr(service, "created_at_utc", None)
                            provisioning_state = getattr(service, "provisioning_state", "Unknown")

                            # Gateway URL
                            gateway_url = getattr(service, "gateway_url", None)

                            # Public IP addresses (for Premium multi-region)
                            public_ip_addresses = getattr(service, "public_ip_addresses", [])

                            # Tags
                            tags = getattr(service, "tags", {}) or {}

                            # Calculate age
                            age_days = 0
                            if creation_time:
                                age_days = (datetime.now(timezone.utc) - creation_time).days

                            # Count APIs and revisions (requires API call)
                            # For simplicity, we'll estimate based on tier
                            estimated_apis = 0
                            estimated_revisions = 0
                            try:
                                apis = apim_client.api.list_by_service(rg_name, service_name)
                                api_list = list(apis)
                                estimated_apis = len(api_list)

                                # Count total revisions across all APIs
                                for api in api_list:
                                    try:
                                        revisions = apim_client.api_revision.list_by_service(
                                            rg_name, service_name, api.name
                                        )
                                        estimated_revisions += len(list(revisions))
                                    except Exception:
                                        pass
                            except Exception as e:
                                logger.debug(f"Error counting APIs for {service_name}: {e}")
                                # Fallback estimate
                                estimated_apis = 5 if sku_name in ["standard", "premium"] else 2

                            # Estimate daily requests (no direct metrics available, estimate based on tier)
                            estimated_daily_requests = 0
                            if sku_name == "consumption":
                                estimated_daily_requests = 10000  # 10K
                            elif sku_name == "developer":
                                estimated_daily_requests = 50000  # 50K
                            elif sku_name == "basic":
                                estimated_daily_requests = 100000  # 100K
                            elif sku_name == "standard":
                                estimated_daily_requests = 500000  # 500K
                            elif sku_name == "premium":
                                estimated_daily_requests = 2000000  # 2M

                            # Estimate monthly cost
                            monthly_cost = self._estimate_api_management_cost(
                                sku_name=sku_name,
                                sku_capacity=sku_capacity,
                                estimated_daily_requests=estimated_daily_requests
                            )

                            # Check optimization opportunities
                            is_optimizable, optimization_score, priority, potential_savings, recommendations = (
                                self._calculate_api_management_optimization(
                                    sku_name=sku_name,
                                    sku_capacity=sku_capacity,
                                    age_days=age_days,
                                    estimated_daily_requests=estimated_daily_requests,
                                    estimated_apis=estimated_apis,
                                    estimated_revisions=estimated_revisions,
                                    monthly_cost=monthly_cost,
                                )
                            )

                            # Build metadata
                            metadata = {
                                "service_id": service_id,
                                "service_name": service_name,
                                "sku": sku_name,
                                "sku_capacity": sku_capacity,
                                "location": service_location,
                                "resource_group": rg_name,
                                "age_days": age_days,
                                "provisioning_state": provisioning_state,
                                "gateway_url": gateway_url,
                                "estimated_daily_requests": estimated_daily_requests,
                                "estimated_apis": estimated_apis,
                                "estimated_revisions": estimated_revisions,
                                "public_ip_addresses": public_ip_addresses,
                                "tags": tags,
                            }

                            resource = AllCloudResourceData(
                                resource_id=service_id,
                                resource_type="azure_api_management",
                                resource_name=service_name,
                                region=service_location,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=metadata,
                                created_at_cloud=creation_time,
                                is_orphan=False,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                            )

                            resources.append(resource)
                            logger.info(
                                f"Found API Management service: {service_name} in {service_location} "
                                f"(SKU: {sku_name}, Optimizable: {is_optimizable}, Score: {optimization_score})"
                            )

                        except Exception as e:
                            logger.error(f"Error processing API Management service {getattr(service, 'name', 'unknown')}: {e}")
                            continue

                except Exception as e:
                    logger.error(f"Error listing API Management services in resource group {rg_name}: {e}")
                    continue

        except Exception as e:
            logger.error(f"Error scanning Azure API Management services: {e}")

        logger.info(f"Completed Azure API Management scan in region {region}: {len(resources)} services found")
        return resources

    def _estimate_api_management_cost(
        self,
        sku_name: str,
        sku_capacity: int,
        estimated_daily_requests: int,
    ) -> float:
        """
        Estimate monthly cost for Azure API Management.

        Pricing:
        - Consumption: $0.035 per 10K calls + $3.50 per GB
        - Developer: $49.79/mois (1M calls inclus)
        - Basic: $147.24/mois (pas de SLA)
        - Standard: $735.48/mois + scale units
        - Premium: $2943.04/mois + scale units + multi-region
        """
        if sku_name == "consumption":
            # Consumption pricing: $0.035 per 10K calls
            monthly_calls = estimated_daily_requests * 30
            cost = (monthly_calls / 10000) * 0.035
            # Add data transfer estimate ($3.50/GB, assume 1KB per call)
            data_gb = (monthly_calls / 1000000)  # 1KB per call
            cost += data_gb * 3.50
            return round(cost, 2)

        elif sku_name == "developer":
            return 49.79  # Fixed cost

        elif sku_name == "basic":
            return 147.24 * sku_capacity  # Per unit

        elif sku_name == "standard":
            return 735.48 * sku_capacity  # Per unit

        elif sku_name == "premium":
            return 2943.04 * sku_capacity  # Per unit

        else:
            return 50.0  # Fallback estimate

    def _calculate_api_management_optimization(
        self,
        sku_name: str,
        sku_capacity: int,
        age_days: int,
        estimated_daily_requests: int,
        estimated_apis: int,
        estimated_revisions: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Azure API Management.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Service with 0 requests for 90+ days
        if estimated_daily_requests == 0 and age_days >= 90:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost  # Full cost
            recommendations.append(
                "API Management service has 0 requests for 90+ days. "
                "Delete the service or downgrade to Consumption tier if keeping for testing. "
                f"Potential savings: ${potential_savings:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Premium tier with low usage (<100K requests/day)
        if sku_name == "premium" and estimated_daily_requests < 100000:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            # Premium costs $2943/month, Standard costs $735/month
            potential_savings = monthly_cost - (735.48 * sku_capacity)
            recommendations.append(
                f"Premium tier API Management service has low usage ({estimated_daily_requests} requests/day). "
                f"Downgrade to Standard tier to save ${potential_savings:.2f}/month. "
                "Premium features (multi-region, VNet) may not be needed."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Too many API revisions (>10 revisions per API)
        if estimated_apis > 0:
            avg_revisions_per_api = estimated_revisions / estimated_apis
            if avg_revisions_per_api > 10:
                is_optimizable = True
                optimization_score = 70
                priority = "high"
                potential_savings = 0.0  # No direct cost savings, but maintenance waste
                recommendations.append(
                    f"API Management service has {estimated_revisions} revisions for {estimated_apis} APIs "
                    f"(avg {avg_revisions_per_api:.1f} revisions/API). "
                    "Clean up old/unused revisions to improve performance and reduce complexity."
                )
                return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Standard/Premium tier with very low usage (<10K requests/day)
        if sku_name in ["standard", "premium"] and estimated_daily_requests < 10000:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Downgrade to Consumption tier
            consumption_cost = (estimated_daily_requests * 30 / 10000) * 0.035
            potential_savings = monthly_cost - consumption_cost
            recommendations.append(
                f"{sku_name.title()} tier API Management service has very low usage ({estimated_daily_requests} requests/day). "
                f"Downgrade to Consumption tier to save ${potential_savings:.2f}/month. "
                "Consumption tier is ideal for dev/test and low-volume APIs."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - No monitoring/alerting configured (best practice)
        # Placeholder - would require checking diagnostic settings
        if not is_optimizable:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = 0.0
            recommendations.append(
                "LOW: Configure monitoring and alerting for API Management service. "
                "Enable Application Insights integration and set up alerts for high latency, errors, and throttling."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_logic_apps(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Logic Apps for cost intelligence.

        Pricing (Azure Logic Apps):
        - Consumption: $0.000025 per action + $0.000125 per trigger
        - Standard: Starts at ~$200/mois (App Service Plan)

        Typical cost: $5-100/month

        Returns:
            List of AllCloudResourceData (is_orphan=False, is_optimizable=True)
        """
        try:
            from azure.mgmt.logic import LogicManagementClient
        except ImportError:
            logger.warning("azure-mgmt-logic not installed, skipping Logic Apps scan")
            return []

        resources = []

        try:
            credential = self._get_azure_credential()
            logic_client = LogicManagementClient(credential, self.subscription_id)

            # Iterate over resource groups
            for rg_name in self.resource_groups:
                try:
                    # List Logic Apps in resource group
                    workflows = logic_client.workflows.list_by_resource_group(rg_name)

                    for workflow in workflows:
                        try:
                            # Extract location
                            workflow_location = getattr(workflow, "location", "unknown").lower()

                            # Filter by region if not global
                            if region.lower() != "global" and workflow_location != region.lower():
                                continue

                            # Extract metadata
                            workflow_id = workflow.id
                            workflow_name = workflow.name
                            state = getattr(workflow, "state", "Unknown")  # Enabled, Disabled, etc.
                            sku_name = getattr(getattr(workflow, "sku", None), "name", "NotSpecified")  # NotSpecified (Consumption), Standard, etc.
                            created_time = getattr(workflow, "created_time", None)
                            changed_time = getattr(workflow, "changed_time", None)

                            # Workflow definition
                            definition = getattr(workflow, "definition", {})

                            # Tags
                            tags = getattr(workflow, "tags", {}) or {}

                            # Calculate age
                            age_days = 0
                            if created_time:
                                age_days = (datetime.now(timezone.utc) - created_time).days

                            # Get workflow run history (to detect activity)
                            runs_count_30d = 0
                            failed_runs_count = 0
                            try:
                                # Get runs from last 30 days
                                thirty_days_ago = datetime.now(timezone.utc) - timedelta(days=30)
                                runs = logic_client.workflow_runs.list(rg_name, workflow_name)

                                for run in runs:
                                    run_start_time = getattr(run, "start_time", None)
                                    run_status = getattr(run, "status", "Unknown")

                                    if run_start_time and run_start_time >= thirty_days_ago:
                                        runs_count_30d += 1
                                        if run_status in ["Failed", "Cancelled", "TimedOut"]:
                                            failed_runs_count += 1

                                    # Limit iteration for performance
                                    if runs_count_30d >= 1000:
                                        break
                            except Exception as e:
                                logger.debug(f"Error fetching runs for workflow {workflow_name}: {e}")
                                # Fallback estimate
                                if state == "Enabled":
                                    runs_count_30d = 100  # Estimate
                                else:
                                    runs_count_30d = 0

                            # Count actions in workflow definition
                            actions_count = 0
                            if isinstance(definition, dict):
                                actions = definition.get("actions", {})
                                actions_count = len(actions) if isinstance(actions, dict) else 0

                            # Estimate monthly executions
                            estimated_monthly_executions = int(runs_count_30d)

                            # Estimate monthly cost
                            monthly_cost = self._estimate_logic_app_cost(
                                sku_name=sku_name,
                                estimated_monthly_executions=estimated_monthly_executions,
                                actions_count=actions_count
                            )

                            # Check optimization opportunities
                            is_optimizable, optimization_score, priority, potential_savings, recommendations = (
                                self._calculate_logic_app_optimization(
                                    state=state,
                                    sku_name=sku_name,
                                    age_days=age_days,
                                    runs_count_30d=runs_count_30d,
                                    failed_runs_count=failed_runs_count,
                                    actions_count=actions_count,
                                    monthly_cost=monthly_cost,
                                )
                            )

                            # Build metadata
                            metadata = {
                                "workflow_id": workflow_id,
                                "workflow_name": workflow_name,
                                "state": state,
                                "sku": sku_name,
                                "location": workflow_location,
                                "resource_group": rg_name,
                                "age_days": age_days,
                                "runs_count_30d": runs_count_30d,
                                "failed_runs_count": failed_runs_count,
                                "actions_count": actions_count,
                                "estimated_monthly_executions": estimated_monthly_executions,
                                "created_time": created_time.isoformat() if created_time else None,
                                "changed_time": changed_time.isoformat() if changed_time else None,
                                "tags": tags,
                            }

                            resource = AllCloudResourceData(
                                resource_id=workflow_id,
                                resource_type="azure_logic_app",
                                resource_name=workflow_name,
                                region=workflow_location,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=metadata,
                                created_at_cloud=created_time,
                                is_orphan=False,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                            )

                            resources.append(resource)
                            logger.info(
                                f"Found Logic App: {workflow_name} in {workflow_location} "
                                f"(State: {state}, Optimizable: {is_optimizable}, Score: {optimization_score})"
                            )

                        except Exception as e:
                            logger.error(f"Error processing Logic App {getattr(workflow, 'name', 'unknown')}: {e}")
                            continue

                except Exception as e:
                    logger.error(f"Error listing Logic Apps in resource group {rg_name}: {e}")
                    continue

        except Exception as e:
            logger.error(f"Error scanning Azure Logic Apps: {e}")

        logger.info(f"Completed Azure Logic Apps scan in region {region}: {len(resources)} workflows found")
        return resources

    def _estimate_logic_app_cost(
        self,
        sku_name: str,
        estimated_monthly_executions: int,
        actions_count: int,
    ) -> float:
        """
        Estimate monthly cost for Azure Logic Apps.

        Pricing:
        - Consumption: $0.000025 per action execution
        - Standard: ~$200/month (App Service Plan based)
        """
        if sku_name.lower() in ["notspecified", "consumption"]:
            # Consumption pricing
            # Assume average of 5 actions per execution
            total_actions = estimated_monthly_executions * max(actions_count, 5)
            cost = total_actions * 0.000025
            return round(cost, 2)
        elif sku_name.lower() == "standard":
            # Standard tier (App Service Plan)
            return 200.0  # Base estimate
        else:
            # Fallback
            return 10.0

    def _calculate_logic_app_optimization(
        self,
        state: str,
        sku_name: str,
        age_days: int,
        runs_count_30d: int,
        failed_runs_count: int,
        actions_count: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Azure Logic Apps.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Disabled workflow for 90+ days
        if state.lower() == "disabled" and age_days >= 90:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost  # Full savings if deleted
            recommendations.append(
                f"Logic App workflow is disabled for {age_days} days. "
                "Delete the workflow if no longer needed to eliminate any residual costs. "
                f"Potential savings: ${potential_savings:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Standard tier with low executions (<100 runs/month)
        if sku_name.lower() == "standard" and runs_count_30d < 100:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            # Consumption would cost: 100 runs * 5 actions * $0.000025 = $0.0125
            consumption_cost = (runs_count_30d * max(actions_count, 5)) * 0.000025
            potential_savings = monthly_cost - consumption_cost
            recommendations.append(
                f"Standard tier Logic App has very low usage ({runs_count_30d} runs/month). "
                f"Downgrade to Consumption tier to save ${potential_savings:.2f}/month. "
                "Standard tier is only cost-effective for high-volume workflows."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - High failure rate (>50% failed runs)
        if runs_count_30d > 0:
            failure_rate = (failed_runs_count / runs_count_30d) * 100
            if failure_rate > 50:
                is_optimizable = True
                optimization_score = 70
                priority = "high"
                potential_savings = monthly_cost * 0.5  # Waste due to failed runs
                recommendations.append(
                    f"Logic App has high failure rate ({failure_rate:.1f}% of {runs_count_30d} runs failed). "
                    "Review and fix workflow errors to reduce wasted executions. "
                    f"Potential savings: ${potential_savings:.2f}/month."
                )
                return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - No executions in 30 days (but enabled)
        if state.lower() == "enabled" and runs_count_30d == 0 and age_days >= 30:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            potential_savings = monthly_cost * 0.8  # Partial savings
            recommendations.append(
                "Logic App workflow is enabled but has 0 executions in 30 days. "
                "Disable or delete if not needed, or verify trigger configuration. "
                f"Potential savings: ${potential_savings:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - No error handling configured
        if actions_count > 0:
            # Best practice recommendation (no direct cost savings)
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = 0.0
            recommendations.append(
                f"Logic App has {actions_count} actions but no explicit error handling. "
                "Add retry policies and error scopes to improve reliability and reduce failed runs."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_data_factories(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Data Factory instances for cost intelligence.

        Pricing (Azure Data Factory):
        - Pipeline orchestration: $0.001 per activity run
        - Data movement (Copy): $0.25 per DIU-hour
        - Integration Runtime (IR): $0.274/hour (Azure IR), $0.10-0.84/hour (Self-hosted IR)

        Typical cost: $10-200/month

        Returns:
            List of AllCloudResourceData (is_orphan=False, is_optimizable=True)
        """
        try:
            from azure.mgmt.datafactory import DataFactoryManagementClient
        except ImportError:
            logger.warning("azure-mgmt-datafactory not installed, skipping Data Factory scan")
            return []

        resources = []

        try:
            credential = self._get_azure_credential()
            df_client = DataFactoryManagementClient(credential, self.subscription_id)

            # Iterate over resource groups
            for rg_name in self.resource_groups:
                try:
                    # List Data Factories in resource group
                    factories = df_client.factories.list_by_resource_group(rg_name)

                    for factory in factories:
                        try:
                            # Extract location
                            factory_location = getattr(factory, "location", "unknown").lower()

                            # Filter by region if not global
                            if region.lower() != "global" and factory_location != region.lower():
                                continue

                            # Extract metadata
                            factory_id = getattr(factory, "id", "")
                            factory_name = getattr(factory, "name", "unknown")
                            provisioning_state = getattr(factory, "provisioning_state", "Unknown")
                            version = getattr(factory, "version", "V2")
                            created_time = getattr(factory, "create_time", None)

                            # Tags
                            tags = getattr(factory, "tags", {}) or {}

                            # Calculate age
                            age_days = 0
                            if created_time:
                                age_days = (datetime.now(timezone.utc) - created_time).days

                            # Count pipelines
                            pipelines_count = 0
                            active_pipelines_count = 0
                            try:
                                pipelines = df_client.pipelines.list_by_factory(rg_name, factory_name)
                                pipelines_list = list(pipelines)
                                pipelines_count = len(pipelines_list)

                                # Check which pipelines are actively used (heuristic)
                                for pipeline in pipelines_list[:20]:  # Limit to first 20 for performance
                                    try:
                                        # Try to get recent runs
                                        runs = df_client.pipeline_runs.query_by_factory(
                                            rg_name,
                                            factory_name,
                                            {
                                                "lastUpdatedAfter": (datetime.now(timezone.utc) - timedelta(days=30)).isoformat(),
                                                "lastUpdatedBefore": datetime.now(timezone.utc).isoformat(),
                                                "filters": [
                                                    {
                                                        "operand": "PipelineName",
                                                        "operator": "Equals",
                                                        "values": [pipeline.name]
                                                    }
                                                ]
                                            }
                                        )
                                        if runs.value and len(runs.value) > 0:
                                            active_pipelines_count += 1
                                    except Exception:
                                        pass
                            except Exception as e:
                                logger.debug(f"Error counting pipelines for {factory_name}: {e}")
                                # Fallback estimate
                                pipelines_count = 10

                            # Count Integration Runtimes
                            ir_count = 0
                            self_hosted_ir_count = 0
                            try:
                                irs = df_client.integration_runtimes.list_by_factory(rg_name, factory_name)
                                for ir in irs:
                                    ir_count += 1
                                    ir_type = getattr(getattr(ir, "properties", None), "type", "Unknown")
                                    if ir_type == "SelfHosted":
                                        self_hosted_ir_count += 1
                            except Exception:
                                ir_count = 1  # Fallback

                            # Estimate activity runs per month (no direct metric)
                            estimated_monthly_runs = active_pipelines_count * 30  # Estimate 1 run/day per active pipeline

                            # Estimate monthly cost
                            monthly_cost = self._estimate_data_factory_cost(
                                estimated_monthly_runs=estimated_monthly_runs,
                                ir_count=ir_count,
                                self_hosted_ir_count=self_hosted_ir_count
                            )

                            # Check optimization opportunities
                            is_optimizable, optimization_score, priority, potential_savings, recommendations = (
                                self._calculate_data_factory_optimization(
                                    age_days=age_days,
                                    pipelines_count=pipelines_count,
                                    active_pipelines_count=active_pipelines_count,
                                    ir_count=ir_count,
                                    self_hosted_ir_count=self_hosted_ir_count,
                                    estimated_monthly_runs=estimated_monthly_runs,
                                    monthly_cost=monthly_cost,
                                )
                            )

                            # Build metadata
                            metadata = {
                                "factory_id": factory_id,
                                "factory_name": factory_name,
                                "version": version,
                                "provisioning_state": provisioning_state,
                                "location": factory_location,
                                "resource_group": rg_name,
                                "age_days": age_days,
                                "pipelines_count": pipelines_count,
                                "active_pipelines_count": active_pipelines_count,
                                "ir_count": ir_count,
                                "self_hosted_ir_count": self_hosted_ir_count,
                                "estimated_monthly_runs": estimated_monthly_runs,
                                "tags": tags,
                            }

                            resource = AllCloudResourceData(
                                resource_id=factory_id,
                                resource_type="azure_data_factory",
                                resource_name=factory_name,
                                region=factory_location,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=metadata,
                                created_at_cloud=created_time,
                                is_orphan=False,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                            )

                            resources.append(resource)
                            logger.info(
                                f"Found Data Factory: {factory_name} in {factory_location} "
                                f"(Pipelines: {pipelines_count}, Optimizable: {is_optimizable}, Score: {optimization_score})"
                            )

                        except Exception as e:
                            logger.error(f"Error processing Data Factory {getattr(factory, 'name', 'unknown')}: {e}")
                            continue

                except Exception as e:
                    logger.error(f"Error listing Data Factories in resource group {rg_name}: {e}")
                    continue

        except Exception as e:
            logger.error(f"Error scanning Azure Data Factories: {e}")

        logger.info(f"Completed Azure Data Factory scan in region {region}: {len(resources)} factories found")
        return resources

    def _estimate_data_factory_cost(
        self,
        estimated_monthly_runs: int,
        ir_count: int,
        self_hosted_ir_count: int,
    ) -> float:
        """
        Estimate monthly cost for Azure Data Factory.

        Pricing:
        - Pipeline orchestration: $0.001 per activity run
        - Integration Runtime: $0.274/hour (Azure IR)
        """
        # Activity runs cost
        activity_runs_cost = estimated_monthly_runs * 0.001

        # IR cost (assume IR runs 8 hours/day for active factories)
        ir_hours_per_month = 8 * 30 * ir_count  # 8 hours/day * 30 days
        ir_cost = ir_hours_per_month * 0.274

        total_cost = activity_runs_cost + ir_cost
        return round(total_cost, 2)

    def _calculate_data_factory_optimization(
        self,
        age_days: int,
        pipelines_count: int,
        active_pipelines_count: int,
        ir_count: int,
        self_hosted_ir_count: int,
        estimated_monthly_runs: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Azure Data Factory.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - No active pipelines for 90+ days
        if active_pipelines_count == 0 and age_days >= 90:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append(
                f"Data Factory has 0 active pipelines for {age_days} days. "
                "Delete the Data Factory if no longer needed to eliminate all costs. "
                f"Potential savings: ${potential_savings:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Many inactive pipelines (>50% inactive)
        if pipelines_count > 0:
            inactive_pipelines = pipelines_count - active_pipelines_count
            inactive_ratio = (inactive_pipelines / pipelines_count) * 100
            if inactive_ratio > 50 and inactive_pipelines > 5:
                is_optimizable = True
                optimization_score = 75
                priority = "high"
                potential_savings = monthly_cost * 0.3  # Estimate 30% savings
                recommendations.append(
                    f"Data Factory has {inactive_pipelines} inactive pipelines out of {pipelines_count} total ({inactive_ratio:.1f}%). "
                    "Delete unused pipelines to reduce complexity and improve performance. "
                    f"Potential savings: ${potential_savings:.2f}/month."
                )
                return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Unused Integration Runtimes
        if ir_count > 1 and active_pipelines_count < ir_count:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Each unused IR costs ~$65/month (8h/day * 30 days * $0.274/hour)
            unused_irs = ir_count - max(active_pipelines_count, 1)
            potential_savings = unused_irs * 65
            recommendations.append(
                f"Data Factory has {ir_count} Integration Runtimes but only {active_pipelines_count} active pipelines. "
                f"Remove {unused_irs} unused Integration Runtimes to save ${potential_savings:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Low activity (< 100 runs/month)
        if estimated_monthly_runs < 100 and estimated_monthly_runs > 0:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            potential_savings = monthly_cost * 0.5
            recommendations.append(
                f"Data Factory has very low activity ({estimated_monthly_runs} runs/month). "
                "Consider consolidating with other Data Factories or using serverless alternatives (Logic Apps). "
                f"Potential savings: ${potential_savings:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - No monitoring/alerting configured
        if not is_optimizable:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = 0.0
            recommendations.append(
                "LOW: Configure monitoring and alerting for Data Factory pipelines. "
                "Enable diagnostic logs and set up alerts for pipeline failures and long-running activities."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_static_web_apps(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Static Web Apps for cost intelligence.

        Pricing (Azure Static Web Apps):
        - Free tier: $0/mois (100GB bandwidth, 250MB storage)
        - Standard tier: $9/mois per app + $0.20/GB bandwidth

        Typical cost: $0-100/month

        Returns:
            List of AllCloudResourceData (is_orphan=False, is_optimizable=True)
        """
        try:
            from azure.mgmt.web import WebSiteManagementClient
        except ImportError:
            logger.warning("azure-mgmt-web not installed, skipping Static Web Apps scan")
            return []

        resources = []

        try:
            credential = self._get_azure_credential()
            web_client = WebSiteManagementClient(credential, self.subscription_id)

            # Iterate over resource groups
            for rg_name in self.resource_groups:
                try:
                    # List Static Web Apps in resource group
                    static_sites = web_client.static_sites.list(rg_name)

                    for site in static_sites:
                        try:
                            # Extract location
                            site_location = getattr(site, "location", "unknown").lower()

                            # Filter by region if not global
                            if region.lower() != "global" and site_location != region.lower():
                                continue

                            # Extract metadata
                            site_id = getattr(site, "id", "")
                            site_name = getattr(site, "name", "unknown")
                            sku_tier = getattr(getattr(site, "sku", None), "tier", "Free")  # Free or Standard
                            created_time = getattr(site, "created_time", None)

                            # Default hostname
                            default_hostname = getattr(site, "default_hostname", None)

                            # Custom domains
                            custom_domains = []
                            try:
                                domains_list = web_client.static_sites.list_static_site_custom_domains(rg_name, site_name)
                                custom_domains = [d.name for d in domains_list]
                            except Exception:
                                pass

                            # Repository info
                            repository_url = getattr(site, "repository_url", None)
                            branch = getattr(site, "branch", None)

                            # Tags
                            tags = getattr(site, "tags", {}) or {}

                            # Calculate age
                            age_days = 0
                            if created_time:
                                age_days = (datetime.now(timezone.utc) - created_time).days

                            # Estimate bandwidth usage (no direct metric)
                            # Assume Standard tier = high traffic, Free tier = low traffic
                            estimated_monthly_bandwidth_gb = 50 if sku_tier == "Standard" else 10

                            # Estimate monthly cost
                            monthly_cost = self._estimate_static_web_app_cost(
                                sku_tier=sku_tier,
                                estimated_monthly_bandwidth_gb=estimated_monthly_bandwidth_gb
                            )

                            # Check optimization opportunities
                            is_optimizable, optimization_score, priority, potential_savings, recommendations = (
                                self._calculate_static_web_app_optimization(
                                    sku_tier=sku_tier,
                                    age_days=age_days,
                                    estimated_monthly_bandwidth_gb=estimated_monthly_bandwidth_gb,
                                    custom_domains_count=len(custom_domains),
                                    repository_url=repository_url,
                                    monthly_cost=monthly_cost,
                                )
                            )

                            # Build metadata
                            metadata = {
                                "site_id": site_id,
                                "site_name": site_name,
                                "sku_tier": sku_tier,
                                "location": site_location,
                                "resource_group": rg_name,
                                "age_days": age_days,
                                "default_hostname": default_hostname,
                                "custom_domains": custom_domains,
                                "repository_url": repository_url,
                                "branch": branch,
                                "estimated_monthly_bandwidth_gb": estimated_monthly_bandwidth_gb,
                                "tags": tags,
                            }

                            resource = AllCloudResourceData(
                                resource_id=site_id,
                                resource_type="azure_static_web_app",
                                resource_name=site_name,
                                region=site_location,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=metadata,
                                created_at_cloud=created_time,
                                is_orphan=False,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                            )

                            resources.append(resource)
                            logger.info(
                                f"Found Static Web App: {site_name} in {site_location} "
                                f"(SKU: {sku_tier}, Optimizable: {is_optimizable}, Score: {optimization_score})"
                            )

                        except Exception as e:
                            logger.error(f"Error processing Static Web App {getattr(site, 'name', 'unknown')}: {e}")
                            continue

                except Exception as e:
                    logger.error(f"Error listing Static Web Apps in resource group {rg_name}: {e}")
                    continue

        except Exception as e:
            logger.error(f"Error scanning Azure Static Web Apps: {e}")

        logger.info(f"Completed Azure Static Web Apps scan in region {region}: {len(resources)} sites found")
        return resources

    def _estimate_static_web_app_cost(
        self,
        sku_tier: str,
        estimated_monthly_bandwidth_gb: int,
    ) -> float:
        """
        Estimate monthly cost for Azure Static Web Apps.

        Pricing:
        - Free tier: $0 (100GB bandwidth, 250MB storage included)
        - Standard tier: $9/month + $0.20/GB bandwidth
        """
        if sku_tier.lower() == "free":
            # Free tier - no cost unless exceeding limits
            if estimated_monthly_bandwidth_gb > 100:
                # Overage charges (rare)
                overage_gb = estimated_monthly_bandwidth_gb - 100
                return round(overage_gb * 0.20, 2)
            return 0.0

        # Standard tier
        base_cost = 9.0  # $9/month
        bandwidth_cost = estimated_monthly_bandwidth_gb * 0.20
        return round(base_cost + bandwidth_cost, 2)

    def _calculate_static_web_app_optimization(
        self,
        sku_tier: str,
        age_days: int,
        estimated_monthly_bandwidth_gb: int,
        custom_domains_count: int,
        repository_url: str,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Azure Static Web Apps.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Standard tier with no traffic for 90+ days
        if sku_tier.lower() == "standard" and estimated_monthly_bandwidth_gb < 1 and age_days >= 90:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost  # Full savings by downgrading or deleting
            recommendations.append(
                f"Standard tier Static Web App has no traffic for {age_days} days. "
                "Downgrade to Free tier or delete if unused. "
                f"Potential savings: ${potential_savings:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Standard tier with very low traffic (<5GB/month)
        if sku_tier.lower() == "standard" and estimated_monthly_bandwidth_gb < 5:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            # Free tier would cost $0, Standard costs $9 + bandwidth
            potential_savings = 9.0  # Base cost savings
            recommendations.append(
                f"Standard tier Static Web App has very low traffic ({estimated_monthly_bandwidth_gb}GB/month). "
                "Free tier includes 100GB bandwidth. "
                f"Downgrade to Free tier to save ${potential_savings:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Free tier approaching bandwidth limit (>80GB/month)
        if sku_tier.lower() == "free" and estimated_monthly_bandwidth_gb > 80:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = 0.0  # No savings, but avoiding overage charges
            recommendations.append(
                f"Free tier Static Web App is approaching bandwidth limit ({estimated_monthly_bandwidth_gb}GB/100GB). "
                "Upgrade to Standard tier or optimize bandwidth usage (CDN, compression, caching) "
                "to avoid overage charges."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - No custom domain configured (best practice)
        if custom_domains_count == 0 and age_days > 30:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            potential_savings = 0.0
            recommendations.append(
                "Static Web App has no custom domain configured. "
                "Configure a custom domain for production apps to improve branding and SEO."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - No monitoring/alerting configured
        if not is_optimizable:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = 0.0
            recommendations.append(
                "LOW: Configure monitoring and alerting for Static Web App. "
                "Enable Application Insights to track performance, errors, and user behavior."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_dedicated_hsms(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Dedicated HSM instances for cost intelligence.

        Pricing (Azure Dedicated HSM):
        - Standard HSM: ~$4,500/mois (bare metal appliance)
        - High Performance HSM: ~$8,000/mois

        Typical cost: $4,500-8,000/month (VERY EXPENSIVE)

        Returns:
            List of AllCloudResourceData (is_orphan=False, is_optimizable=True)
        """
        try:
            from azure.mgmt.hardwaresecuritymodules import AzureDedicatedHSMResourceProvider
        except ImportError:
            logger.warning("azure-mgmt-hardwaresecuritymodules not installed, skipping Dedicated HSM scan")
            return []

        resources = []

        try:
            credential = self._get_azure_credential()
            hsm_client = AzureDedicatedHSMResourceProvider(credential, self.subscription_id)

            # Iterate over resource groups
            for rg_name in self.resource_groups:
                try:
                    # List Dedicated HSMs in resource group
                    hsms = hsm_client.dedicated_hsm.list_by_resource_group(rg_name)

                    for hsm in hsms:
                        try:
                            # Extract location
                            hsm_location = getattr(hsm, "location", "unknown").lower()

                            # Filter by region if not global
                            if region.lower() != "global" and hsm_location != region.lower():
                                continue

                            # Extract metadata
                            hsm_id = getattr(hsm, "id", "")
                            hsm_name = getattr(hsm, "name", "unknown")
                            sku_name = getattr(getattr(hsm, "sku", None), "name", "Standard")  # Standard or High Performance
                            provisioning_state = getattr(getattr(hsm, "properties", None), "provisioning_state", "Unknown")

                            # HSM properties
                            properties = getattr(hsm, "properties", None)
                            stamp_id = getattr(properties, "stamp_id", None) if properties else None
                            status_message = getattr(properties, "status_message", None) if properties else None

                            # Network profile
                            network_profile = getattr(properties, "network_profile", None) if properties else None
                            subnet_id = getattr(network_profile, "subnet", {}).get("id", None) if network_profile else None

                            # Tags
                            tags = getattr(hsm, "tags", {}) or {}

                            # Calculate age (estimate based on tags or assume old)
                            age_days = 90  # Default assumption for HSMs

                            # Estimate key usage (no direct metric available)
                            # Dedicated HSMs can store thousands of keys
                            estimated_keys_count = 100  # Estimate
                            estimated_keys_capacity = 10000  # Typical capacity

                            # Estimate monthly cost based on SKU
                            monthly_cost = self._estimate_dedicated_hsm_cost(sku_name=sku_name)

                            # Check optimization opportunities
                            is_optimizable, optimization_score, priority, potential_savings, recommendations = (
                                self._calculate_dedicated_hsm_optimization(
                                    hsm_name=hsm_name,
                                    sku_name=sku_name,
                                    provisioning_state=provisioning_state,
                                    age_days=age_days,
                                    estimated_keys_count=estimated_keys_count,
                                    estimated_keys_capacity=estimated_keys_capacity,
                                    status_message=status_message,
                                    monthly_cost=monthly_cost,
                                )
                            )

                            # Build metadata
                            metadata = {
                                "hsm_id": hsm_id,
                                "hsm_name": hsm_name,
                                "sku": sku_name,
                                "location": hsm_location,
                                "resource_group": rg_name,
                                "provisioning_state": provisioning_state,
                                "stamp_id": stamp_id,
                                "status_message": status_message,
                                "subnet_id": subnet_id,
                                "age_days": age_days,
                                "estimated_keys_count": estimated_keys_count,
                                "estimated_keys_capacity": estimated_keys_capacity,
                                "tags": tags,
                            }

                            resource = AllCloudResourceData(
                                resource_id=hsm_id,
                                resource_type="azure_dedicated_hsm",
                                resource_name=hsm_name,
                                region=hsm_location,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=metadata,
                                created_at_cloud=None,  # Not available in API
                                is_orphan=False,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                            )

                            resources.append(resource)
                            logger.info(
                                f"Found Dedicated HSM: {hsm_name} in {hsm_location} "
                                f"(SKU: {sku_name}, Optimizable: {is_optimizable}, Score: {optimization_score})"
                            )

                        except Exception as e:
                            logger.error(f"Error processing Dedicated HSM {getattr(hsm, 'name', 'unknown')}: {e}")
                            continue

                except Exception as e:
                    logger.error(f"Error listing Dedicated HSMs in resource group {rg_name}: {e}")
                    continue

        except Exception as e:
            logger.error(f"Error scanning Azure Dedicated HSMs: {e}")

        logger.info(f"Completed Azure Dedicated HSM scan in region {region}: {len(resources)} HSMs found")
        return resources

    def _estimate_dedicated_hsm_cost(self, sku_name: str) -> float:
        """
        Estimate monthly cost for Azure Dedicated HSM.

        Pricing:
        - Standard HSM: ~$4,500/month
        - High Performance HSM: ~$8,000/month
        """
        if "high" in sku_name.lower() or "performance" in sku_name.lower():
            return 8000.0
        return 4500.0  # Standard

    def _calculate_dedicated_hsm_optimization(
        self,
        hsm_name: str,
        sku_name: str,
        provisioning_state: str,
        age_days: int,
        estimated_keys_count: int,
        estimated_keys_capacity: int,
        status_message: str,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Azure Dedicated HSM.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - HSM provisioned but never connected for 30+ days
        if provisioning_state.lower() == "succeeded" and age_days >= 30:
            # Check if there's a status message indicating no connections
            if status_message and ("not connected" in status_message.lower() or "no activity" in status_message.lower()):
                is_optimizable = True
                optimization_score = 90
                priority = "critical"
                potential_savings = monthly_cost  # Full cost if unused
                recommendations.append(
                    f"Dedicated HSM is provisioned but never connected for {age_days} days. "
                    f"This is VERY EXPENSIVE (${monthly_cost:.2f}/month). "
                    "Delete immediately if not needed or investigate connectivity issues."
                )
                return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Utilization <10% of key capacity
        if estimated_keys_capacity > 0:
            utilization_pct = (estimated_keys_count / estimated_keys_capacity) * 100
            if utilization_pct < 10:
                is_optimizable = True
                optimization_score = 75
                priority = "high"
                potential_savings = monthly_cost * 0.7  # Partial waste
                recommendations.append(
                    f"Dedicated HSM has very low utilization ({utilization_pct:.1f}% of key capacity). "
                    f"This costs ${monthly_cost:.2f}/month. "
                    "Consider migrating to Azure Key Vault Managed HSM (much cheaper) or consolidating HSMs."
                )
                return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - No backup configured (risk of data loss)
        # Placeholder - would require checking backup settings
        if not is_optimizable:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = 0.0  # No cost savings, but critical risk
            recommendations.append(
                f"Dedicated HSM backup configuration not detected (costs ${monthly_cost:.2f}/month). "
                "Configure backup and disaster recovery to protect critical cryptographic keys. "
                "Data loss would be catastrophic."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Redundant HSM unused (DR not tested)
        if "dr" in hsm_name.lower() or "secondary" in hsm_name.lower():
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            potential_savings = 0.0
            recommendations.append(
                f"Dedicated HSM appears to be a DR/secondary instance (costs ${monthly_cost:.2f}/month). "
                "Verify DR testing is performed regularly. "
                "Untested DR is wasted money."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - No monitoring/alerting configured
        if not is_optimizable:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = 0.0
            recommendations.append(
                f"LOW: Configure monitoring and alerting for Dedicated HSM (costs ${monthly_cost:.2f}/month). "
                "Monitor key operations, connection status, and performance metrics. "
                "Set up alerts for failures and capacity limits."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_iot_hub_message_routing(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure IoT Hub message routing configurations for cost intelligence.

        This scans all IoT Hubs across all resource groups in the specified region
        and analyzes their message routing configuration for optimization opportunities.

        Pricing:
            - Free tier: Free (8,000 messages/day)
            - Basic tier: $10/month + $0.0004/1K messages
            - Standard tier: $25-200/month (S1-S3) + $0.0004/1K messages
            - Message routing: Additional $0.50/million routing operations

        Typical monthly cost: $10-500/month

        Optimization scenarios:
            1. CRITICAL (90): Custom routes configured but 0 messages routed for 90+ days
            2. HIGH (75): Fallback route activated with 100% of messages (routing misconfigured)
            3. HIGH (70): Redundant or conflicting routes (duplicate queries/endpoints)
            4. MEDIUM (50): Dead endpoints (Storage/Event Hub/Service Bus unavailable)
            5. LOW (30): No dead-letter endpoint configured (message loss risk)

        Returns:
            List of AllCloudResourceData objects with is_optimizable=True and optimization scores.
        """
        resources = []

        try:
            from azure.mgmt.iothub import IotHubClient
        except ImportError:
            logger.error("azure-mgmt-iothub not installed - cannot scan IoT Hub routing")
            return resources

        try:
            credential = DefaultAzureCredential()
            client = IotHubClient(credential, self.subscription_id)

            # Iterate through all resource groups
            for rg_name in await self._get_resource_group_names():
                try:
                    # List all IoT Hubs in resource group
                    iot_hubs = client.iot_hub_resource.list_by_resource_group(rg_name)

                    for hub in iot_hubs:
                        try:
                            # Filter by region
                            hub_region = hub.location.lower().replace(" ", "")
                            normalized_region = region.lower().replace(" ", "")
                            if hub_region != normalized_region:
                                continue

                            # Get routing configuration
                            routing_config = hub.properties.routing if hub.properties else None
                            sku = hub.sku.name if hub.sku else "unknown"
                            tier = hub.sku.tier if hub.sku else "unknown"

                            # Extract route metadata
                            custom_routes = routing_config.routes if routing_config and routing_config.routes else []
                            fallback_route_enabled = (
                                routing_config.fallback_route.is_enabled
                                if routing_config and routing_config.fallback_route
                                else False
                            )
                            endpoints = (
                                routing_config.endpoints if routing_config and routing_config.endpoints else None
                            )

                            # Count endpoints by type
                            storage_endpoints = len(endpoints.storage_containers) if endpoints and endpoints.storage_containers else 0
                            eventhub_endpoints = len(endpoints.event_hubs) if endpoints and endpoints.event_hubs else 0
                            servicebus_queue_endpoints = (
                                len(endpoints.service_bus_queues) if endpoints and endpoints.service_bus_queues else 0
                            )
                            servicebus_topic_endpoints = (
                                len(endpoints.service_bus_topics) if endpoints and endpoints.service_bus_topics else 0
                            )

                            total_custom_endpoints = (
                                storage_endpoints
                                + eventhub_endpoints
                                + servicebus_queue_endpoints
                                + servicebus_topic_endpoints
                            )

                            # Estimate message routing activity (placeholder - real implementation would query metrics)
                            # For MVP, we estimate based on tier and routes
                            estimated_monthly_messages = 0
                            if tier.lower() == "free":
                                estimated_monthly_messages = 8000 * 30  # 8K/day limit
                            elif tier.lower() == "basic":
                                estimated_monthly_messages = 100_000  # Conservative estimate
                            elif tier.lower() == "standard":
                                if "s1" in sku.lower():
                                    estimated_monthly_messages = 1_000_000
                                elif "s2" in sku.lower():
                                    estimated_monthly_messages = 10_000_000
                                elif "s3" in sku.lower():
                                    estimated_monthly_messages = 100_000_000

                            # Calculate routing efficiency
                            routes_count = len(custom_routes)
                            fallback_percentage = 100 if fallback_route_enabled and routes_count == 0 else 0
                            if routes_count > 0 and fallback_route_enabled:
                                # Estimate: if routes exist, assume 70% custom, 30% fallback
                                fallback_percentage = 30

                            # Calculate cost
                            monthly_cost = self._calculate_iot_hub_routing_cost(
                                tier, sku, estimated_monthly_messages, routes_count
                            )

                            # Calculate optimization
                            (
                                is_optimizable,
                                optimization_score,
                                priority,
                                potential_savings,
                                recommendations,
                            ) = self._calculate_iot_hub_routing_optimization(
                                tier,
                                routes_count,
                                total_custom_endpoints,
                                fallback_route_enabled,
                                fallback_percentage,
                                estimated_monthly_messages,
                                monthly_cost,
                            )

                            resource = AllCloudResourceData(
                                resource_id=hub.id,
                                resource_type="azure_iot_hub_routing",
                                resource_name=hub.name,
                                region=hub.location,
                                is_orphan=False,  # IoT Hub routing cannot be orphan
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata={
                                    "sku": sku,
                                    "tier": tier,
                                    "resource_group": rg_name,
                                    "custom_routes_count": routes_count,
                                    "fallback_route_enabled": fallback_route_enabled,
                                    "fallback_percentage": fallback_percentage,
                                    "storage_endpoints": storage_endpoints,
                                    "eventhub_endpoints": eventhub_endpoints,
                                    "servicebus_queue_endpoints": servicebus_queue_endpoints,
                                    "servicebus_topic_endpoints": servicebus_topic_endpoints,
                                    "total_custom_endpoints": total_custom_endpoints,
                                    "estimated_monthly_messages": estimated_monthly_messages,
                                },
                                last_used_at=None,
                                created_at_cloud=None,
                            )

                            resources.append(resource)
                            logger.info(
                                f"Scanned Azure IoT Hub routing: {hub.name} in {hub.location} "
                                f"(routes={routes_count}, endpoints={total_custom_endpoints}, "
                                f"optimizable={is_optimizable}, cost=${monthly_cost:.2f}/month)"
                            )

                        except Exception as e:
                            logger.error(
                                f"Error scanning IoT Hub {hub.name}: {e}",
                                exc_info=True,
                            )
                            continue

                except Exception as e:
                    logger.error(
                        f"Error listing IoT Hubs in resource group {rg_name}: {e}",
                        exc_info=True,
                    )
                    continue

        except Exception as e:
            logger.error(f"Error scanning Azure IoT Hub routing: {e}", exc_info=True)

        return resources

    def _calculate_iot_hub_routing_cost(
        self, tier: str, sku: str, monthly_messages: int, routes_count: int
    ) -> float:
        """
        Calculate monthly cost for IoT Hub message routing.

        Pricing breakdown (as of 2025):
            - Free tier: $0 (8,000 messages/day limit)
            - Basic B1: $10/month + $0.0004/1K messages
            - Standard S1: $25/month + $0.0004/1K messages (400K/day)
            - Standard S2: $250/month + $0.0004/1K messages (6M/day)
            - Standard S3: $2,500/month + $0.0004/1K messages (300M/day)
            - Message routing: $0.50/million routing operations

        Returns:
            Estimated monthly cost in USD
        """
        tier_lower = tier.lower()
        sku_lower = sku.lower()

        # Base cost by SKU
        base_cost = 0.0
        if tier_lower == "free":
            base_cost = 0.0
        elif tier_lower == "basic":
            base_cost = 10.0
        elif tier_lower == "standard":
            if "s1" in sku_lower:
                base_cost = 25.0
            elif "s2" in sku_lower:
                base_cost = 250.0
            elif "s3" in sku_lower:
                base_cost = 2500.0
            else:
                base_cost = 25.0  # Default to S1

        # Message cost ($0.0004 per 1K messages)
        message_cost = (monthly_messages / 1000) * 0.0004

        # Routing operations cost ($0.50 per million operations)
        # Each message can trigger multiple route evaluations
        routing_operations = monthly_messages * routes_count if routes_count > 0 else monthly_messages
        routing_cost = (routing_operations / 1_000_000) * 0.50

        total_cost = base_cost + message_cost + routing_cost
        return round(total_cost, 2)

    def _calculate_iot_hub_routing_optimization(
        self,
        tier: str,
        routes_count: int,
        total_custom_endpoints: int,
        fallback_route_enabled: bool,
        fallback_percentage: int,
        monthly_messages: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Azure IoT Hub message routing.

        5 optimization scenarios (ordered by severity):
            1. CRITICAL (90): Custom routes configured but 0 messages routed for 90+ days
            2. HIGH (75): Fallback route activated with 100% of messages (routing misconfigured)
            3. HIGH (70): Redundant or conflicting routes (>5 routes to same endpoint type)
            4. MEDIUM (50): Dead endpoints (custom endpoints > 0 but fallback 100%)
            5. LOW (30): No dead-letter endpoint configured (message loss risk)

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations: list[str] = []

        # Scenario 1: CRITICAL - Custom routes configured but 0 messages for 90+ days
        # (In real implementation, would check metrics for actual message counts over 90 days)
        if routes_count > 0 and monthly_messages == 0:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            # If no messages, can downgrade to Free tier
            if tier.lower() != "free":
                potential_savings = monthly_cost  # Save full cost by removing hub or downgrading
            recommendations.append(
                f"CRITICAL: IoT Hub has {routes_count} custom route(s) configured but 0 messages routed in 90+ days. "
                f"Consider deleting unused routes or removing the IoT Hub (saves ${monthly_cost:.2f}/month). "
                "Review device connectivity and message flow."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Fallback route with 100% of messages (routing misconfigured)
        if fallback_route_enabled and fallback_percentage >= 90 and routes_count > 0:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            # Routing cost wasted on ineffective rules
            potential_savings = round((monthly_messages * routes_count / 1_000_000) * 0.50, 2)
            recommendations.append(
                f"HIGH: Fallback route receives {fallback_percentage}% of messages despite {routes_count} custom route(s). "
                f"Review routing queries - they may never match (saves ${potential_savings:.2f}/month in routing costs). "
                "Fix route conditions or remove ineffective routes."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Redundant or conflicting routes
        if routes_count > 5:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Estimate 30% of routing operations are redundant
            redundant_routing_cost = round((monthly_messages * routes_count * 0.3 / 1_000_000) * 0.50, 2)
            potential_savings = redundant_routing_cost
            recommendations.append(
                f"HIGH: IoT Hub has {routes_count} custom routes (>5). "
                f"Review for redundant or conflicting routing queries (saves ${potential_savings:.2f}/month). "
                "Consolidate routes where possible to reduce routing overhead."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Dead endpoints (endpoints exist but fallback gets all messages)
        if total_custom_endpoints > 0 and fallback_route_enabled and fallback_percentage >= 70:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Endpoints configured but not receiving messages
            potential_savings = round(monthly_cost * 0.1, 2)  # 10% savings from cleanup
            recommendations.append(
                f"MEDIUM: {total_custom_endpoints} custom endpoint(s) configured but fallback route receives {fallback_percentage}% of messages. "
                f"Endpoints may be unreachable or misconfigured (saves ${potential_savings:.2f}/month). "
                "Verify endpoint connectivity (Storage/Event Hub/Service Bus)."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - No dead-letter endpoint configured
        if routes_count > 0 and not fallback_route_enabled:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = 0.0  # No direct cost savings, but prevents message loss
            recommendations.append(
                f"LOW: {routes_count} custom route(s) configured but no fallback route enabled. "
                "Messages that don't match any route will be dropped. "
                "Enable fallback route to prevent message loss and improve reliability."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_ml_online_endpoints(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure ML Online Endpoints for cost intelligence.

        This scans all Azure ML workspaces across all resource groups in the specified region
        and analyzes their online endpoints (real-time inference) for optimization opportunities.

        Pricing:
            - Basic instance (CPU): ~$50/month
            - Standard instance (CPU): ~$200/month
            - Premium instance (GPU): ~$500-1,000/month
            - Autoscaling: Additional costs based on instance count

        Typical monthly cost: $50-1,000/month

        Optimization scenarios:
            1. CRITICAL (90): Endpoint deployed but 0 inference requests for 90+ days
            2. HIGH (75): Premium GPU instance with <5% utilization (forecasting/recommendations models)
            3. HIGH (70): Multiple endpoints with same model version (consolidation opportunity)
            4. MEDIUM (50): Standard instance with <20% utilization
            5. LOW (30): No autoscaling configured (cost efficiency opportunity)

        Returns:
            List of AllCloudResourceData objects with is_optimizable=True and optimization scores.
        """
        resources = []

        try:
            from azure.ai.ml import MLClient
        except ImportError:
            logger.error("azure-ai-ml not installed - cannot scan ML Online Endpoints")
            return resources

        try:
            credential = DefaultAzureCredential()

            # Iterate through all resource groups
            for rg_name in await self._get_resource_group_names():
                try:
                    # List all ML workspaces in resource group
                    from azure.mgmt.machinelearningservices import AzureMachineLearningWorkspaces
                    ml_mgmt_client = AzureMachineLearningWorkspaces(credential, self.subscription_id)
                    workspaces = ml_mgmt_client.workspaces.list_by_resource_group(rg_name)

                    for workspace in workspaces:
                        try:
                            # Filter by region
                            workspace_region = workspace.location.lower().replace(" ", "")
                            normalized_region = region.lower().replace(" ", "")
                            if workspace_region != normalized_region:
                                continue

                            # Create ML Client for this workspace
                            ml_client = MLClient(
                                credential=credential,
                                subscription_id=self.subscription_id,
                                resource_group_name=rg_name,
                                workspace_name=workspace.name
                            )

                            # List online endpoints in workspace
                            online_endpoints = ml_client.online_endpoints.list()

                            for endpoint in online_endpoints:
                                try:
                                    # Get endpoint details
                                    endpoint_name = endpoint.name
                                    provisioning_state = endpoint.provisioning_state
                                    endpoint_type = endpoint.kind if hasattr(endpoint, 'kind') else "unknown"

                                    # Get deployments for this endpoint
                                    deployments = list(ml_client.online_deployments.list(endpoint_name=endpoint_name))
                                    deployment_count = len(deployments)

                                    # Extract instance info from first deployment (if exists)
                                    instance_type = "unknown"
                                    instance_count = 0
                                    if deployments:
                                        first_deployment = deployments[0]
                                        instance_type = first_deployment.instance_type if hasattr(first_deployment, 'instance_type') else "unknown"
                                        instance_count = first_deployment.instance_count if hasattr(first_deployment, 'instance_count') else 0

                                    # Estimate monthly messages/requests (placeholder - real implementation would query metrics)
                                    # For MVP, we estimate based on instance type
                                    estimated_monthly_requests = 0
                                    if "gpu" in instance_type.lower() or "premium" in instance_type.lower():
                                        estimated_monthly_requests = 100_000  # Premium instances expected high traffic
                                    elif "standard" in instance_type.lower():
                                        estimated_monthly_requests = 50_000
                                    else:
                                        estimated_monthly_requests = 10_000  # Basic

                                    # Calculate cost
                                    monthly_cost = self._calculate_ml_online_endpoint_cost(
                                        instance_type, instance_count
                                    )

                                    # Calculate optimization
                                    (
                                        is_optimizable,
                                        optimization_score,
                                        priority,
                                        potential_savings,
                                        recommendations,
                                    ) = self._calculate_ml_online_endpoint_optimization(
                                        instance_type,
                                        instance_count,
                                        deployment_count,
                                        estimated_monthly_requests,
                                        monthly_cost,
                                    )

                                    resource = AllCloudResourceData(
                                        resource_id=f"/subscriptions/{self.subscription_id}/resourceGroups/{rg_name}/providers/Microsoft.MachineLearningServices/workspaces/{workspace.name}/onlineEndpoints/{endpoint_name}",
                                        resource_type="azure_ml_online_endpoint",
                                        resource_name=endpoint_name,
                                        region=workspace.location,
                                        is_orphan=False,  # ML endpoints cannot be orphan
                                        is_optimizable=is_optimizable,
                                        optimization_score=optimization_score,
                                        optimization_priority=priority,
                                        potential_monthly_savings=potential_savings,
                                        optimization_recommendations=recommendations,
                                        estimated_monthly_cost=monthly_cost,
                                        currency="USD",
                                        resource_metadata={
                                            "workspace_name": workspace.name,
                                            "resource_group": rg_name,
                                            "provisioning_state": provisioning_state,
                                            "endpoint_type": endpoint_type,
                                            "instance_type": instance_type,
                                            "instance_count": instance_count,
                                            "deployment_count": deployment_count,
                                            "estimated_monthly_requests": estimated_monthly_requests,
                                        },
                                        last_used_at=None,
                                        created_at_cloud=None,
                                    )

                                    resources.append(resource)
                                    logger.info(
                                        f"Scanned Azure ML Online Endpoint: {endpoint_name} in workspace {workspace.name} "
                                        f"(instance={instance_type}, count={instance_count}, optimizable={is_optimizable}, cost=${monthly_cost:.2f}/month)"
                                    )

                                except Exception as e:
                                    logger.error(
                                        f"Error scanning ML Online Endpoint {endpoint.name}: {e}",
                                        exc_info=True,
                                    )
                                    continue

                        except Exception as e:
                            logger.error(
                                f"Error scanning ML workspace {workspace.name}: {e}",
                                exc_info=True,
                            )
                            continue

                except Exception as e:
                    logger.error(
                        f"Error listing ML workspaces in resource group {rg_name}: {e}",
                        exc_info=True,
                    )
                    continue

        except Exception as e:
            logger.error(f"Error scanning Azure ML Online Endpoints: {e}", exc_info=True)

        return resources

    def _calculate_ml_online_endpoint_cost(
        self, instance_type: str, instance_count: int
    ) -> float:
        """
        Calculate monthly cost for Azure ML Online Endpoint.

        Pricing breakdown (as of 2025):
            - Basic CPU instances: ~$50/month per instance
            - Standard CPU instances: ~$200/month per instance
            - Premium GPU instances: ~$500-1,000/month per instance
            - Data transfer: $0.087/GB (typically negligible)

        Returns:
            Estimated monthly cost in USD
        """
        instance_type_lower = instance_type.lower()

        # Base cost per instance per month
        cost_per_instance = 50.0  # Default to Basic

        if "gpu" in instance_type_lower or "premium" in instance_type_lower:
            cost_per_instance = 750.0  # Premium GPU average
        elif "standard" in instance_type_lower:
            cost_per_instance = 200.0  # Standard CPU
        elif "basic" in instance_type_lower or "cpu" in instance_type_lower:
            cost_per_instance = 50.0  # Basic CPU

        total_cost = cost_per_instance * max(instance_count, 1)
        return round(total_cost, 2)

    def _calculate_ml_online_endpoint_optimization(
        self,
        instance_type: str,
        instance_count: int,
        deployment_count: int,
        monthly_requests: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Azure ML Online Endpoints.

        5 optimization scenarios (ordered by severity):
            1. CRITICAL (90): Endpoint deployed but 0 inference requests for 90+ days
            2. HIGH (75): Premium GPU instance with <5% utilization (forecasting/recommendations)
            3. HIGH (70): Multiple deployments with same model (consolidation opportunity)
            4. MEDIUM (50): Standard instance with <20% utilization
            5. LOW (30): No autoscaling configured (cost efficiency)

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations: list[str] = []

        instance_type_lower = instance_type.lower()
        is_gpu = "gpu" in instance_type_lower or "premium" in instance_type_lower

        # Scenario 1: CRITICAL - Endpoint deployed but 0 requests for 90+ days
        if monthly_requests == 0:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost  # Save full cost by deleting endpoint
            recommendations.append(
                f"CRITICAL: ML Online Endpoint deployed but 0 inference requests for 90+ days (costs ${monthly_cost:.2f}/month). "
                f"Consider deleting the endpoint or putting it in standby mode. "
                "Review model usage and deployment necessity."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Premium GPU instance with <5% utilization
        if is_gpu and monthly_requests < 5_000:  # <5% of expected 100K
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            # Downgrade to Standard CPU saves ~75% cost
            potential_savings = round(monthly_cost * 0.75, 2)
            recommendations.append(
                f"HIGH: Premium GPU instance with very low utilization ({monthly_requests:,} requests/month, <5%). "
                f"Downgrade to Standard CPU instance (saves ${potential_savings:.2f}/month). "
                "GPU recommended only for complex deep learning models (forecasting, recommendations, image processing)."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Multiple deployments (consolidation opportunity)
        if deployment_count > 3:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Consolidate deployments saves ~50% cost
            potential_savings = round(monthly_cost * 0.5, 2)
            recommendations.append(
                f"HIGH: Endpoint has {deployment_count} deployments. "
                f"Consider consolidating similar model versions (saves ${potential_savings:.2f}/month). "
                "Use blue-green deployment strategy instead of keeping multiple versions live."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Standard instance with <20% utilization
        if "standard" in instance_type_lower and monthly_requests < 10_000:  # <20% of expected 50K
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Downgrade to Basic saves ~75% cost
            potential_savings = round(monthly_cost * 0.75, 2)
            recommendations.append(
                f"MEDIUM: Standard instance with low utilization ({monthly_requests:,} requests/month, <20%). "
                f"Downgrade to Basic instance (saves ${potential_savings:.2f}/month). "
                "Review traffic patterns and adjust instance type accordingly."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - No autoscaling configured (single instance)
        if instance_count == 1:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = round(monthly_cost * 0.2, 2)  # 20% savings from autoscaling
            recommendations.append(
                f"LOW: Single instance deployment without autoscaling (costs ${monthly_cost:.2f}/month). "
                f"Enable autoscaling to optimize costs during low-traffic periods (saves ${potential_savings:.2f}/month). "
                "Configure min=0 instances to scale down to zero when idle."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_ml_batch_endpoints(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure ML Batch Endpoints for cost intelligence.

        This scans all Azure ML workspaces across all resource groups in the specified region
        and analyzes their batch endpoints (batch scoring/inference) for optimization opportunities.

        Pricing:
            - Compute cluster: $0.10-2.00/hour per node (depends on VM size)
            - Storage: ~$10-50/month for batch outputs
            - Data transfer: $0.087/GB

        Typical monthly cost: $10-500/month (pay-per-use model)

        Optimization scenarios:
            1. CRITICAL (90): Batch endpoint deployed but 0 batch jobs for 90+ days
            2. HIGH (75): Compute cluster running 24/7 instead of on-demand
            3. HIGH (70): Oversized cluster (too many nodes for workload)
            4. MEDIUM (50): No job scheduling optimization (peak hour waste)
            5. LOW (30): Missing cost allocation tags

        Returns:
            List of AllCloudResourceData objects with is_optimizable=True and optimization scores.
        """
        resources = []

        try:
            from azure.ai.ml import MLClient
        except ImportError:
            logger.error("azure-ai-ml not installed - cannot scan ML Batch Endpoints")
            return resources

        try:
            credential = DefaultAzureCredential()

            # Iterate through all resource groups
            for rg_name in await self._get_resource_group_names():
                try:
                    # List all ML workspaces in resource group
                    from azure.mgmt.machinelearningservices import AzureMachineLearningWorkspaces
                    ml_mgmt_client = AzureMachineLearningWorkspaces(credential, self.subscription_id)
                    workspaces = ml_mgmt_client.workspaces.list_by_resource_group(rg_name)

                    for workspace in workspaces:
                        try:
                            # Filter by region
                            workspace_region = workspace.location.lower().replace(" ", "")
                            normalized_region = region.lower().replace(" ", "")
                            if workspace_region != normalized_region:
                                continue

                            # Create ML Client for this workspace
                            ml_client = MLClient(
                                credential=credential,
                                subscription_id=self.subscription_id,
                                resource_group_name=rg_name,
                                workspace_name=workspace.name
                            )

                            # List batch endpoints in workspace
                            batch_endpoints = ml_client.batch_endpoints.list()

                            for endpoint in batch_endpoints:
                                try:
                                    # Get endpoint details
                                    endpoint_name = endpoint.name
                                    provisioning_state = endpoint.provisioning_state

                                    # Get deployments for this endpoint
                                    deployments = list(ml_client.batch_deployments.list(endpoint_name=endpoint_name))
                                    deployment_count = len(deployments)

                                    # Extract compute info from first deployment (if exists)
                                    compute_type = "unknown"
                                    instance_count = 0
                                    if deployments:
                                        first_deployment = deployments[0]
                                        compute_name = first_deployment.compute if hasattr(first_deployment, 'compute') else None
                                        if compute_name:
                                            # Try to get compute cluster details
                                            try:
                                                compute = ml_client.compute.get(compute_name)
                                                compute_type = compute.type if hasattr(compute, 'type') else "unknown"
                                                if hasattr(compute, 'size'):
                                                    instance_count = compute.size.max_instances if hasattr(compute.size, 'max_instances') else 1
                                            except:
                                                pass

                                    # Estimate monthly batch jobs (placeholder - real implementation would query job history)
                                    # For MVP, assume some baseline activity
                                    estimated_monthly_jobs = 10  # Conservative estimate

                                    # Check if cluster is always running (critical waste)
                                    is_always_running = False
                                    if compute_type.lower() == "amlcompute":
                                        # AMLCompute clusters can be set to always-on or auto-scale to 0
                                        is_always_running = True  # Assume worst case for MVP

                                    # Calculate cost
                                    monthly_cost = self._calculate_ml_batch_endpoint_cost(
                                        compute_type, instance_count, is_always_running
                                    )

                                    # Calculate optimization
                                    (
                                        is_optimizable,
                                        optimization_score,
                                        priority,
                                        potential_savings,
                                        recommendations,
                                    ) = self._calculate_ml_batch_endpoint_optimization(
                                        compute_type,
                                        instance_count,
                                        deployment_count,
                                        estimated_monthly_jobs,
                                        is_always_running,
                                        monthly_cost,
                                    )

                                    resource = AllCloudResourceData(
                                        resource_id=f"/subscriptions/{self.subscription_id}/resourceGroups/{rg_name}/providers/Microsoft.MachineLearningServices/workspaces/{workspace.name}/batchEndpoints/{endpoint_name}",
                                        resource_type="azure_ml_batch_endpoint",
                                        resource_name=endpoint_name,
                                        region=workspace.location,
                                        is_orphan=False,  # ML batch endpoints cannot be orphan
                                        is_optimizable=is_optimizable,
                                        optimization_score=optimization_score,
                                        optimization_priority=priority,
                                        potential_monthly_savings=potential_savings,
                                        optimization_recommendations=recommendations,
                                        estimated_monthly_cost=monthly_cost,
                                        currency="USD",
                                        resource_metadata={
                                            "workspace_name": workspace.name,
                                            "resource_group": rg_name,
                                            "provisioning_state": provisioning_state,
                                            "compute_type": compute_type,
                                            "instance_count": instance_count,
                                            "deployment_count": deployment_count,
                                            "estimated_monthly_jobs": estimated_monthly_jobs,
                                            "is_always_running": is_always_running,
                                        },
                                        last_used_at=None,
                                        created_at_cloud=None,
                                    )

                                    resources.append(resource)
                                    logger.info(
                                        f"Scanned Azure ML Batch Endpoint: {endpoint_name} in workspace {workspace.name} "
                                        f"(compute={compute_type}, nodes={instance_count}, optimizable={is_optimizable}, cost=${monthly_cost:.2f}/month)"
                                    )

                                except Exception as e:
                                    logger.error(
                                        f"Error scanning ML Batch Endpoint {endpoint.name}: {e}",
                                        exc_info=True,
                                    )
                                    continue

                        except Exception as e:
                            logger.error(
                                f"Error scanning ML workspace {workspace.name}: {e}",
                                exc_info=True,
                            )
                            continue

                except Exception as e:
                    logger.error(
                        f"Error listing ML workspaces in resource group {rg_name}: {e}",
                        exc_info=True,
                    )
                    continue

        except Exception as e:
            logger.error(f"Error scanning Azure ML Batch Endpoints: {e}", exc_info=True)

        return resources

    def _calculate_ml_batch_endpoint_cost(
        self, compute_type: str, instance_count: int, is_always_running: bool
    ) -> float:
        """
        Calculate monthly cost for Azure ML Batch Endpoint.

        Pricing breakdown (as of 2025):
            - Standard D4s v3 (always running): ~$200/month per node
            - Standard D4s v3 (on-demand, 50 hours/month): ~$25/month per node
            - Storage for batch outputs: ~$10/month

        Returns:
            Estimated monthly cost in USD
        """
        compute_type_lower = compute_type.lower()

        if is_always_running:
            # Cluster running 24/7 - very expensive
            cost_per_node_monthly = 200.0  # Average for Standard D4s
            compute_cost = cost_per_node_monthly * max(instance_count, 1)
        else:
            # On-demand usage - assume 50 hours/month average
            cost_per_hour = 0.50  # Average for Standard D4s
            hours_per_month = 50
            compute_cost = cost_per_hour * hours_per_month * max(instance_count, 1)

        storage_cost = 10.0  # Batch output storage
        total_cost = compute_cost + storage_cost
        return round(total_cost, 2)

    def _calculate_ml_batch_endpoint_optimization(
        self,
        compute_type: str,
        instance_count: int,
        deployment_count: int,
        monthly_jobs: int,
        is_always_running: bool,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Azure ML Batch Endpoints.

        5 optimization scenarios (ordered by severity):
            1. CRITICAL (90): Batch endpoint deployed but 0 batch jobs for 90+ days
            2. HIGH (75): Compute cluster running 24/7 instead of on-demand
            3. HIGH (70): Oversized cluster (too many nodes for workload)
            4. MEDIUM (50): No job scheduling optimization
            5. LOW (30): Missing cost allocation tags

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations: list[str] = []

        # Scenario 1: CRITICAL - Endpoint deployed but 0 batch jobs for 90+ days
        if monthly_jobs == 0:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost  # Save full cost by deleting endpoint
            recommendations.append(
                f"CRITICAL: ML Batch Endpoint deployed but 0 batch jobs executed for 90+ days (costs ${monthly_cost:.2f}/month). "
                f"Consider deleting the endpoint or archiving unused batch models. "
                "Review batch scoring requirements and model lifecycle."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Cluster running 24/7 instead of on-demand
        if is_always_running:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            # Switch to on-demand saves ~75% cost
            potential_savings = round(monthly_cost * 0.75, 2)
            recommendations.append(
                f"HIGH: Compute cluster running 24/7 instead of on-demand (costs ${monthly_cost:.2f}/month). "
                f"Configure auto-scale to 0 nodes when idle (saves ${potential_savings:.2f}/month). "
                "Batch endpoints should only spin up compute when jobs are submitted."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Oversized cluster (too many nodes for workload)
        if instance_count > 10 and monthly_jobs < 5:  # High node count, low job frequency
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Reduce cluster size saves ~50% cost
            potential_savings = round(monthly_cost * 0.5, 2)
            recommendations.append(
                f"HIGH: Oversized cluster ({instance_count} nodes) for low workload ({monthly_jobs} jobs/month). "
                f"Reduce max cluster size to 3-5 nodes (saves ${potential_savings:.2f}/month). "
                "Right-size your cluster based on actual batch job requirements."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - No job scheduling optimization
        if monthly_jobs > 20:  # High job frequency
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Better scheduling saves ~20% cost
            potential_savings = round(monthly_cost * 0.2, 2)
            recommendations.append(
                f"MEDIUM: High batch job frequency ({monthly_jobs} jobs/month) without scheduling optimization. "
                f"Implement job batching and off-peak scheduling (saves ${potential_savings:.2f}/month). "
                "Schedule non-urgent jobs during low-cost hours to optimize compute usage."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - Missing cost allocation tags
        is_optimizable = True
        optimization_score = 30
        priority = "low"
        potential_savings = 0.0  # No direct savings, but improves cost tracking
        recommendations.append(
            f"LOW: Batch endpoint lacks cost allocation tags (costs ${monthly_cost:.2f}/month). "
            "Add tags for project, team, and environment to improve cost attribution. "
            "Proper tagging enables chargeback and budget tracking."
        )
        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_automation_accounts(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Automation Accounts for cost intelligence.

        This scans all Automation Accounts across all resource groups in the specified region
        and analyzes their runbooks, jobs, and schedules for optimization opportunities.

        Pricing:
            - Process automation: $0.002/minute runtime
            - Update management: $5/month per managed machine
            - Configuration management: $6/month per node
            - Storage: ~$1/month for job logs

        Typical monthly cost: $5-100/month

        Optimization scenarios:
            1. CRITICAL (90): Account active but 0 runbook executions for 90+ days
            2. HIGH (75): Scheduled runbooks disabled for 60+ days
            3. HIGH (70): Multiple accounts in same region (consolidation opportunity)
            4. MEDIUM (50): Runbooks with high failure rate (>30% wasted executions)
            5. LOW (30): No job monitoring configured

        Returns:
            List of AllCloudResourceData objects with is_optimizable=True and optimization scores.
        """
        resources = []

        try:
            from azure.mgmt.automation import AutomationClient
        except ImportError:
            logger.error("azure-mgmt-automation not installed - cannot scan Automation Accounts")
            return resources

        try:
            credential = DefaultAzureCredential()
            automation_client = AutomationClient(credential, self.subscription_id)

            # Iterate through all resource groups
            for rg_name in await self._get_resource_group_names():
                try:
                    # List all Automation Accounts in resource group
                    accounts = automation_client.automation_account.list_by_resource_group(rg_name)

                    for account in accounts:
                        try:
                            # Filter by region
                            account_region = account.location.lower().replace(" ", "")
                            normalized_region = region.lower().replace(" ", "")
                            if account_region != normalized_region:
                                continue

                            account_name = account.name
                            state = account.state if hasattr(account, 'state') else "unknown"

                            # Get runbooks count
                            runbooks = list(automation_client.runbook.list_by_automation_account(rg_name, account_name))
                            runbook_count = len(runbooks)

                            # Count enabled/disabled runbooks
                            enabled_runbooks = sum(1 for rb in runbooks if hasattr(rb, 'state') and rb.state.lower() == 'published')
                            disabled_runbooks = runbook_count - enabled_runbooks

                            # Get schedules count
                            try:
                                schedules = list(automation_client.schedule.list_by_automation_account(rg_name, account_name))
                                schedule_count = len(schedules)
                                disabled_schedules = sum(1 for s in schedules if hasattr(s, 'is_enabled') and not s.is_enabled)
                            except:
                                schedule_count = 0
                                disabled_schedules = 0

                            # Estimate monthly executions (placeholder - real implementation would query job history)
                            # For MVP, assume some baseline activity
                            estimated_monthly_executions = enabled_runbooks * 30  # 1 execution per day per runbook

                            # Estimate failure rate
                            failure_rate_percent = 0  # Placeholder

                            # Calculate cost
                            monthly_cost = self._calculate_automation_account_cost(
                                runbook_count, estimated_monthly_executions
                            )

                            # Calculate optimization
                            (
                                is_optimizable,
                                optimization_score,
                                priority,
                                potential_savings,
                                recommendations,
                            ) = self._calculate_automation_account_optimization(
                                runbook_count,
                                enabled_runbooks,
                                disabled_runbooks,
                                schedule_count,
                                disabled_schedules,
                                estimated_monthly_executions,
                                failure_rate_percent,
                                monthly_cost,
                            )

                            resource = AllCloudResourceData(
                                resource_id=account.id,
                                resource_type="azure_automation_account",
                                resource_name=account_name,
                                region=account.location,
                                is_orphan=False,  # Automation accounts cannot be orphan
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata={
                                    "resource_group": rg_name,
                                    "state": state,
                                    "runbook_count": runbook_count,
                                    "enabled_runbooks": enabled_runbooks,
                                    "disabled_runbooks": disabled_runbooks,
                                    "schedule_count": schedule_count,
                                    "disabled_schedules": disabled_schedules,
                                    "estimated_monthly_executions": estimated_monthly_executions,
                                    "failure_rate_percent": failure_rate_percent,
                                },
                                last_used_at=None,
                                created_at_cloud=None,
                            )

                            resources.append(resource)
                            logger.info(
                                f"Scanned Azure Automation Account: {account_name} in {account.location} "
                                f"(runbooks={runbook_count}, schedules={schedule_count}, optimizable={is_optimizable}, cost=${monthly_cost:.2f}/month)"
                            )

                        except Exception as e:
                            logger.error(
                                f"Error scanning Automation Account {account.name}: {e}",
                                exc_info=True,
                            )
                            continue

                except Exception as e:
                    logger.error(
                        f"Error listing Automation Accounts in resource group {rg_name}: {e}",
                        exc_info=True,
                    )
                    continue

        except Exception as e:
            logger.error(f"Error scanning Azure Automation Accounts: {e}", exc_info=True)

        return resources

    def _calculate_automation_account_cost(
        self, runbook_count: int, monthly_executions: int
    ) -> float:
        """
        Calculate monthly cost for Azure Automation Account.

        Pricing breakdown (as of 2025):
            - Process automation: $0.002/minute runtime
            - Assume average runbook runtime: 5 minutes
            - Storage: ~$1/month for job logs
            - Update management: $5/month (if used)

        Returns:
            Estimated monthly cost in USD
        """
        # Process automation cost
        avg_runtime_minutes = 5.0
        cost_per_minute = 0.002
        execution_cost = monthly_executions * avg_runtime_minutes * cost_per_minute

        # Storage cost for job logs
        storage_cost = 1.0

        # Base update management cost (if any runbooks exist)
        update_mgmt_cost = 5.0 if runbook_count > 0 else 0.0

        total_cost = execution_cost + storage_cost + update_mgmt_cost
        return round(total_cost, 2)

    def _calculate_automation_account_optimization(
        self,
        runbook_count: int,
        enabled_runbooks: int,
        disabled_runbooks: int,
        schedule_count: int,
        disabled_schedules: int,
        monthly_executions: int,
        failure_rate_percent: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization opportunities for Azure Automation Accounts.

        5 optimization scenarios (ordered by severity):
            1. CRITICAL (90): Account active but 0 runbook executions for 90+ days
            2. HIGH (75): Scheduled runbooks disabled for 60+ days
            3. HIGH (70): Multiple accounts in same region (consolidation opportunity)
            4. MEDIUM (50): Runbooks with high failure rate (>30%)
            5. LOW (30): No job monitoring configured

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations: list[str] = []

        # Scenario 1: CRITICAL - Account active but 0 runbook executions for 90+ days
        if monthly_executions == 0 and runbook_count > 0:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost  # Save full cost by deleting account
            recommendations.append(
                f"CRITICAL: Automation Account has {runbook_count} runbook(s) but 0 executions for 90+ days (costs ${monthly_cost:.2f}/month). "
                f"Consider deleting the account or archiving unused runbooks. "
                "Review automation requirements and runbook lifecycle."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Scheduled runbooks disabled for 60+ days
        if disabled_schedules > 5:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            # Cleanup saves ~50% cost
            potential_savings = round(monthly_cost * 0.5, 2)
            recommendations.append(
                f"HIGH: {disabled_schedules} schedule(s) disabled for 60+ days (costs ${monthly_cost:.2f}/month). "
                f"Delete unused schedules and associated runbooks (saves ${potential_savings:.2f}/month). "
                "Clean up deprecated automation workflows."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Too many disabled runbooks (consolidation opportunity)
        if disabled_runbooks > enabled_runbooks and disabled_runbooks > 5:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Consolidate accounts saves ~30% cost
            potential_savings = round(monthly_cost * 0.3, 2)
            recommendations.append(
                f"HIGH: {disabled_runbooks} disabled runbooks vs {enabled_runbooks} enabled. "
                f"Archive or delete unused runbooks (saves ${potential_savings:.2f}/month). "
                "Consolidate active automation workflows into fewer accounts."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - High failure rate (wasted executions)
        if failure_rate_percent > 30:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            # Fix failures saves ~30% cost
            potential_savings = round(monthly_cost * 0.3, 2)
            recommendations.append(
                f"MEDIUM: High runbook failure rate ({failure_rate_percent}% failures). "
                f"Fix failing runbooks to reduce wasted executions (saves ${potential_savings:.2f}/month). "
                "Review error logs and improve runbook error handling."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - No job monitoring configured
        if schedule_count > 0:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = 0.0  # No direct savings, but improves reliability
            recommendations.append(
                f"LOW: {schedule_count} schedule(s) without job monitoring alerts (costs ${monthly_cost:.2f}/month). "
                "Configure Azure Monitor alerts for job failures and long-running jobs. "
                "Proactive monitoring prevents waste from failed automation workflows."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_advisor_recommendations(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Advisor cost optimization recommendations for cost intelligence.

        Azure Advisor is a FREE native Azure service that provides personalized recommendations
        to optimize Azure deployments (cost, security, reliability, performance).

        Pricing: Azure Advisor is FREE (no additional cost)

        Detection Logic:
        - Scans Advisor recommendations with category="Cost" (subscription-wide, not region-specific)
        - Groups recommendations by impacted resource ID
        - Analyzes recommendation impact level and age (how long ignored)
        - Recommendations marked as optimizable if not acted upon

        Waste Detection (5 scenarios):
        1. CRITICAL (90): Critical-impact recommendation ignored for 90+ days (potential high waste)
        2. HIGH (75): High-impact recommendation not acted on for 30+ days (significant waste)
        3. HIGH (70): Multiple recommendations for same resource (indicates systemic issues)
        4. MEDIUM (50): Medium-impact recommendation ignored for 60+ days (moderate waste)
        5. LOW (30): Low-impact recommendation not reviewed for 90+ days (minor waste)

        Returns:
            List of ALL Advisor cost recommendations as AllCloudResourceData objects
        """
        logger.info("inventory.scanning_advisor_recommendations", region=region)

        try:
            from azure.mgmt.advisor import AdvisorManagementClient
            from datetime import datetime, timezone

            advisor_client = AdvisorManagementClient(
                credential=self.credentials,
                subscription_id=self.subscription_id
            )

            all_recommendations: list[AllCloudResourceData] = []

            # List all cost optimization recommendations (subscription-wide)
            # Note: Advisor recommendations are not region-specific, they apply to entire subscription
            recommendations_list = list(
                advisor_client.recommendations.list(filter="Category eq 'Cost'")
            )

            logger.info(
                "inventory.advisor_recommendations_found",
                region=region,
                recommendation_count=len(recommendations_list)
            )

            for rec in recommendations_list:
                try:
                    # Extract recommendation properties
                    recommendation_id = rec.name or "unknown"
                    recommendation_type = rec.recommendation_type_id or "unknown"
                    impacted_resource_id = rec.impacted_value or "unknown"
                    impacted_resource_type = rec.impacted_field or "unknown"
                    short_description = rec.short_description.problem if rec.short_description else "No description"
                    impact_level = rec.impact.lower() if rec.impact else "low"  # High, Medium, Low
                    last_updated = rec.last_updated

                    # Calculate age of recommendation (how long it's been ignored)
                    if last_updated:
                        age_days = (datetime.now(timezone.utc) - last_updated).days
                    else:
                        age_days = 0

                    # Estimate potential savings from extended_properties
                    potential_savings_amount = 0.0
                    currency = "USD"
                    if rec.extended_properties and "annualSavingsAmount" in rec.extended_properties:
                        try:
                            potential_savings_amount = float(rec.extended_properties["annualSavingsAmount"])
                            currency = rec.extended_properties.get("savingsCurrency", "USD")
                        except (ValueError, TypeError):
                            pass

                    # Monthly savings (annualSavingsAmount / 12)
                    monthly_savings = potential_savings_amount / 12.0 if potential_savings_amount > 0 else 10.0

                    # Determine optimization status
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        optimization_savings,
                        recommendations_list_output,
                    ) = self._calculate_advisor_recommendation_optimization(
                        impact_level=impact_level,
                        age_days=age_days,
                        monthly_savings=monthly_savings,
                        short_description=short_description,
                    )

                    # Create AllCloudResourceData
                    resource_data = AllCloudResourceData(
                        resource_id=recommendation_id,
                        resource_type="azure_advisor_recommendation",
                        resource_name=f"Advisor: {short_description[:50]}...",
                        region=region,  # Not region-specific, but required field
                        estimated_monthly_cost=monthly_savings,
                        currency=currency,
                        is_orphan=False,  # Advisor recommendations are not orphans
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_priority=priority,
                        potential_monthly_savings=optimization_savings,
                        optimization_recommendations=recommendations_list_output,
                        resource_metadata={
                            "recommendation_id": recommendation_id,
                            "recommendation_type": recommendation_type,
                            "impacted_resource_id": impacted_resource_id,
                            "impacted_resource_type": impacted_resource_type,
                            "short_description": short_description,
                            "impact_level": impact_level,
                            "age_days": age_days,
                            "last_updated": last_updated.isoformat() if last_updated else None,
                            "annual_savings_amount": potential_savings_amount,
                        },
                    )
                    all_recommendations.append(resource_data)

                except Exception as e:
                    logger.error(
                        "inventory.advisor_recommendation_processing_error",
                        recommendation_id=getattr(rec, "name", "unknown"),
                        error=str(e),
                    )
                    continue

            logger.info(
                "inventory.advisor_recommendations_scan_complete",
                region=region,
                total_recommendations=len(all_recommendations),
                optimizable_count=sum(1 for r in all_recommendations if r.is_optimizable),
            )

            return all_recommendations

        except ImportError:
            logger.error(
                "inventory.missing_advisor_sdk",
                region=region,
                error="azure-mgmt-advisor package not installed",
            )
            return []
        except Exception as e:
            logger.error("inventory.advisor_recommendations_scan_error", region=region, error=str(e))
            return []

    def _calculate_advisor_recommendation_optimization(
        self,
        impact_level: str,
        age_days: int,
        monthly_savings: float,
        short_description: str,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization score for Azure Advisor cost recommendations.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Critical-impact recommendation ignored for 90+ days
        if impact_level == "high" and age_days >= 90:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_savings
            recommendations.append(
                f"CRITICAL: High-impact Advisor recommendation ignored for {age_days} days (potential ${monthly_savings:.2f}/month savings). "
                f"Recommendation: {short_description}. "
                "Act immediately to implement this cost optimization. Long-ignored high-impact recommendations indicate significant waste."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - High-impact recommendation not acted on for 30+ days
        if impact_level == "high" and age_days >= 30:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_savings
            recommendations.append(
                f"HIGH: High-impact Advisor recommendation pending for {age_days} days (saves ${monthly_savings:.2f}/month). "
                f"Recommendation: {short_description}. "
                "Implement this recommendation to realize significant cost savings."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Medium-impact recommendation ignored for 60+ days
        if impact_level == "medium" and age_days >= 60:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = monthly_savings
            recommendations.append(
                f"HIGH: Medium-impact Advisor recommendation ignored for {age_days} days (saves ${monthly_savings:.2f}/month). "
                f"Recommendation: {short_description}. "
                "Review and implement to avoid continued waste. Extended inaction increases cumulative cost."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Medium-impact recommendation pending for 30+ days
        if impact_level == "medium" and age_days >= 30:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            potential_savings = monthly_savings
            recommendations.append(
                f"MEDIUM: Medium-impact Advisor recommendation pending for {age_days} days (saves ${monthly_savings:.2f}/month). "
                f"Recommendation: {short_description}. "
                "Schedule implementation to reduce ongoing costs."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - Low-impact recommendation not reviewed for 90+ days
        if impact_level == "low" and age_days >= 90:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = monthly_savings
            recommendations.append(
                f"LOW: Low-impact Advisor recommendation pending for {age_days} days (saves ${monthly_savings:.2f}/month). "
                f"Recommendation: {short_description}. "
                "Review recommendation and decide whether to implement or suppress."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_arm_deployments(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure ARM Template Deployments for cost intelligence.

        ARM (Azure Resource Manager) templates are JSON files that define infrastructure as code.
        Failed or long-running deployments can waste resources (compute time, storage for logs).

        Pricing: ARM deployments are FREE, but failed deployments waste engineer time and resources
        Estimated cost: $50-200/hour of engineer time wasted debugging failed deployments

        Detection Logic:
        - Scans ARM deployments at resource group level (region-specific)
        - Analyzes deployment status (Succeeded, Failed, Running)
        - Calculates deployment duration and identifies long-running deployments
        - Detects repeated failed deployments (same template failing multiple times)

        Waste Detection (5 scenarios):
        1. CRITICAL (90): Failed deployment running for 90+ days (indicates abandoned infrastructure)
        2. HIGH (75): Multiple failed deployments of same template in 30d (systemic issue)
        3. HIGH (70): Running deployment stuck for 7+ days (blocked deployment)
        4. MEDIUM (50): Failed deployment not cleaned up for 30+ days (technical debt)
        5. LOW (30): Successful deployment with warnings or issues (minor inefficiencies)

        Returns:
            List of ALL ARM deployments as AllCloudResourceData objects
        """
        logger.info("inventory.scanning_arm_deployments", region=region)

        try:
            from azure.mgmt.resource import ResourceManagementClient
            from datetime import datetime, timezone

            resource_client = ResourceManagementClient(
                credential=self.credentials,
                subscription_id=self.subscription_id
            )

            all_deployments: list[AllCloudResourceData] = []

            # List all resource groups (to scan deployments per RG)
            resource_groups = list(resource_client.resource_groups.list())

            # Filter resource groups by region
            region_rgs = [rg for rg in resource_groups if rg.location == region]

            logger.info(
                "inventory.arm_deployments_resource_groups_found",
                region=region,
                resource_group_count=len(region_rgs)
            )

            for rg in region_rgs:
                try:
                    rg_name = rg.name

                    # List deployments in this resource group
                    deployments_list = list(
                        resource_client.deployments.list_by_resource_group(
                            resource_group_name=rg_name
                        )
                    )

                    for deployment in deployments_list:
                        try:
                            deployment_name = deployment.name or "unknown"
                            deployment_id = deployment.id or "unknown"
                            deployment_state = deployment.properties.provisioning_state if deployment.properties else "Unknown"
                            timestamp = deployment.properties.timestamp if deployment.properties else None
                            duration = deployment.properties.duration if deployment.properties else None
                            mode = safe_get_value(deployment.properties.mode if deployment.properties else None, "Unknown")

                            # Calculate age of deployment
                            if timestamp:
                                age_days = (datetime.now(timezone.utc) - timestamp).days
                            else:
                                age_days = 0

                            # Parse duration (ISO 8601 format like "PT1H30M")
                            duration_seconds = 0
                            if duration:
                                try:
                                    # Simple parse for PT format (e.g., PT1H30M5S)
                                    import re
                                    hours = re.search(r'(\d+)H', duration)
                                    minutes = re.search(r'(\d+)M', duration)
                                    seconds = re.search(r'(\d+)S', duration)
                                    duration_seconds = (
                                        (int(hours.group(1)) * 3600 if hours else 0) +
                                        (int(minutes.group(1)) * 60 if minutes else 0) +
                                        (int(seconds.group(1)) if seconds else 0)
                                    )
                                except Exception:
                                    duration_seconds = 0

                            # Estimate cost (engineer time wasted debugging failed deployments)
                            # Failed deployment: $100/hour * hours wasted
                            # Assume 2 hours wasted per failed deployment
                            engineer_hourly_rate = 100.0
                            hours_wasted = 2.0 if deployment_state == "Failed" else 0.0
                            estimated_cost = engineer_hourly_rate * hours_wasted

                            # Monthly cost (amortized over 1 month if still present)
                            monthly_cost = estimated_cost if age_days <= 30 else 0.0

                            # Determine optimization status
                            (
                                is_optimizable,
                                optimization_score,
                                priority,
                                optimization_savings,
                                recommendations_list,
                            ) = self._calculate_arm_deployment_optimization(
                                deployment_state=deployment_state,
                                age_days=age_days,
                                duration_seconds=duration_seconds,
                                monthly_cost=monthly_cost,
                                deployment_name=deployment_name,
                            )

                            # Create AllCloudResourceData
                            resource_data = AllCloudResourceData(
                                resource_id=deployment_id,
                                resource_type="azure_arm_deployment",
                                resource_name=f"ARM: {deployment_name}",
                                region=region,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                is_orphan=False,  # ARM deployments are not orphans
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=optimization_savings,
                                optimization_recommendations=recommendations_list,
                                resource_metadata={
                                    "deployment_id": deployment_id,
                                    "deployment_name": deployment_name,
                                    "resource_group": rg_name,
                                    "provisioning_state": deployment_state,
                                    "deployment_mode": mode,
                                    "timestamp": timestamp.isoformat() if timestamp else None,
                                    "duration": duration,
                                    "duration_seconds": duration_seconds,
                                    "age_days": age_days,
                                },
                            )
                            all_deployments.append(resource_data)

                        except Exception as e:
                            logger.error(
                                "inventory.arm_deployment_processing_error",
                                deployment_name=getattr(deployment, "name", "unknown"),
                                error=str(e),
                            )
                            continue

                except Exception as e:
                    logger.error(
                        "inventory.arm_deployments_rg_error",
                        resource_group=getattr(rg, "name", "unknown"),
                        error=str(e),
                    )
                    continue

            logger.info(
                "inventory.arm_deployments_scan_complete",
                region=region,
                total_deployments=len(all_deployments),
                optimizable_count=sum(1 for r in all_deployments if r.is_optimizable),
            )

            return all_deployments

        except ImportError:
            logger.error(
                "inventory.missing_resource_sdk",
                region=region,
                error="azure-mgmt-resource package not installed",
            )
            return []
        except Exception as e:
            logger.error("inventory.arm_deployments_scan_error", region=region, error=str(e))
            return []

    def _calculate_arm_deployment_optimization(
        self,
        deployment_state: str,
        age_days: int,
        duration_seconds: int,
        monthly_cost: float,
        deployment_name: str,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization score for Azure ARM Template Deployments.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Failed deployment abandoned for 90+ days
        if deployment_state == "Failed" and age_days >= 90:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append(
                f"CRITICAL: Failed ARM deployment '{deployment_name}' abandoned for {age_days} days (wasted ${monthly_cost:.2f} in engineer time). "
                "Delete this failed deployment to clean up technical debt. "
                "Long-abandoned failed deployments indicate process issues and waste resources."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Multiple failed deployments (same template)
        # Note: This scenario requires tracking multiple deployments, simplified here
        if deployment_state == "Failed" and age_days >= 30:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.append(
                f"HIGH: Failed ARM deployment '{deployment_name}' not cleaned up for {age_days} days (wasted ${monthly_cost:.2f}). "
                "Investigate root cause of failure and delete failed deployment. "
                "Repeated failures indicate systemic infrastructure issues."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Running deployment stuck for 7+ days
        if deployment_state == "Running" and age_days >= 7:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = monthly_cost * 2  # Stuck deployments block other work
            recommendations.append(
                f"HIGH: ARM deployment '{deployment_name}' stuck in Running state for {age_days} days. "
                "Cancel this stuck deployment and investigate what's blocking it. "
                "Long-running deployments may indicate timeouts, dependency issues, or resource locks."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Failed deployment not cleaned up for 7+ days
        if deployment_state == "Failed" and age_days >= 7:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            potential_savings = monthly_cost
            recommendations.append(
                f"MEDIUM: Failed ARM deployment '{deployment_name}' not cleaned up for {age_days} days (wasted ${monthly_cost:.2f}). "
                "Delete this failed deployment and fix the underlying issue. "
                "Failed deployments create technical debt and clutter deployment history."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - Very long deployment duration (>1 hour)
        if deployment_state == "Succeeded" and duration_seconds > 3600:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = 10.0  # Small optimization potential
            recommendations.append(
                f"LOW: ARM deployment '{deployment_name}' took {duration_seconds/60:.1f} minutes to complete. "
                "Review template for optimization opportunities (parallel deployments, smaller batches). "
                "Long deployment times slow down development velocity."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_container_instances(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Container Instances (ACI) for cost intelligence.

        Azure Container Instances (ACI) is a serverless container runtime (like AWS Fargate).
        Charges are per-second based on CPU cores and memory allocated.

        Pricing: ~$0.0000012/vCPU-second + ~$0.00000013/GB-second
        Example: 1 vCPU + 1.5 GB = ~$40/month if running 24/7

        Detection Logic:
        - Scans all container groups (ACI deployment unit)
        - Analyzes container state (Running, Stopped, Succeeded, Failed)
        - Detects idle containers (Stopped but still allocated resources)
        - Identifies failed containers (exited with errors)
        - Calculates runtime and cost

        Waste Detection (5 scenarios):
        1. CRITICAL (90): Stopped container allocated for 90+ days (abandoned infrastructure)
        2. HIGH (75): Failed container not cleaned up for 30+ days (technical debt)
        3. HIGH (70): Running container with no restarts in 90d (potentially idle)
        4. MEDIUM (50): Succeeded container not deleted for 7+ days (completed job)
        5. LOW (30): Over-provisioned container (high CPU/memory, low utilization)

        Returns:
            List of ALL container instances as AllCloudResourceData objects
        """
        logger.info("inventory.scanning_container_instances", region=region)

        try:
            from azure.mgmt.containerinstance import ContainerInstanceManagementClient
            from datetime import datetime, timezone

            aci_client = ContainerInstanceManagementClient(
                credential=self.credentials,
                subscription_id=self.subscription_id
            )

            all_container_groups: list[AllCloudResourceData] = []

            # List all resource groups (to scan container groups per RG)
            from azure.mgmt.resource import ResourceManagementClient
            resource_client = ResourceManagementClient(
                credential=self.credentials,
                subscription_id=self.subscription_id
            )

            resource_groups = list(resource_client.resource_groups.list())

            # Filter resource groups by region
            region_rgs = [rg for rg in resource_groups if rg.location == region]

            logger.info(
                "inventory.container_instances_resource_groups_found",
                region=region,
                resource_group_count=len(region_rgs)
            )

            for rg in region_rgs:
                try:
                    rg_name = rg.name

                    # List container groups in this resource group
                    container_groups_list = list(
                        aci_client.container_groups.list_by_resource_group(
                            resource_group_name=rg_name
                        )
                    )

                    for cg in container_groups_list:
                        try:
                            cg_name = cg.name or "unknown"
                            cg_id = cg.id or "unknown"
                            cg_location = cg.location or region
                            provisioning_state = cg.provisioning_state or "Unknown"
                            instance_view = cg.instance_view

                            # Container state (from first container in group)
                            container_state = "Unknown"
                            restart_count = 0
                            if cg.containers and len(cg.containers) > 0:
                                first_container = cg.containers[0]
                                if hasattr(first_container, 'instance_view') and first_container.instance_view:
                                    container_state = first_container.instance_view.current_state.state if first_container.instance_view.current_state else "Unknown"
                                    restart_count = first_container.instance_view.restart_count if hasattr(first_container.instance_view, 'restart_count') else 0

                            # OS type and resources
                            os_type = safe_get_value(cg.os_type, "Linux")
                            cpu_count = 0.0
                            memory_gb = 0.0
                            if cg.containers and len(cg.containers) > 0:
                                for container in cg.containers:
                                    if container.resources and container.resources.requests:
                                        cpu_count += container.resources.requests.cpu or 0.0
                                        memory_gb += container.resources.requests.memory_in_gb or 0.0

                            # Calculate age (creation time)
                            # Note: ACI doesn't expose creation timestamp directly, use resource tags or assume recent
                            age_days = 30  # Default assumption for orphan detection

                            # Calculate monthly cost
                            # Pricing: $0.0000012/vCPU-second + $0.00000013/GB-second (US pricing)
                            cpu_cost_per_second = 0.0000012
                            memory_cost_per_second = 0.00000013
                            seconds_per_month = 30 * 24 * 3600  # ~2.6M seconds

                            # Only charge if container is running or stopped (not succeeded/failed)
                            if container_state in ["Running", "Stopped"]:
                                monthly_cost = (
                                    (cpu_count * cpu_cost_per_second * seconds_per_month) +
                                    (memory_gb * memory_cost_per_second * seconds_per_month)
                                )
                            else:
                                monthly_cost = 0.0

                            # Determine optimization status
                            (
                                is_optimizable,
                                optimization_score,
                                priority,
                                optimization_savings,
                                recommendations_list,
                            ) = self._calculate_container_instance_optimization(
                                container_state=container_state,
                                age_days=age_days,
                                restart_count=restart_count,
                                monthly_cost=monthly_cost,
                                cg_name=cg_name,
                                cpu_count=cpu_count,
                                memory_gb=memory_gb,
                            )

                            # Create AllCloudResourceData
                            resource_data = AllCloudResourceData(
                                resource_id=cg_id,
                                resource_type="azure_container_instance",
                                resource_name=f"ACI: {cg_name}",
                                region=cg_location,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                is_orphan=False,  # Will be determined by optimization logic
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=optimization_savings,
                                optimization_recommendations=recommendations_list,
                                resource_metadata={
                                    "container_group_id": cg_id,
                                    "container_group_name": cg_name,
                                    "resource_group": rg_name,
                                    "provisioning_state": provisioning_state,
                                    "container_state": container_state,
                                    "os_type": os_type,
                                    "cpu_count": cpu_count,
                                    "memory_gb": memory_gb,
                                    "restart_count": restart_count,
                                    "container_count": len(cg.containers) if cg.containers else 0,
                                    "age_days": age_days,
                                },
                            )
                            all_container_groups.append(resource_data)

                        except Exception as e:
                            logger.error(
                                "inventory.container_instance_processing_error",
                                container_group_name=getattr(cg, "name", "unknown"),
                                error=str(e),
                            )
                            continue

                except Exception as e:
                    logger.error(
                        "inventory.container_instances_rg_error",
                        resource_group=getattr(rg, "name", "unknown"),
                        error=str(e),
                    )
                    continue

            logger.info(
                "inventory.container_instances_scan_complete",
                region=region,
                total_container_groups=len(all_container_groups),
                optimizable_count=sum(1 for r in all_container_groups if r.is_optimizable),
            )

            return all_container_groups

        except ImportError:
            logger.error(
                "inventory.missing_container_instance_sdk",
                region=region,
                error="azure-mgmt-containerinstance package not installed",
            )
            return []
        except Exception as e:
            logger.error("inventory.container_instances_scan_error", region=region, error=str(e))
            return []

    def _calculate_container_instance_optimization(
        self,
        container_state: str,
        age_days: int,
        restart_count: int,
        monthly_cost: float,
        cg_name: str,
        cpu_count: float,
        memory_gb: float,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization score for Azure Container Instances.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Stopped container allocated for 90+ days (orphan)
        if container_state == "Stopped" and age_days >= 90:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append(
                f"CRITICAL: Container group '{cg_name}' stopped for {age_days} days but still allocated (wastes ${monthly_cost:.2f}/month). "
                "Delete this abandoned container group immediately. "
                "Long-stopped containers indicate forgotten infrastructure and waste resources."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Failed container not cleaned up for 30+ days
        if container_state == "Failed" and age_days >= 30:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost
            recommendations.append(
                f"HIGH: Container group '{cg_name}' failed {age_days} days ago but not cleaned up (wasted ${monthly_cost:.2f}). "
                "Delete this failed container group and investigate the failure. "
                "Failed containers indicate process issues and waste resources."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Running container with no restarts in 90 days (potentially idle)
        if container_state == "Running" and restart_count == 0 and age_days >= 90:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = monthly_cost * 0.5  # Assume 50% waste
            recommendations.append(
                f"HIGH: Container group '{cg_name}' running for {age_days} days with 0 restarts (costs ${monthly_cost:.2f}/month). "
                "Verify this container is actually doing work. No restarts may indicate idle container. "
                "Consider monitoring CPU/memory utilization or switching to event-driven architecture."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Succeeded container not deleted for 7+ days
        if container_state == "Succeeded" and age_days >= 7:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            potential_savings = monthly_cost
            recommendations.append(
                f"MEDIUM: Container group '{cg_name}' completed successfully {age_days} days ago but not deleted (costs ${monthly_cost:.2f}/month). "
                "Delete this completed container group to free resources. "
                "Succeeded containers from batch jobs should be cleaned up automatically."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - Over-provisioned container (high CPU/memory)
        if cpu_count >= 4 or memory_gb >= 8:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = monthly_cost * 0.3  # Assume 30% over-provisioning
            recommendations.append(
                f"LOW: Container group '{cg_name}' provisioned with {cpu_count} vCPU / {memory_gb:.1f} GB (costs ${monthly_cost:.2f}/month). "
                "Review actual resource utilization to detect over-provisioning. "
                "Right-sizing containers can reduce costs by 30-50%. Consider Azure Monitor metrics."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_batch_jobs(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Batch Jobs for cost intelligence.

        Azure Batch is a cloud-based job scheduling service for large-scale parallel
        and high-performance computing (HPC) workloads.

        Pricing: Batch service is FREE, but you pay for compute nodes
        - VM pricing: $0.10-2.00/hour per node (depending on VM size)
        - Example: 10 nodes  Standard_D2s_v3  24h = ~$40/day

        Detection Logic:
        - Scans all Batch Accounts in subscription
        - Lists jobs (Active, Completed, Failed) per account
        - Analyzes pool allocation and node usage
        - Detects abandoned/stuck jobs with active pools
        - Calculates compute waste from idle nodes

        Waste Detection (5 scenarios):
        1. CRITICAL (90): Failed job with active pool for 90+ days (abandoned infrastructure)
        2. HIGH (75): Multiple failed jobs on same pool in 30d (systemic issue)
        3. HIGH (70): Active job stuck for 7+ days (blocked job, wasting resources)
        4. MEDIUM (50): Completed job with active pool for 7+ days (cleanup missing)
        5. LOW (30): Pool over-provisioned (more nodes than needed for workload)

        Returns:
            List of ALL Batch jobs as AllCloudResourceData objects
        """
        logger.info("inventory.scanning_batch_jobs", region=region)

        try:
            from azure.mgmt.batch import BatchManagementClient
            from datetime import datetime, timezone

            batch_client = BatchManagementClient(
                credential=self.credentials,
                subscription_id=self.subscription_id
            )

            all_batch_jobs: list[AllCloudResourceData] = []

            # List all Batch accounts in subscription
            batch_accounts = list(batch_client.batch_account.list())

            # Filter by region
            region_accounts = [acc for acc in batch_accounts if acc.location == region]

            logger.info(
                "inventory.batch_accounts_found",
                region=region,
                batch_account_count=len(region_accounts)
            )

            for account in region_accounts:
                try:
                    account_name = account.name or "unknown"
                    account_id = account.id or "unknown"
                    resource_group = account.id.split('/')[4] if account.id else "unknown"

                    # List pools in this Batch account
                    try:
                        pools_list = list(
                            batch_client.pool.list_by_batch_account(
                                resource_group_name=resource_group,
                                account_name=account_name
                            )
                        )
                    except Exception as e:
                        logger.warning(
                            "inventory.batch_pools_list_error",
                            account=account_name,
                            error=str(e)
                        )
                        pools_list = []

                    # Since we can't easily list individual jobs via Management API,
                    # we analyze pools as proxy for job activity
                    # Note: Full job details require Batch DataPlane API (azure.batch)
                    for pool in pools_list:
                        try:
                            pool_name = pool.name or "unknown"
                            pool_id = pool.id or "unknown"
                            allocation_state = pool.allocation_state if hasattr(pool, 'allocation_state') else "unknown"
                            vm_size = pool.vm_size if hasattr(pool, 'vm_size') else "unknown"

                            # Pool scale settings
                            target_dedicated_nodes = 0
                            target_low_priority_nodes = 0
                            current_dedicated_nodes = 0
                            current_low_priority_nodes = 0

                            if hasattr(pool, 'scale_settings') and pool.scale_settings:
                                if hasattr(pool.scale_settings, 'fixed_scale') and pool.scale_settings.fixed_scale:
                                    target_dedicated_nodes = pool.scale_settings.fixed_scale.target_dedicated_nodes or 0
                                    target_low_priority_nodes = pool.scale_settings.fixed_scale.target_low_priority_nodes or 0

                            if hasattr(pool, 'current_dedicated_nodes'):
                                current_dedicated_nodes = pool.current_dedicated_nodes or 0
                            if hasattr(pool, 'current_low_priority_nodes'):
                                current_low_priority_nodes = pool.current_low_priority_nodes or 0

                            total_nodes = current_dedicated_nodes + current_low_priority_nodes

                            # Estimate age (creation time)
                            creation_time = pool.creation_time if hasattr(pool, 'creation_time') else None
                            if creation_time:
                                age_days = (datetime.now(timezone.utc) - creation_time).days
                            else:
                                age_days = 30  # Default assumption

                            # Last modified time
                            last_modified = pool.last_modified if hasattr(pool, 'last_modified') else None

                            # Estimate monthly cost based on VM size and node count
                            # Simplified pricing (actual varies by region and VM size)
                            vm_hourly_cost = {
                                "standard_d2s_v3": 0.096,  # 2 vCPU, 8 GB
                                "standard_d4s_v3": 0.192,  # 4 vCPU, 16 GB
                                "standard_f2s_v2": 0.085,  # 2 vCPU, 4 GB
                                "standard_f4s_v2": 0.169,  # 4 vCPU, 8 GB
                            }

                            vm_size_normalized = vm_size.lower() if vm_size != "unknown" else "standard_d2s_v3"
                            hourly_cost_per_node = vm_hourly_cost.get(vm_size_normalized, 0.10)  # Default $0.10/hour

                            hours_per_month = 730  # ~30 days
                            monthly_cost = total_nodes * hourly_cost_per_node * hours_per_month

                            # Determine optimization status (using pool as proxy for job activity)
                            # Note: In real scenario, would use Batch DataPlane API to get actual job states
                            (
                                is_optimizable,
                                optimization_score,
                                priority,
                                optimization_savings,
                                recommendations_list,
                            ) = self._calculate_batch_job_optimization(
                                allocation_state=allocation_state,
                                age_days=age_days,
                                total_nodes=total_nodes,
                                target_nodes=target_dedicated_nodes + target_low_priority_nodes,
                                monthly_cost=monthly_cost,
                                pool_name=pool_name,
                                vm_size=vm_size,
                            )

                            # Create AllCloudResourceData
                            resource_data = AllCloudResourceData(
                                resource_id=pool_id,
                                resource_type="azure_batch_job",
                                resource_name=f"Batch Pool: {pool_name}",
                                region=region,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                is_orphan=False,  # Pools are not orphans (they're infrastructure)
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=optimization_savings,
                                optimization_recommendations=recommendations_list,
                                resource_metadata={
                                    "batch_account": account_name,
                                    "pool_id": pool_id,
                                    "pool_name": pool_name,
                                    "allocation_state": allocation_state,
                                    "vm_size": vm_size,
                                    "current_dedicated_nodes": current_dedicated_nodes,
                                    "current_low_priority_nodes": current_low_priority_nodes,
                                    "target_dedicated_nodes": target_dedicated_nodes,
                                    "target_low_priority_nodes": target_low_priority_nodes,
                                    "total_nodes": total_nodes,
                                    "age_days": age_days,
                                    "creation_time": creation_time.isoformat() if creation_time else None,
                                    "last_modified": last_modified.isoformat() if last_modified else None,
                                },
                            )
                            all_batch_jobs.append(resource_data)

                        except Exception as e:
                            logger.error(
                                "inventory.batch_pool_processing_error",
                                pool_name=getattr(pool, "name", "unknown"),
                                error=str(e),
                            )
                            continue

                except Exception as e:
                    logger.error(
                        "inventory.batch_account_error",
                        account=getattr(account, "name", "unknown"),
                        error=str(e),
                    )
                    continue

            logger.info(
                "inventory.batch_jobs_scan_complete",
                region=region,
                total_batch_pools=len(all_batch_jobs),
                optimizable_count=sum(1 for r in all_batch_jobs if r.is_optimizable),
            )

            return all_batch_jobs

        except ImportError:
            logger.error(
                "inventory.missing_batch_sdk",
                region=region,
                error="azure-mgmt-batch package not installed",
            )
            return []
        except Exception as e:
            logger.error("inventory.batch_jobs_scan_error", region=region, error=str(e))
            return []

    def _calculate_batch_job_optimization(
        self,
        allocation_state: str,
        age_days: int,
        total_nodes: int,
        target_nodes: int,
        monthly_cost: float,
        pool_name: str,
        vm_size: str,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization score for Azure Batch Jobs (using pool as proxy).

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Pool with nodes allocated for 90+ days (likely abandoned)
        if total_nodes > 0 and age_days >= 90:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            potential_savings = monthly_cost
            recommendations.append(
                f"CRITICAL: Batch pool '{pool_name}' with {total_nodes} node(s) allocated for {age_days} days (wastes ${monthly_cost:.2f}/month). "
                "This pool has been running for 90+ days, likely abandoned. Delete pool or reduce node allocation. "
                "Long-running Batch pools indicate forgotten infrastructure and significant waste."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Pool steady-state for 30+ days (not a typical batch workload)
        if allocation_state == "Steady" and total_nodes > 0 and age_days >= 30:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            potential_savings = monthly_cost * 0.7  # Assume 70% waste
            recommendations.append(
                f"HIGH: Batch pool '{pool_name}' in Steady state for {age_days} days with {total_nodes} node(s) (costs ${monthly_cost:.2f}/month). "
                "Batch pools should be ephemeral (created for jobs, deleted after). "
                "Consider switching to auto-scale or deleting pool when not in use. Potential savings: ${monthly_cost * 0.7:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Pool resizing (stuck in transition state)
        if allocation_state == "Resizing" and age_days >= 7:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            potential_savings = monthly_cost * 0.5  # Partial waste
            recommendations.append(
                f"HIGH: Batch pool '{pool_name}' stuck in Resizing state for {age_days} days (costs ${monthly_cost:.2f}/month). "
                "Pool may be stuck due to quota limits, networking issues, or API errors. "
                "Investigate and fix resizing issue, or delete and recreate pool."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Pool with idle nodes (current > target)
        if total_nodes > target_nodes and total_nodes > 0:
            excess_nodes = total_nodes - target_nodes
            excess_cost = (excess_nodes / total_nodes) * monthly_cost
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            potential_savings = excess_cost
            recommendations.append(
                f"MEDIUM: Batch pool '{pool_name}' has {excess_nodes} excess node(s) ({total_nodes} current vs {target_nodes} target, costs ${monthly_cost:.2f}/month). "
                "Pool is not scaling down properly. Review auto-scale formula or manually reduce nodes. "
                f"Removing excess nodes would save ${excess_cost:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - Large pool (potential over-provisioning)
        if total_nodes >= 10:
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = monthly_cost * 0.2  # Assume 20% over-provisioning
            recommendations.append(
                f"LOW: Batch pool '{pool_name}' has {total_nodes} node(s) with VM size {vm_size} (costs ${monthly_cost:.2f}/month). "
                "Large pools should be monitored for utilization. "
                "Review job metrics to ensure nodes are fully utilized. Consider right-sizing VM or reducing node count."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_storage_lifecycle_policies(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Azure Storage Lifecycle Management Policies for cost intelligence.

        Azure Storage Lifecycle Management allows automatic transition of blobs between
        access tiers (Hot  Cool  Archive) and automatic deletion based on rules.

        Pricing: Lifecycle Management is FREE (included with Azure Storage)
        BUT improper configuration leads to massive waste:
        - Hot tier: $0.018/GB/month
        - Cool tier: $0.010/GB/month (45% cheaper)
        - Archive tier: $0.002/GB/month (89% cheaper)

        Detection Logic:
        - Scans all Storage Accounts in subscription
        - Retrieves management policies (name='default')
        - Analyzes lifecycle rules for blob transitions
        - Detects missing or misconfigured policies
        - Calculates waste from data stored in wrong tier

        Waste Detection (5 scenarios):
        1. CRITICAL (90): No policy + >1TB data in Hot tier 90+ days (massive waste)
        2. HIGH (75): Policy exists but poorly configured (no Archive transition)
        3. HIGH (70): Large blobs in Hot tier >180 days with no access (should be Archive)
        4. MEDIUM (50): Transition to Cool delayed (>30d in Hot instead of Cool)
        5. LOW (30): Policy exists but suboptimal (can be improved)

        Returns:
            List of ALL Storage Accounts with lifecycle policy analysis as AllCloudResourceData objects
        """
        logger.info("inventory.scanning_storage_lifecycle_policies", region=region)

        try:
            from azure.mgmt.storage import StorageManagementClient
            from datetime import datetime, timezone

            storage_client = StorageManagementClient(
                credential=self.credentials,
                subscription_id=self.subscription_id
            )

            all_policies: list[AllCloudResourceData] = []

            # List all storage accounts
            storage_accounts = list(storage_client.storage_accounts.list())

            # Filter by region
            region_accounts = [acc for acc in storage_accounts if acc.location == region]

            logger.info(
                "inventory.storage_accounts_found",
                region=region,
                storage_account_count=len(region_accounts)
            )

            for account in region_accounts:
                try:
                    account_name = account.name or "unknown"
                    account_id = account.id or "unknown"
                    resource_group = account.id.split('/')[4] if account.id else "unknown"
                    sku_tier = safe_get_value(account.sku.tier if account.sku else None, "Standard")

                    # Try to get management policy (name is always 'default')
                    policy = None
                    has_policy = False
                    try:
                        policy = storage_client.management_policies.get(
                            resource_group_name=resource_group,
                            account_name=account_name,
                            management_policy_name='default'
                        )
                        has_policy = True
                    except Exception:
                        # No policy exists (404 is expected if no policy configured)
                        has_policy = False

                    # Estimate storage size (would need Azure Monitor API for actual usage)
                    # For now, assume moderate usage (100 GB) for cost calculation
                    estimated_storage_gb = 100.0

                    # Analyze policy configuration
                    policy_config_status = "none"
                    has_cool_transition = False
                    has_archive_transition = False
                    cool_transition_days = None
                    archive_transition_days = None

                    if has_policy and policy and hasattr(policy, 'policy') and policy.policy:
                        policy_config_status = "configured"
                        if hasattr(policy.policy, 'rules') and policy.policy.rules:
                            for rule in policy.policy.rules:
                                if not rule.enabled:
                                    continue

                                # Check for tier transitions in rule definition
                                if hasattr(rule, 'definition') and rule.definition:
                                    actions = rule.definition.actions if hasattr(rule.definition, 'actions') else None
                                    if actions and hasattr(actions, 'base_blob') and actions.base_blob:
                                        base_blob = actions.base_blob

                                        # Cool transition
                                        if hasattr(base_blob, 'tier_to_cool') and base_blob.tier_to_cool:
                                            has_cool_transition = True
                                            if hasattr(base_blob.tier_to_cool, 'days_after_modification_greater_than'):
                                                cool_transition_days = base_blob.tier_to_cool.days_after_modification_greater_than

                                        # Archive transition
                                        if hasattr(base_blob, 'tier_to_archive') and base_blob.tier_to_archive:
                                            has_archive_transition = True
                                            if hasattr(base_blob.tier_to_archive, 'days_after_modification_greater_than'):
                                                archive_transition_days = base_blob.tier_to_archive.days_after_modification_greater_than

                    # Calculate monthly cost (assume all data in Hot tier if no policy)
                    hot_tier_cost_per_gb = 0.018
                    cool_tier_cost_per_gb = 0.010
                    archive_tier_cost_per_gb = 0.002

                    # If no policy, all data in Hot tier
                    if not has_policy:
                        monthly_cost = estimated_storage_gb * hot_tier_cost_per_gb
                    else:
                        # With policy, assume 50% in Cool, 30% in Archive, 20% in Hot
                        monthly_cost = (
                            estimated_storage_gb * 0.2 * hot_tier_cost_per_gb +
                            estimated_storage_gb * 0.5 * cool_tier_cost_per_gb +
                            estimated_storage_gb * 0.3 * archive_tier_cost_per_gb
                        )

                    # Determine optimization status
                    (
                        is_optimizable,
                        optimization_score,
                        priority,
                        optimization_savings,
                        recommendations_list,
                    ) = self._calculate_storage_lifecycle_optimization(
                        has_policy=has_policy,
                        has_cool_transition=has_cool_transition,
                        has_archive_transition=has_archive_transition,
                        cool_transition_days=cool_transition_days,
                        archive_transition_days=archive_transition_days,
                        estimated_storage_gb=estimated_storage_gb,
                        monthly_cost=monthly_cost,
                        account_name=account_name,
                    )

                    # Create AllCloudResourceData
                    resource_data = AllCloudResourceData(
                        resource_id=account_id,
                        resource_type="azure_storage_lifecycle_policy",
                        resource_name=f"Storage: {account_name}",
                        region=region,
                        estimated_monthly_cost=monthly_cost,
                        currency="USD",
                        is_orphan=False,  # Lifecycle policies are not orphans
                        is_optimizable=is_optimizable,
                        optimization_score=optimization_score,
                        optimization_priority=priority,
                        potential_monthly_savings=optimization_savings,
                        optimization_recommendations=recommendations_list,
                        resource_metadata={
                            "storage_account_id": account_id,
                            "storage_account_name": account_name,
                            "resource_group": resource_group,
                            "sku_tier": sku_tier,
                            "has_lifecycle_policy": has_policy,
                            "policy_config_status": policy_config_status,
                            "has_cool_transition": has_cool_transition,
                            "has_archive_transition": has_archive_transition,
                            "cool_transition_days": cool_transition_days,
                            "archive_transition_days": archive_transition_days,
                            "estimated_storage_gb": estimated_storage_gb,
                        },
                    )
                    all_policies.append(resource_data)

                except Exception as e:
                    logger.error(
                        "inventory.storage_lifecycle_policy_processing_error",
                        storage_account=getattr(account, "name", "unknown"),
                        error=str(e),
                    )
                    continue

            logger.info(
                "inventory.storage_lifecycle_policies_scan_complete",
                region=region,
                total_storage_accounts=len(all_policies),
                optimizable_count=sum(1 for r in all_policies if r.is_optimizable),
            )

            return all_policies

        except ImportError:
            logger.error(
                "inventory.missing_storage_sdk",
                region=region,
                error="azure-mgmt-storage package not installed",
            )
            return []
        except Exception as e:
            logger.error("inventory.storage_lifecycle_policies_scan_error", region=region, error=str(e))
            return []

    def _calculate_storage_lifecycle_optimization(
        self,
        has_policy: bool,
        has_cool_transition: bool,
        has_archive_transition: bool,
        cool_transition_days: int | None,
        archive_transition_days: int | None,
        estimated_storage_gb: float,
        monthly_cost: float,
        account_name: str,
    ) -> tuple[bool, int, str, float, list[str]]:
        """
        Calculate optimization score for Azure Storage Lifecycle Policies.

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - No policy + significant storage (>100 GB)
        if not has_policy and estimated_storage_gb >= 100:
            is_optimizable = True
            optimization_score = 90
            priority = "critical"
            # Potential savings: transition 70% to Cool (45% cheaper) and 20% to Archive (89% cheaper)
            hot_cost = estimated_storage_gb * 0.018
            optimized_cost = (
                estimated_storage_gb * 0.1 * 0.018 +  # 10% Hot
                estimated_storage_gb * 0.7 * 0.010 +  # 70% Cool
                estimated_storage_gb * 0.2 * 0.002    # 20% Archive
            )
            potential_savings = hot_cost - optimized_cost
            recommendations.append(
                f"CRITICAL: Storage account '{account_name}' has NO lifecycle policy configured ({estimated_storage_gb:.0f} GB, costs ${monthly_cost:.2f}/month). "
                "All blobs likely in Hot tier (most expensive). Create lifecycle policy to transition old blobs to Cool (30d) and Archive (90d). "
                f"Implementing tiering would save ${potential_savings:.2f}/month (70% reduction)."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Policy exists but no Archive transition
        if has_policy and has_cool_transition and not has_archive_transition:
            is_optimizable = True
            optimization_score = 75
            priority = "high"
            # Assume 30% of data could move to Archive
            current_cool_cost = estimated_storage_gb * 0.3 * 0.010
            archive_cost = estimated_storage_gb * 0.3 * 0.002
            potential_savings = current_cool_cost - archive_cost
            recommendations.append(
                f"HIGH: Storage account '{account_name}' has lifecycle policy with Cool transition but NO Archive transition (costs ${monthly_cost:.2f}/month). "
                "Add Archive tier rules for blobs older than 180 days to maximize savings. "
                f"Archive tier is 80% cheaper than Cool. Potential savings: ${potential_savings:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Cool transition delayed (>60 days)
        if has_policy and has_cool_transition and cool_transition_days and cool_transition_days > 60:
            is_optimizable = True
            optimization_score = 70
            priority = "high"
            # Calculate waste from keeping data in Hot for too long
            extra_days = cool_transition_days - 30  # Optimal is 30 days
            extra_cost_per_gb = (0.018 - 0.010) * (extra_days / 30)
            potential_savings = estimated_storage_gb * extra_cost_per_gb
            recommendations.append(
                f"HIGH: Storage account '{account_name}' transitions to Cool after {cool_transition_days} days (costs ${monthly_cost:.2f}/month). "
                f"This is too late. Reduce to 30 days to save ${potential_savings:.2f}/month. "
                "Blobs rarely accessed after 30 days should move to Cool tier immediately."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Archive transition delayed (>180 days)
        if has_policy and has_archive_transition and archive_transition_days and archive_transition_days > 180:
            is_optimizable = True
            optimization_score = 50
            priority = "medium"
            extra_days = archive_transition_days - 180
            extra_cost_per_gb = (0.010 - 0.002) * (extra_days / 90)
            potential_savings = estimated_storage_gb * 0.2 * extra_cost_per_gb
            recommendations.append(
                f"MEDIUM: Storage account '{account_name}' transitions to Archive after {archive_transition_days} days (costs ${monthly_cost:.2f}/month). "
                f"Reduce to 180 days to save ${potential_savings:.2f}/month. "
                "Long-term archival data should move to Archive tier sooner."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: LOW - Policy exists but could be optimized
        if has_policy and (not has_cool_transition or not has_archive_transition):
            is_optimizable = True
            optimization_score = 30
            priority = "low"
            potential_savings = monthly_cost * 0.1  # Assume 10% improvement
            recommendations.append(
                f"LOW: Storage account '{account_name}' has lifecycle policy but incomplete (costs ${monthly_cost:.2f}/month). "
                "Review and optimize: ensure Cool transition (30d) and Archive transition (180d) are configured. "
                f"Fine-tuning could save ${potential_savings:.2f}/month."
            )
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    # ============================================================================
    # AWS - EKS CLUSTER (Cost Intelligence / Inventory Mode)
    # ============================================================================

    def _calculate_eks_monthly_cost(
        self,
        node_group_costs: float,
        fargate_costs: float,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS EKS Cluster.

        Cost structure:
        - Control plane: $73/month (0.10/hour * 730 hours)
        - Node groups: Sum of EC2 instance costs
        - Fargate pods: vCPU + memory costs

        Args:
            node_group_costs: Total monthly cost of all node groups (EC2 instances)
            fargate_costs: Total monthly cost of Fargate pods
            region: AWS region

        Returns:
            Total estimated monthly cost
        """
        # EKS control plane cost (fixed)
        control_plane_cost = 73.0  # $0.10/hour * 730 hours/month

        total_cost = control_plane_cost + node_group_costs + fargate_costs

        return total_cost

    def _calculate_eks_optimization(
        self,
        cluster: dict[str, Any],
        node_groups: list[dict[str, Any]],
        fargate_profiles: list[dict[str, Any]],
        monthly_cost: float,
        avg_cpu_utilization: float,
        node_instance_types: list[str],
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Calculate AWS EKS Cluster optimization metrics.

        Optimization scenarios (5):
        1. No worker nodes - Cluster with 0 nodes (paying control plane only)
        2. All nodes unhealthy - All nodes in degraded/failed state
        3. Over-provisioned nodes - All nodes with CPU <20% (right-sizing opportunity)
        4. Old generation nodes - Using t2/m4/c4/r4 instance types
        5. Spot instances not used - 100% On-Demand nodes (Spot 70% cheaper)

        Args:
            cluster: EKS cluster details
            node_groups: List of node groups
            fargate_profiles: List of Fargate profiles
            monthly_cost: Total monthly cost
            avg_cpu_utilization: Average CPU utilization across all nodes
            node_instance_types: List of instance types used by nodes

        Returns:
            Tuple of (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations: list[dict[str, Any]] = []

        cluster_name = cluster.get("name", "Unknown")
        total_node_count = sum(ng.get("scalingConfig", {}).get("desiredSize", 0) for ng in node_groups)

        # Scenario 1: CRITICAL - No worker nodes (paying control plane only)
        if total_node_count == 0 and not fargate_profiles:
            is_optimizable = True
            optimization_score = 95
            priority = "critical"
            potential_savings = monthly_cost  # Full cluster cost is waste
            recommendations.append({
                "type": "no_nodes",
                "severity": "critical",
                "message": (
                    f"CRITICAL: EKS cluster '{cluster_name}' has NO worker nodes or Fargate profiles (costs ${monthly_cost:.2f}/month). "
                    f"You're paying ${73:.2f}/month for control plane with no compute resources. "
                    "Delete this cluster immediately or add node groups/Fargate profiles if needed."
                ),
                "impact": "Paying full EKS costs with zero workload capacity",
                "action": "Delete cluster or add worker nodes/Fargate profiles",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - All nodes unhealthy (dead cluster)
        unhealthy_nodes = sum(
            1 for ng in node_groups
            if ng.get("health", {}).get("issues", [])
        )
        if total_node_count > 0 and unhealthy_nodes == len(node_groups):
            is_optimizable = True
            optimization_score = 90
            priority = "high"
            potential_savings = monthly_cost  # Entire cluster is waste
            recommendations.append({
                "type": "all_unhealthy",
                "severity": "high",
                "message": (
                    f"HIGH: EKS cluster '{cluster_name}' has ALL node groups unhealthy (costs ${monthly_cost:.2f}/month). "
                    f"All {len(node_groups)} node groups are in degraded/failed state. "
                    "This cluster is not serving any workloads. Investigate node group health or delete cluster."
                ),
                "impact": "Cluster not operational, all costs are waste",
                "action": "Fix node group health issues or delete cluster",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: MEDIUM - Over-provisioned nodes (CPU <20%)
        if avg_cpu_utilization > 0 and avg_cpu_utilization < 20.0:
            is_optimizable = True
            optimization_score = 70
            priority = "medium"
            # Assume 30% savings from right-sizing (reduce node count or instance types)
            potential_savings = monthly_cost * 0.30
            recommendations.append({
                "type": "over_provisioned",
                "severity": "medium",
                "message": (
                    f"MEDIUM: EKS cluster '{cluster_name}' has very low CPU utilization ({avg_cpu_utilization:.1f}%, costs ${monthly_cost:.2f}/month). "
                    f"Nodes are under-utilized. Right-size node groups: reduce node count or use smaller instance types. "
                    f"Potential savings: ${potential_savings:.2f}/month (30% reduction)."
                ),
                "impact": f"Wasting {100 - avg_cpu_utilization:.1f}% of node capacity",
                "action": "Reduce node count or downgrade instance types",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Old generation nodes (t2/m4/c4/r4)
        old_generation_types = ["t2", "m4", "c4", "r4"]
        old_instances = [it for it in node_instance_types if any(it.startswith(old) for old in old_generation_types)]
        if old_instances:
            is_optimizable = True
            optimization_score = 65
            priority = "medium"
            # Assume 15% savings from upgrading to newer generation
            potential_savings = monthly_cost * 0.15
            recommendations.append({
                "type": "old_generation",
                "severity": "medium",
                "message": (
                    f"MEDIUM: EKS cluster '{cluster_name}' uses old generation instance types (costs ${monthly_cost:.2f}/month). "
                    f"Found: {', '.join(set(old_instances))}. "
                    "Upgrade to newer generations (t3, m5, c5, r5) for 15% cost savings and better performance."
                ),
                "impact": "Missing out on 15% cost savings and improved performance",
                "action": f"Upgrade to newer generation: {', '.join(set(old_instances)).replace('t2', 't3').replace('m4', 'm5').replace('c4', 'c5').replace('r4', 'r5')}",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: MEDIUM - Spot instances not used (100% On-Demand)
        # Check if any node group uses Spot instances
        has_spot = any(ng.get("capacityType", "ON_DEMAND") == "SPOT" for ng in node_groups)
        if not has_spot and total_node_count >= 3:
            is_optimizable = True
            optimization_score = 60
            priority = "medium"
            # Spot instances are typically 70% cheaper
            # Recommend 60% of nodes on Spot (conservative mix)
            potential_savings = monthly_cost * 0.60 * 0.70
            recommendations.append({
                "type": "no_spot",
                "severity": "medium",
                "message": (
                    f"MEDIUM: EKS cluster '{cluster_name}' uses 100% On-Demand nodes (costs ${monthly_cost:.2f}/month). "
                    f"Cluster has {total_node_count} nodes. "
                    "Use Spot instances for 60% of nodes to save 70% on those nodes. "
                    f"Potential savings: ${potential_savings:.2f}/month (42% total reduction)."
                ),
                "impact": "Missing out on 42% cost savings from Spot instances",
                "action": "Create Spot node groups for stateless/fault-tolerant workloads",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # No optimization opportunities found
        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_eks_clusters(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS EKS Clusters for cost intelligence.

        This method scans EKS clusters to provide cost visibility and optimization
        recommendations. Unlike orphan detection, this scans ALL clusters regardless
        of usage patterns.

        Args:
            region: AWS region to scan

        Returns:
            List of AllCloudResourceData objects for all EKS clusters
        """
        resources: list[AllCloudResourceData] = []

        try:
            async with self.session.client("eks", region_name=region) as eks:
                async with self.session.client("ec2", region_name=region) as ec2:
                    async with self.session.client("cloudwatch", region_name=region) as cloudwatch:
                        # List all clusters
                        response = await eks.list_clusters()
                        cluster_names = response.get("clusters", [])

                        for cluster_name in cluster_names:
                            try:
                                # Get cluster details
                                cluster_response = await eks.describe_cluster(name=cluster_name)
                                cluster = cluster_response["cluster"]

                                cluster_arn = cluster.get("arn", "")
                                created_at = cluster.get("createdAt")
                                status = cluster.get("status", "UNKNOWN")

                                # List node groups
                                node_groups_response = await eks.list_nodegroups(clusterName=cluster_name)
                                node_group_names = node_groups_response.get("nodegroups", [])

                                node_groups = []
                                node_group_costs = 0.0
                                node_instance_types = []
                                total_nodes = 0

                                for ng_name in node_group_names:
                                    ng_response = await eks.describe_nodegroup(
                                        clusterName=cluster_name,
                                        nodegroupName=ng_name
                                    )
                                    ng = ng_response["nodegroup"]
                                    node_groups.append(ng)

                                    # Calculate node group cost
                                    scaling_config = ng.get("scalingConfig", {})
                                    desired_size = scaling_config.get("desiredSize", 0)
                                    total_nodes += desired_size

                                    instance_types = ng.get("instanceTypes", [])
                                    node_instance_types.extend(instance_types)

                                    # Simplified cost calculation (assume $0.05/hour per node avg)
                                    # In reality, this varies by instance type
                                    node_group_costs += desired_size * 0.05 * 730  # $0.05/hour * 730 hours

                                # List Fargate profiles
                                fargate_response = await eks.list_fargate_profiles(clusterName=cluster_name)
                                fargate_profile_names = fargate_response.get("fargateProfileNames", [])

                                fargate_profiles = []
                                fargate_costs = 0.0

                                for fp_name in fargate_profile_names:
                                    fp_response = await eks.describe_fargate_profile(
                                        clusterName=cluster_name,
                                        fargateProfileName=fp_name
                                    )
                                    fp = fp_response["fargateProfile"]
                                    fargate_profiles.append(fp)

                                    # Simplified Fargate cost (hard to estimate without pod metrics)
                                    # Assume minimal cost for now
                                    fargate_costs += 10.0  # Placeholder

                                # Get CloudWatch CPU metrics (if nodes exist)
                                avg_cpu_utilization = 0.0
                                if total_nodes > 0:
                                    try:
                                        # Get average CPU utilization for the cluster (last 14 days)
                                        end_time = datetime.utcnow()
                                        start_time = end_time - timedelta(days=14)

                                        cpu_response = await cloudwatch.get_metric_statistics(
                                            Namespace="ContainerInsights",
                                            MetricName="node_cpu_utilization",
                                            Dimensions=[
                                                {"Name": "ClusterName", "Value": cluster_name},
                                            ],
                                            StartTime=start_time,
                                            EndTime=end_time,
                                            Period=86400,  # 1 day
                                            Statistics=["Average"],
                                        )

                                        datapoints = cpu_response.get("Datapoints", [])
                                        if datapoints:
                                            avg_cpu_utilization = sum(dp["Average"] for dp in datapoints) / len(datapoints)
                                    except Exception as e:
                                        logger.warning(
                                            "eks.cloudwatch_metrics_failed",
                                            cluster_name=cluster_name,
                                            error=str(e),
                                        )

                                # Calculate total monthly cost
                                monthly_cost = self._calculate_eks_monthly_cost(
                                    node_group_costs=node_group_costs,
                                    fargate_costs=fargate_costs,
                                    region=region,
                                )

                                # Calculate optimization metrics
                                (
                                    is_optimizable,
                                    optimization_score,
                                    optimization_priority,
                                    potential_savings,
                                    optimization_recommendations,
                                ) = self._calculate_eks_optimization(
                                    cluster=cluster,
                                    node_groups=node_groups,
                                    fargate_profiles=fargate_profiles,
                                    monthly_cost=monthly_cost,
                                    avg_cpu_utilization=avg_cpu_utilization,
                                    node_instance_types=node_instance_types,
                                )

                                # Build metadata
                                metadata = {
                                    "cluster_name": cluster_name,
                                    "cluster_arn": cluster_arn,
                                    "status": status,
                                    "kubernetes_version": cluster.get("version", "unknown"),
                                    "node_groups_count": len(node_groups),
                                    "total_nodes": total_nodes,
                                    "fargate_profiles_count": len(fargate_profiles),
                                    "avg_cpu_utilization": round(avg_cpu_utilization, 2),
                                    "node_instance_types": list(set(node_instance_types)),
                                }

                                # Create resource entry
                                resource = AllCloudResourceData(
                                    resource_id=cluster_arn,
                                    resource_type="eks_cluster",
                                    resource_name=cluster_name,
                                    region=region,
                                    estimated_monthly_cost=monthly_cost,
                                    currency="USD",
                                    resource_metadata=metadata,
                                    created_at_cloud=created_at,
                                    is_optimizable=is_optimizable,
                                    optimization_score=optimization_score,
                                    optimization_priority=optimization_priority,
                                    potential_monthly_savings=potential_savings,
                                    optimization_recommendations=optimization_recommendations,
                                )

                                resources.append(resource)

                            except Exception as e:
                                logger.error(
                                    "eks.cluster_scan_failed",
                                    cluster_name=cluster_name,
                                    region=region,
                                    error=str(e),
                                )
                                continue

        except Exception as e:
            logger.error(
                "eks.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # ============================================================================
    # AWS - LAMBDA FUNCTION (Cost Intelligence / Inventory Mode)
    # ============================================================================

    def _calculate_lambda_monthly_cost(
        self,
        invocations_monthly: int,
        avg_duration_ms: float,
        memory_mb: int,
        provisioned_concurrency: int,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS Lambda Function.

        Cost structure:
        - Request cost: $0.20 per 1 million requests
        - Compute cost: $0.0000166667 per GB-second (us-east-1 pricing)
        - Provisioned concurrency: $0.0000041667 per GB-hour

        Args:
            invocations_monthly: Number of invocations per month
            avg_duration_ms: Average duration in milliseconds
            memory_mb: Memory allocation in MB
            provisioned_concurrency: Provisioned concurrency units
            region: AWS region

        Returns:
            Total estimated monthly cost
        """
        # Request cost
        request_cost = (invocations_monthly / 1_000_000) * 0.20

        # Compute cost
        compute_seconds = (invocations_monthly * avg_duration_ms) / 1000
        memory_gb = memory_mb / 1024
        compute_cost = compute_seconds * memory_gb * 0.0000166667

        # Provisioned concurrency cost (if configured)
        provisioned_cost = 0.0
        if provisioned_concurrency > 0:
            # $0.0000041667 per GB-hour * 730 hours/month
            provisioned_cost = provisioned_concurrency * memory_gb * 0.0000041667 * 730

        total_cost = request_cost + compute_cost + provisioned_cost

        return total_cost

    def _calculate_lambda_optimization(
        self,
        function: dict[str, Any],
        invocations_monthly: int,
        error_count: int,
        avg_duration_ms: float,
        memory_mb: int,
        timeout_seconds: int,
        provisioned_concurrency: int,
        runtime: str,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Calculate AWS Lambda Function optimization metrics.

        Optimization scenarios (5):
        1. Unused provisioned concurrency - VERY EXPENSIVE (highest priority)
        2. Never invoked - Function created but never executed
        3. 100% failures - All invocations fail (dead function)
        4. Over-provisioned memory - >50% unused memory
        5. Old/deprecated runtime - Security risk + no support

        Args:
            function: Lambda function details
            invocations_monthly: Monthly invocation count
            error_count: Number of errors in the period
            avg_duration_ms: Average duration in milliseconds
            memory_mb: Memory allocation in MB
            timeout_seconds: Function timeout in seconds
            provisioned_concurrency: Provisioned concurrency units
            runtime: Lambda runtime (python3.11, nodejs20.x, etc.)
            monthly_cost: Total monthly cost

        Returns:
            Tuple of (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations: list[dict[str, Any]] = []

        function_name = function.get("FunctionName", "Unknown")

        # Scenario 1: CRITICAL - Unused provisioned concurrency (VERY EXPENSIVE)
        if provisioned_concurrency > 0 and invocations_monthly < 100:
            is_optimizable = True
            optimization_score = 95
            priority = "critical"
            # Provisioned concurrency is typically 90% of the cost for low-traffic functions
            memory_gb = memory_mb / 1024
            provisioned_monthly_cost = provisioned_concurrency * memory_gb * 0.0000041667 * 730
            potential_savings = provisioned_monthly_cost
            recommendations.append({
                "type": "unused_provisioned",
                "severity": "critical",
                "message": (
                    f"CRITICAL: Lambda function '{function_name}' has PROVISIONED CONCURRENCY configured but very low usage (costs ${monthly_cost:.2f}/month). "
                    f"Provisioned concurrency: {provisioned_concurrency} units, Invocations: {invocations_monthly}/month. "
                    f"Provisioned concurrency costs ${provisioned_monthly_cost:.2f}/month. "
                    "This is the MOST EXPENSIVE Lambda configuration. Remove provisioned concurrency immediately."
                ),
                "impact": f"Wasting ${provisioned_monthly_cost:.2f}/month on unused provisioned capacity",
                "action": "Remove provisioned concurrency configuration",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Never invoked (function created but never used)
        if invocations_monthly == 0:
            is_optimizable = True
            optimization_score = 90
            priority = "high"
            potential_savings = monthly_cost  # All cost is waste
            recommendations.append({
                "type": "never_invoked",
                "severity": "high",
                "message": (
                    f"HIGH: Lambda function '{function_name}' has NEVER been invoked (costs ${monthly_cost:.2f}/month). "
                    "This function is not being used by any service. "
                    "Delete this function or integrate it if it was meant to be used."
                ),
                "impact": "Paying for a function that is never used",
                "action": "Delete function or integrate into application",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - 100% failures (all invocations fail)
        if invocations_monthly > 0 and error_count > 0:
            error_rate = (error_count / invocations_monthly) * 100
            if error_rate >= 95.0:
                is_optimizable = True
                optimization_score = 85
                priority = "high"
                potential_savings = monthly_cost  # Dead function is waste
                recommendations.append({
                    "type": "all_failures",
                    "severity": "high",
                    "message": (
                        f"HIGH: Lambda function '{function_name}' has {error_rate:.1f}% error rate (costs ${monthly_cost:.2f}/month). "
                        f"Out of {invocations_monthly} invocations, {error_count} failed. "
                        "This function is effectively broken. Fix errors or delete function."
                    ),
                    "impact": "Paying for a broken function that always fails",
                    "action": "Fix errors in function code or delete function",
                })
                return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Over-provisioned memory (>50% unused)
        # Estimate: If avg_duration is very low relative to timeout, memory is likely over-provisioned
        # This is a simplified heuristic (real memory usage requires custom metrics)
        if invocations_monthly > 100 and avg_duration_ms < (timeout_seconds * 1000 * 0.3):
            is_optimizable = True
            optimization_score = 65
            priority = "medium"
            # Assume 30% savings from right-sizing memory
            potential_savings = monthly_cost * 0.30
            recommendations.append({
                "type": "over_provisioned_memory",
                "severity": "medium",
                "message": (
                    f"MEDIUM: Lambda function '{function_name}' may have over-provisioned memory (costs ${monthly_cost:.2f}/month). "
                    f"Memory: {memory_mb}MB, Avg duration: {avg_duration_ms:.0f}ms, Timeout: {timeout_seconds}s. "
                    f"Function completes very quickly ({avg_duration_ms:.0f}ms) suggesting memory may be excessive. "
                    f"Test with lower memory (e.g., {int(memory_mb * 0.7)}MB) to save ${potential_savings:.2f}/month (30% reduction)."
                ),
                "impact": "Paying for unused memory capacity",
                "action": f"Reduce memory allocation from {memory_mb}MB to {int(memory_mb * 0.7)}MB",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: MEDIUM - Old/deprecated runtime (security risk)
        deprecated_runtimes = [
            "python2.7", "python3.6", "python3.7",
            "nodejs", "nodejs4.3", "nodejs6.10", "nodejs8.10", "nodejs10.x", "nodejs12.x", "nodejs14.x",
            "ruby2.5", "ruby2.7",
            "java8",
            "dotnetcore2.0", "dotnetcore2.1", "dotnetcore3.1",
            "go1.x"
        ]
        if runtime in deprecated_runtimes:
            is_optimizable = True
            optimization_score = 60
            priority = "medium"
            potential_savings = 0.0  # No direct cost savings, but security risk
            recommendations.append({
                "type": "old_runtime",
                "severity": "medium",
                "message": (
                    f"MEDIUM: Lambda function '{function_name}' uses DEPRECATED runtime '{runtime}' (costs ${monthly_cost:.2f}/month). "
                    "This runtime is no longer supported by AWS and has known security vulnerabilities. "
                    "Upgrade to latest runtime: python3.11+, nodejs20.x+, java17+, etc."
                ),
                "impact": "Security vulnerabilities, no AWS support, future breaking changes",
                "action": f"Upgrade runtime from '{runtime}' to latest version",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # No optimization opportunities found
        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_lambda_functions(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS Lambda Functions for cost intelligence.

        This method scans Lambda functions to provide cost visibility and optimization
        recommendations. Unlike orphan detection, this scans ALL functions regardless
        of usage patterns.

        Args:
            region: AWS region to scan

        Returns:
            List of AllCloudResourceData objects for all Lambda functions
        """
        resources: list[AllCloudResourceData] = []

        try:
            async with self.session.client("lambda", region_name=region) as lambda_client:
                async with self.session.client("cloudwatch", region_name=region) as cloudwatch:
                    # List all Lambda functions
                    paginator = lambda_client.get_paginator("list_functions")
                    async for page in paginator.paginate():
                        functions = page.get("Functions", [])

                        for function in functions:
                            try:
                                function_name = function.get("FunctionName", "")
                                function_arn = function.get("FunctionArn", "")
                                runtime = function.get("Runtime", "unknown")
                                memory_mb = function.get("MemorySize", 128)
                                timeout_seconds = function.get("Timeout", 3)

                                # Get last modified date
                                last_modified = function.get("LastModified", "")
                                try:
                                    created_at = datetime.fromisoformat(last_modified.replace("Z", "+00:00"))
                                except Exception:
                                    created_at = None

                                # Check for provisioned concurrency
                                provisioned_concurrency = 0
                                try:
                                    pc_response = await lambda_client.list_provisioned_concurrency_configs(
                                        FunctionName=function_name
                                    )
                                    configs = pc_response.get("ProvisionedConcurrencyConfigs", [])
                                    if configs:
                                        # Sum all provisioned concurrency
                                        provisioned_concurrency = sum(c.get("AllocatedConcurrentExecutions", 0) for c in configs)
                                except Exception:
                                    pass

                                # Get CloudWatch metrics (last 30 days)
                                end_time = datetime.utcnow()
                                start_time = end_time - timedelta(days=30)

                                # Get invocations count
                                invocations_monthly = 0
                                try:
                                    invocations_response = await cloudwatch.get_metric_statistics(
                                        Namespace="AWS/Lambda",
                                        MetricName="Invocations",
                                        Dimensions=[
                                            {"Name": "FunctionName", "Value": function_name},
                                        ],
                                        StartTime=start_time,
                                        EndTime=end_time,
                                        Period=2592000,  # 30 days
                                        Statistics=["Sum"],
                                    )

                                    datapoints = invocations_response.get("Datapoints", [])
                                    if datapoints:
                                        invocations_monthly = int(datapoints[0].get("Sum", 0))
                                except Exception as e:
                                    logger.warning(
                                        "lambda.invocations_metrics_failed",
                                        function_name=function_name,
                                        error=str(e),
                                    )

                                # Get error count
                                error_count = 0
                                try:
                                    errors_response = await cloudwatch.get_metric_statistics(
                                        Namespace="AWS/Lambda",
                                        MetricName="Errors",
                                        Dimensions=[
                                            {"Name": "FunctionName", "Value": function_name},
                                        ],
                                        StartTime=start_time,
                                        EndTime=end_time,
                                        Period=2592000,  # 30 days
                                        Statistics=["Sum"],
                                    )

                                    datapoints = errors_response.get("Datapoints", [])
                                    if datapoints:
                                        error_count = int(datapoints[0].get("Sum", 0))
                                except Exception:
                                    pass

                                # Get average duration
                                avg_duration_ms = 0.0
                                try:
                                    duration_response = await cloudwatch.get_metric_statistics(
                                        Namespace="AWS/Lambda",
                                        MetricName="Duration",
                                        Dimensions=[
                                            {"Name": "FunctionName", "Value": function_name},
                                        ],
                                        StartTime=start_time,
                                        EndTime=end_time,
                                        Period=2592000,  # 30 days
                                        Statistics=["Average"],
                                    )

                                    datapoints = duration_response.get("Datapoints", [])
                                    if datapoints:
                                        avg_duration_ms = datapoints[0].get("Average", 0.0)
                                except Exception:
                                    # Fallback to a reasonable default
                                    avg_duration_ms = 100.0

                                # Calculate monthly cost
                                monthly_cost = self._calculate_lambda_monthly_cost(
                                    invocations_monthly=invocations_monthly,
                                    avg_duration_ms=avg_duration_ms,
                                    memory_mb=memory_mb,
                                    provisioned_concurrency=provisioned_concurrency,
                                    region=region,
                                )

                                # Calculate optimization metrics
                                (
                                    is_optimizable,
                                    optimization_score,
                                    optimization_priority,
                                    potential_savings,
                                    optimization_recommendations,
                                ) = self._calculate_lambda_optimization(
                                    function=function,
                                    invocations_monthly=invocations_monthly,
                                    error_count=error_count,
                                    avg_duration_ms=avg_duration_ms,
                                    memory_mb=memory_mb,
                                    timeout_seconds=timeout_seconds,
                                    provisioned_concurrency=provisioned_concurrency,
                                    runtime=runtime,
                                    monthly_cost=monthly_cost,
                                )

                                # Build metadata
                                metadata = {
                                    "function_name": function_name,
                                    "function_arn": function_arn,
                                    "runtime": runtime,
                                    "memory_mb": memory_mb,
                                    "timeout_seconds": timeout_seconds,
                                    "invocations_monthly": invocations_monthly,
                                    "error_count": error_count,
                                    "error_rate": round((error_count / invocations_monthly * 100) if invocations_monthly > 0 else 0.0, 2),
                                    "avg_duration_ms": round(avg_duration_ms, 2),
                                    "provisioned_concurrency": provisioned_concurrency,
                                }

                                # Create resource entry
                                resource = AllCloudResourceData(
                                    resource_id=function_arn,
                                    resource_type="lambda_function",
                                    resource_name=function_name,
                                    region=region,
                                    estimated_monthly_cost=monthly_cost,
                                    currency="USD",
                                    resource_metadata=metadata,
                                    created_at_cloud=created_at,
                                    is_optimizable=is_optimizable,
                                    optimization_score=optimization_score,
                                    optimization_priority=optimization_priority,
                                    potential_monthly_savings=potential_savings,
                                    optimization_recommendations=optimization_recommendations,
                                )

                                resources.append(resource)

                            except Exception as e:
                                logger.error(
                                    "lambda.function_scan_failed",
                                    function_name=function.get("FunctionName", "Unknown"),
                                    region=region,
                                    error=str(e),
                                )
                                continue

        except Exception as e:
            logger.error(
                "lambda.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # ============================================================================
    # AWS - DYNAMODB TABLE (Cost Intelligence / Inventory Mode)
    # ============================================================================

    def _calculate_dynamodb_monthly_cost(
        self,
        billing_mode: str,
        read_capacity: int,
        write_capacity: int,
        storage_gb: float,
        gsi_count: int,
        gsi_read_capacity: int,
        gsi_write_capacity: int,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS DynamoDB Table.

        Cost structure (us-east-1 pricing):

        Provisioned mode:
        - Read Capacity Unit (RCU): $0.00065/hour = $0.47/month per RCU
        - Write Capacity Unit (WCU): $0.00013/hour = $0.095/month per WCU
        - Storage: $0.25/GB-month
        - GSI: Same pricing as base table (doubles cost if same capacity)

        On-Demand mode:
        - Write Request Unit (WRU): $1.25 per million writes
        - Read Request Unit (RRU): $0.25 per million reads
        - Storage: $0.25/GB-month
        - GSI: Same pricing as base table

        Args:
            billing_mode: "PROVISIONED" or "PAY_PER_REQUEST" (On-Demand)
            read_capacity: Provisioned read capacity units (0 for On-Demand)
            write_capacity: Provisioned write capacity units (0 for On-Demand)
            storage_gb: Table size in GB
            gsi_count: Number of Global Secondary Indexes
            gsi_read_capacity: Total GSI read capacity (Provisioned only)
            gsi_write_capacity: Total GSI write capacity (Provisioned only)
            region: AWS region

        Returns:
            Total estimated monthly cost
        """
        storage_cost = storage_gb * 0.25  # $0.25/GB-month

        if billing_mode == "PROVISIONED":
            # Base table RCU/WCU costs
            base_read_cost = read_capacity * 0.00065 * 730  # $0.00065/hour * 730 hours
            base_write_cost = write_capacity * 0.00013 * 730  # $0.00013/hour * 730 hours

            # GSI RCU/WCU costs (same pricing as base table)
            gsi_read_cost = gsi_read_capacity * 0.00065 * 730
            gsi_write_cost = gsi_write_capacity * 0.00013 * 730

            total_cost = base_read_cost + base_write_cost + gsi_read_cost + gsi_write_cost + storage_cost
        else:  # PAY_PER_REQUEST (On-Demand)
            # For On-Demand, we don't know the actual request count without CloudWatch metrics
            # Return storage cost only (actual cost calculated later with metrics)
            total_cost = storage_cost

        return total_cost

    def _calculate_dynamodb_optimization(
        self,
        table: dict[str, Any],
        billing_mode: str,
        read_capacity: int,
        write_capacity: int,
        consumed_read_capacity: float,
        consumed_write_capacity: float,
        gsi_count: int,
        gsi_read_capacity: int,
        gsi_write_capacity: int,
        item_count: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Calculate AWS DynamoDB Table optimization metrics.

        Optimization scenarios (5):
        1. Over-provisioned capacity - <10% utilization (VERY EXPENSIVE)
        2. Unused Global Secondary Indexes - GSI never queried (doubles cost)
        3. Never used tables (Provisioned) - 0 usage since creation
        4. Never used tables (On-Demand) - 0 usage in 60 days
        5. Empty tables - 0 items for 90+ days

        Args:
            table: DynamoDB table details
            billing_mode: "PROVISIONED" or "PAY_PER_REQUEST"
            read_capacity: Provisioned read capacity units
            write_capacity: Provisioned write capacity units
            consumed_read_capacity: Actual consumed RCU (from CloudWatch)
            consumed_write_capacity: Actual consumed WCU (from CloudWatch)
            gsi_count: Number of GSIs
            gsi_read_capacity: Total GSI read capacity
            gsi_write_capacity: Total GSI write capacity
            item_count: Number of items in table
            monthly_cost: Total monthly cost

        Returns:
            Tuple of (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations: list[dict[str, Any]] = []

        table_name = table.get("TableName", "Unknown")

        # Scenario 1: CRITICAL - Over-provisioned capacity (<10% utilization)
        if billing_mode == "PROVISIONED":
            # Calculate utilization percentages
            read_utilization = (consumed_read_capacity / read_capacity * 100) if read_capacity > 0 else 0.0
            write_utilization = (consumed_write_capacity / write_capacity * 100) if write_capacity > 0 else 0.0
            avg_utilization = (read_utilization + write_utilization) / 2

            if avg_utilization < 10.0 and (read_capacity > 0 or write_capacity > 0):
                is_optimizable = True
                optimization_score = 95
                priority = "critical"
                # Assume 70% savings from right-sizing to actual usage
                potential_savings = monthly_cost * 0.70
                recommendations.append({
                    "type": "over_provisioned",
                    "severity": "critical",
                    "message": (
                        f"CRITICAL: DynamoDB table '{table_name}' has VERY LOW capacity utilization (costs ${monthly_cost:.2f}/month). "
                        f"Read: {read_utilization:.1f}% ({consumed_read_capacity:.0f}/{read_capacity} RCU), "
                        f"Write: {write_utilization:.1f}% ({consumed_write_capacity:.0f}/{write_capacity} WCU). "
                        f"You're wasting {100 - avg_utilization:.1f}% of provisioned capacity. "
                        f"Right-size to actual usage or switch to On-Demand mode. Potential savings: ${potential_savings:.2f}/month (70% reduction)."
                    ),
                    "impact": f"Wasting {100 - avg_utilization:.1f}% of provisioned capacity",
                    "action": f"Reduce capacity to {int(consumed_read_capacity * 1.5)} RCU / {int(consumed_write_capacity * 1.5)} WCU or switch to On-Demand",
                })
                return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Unused Global Secondary Indexes (doubles cost)
        if gsi_count > 0 and (gsi_read_capacity > 0 or gsi_write_capacity > 0):
            is_optimizable = True
            optimization_score = 85
            priority = "high"
            # GSI cost is typically 50% of total cost for tables with GSIs
            gsi_monthly_cost = monthly_cost * 0.50
            potential_savings = gsi_monthly_cost
            recommendations.append({
                "type": "unused_gsi",
                "severity": "high",
                "message": (
                    f"HIGH: DynamoDB table '{table_name}' has {gsi_count} Global Secondary Indexes (costs ${monthly_cost:.2f}/month). "
                    f"GSIs double your costs ({gsi_count} GSIs = ~${gsi_monthly_cost:.2f}/month). "
                    f"Verify all GSIs are actively used in queries. Delete unused GSIs immediately. "
                    f"Each unused GSI wastes ~${gsi_monthly_cost / gsi_count:.2f}/month."
                ),
                "impact": f"GSIs cost ${gsi_monthly_cost:.2f}/month - verify usage",
                "action": f"Delete unused GSIs (check CloudWatch GSI metrics)",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - Never used tables (Provisioned mode)
        if billing_mode == "PROVISIONED" and consumed_read_capacity == 0 and consumed_write_capacity == 0:
            is_optimizable = True
            optimization_score = 90
            priority = "high"
            potential_savings = monthly_cost  # All cost is waste
            recommendations.append({
                "type": "never_used_provisioned",
                "severity": "high",
                "message": (
                    f"HIGH: DynamoDB table '{table_name}' in PROVISIONED mode has NEVER been used (costs ${monthly_cost:.2f}/month). "
                    f"0 reads and 0 writes detected. "
                    "This table is not integrated with any application. Delete table or integrate if needed."
                ),
                "impact": "Paying for provisioned capacity with zero usage",
                "action": "Delete table or integrate into application",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: HIGH - Never used tables (On-Demand mode)
        if billing_mode == "PAY_PER_REQUEST" and consumed_read_capacity == 0 and consumed_write_capacity == 0:
            is_optimizable = True
            optimization_score = 85
            priority = "high"
            potential_savings = monthly_cost  # Storage cost is waste
            recommendations.append({
                "type": "never_used_ondemand",
                "severity": "high",
                "message": (
                    f"HIGH: DynamoDB table '{table_name}' in ON-DEMAND mode has NO usage (costs ${monthly_cost:.2f}/month). "
                    f"0 reads and 0 writes in last 60 days. "
                    "This table is not being accessed. Delete table if no longer needed."
                ),
                "impact": f"Paying storage cost (${monthly_cost:.2f}/month) with no access",
                "action": "Delete table or verify integration",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 5: MEDIUM - Empty tables (0 items for 90+ days)
        if item_count == 0:
            is_optimizable = True
            optimization_score = 75
            priority = "medium"
            potential_savings = monthly_cost  # Empty table is waste
            recommendations.append({
                "type": "empty_table",
                "severity": "medium",
                "message": (
                    f"MEDIUM: DynamoDB table '{table_name}' is EMPTY (costs ${monthly_cost:.2f}/month). "
                    f"Table has 0 items. "
                    "Empty table has been idle for extended period. Delete if no longer needed."
                ),
                "impact": f"Paying ${monthly_cost:.2f}/month for empty table",
                "action": "Delete table if no longer needed",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # No optimization opportunities found
        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_dynamodb_tables(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS DynamoDB Tables for cost intelligence.

        This method scans DynamoDB tables to provide cost visibility and optimization
        recommendations. Unlike orphan detection, this scans ALL tables regardless
        of usage patterns.

        Args:
            region: AWS region to scan

        Returns:
            List of AllCloudResourceData objects for all DynamoDB tables
        """
        resources: list[AllCloudResourceData] = []

        try:
            async with self.session.client("dynamodb", region_name=region) as dynamodb:
                async with self.session.client("cloudwatch", region_name=region) as cloudwatch:
                    # List all tables
                    paginator = dynamodb.get_paginator("list_tables")
                    async for page in paginator.paginate():
                        table_names = page.get("TableNames", [])

                        for table_name in table_names:
                            try:
                                # Get table details
                                response = await dynamodb.describe_table(TableName=table_name)
                                table = response["Table"]

                                table_arn = table.get("TableArn", "")
                                billing_mode_summary = table.get("BillingModeSummary", {})
                                billing_mode = billing_mode_summary.get("BillingMode", "PROVISIONED")

                                # Get creation date
                                created_at = table.get("CreationDateTime")

                                # Get provisioned throughput (for Provisioned mode)
                                provisioned_throughput = table.get("ProvisionedThroughput", {})
                                read_capacity = provisioned_throughput.get("ReadCapacityUnits", 0)
                                write_capacity = provisioned_throughput.get("WriteCapacityUnits", 0)

                                # Get table size and item count
                                storage_gb = table.get("TableSizeBytes", 0) / (1024 ** 3)  # Convert bytes to GB
                                item_count = table.get("ItemCount", 0)

                                # Get GSI information
                                global_secondary_indexes = table.get("GlobalSecondaryIndexes", [])
                                gsi_count = len(global_secondary_indexes)
                                gsi_read_capacity = 0
                                gsi_write_capacity = 0

                                for gsi in global_secondary_indexes:
                                    gsi_throughput = gsi.get("ProvisionedThroughput", {})
                                    gsi_read_capacity += gsi_throughput.get("ReadCapacityUnits", 0)
                                    gsi_write_capacity += gsi_throughput.get("WriteCapacityUnits", 0)

                                # Get CloudWatch metrics (last 14 days)
                                end_time = datetime.utcnow()
                                start_time = end_time - timedelta(days=14)

                                # Get consumed read capacity
                                consumed_read_capacity = 0.0
                                try:
                                    read_response = await cloudwatch.get_metric_statistics(
                                        Namespace="AWS/DynamoDB",
                                        MetricName="ConsumedReadCapacityUnits",
                                        Dimensions=[
                                            {"Name": "TableName", "Value": table_name},
                                        ],
                                        StartTime=start_time,
                                        EndTime=end_time,
                                        Period=1209600,  # 14 days
                                        Statistics=["Average"],
                                    )

                                    datapoints = read_response.get("Datapoints", [])
                                    if datapoints:
                                        consumed_read_capacity = datapoints[0].get("Average", 0.0)
                                except Exception:
                                    pass

                                # Get consumed write capacity
                                consumed_write_capacity = 0.0
                                try:
                                    write_response = await cloudwatch.get_metric_statistics(
                                        Namespace="AWS/DynamoDB",
                                        MetricName="ConsumedWriteCapacityUnits",
                                        Dimensions=[
                                            {"Name": "TableName", "Value": table_name},
                                        ],
                                        StartTime=start_time,
                                        EndTime=end_time,
                                        Period=1209600,  # 14 days
                                        Statistics=["Average"],
                                    )

                                    datapoints = write_response.get("Datapoints", [])
                                    if datapoints:
                                        consumed_write_capacity = datapoints[0].get("Average", 0.0)
                                except Exception:
                                    pass

                                # Calculate monthly cost
                                monthly_cost = self._calculate_dynamodb_monthly_cost(
                                    billing_mode=billing_mode,
                                    read_capacity=read_capacity,
                                    write_capacity=write_capacity,
                                    storage_gb=storage_gb,
                                    gsi_count=gsi_count,
                                    gsi_read_capacity=gsi_read_capacity,
                                    gsi_write_capacity=gsi_write_capacity,
                                    region=region,
                                )

                                # Calculate optimization metrics
                                (
                                    is_optimizable,
                                    optimization_score,
                                    optimization_priority,
                                    potential_savings,
                                    optimization_recommendations,
                                ) = self._calculate_dynamodb_optimization(
                                    table=table,
                                    billing_mode=billing_mode,
                                    read_capacity=read_capacity,
                                    write_capacity=write_capacity,
                                    consumed_read_capacity=consumed_read_capacity,
                                    consumed_write_capacity=consumed_write_capacity,
                                    gsi_count=gsi_count,
                                    gsi_read_capacity=gsi_read_capacity,
                                    gsi_write_capacity=gsi_write_capacity,
                                    item_count=item_count,
                                    monthly_cost=monthly_cost,
                                )

                                # Build metadata
                                metadata = {
                                    "table_name": table_name,
                                    "table_arn": table_arn,
                                    "billing_mode": billing_mode,
                                    "read_capacity": read_capacity,
                                    "write_capacity": write_capacity,
                                    "consumed_read_capacity": round(consumed_read_capacity, 2),
                                    "consumed_write_capacity": round(consumed_write_capacity, 2),
                                    "storage_gb": round(storage_gb, 2),
                                    "item_count": item_count,
                                    "gsi_count": gsi_count,
                                    "gsi_read_capacity": gsi_read_capacity,
                                    "gsi_write_capacity": gsi_write_capacity,
                                }

                                # Create resource entry
                                resource = AllCloudResourceData(
                                    resource_id=table_arn,
                                    resource_type="dynamodb_table",
                                    resource_name=table_name,
                                    region=region,
                                    estimated_monthly_cost=monthly_cost,
                                    currency="USD",
                                    resource_metadata=metadata,
                                    created_at_cloud=created_at,
                                    is_optimizable=is_optimizable,
                                    optimization_score=optimization_score,
                                    optimization_priority=optimization_priority,
                                    potential_monthly_savings=potential_savings,
                                    optimization_recommendations=optimization_recommendations,
                                )

                                resources.append(resource)

                            except Exception as e:
                                logger.error(
                                    "dynamodb.table_scan_failed",
                                    table_name=table_name,
                                    region=region,
                                    error=str(e),
                                )
                                continue

        except Exception as e:
            logger.error(
                "dynamodb.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # ============================================================================
    # AWS - FARGATE TASK (Cost Intelligence / Inventory Mode)
    # ============================================================================

    def _calculate_fargate_monthly_cost(
        self,
        vcpu: float,
        memory_gb: float,
        hours_running: float,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS Fargate Task.

        Cost structure (us-east-1 pricing, Linux x86_64):
        - vCPU: $0.04048 per vCPU-hour
        - Memory: $0.004445 per GB-hour

        Args:
            vcpu: Number of vCPUs (e.g., 0.25, 0.5, 1, 2, 4)
            memory_gb: Memory in GB (e.g., 0.5, 1, 2, 4, 8, 16)
            hours_running: Hours running in the month (0-730)
            region: AWS region

        Returns:
            Total estimated monthly cost
        """
        vcpu_cost = vcpu * 0.04048 * hours_running
        memory_cost = memory_gb * 0.004445 * hours_running

        total_cost = vcpu_cost + memory_cost

        return total_cost

    def _calculate_fargate_optimization(
        self,
        task: dict[str, Any],
        task_definition: dict[str, Any],
        last_status: str,
        avg_cpu_utilization: float,
        avg_memory_utilization: float,
        hours_running_monthly: float,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Calculate AWS Fargate Task optimization metrics.

        Optimization scenarios (4):
        1. Stopped tasks - Task stopped but still allocated (critical)
        2. Low CPU utilization - CPU <10% (high)
        3. EC2 would be cheaper - Long-running tasks (medium)
        4. Over-provisioned vCPU/memory - Underutilized (low)

        Args:
            task: ECS task details
            task_definition: Task definition details
            last_status: Task status (RUNNING, STOPPED, etc.)
            avg_cpu_utilization: Average CPU utilization percentage
            avg_memory_utilization: Average memory utilization percentage
            hours_running_monthly: Hours running in the month
            monthly_cost: Total monthly cost

        Returns:
            Tuple of (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations: list[dict[str, Any]] = []

        task_arn = task.get("taskArn", "")
        task_id = task_arn.split("/")[-1][:8] if task_arn else "Unknown"

        # Extract vCPU and memory from task definition
        vcpu = float(task_definition.get("cpu", "256")) / 1024  # Convert CPU units to vCPU
        memory_gb = float(task_definition.get("memory", "512")) / 1024  # Convert MB to GB

        # Scenario 1: CRITICAL - Stopped tasks (allocated but not running)
        if last_status != "RUNNING" and hours_running_monthly > 0:
            is_optimizable = True
            optimization_score = 95
            priority = "critical"
            potential_savings = monthly_cost  # All cost is waste
            recommendations.append({
                "type": "stopped_task",
                "severity": "critical",
                "message": (
                    f"CRITICAL: Fargate task '{task_id}' is STOPPED but still allocated (costs ${monthly_cost:.2f}/month). "
                    f"Status: {last_status}. "
                    "Stopped tasks should not be allocated. Stop or delete this task immediately."
                ),
                "impact": "Paying for stopped task capacity",
                "action": "Stop or delete Fargate task",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Low CPU utilization (CPU <10%)
        if avg_cpu_utilization > 0 and avg_cpu_utilization < 10.0:
            is_optimizable = True
            optimization_score = 85
            priority = "high"
            # Assume 40% savings from right-sizing CPU
            potential_savings = monthly_cost * 0.40
            recommendations.append({
                "type": "low_cpu",
                "severity": "high",
                "message": (
                    f"HIGH: Fargate task '{task_id}' has VERY LOW CPU utilization (costs ${monthly_cost:.2f}/month). "
                    f"CPU: {avg_cpu_utilization:.1f}% (vCPU: {vcpu}). "
                    f"Task is using only {avg_cpu_utilization:.1f}% of allocated CPU. "
                    f"Reduce vCPU allocation to save ${potential_savings:.2f}/month (40% reduction)."
                ),
                "impact": f"Wasting {100 - avg_cpu_utilization:.1f}% of CPU capacity",
                "action": f"Reduce vCPU from {vcpu} to {vcpu * 0.5:.2f}",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: MEDIUM - EC2 would be cheaper (long-running tasks)
        # Fargate break-even is ~10-15 pods; for long-running tasks, EC2 is cheaper
        if hours_running_monthly >= 720:  # Running 24/7
            is_optimizable = True
            optimization_score = 70
            priority = "medium"
            # EC2 is typically 30% cheaper for 24/7 workloads
            potential_savings = monthly_cost * 0.30
            recommendations.append({
                "type": "ec2_cheaper",
                "severity": "medium",
                "message": (
                    f"MEDIUM: Fargate task '{task_id}' runs 24/7 (costs ${monthly_cost:.2f}/month). "
                    f"Running {hours_running_monthly:.0f} hours/month. "
                    "Fargate is optimized for sporadic/burst workloads. "
                    f"For 24/7 workloads, EC2 instances are 30% cheaper. Consider migrating to ECS on EC2 to save ${potential_savings:.2f}/month."
                ),
                "impact": "Fargate is 30% more expensive than EC2 for 24/7 workloads",
                "action": "Migrate to ECS on EC2 instances",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: LOW - Over-provisioned vCPU/memory (underutilized)
        if avg_cpu_utilization > 0 and avg_cpu_utilization < 30.0 and avg_memory_utilization < 30.0:
            is_optimizable = True
            optimization_score = 50
            priority = "low"
            # Assume 20% savings from right-sizing
            potential_savings = monthly_cost * 0.20
            recommendations.append({
                "type": "over_provisioned",
                "severity": "low",
                "message": (
                    f"LOW: Fargate task '{task_id}' has low CPU and memory utilization (costs ${monthly_cost:.2f}/month). "
                    f"CPU: {avg_cpu_utilization:.1f}%, Memory: {avg_memory_utilization:.1f}%. "
                    f"Current: {vcpu} vCPU, {memory_gb:.2f} GB memory. "
                    f"Right-size to {vcpu * 0.8:.2f} vCPU / {memory_gb * 0.8:.2f} GB to save ${potential_savings:.2f}/month (20% reduction)."
                ),
                "impact": "Underutilized CPU and memory resources",
                "action": f"Reduce to {vcpu * 0.8:.2f} vCPU / {memory_gb * 0.8:.2f} GB",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # No optimization opportunities found
        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_fargate_tasks(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS Fargate Tasks for cost intelligence.

        This method scans Fargate tasks running in ECS clusters to provide cost
        visibility and optimization recommendations. Unlike orphan detection, this
        scans ALL tasks regardless of usage patterns.

        Args:
            region: AWS region to scan

        Returns:
            List of AllCloudResourceData objects for all Fargate tasks
        """
        resources: list[AllCloudResourceData] = []

        try:
            async with self.session.client("ecs", region_name=region) as ecs:
                async with self.session.client("cloudwatch", region_name=region) as cloudwatch:
                    # List all ECS clusters
                    clusters_response = await ecs.list_clusters()
                    cluster_arns = clusters_response.get("clusterArns", [])

                    for cluster_arn in cluster_arns:
                        try:
                            # List tasks in cluster with Fargate launch type
                            tasks_response = await ecs.list_tasks(
                                cluster=cluster_arn,
                                launchType="FARGATE",
                            )
                            task_arns = tasks_response.get("taskArns", [])

                            if not task_arns:
                                continue

                            # Describe tasks
                            described_tasks_response = await ecs.describe_tasks(
                                cluster=cluster_arn,
                                tasks=task_arns,
                            )
                            tasks = described_tasks_response.get("tasks", [])

                            for task in tasks:
                                try:
                                    task_arn = task.get("taskArn", "")
                                    task_definition_arn = task.get("taskDefinitionArn", "")
                                    last_status = task.get("lastStatus", "UNKNOWN")
                                    created_at = task.get("createdAt")

                                    # Get task definition details
                                    task_def_response = await ecs.describe_task_definition(
                                        taskDefinition=task_definition_arn
                                    )
                                    task_definition = task_def_response["taskDefinition"]

                                    # Extract vCPU and memory
                                    cpu_str = task_definition.get("cpu", "256")  # CPU units
                                    memory_str = task_definition.get("memory", "512")  # MB
                                    vcpu = float(cpu_str) / 1024  # Convert to vCPU
                                    memory_gb = float(memory_str) / 1024  # Convert to GB

                                    # Calculate hours running (estimate from creation time)
                                    hours_running_monthly = 0.0
                                    if created_at:
                                        age = datetime.utcnow() - created_at.replace(tzinfo=None)
                                        hours_running_monthly = min(age.total_seconds() / 3600, 730)

                                    # Get CloudWatch CPU metrics (last 7 days)
                                    avg_cpu_utilization = 0.0
                                    avg_memory_utilization = 0.0

                                    if last_status == "RUNNING":
                                        try:
                                            end_time = datetime.utcnow()
                                            start_time = end_time - timedelta(days=7)

                                            # Get CPU utilization
                                            cpu_response = await cloudwatch.get_metric_statistics(
                                                Namespace="ECS/ContainerInsights",
                                                MetricName="CpuUtilized",
                                                Dimensions=[
                                                    {"Name": "ClusterName", "Value": cluster_arn.split("/")[-1]},
                                                    {"Name": "TaskId", "Value": task_arn.split("/")[-1]},
                                                ],
                                                StartTime=start_time,
                                                EndTime=end_time,
                                                Period=604800,  # 7 days
                                                Statistics=["Average"],
                                            )

                                            datapoints = cpu_response.get("Datapoints", [])
                                            if datapoints:
                                                avg_cpu_utilization = datapoints[0].get("Average", 0.0)
                                        except Exception:
                                            pass

                                        try:
                                            # Get Memory utilization
                                            memory_response = await cloudwatch.get_metric_statistics(
                                                Namespace="ECS/ContainerInsights",
                                                MetricName="MemoryUtilized",
                                                Dimensions=[
                                                    {"Name": "ClusterName", "Value": cluster_arn.split("/")[-1]},
                                                    {"Name": "TaskId", "Value": task_arn.split("/")[-1]},
                                                ],
                                                StartTime=start_time,
                                                EndTime=end_time,
                                                Period=604800,  # 7 days
                                                Statistics=["Average"],
                                            )

                                            datapoints = memory_response.get("Datapoints", [])
                                            if datapoints:
                                                avg_memory_utilization = datapoints[0].get("Average", 0.0)
                                        except Exception:
                                            pass

                                    # Calculate monthly cost
                                    monthly_cost = self._calculate_fargate_monthly_cost(
                                        vcpu=vcpu,
                                        memory_gb=memory_gb,
                                        hours_running=hours_running_monthly,
                                        region=region,
                                    )

                                    # Calculate optimization metrics
                                    (
                                        is_optimizable,
                                        optimization_score,
                                        optimization_priority,
                                        potential_savings,
                                        optimization_recommendations,
                                    ) = self._calculate_fargate_optimization(
                                        task=task,
                                        task_definition=task_definition,
                                        last_status=last_status,
                                        avg_cpu_utilization=avg_cpu_utilization,
                                        avg_memory_utilization=avg_memory_utilization,
                                        hours_running_monthly=hours_running_monthly,
                                        monthly_cost=monthly_cost,
                                    )

                                    # Build metadata
                                    task_id = task_arn.split("/")[-1] if task_arn else "unknown"
                                    metadata = {
                                        "task_id": task_id,
                                        "task_arn": task_arn,
                                        "cluster_arn": cluster_arn,
                                        "last_status": last_status,
                                        "vcpu": vcpu,
                                        "memory_gb": memory_gb,
                                        "hours_running_monthly": round(hours_running_monthly, 2),
                                        "avg_cpu_utilization": round(avg_cpu_utilization, 2),
                                        "avg_memory_utilization": round(avg_memory_utilization, 2),
                                    }

                                    # Apply detection rules filtering
                                    resource_age_days = None
                                    if created_at:
                                        resource_age_days = (datetime.utcnow() - created_at.replace(tzinfo=None)).days

                                    if not self._should_include_resource("fargate_task", resource_age_days):
                                        logger.debug("inventory.fargate_filtered", task_id=task_id, age=resource_age_days)
                                        continue

                                    # Create resource entry
                                    resource = AllCloudResourceData(
                                        resource_id=task_arn,
                                        resource_type="fargate_task",
                                        resource_name=f"fargate-{task_id}",
                                        region=region,
                                        estimated_monthly_cost=monthly_cost,
                                        currency="USD",
                                        resource_metadata=metadata,
                                        created_at_cloud=created_at,
                                        is_optimizable=is_optimizable,
                                        optimization_score=optimization_score,
                                        optimization_priority=optimization_priority,
                                        potential_monthly_savings=potential_savings,
                                        optimization_recommendations=optimization_recommendations,
                                    )

                                    resources.append(resource)

                                except Exception as e:
                                    logger.error(
                                        "fargate.task_scan_failed",
                                        task_arn=task.get("taskArn", "Unknown"),
                                        cluster_arn=cluster_arn,
                                        region=region,
                                        error=str(e),
                                    )
                                    continue

                        except Exception as e:
                            logger.error(
                                "fargate.cluster_scan_failed",
                                cluster_arn=cluster_arn,
                                region=region,
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "fargate.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # ============================================================================
    # AWS - ELASTICACHE CLUSTER (Cost Intelligence / Inventory Mode)
    # ============================================================================

    def _calculate_elasticache_monthly_cost(
        self,
        node_type: str,
        num_nodes: int,
        engine: str,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS ElastiCache Cluster.

        Cost structure (us-east-1 pricing):
        - Node cost varies by type (t3.micro to r6g.xlarge)
        - Charged per node per hour
        - Redis and Memcached same pricing

        Args:
            node_type: Node type (e.g., cache.t3.micro, cache.m5.large)
            num_nodes: Number of cache nodes
            engine: redis or memcached
            region: AWS region

        Returns:
            Total estimated monthly cost
        """
        # Map node_type to pricing key
        # cache.t3.micro -> elasticache_t3_micro
        pricing_key = f"elasticache_{node_type.replace('cache.', '').replace('.', '_')}"

        # Get hourly cost from pricing dict, fallback to m5.large if not found
        hourly_cost = self.PRICING.get(pricing_key, 0.126)

        # Calculate monthly cost (730 hours/month)
        monthly_cost = hourly_cost * 730 * num_nodes

        return monthly_cost

    def _calculate_elasticache_optimization(
        self,
        cluster: dict[str, Any],
        cache_hits: float,
        cache_misses: float,
        curr_connections: float,
        memory_usage_percent: float,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Calculate AWS ElastiCache Cluster optimization metrics.

        Optimization scenarios (4):
        1. Zero cache hits - No activity (critical)
        2. Low hit rate - <50% hits, cache inefficient (high)
        3. No connections - Nobody connects (high)
        4. Over-provisioned memory - <20% memory used (medium)

        Args:
            cluster: ElastiCache cluster details
            cache_hits: Total cache hits in period
            cache_misses: Total cache misses in period
            curr_connections: Current connections count
            memory_usage_percent: Memory usage percentage
            monthly_cost: Total monthly cost

        Returns:
            Tuple of (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "none"
        potential_savings = 0.0
        recommendations: list[dict[str, Any]] = []

        cluster_id = cluster.get("CacheClusterId", "Unknown")

        # Scenario 1: CRITICAL - Zero cache hits (no activity)
        if cache_hits == 0.0 and cache_misses == 0.0:
            is_optimizable = True
            optimization_score = 95
            priority = "critical"
            potential_savings = monthly_cost  # All cost is waste
            recommendations.append({
                "type": "zero_cache_hits",
                "severity": "critical",
                "message": (
                    f"CRITICAL: ElastiCache cluster '{cluster_id}' has ZERO cache activity (costs ${monthly_cost:.2f}/month). "
                    "0 cache hits and 0 cache misses detected. "
                    "This cluster is not being used by any application. Delete cluster or integrate if needed."
                ),
                "impact": "Paying for a cache that is never accessed",
                "action": "Delete cluster or integrate into application",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 2: HIGH - Low hit rate (<50% hits, cache inefficient)
        total_requests = cache_hits + cache_misses
        if total_requests > 0:
            hit_rate = (cache_hits / total_requests) * 100
            if hit_rate < 50.0:
                is_optimizable = True
                optimization_score = 85
                priority = "high"
                # Low hit rate means cache is not effective, consider alternative solutions
                potential_savings = monthly_cost * 0.50  # 50% of cost could be saved
                recommendations.append({
                    "type": "low_hit_rate",
                    "severity": "high",
                    "message": (
                        f"HIGH: ElastiCache cluster '{cluster_id}' has LOW cache hit rate (costs ${monthly_cost:.2f}/month). "
                        f"Hit rate: {hit_rate:.1f}% (Hits: {cache_hits:.0f}, Misses: {cache_misses:.0f}). "
                        "Cache hit rate below 50% indicates ineffective caching. "
                        f"Review cache key design or consider deleting cluster to save ${potential_savings:.2f}/month (50% of cost)."
                    ),
                    "impact": "Cache is ineffective with less than 50% hit rate",
                    "action": "Optimize cache key design or delete cluster",
                })
                return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 3: HIGH - No connections (nobody connects)
        if curr_connections == 0.0 and total_requests > 0:
            is_optimizable = True
            optimization_score = 90
            priority = "high"
            potential_savings = monthly_cost  # Cluster not actively used
            recommendations.append({
                "type": "no_connections",
                "severity": "high",
                "message": (
                    f"HIGH: ElastiCache cluster '{cluster_id}' has NO active connections (costs ${monthly_cost:.2f}/month). "
                    "0 current connections detected. "
                    "No applications are actively connected to this cluster. Delete or verify integration."
                ),
                "impact": "Paying for a cluster with no active connections",
                "action": "Delete cluster or verify application integration",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # Scenario 4: MEDIUM - Over-provisioned memory (<20% memory used)
        if memory_usage_percent > 0 and memory_usage_percent < 20.0:
            is_optimizable = True
            optimization_score = 65
            priority = "medium"
            # Assume 40% savings from right-sizing to smaller node type
            potential_savings = monthly_cost * 0.40
            recommendations.append({
                "type": "over_provisioned_memory",
                "severity": "medium",
                "message": (
                    f"MEDIUM: ElastiCache cluster '{cluster_id}' has LOW memory usage (costs ${monthly_cost:.2f}/month). "
                    f"Memory usage: {memory_usage_percent:.1f}%. "
                    f"Cluster is using only {memory_usage_percent:.1f}% of allocated memory. "
                    f"Downgrade to smaller node type to save ${potential_savings:.2f}/month (40% reduction)."
                ),
                "impact": f"Wasting {100 - memory_usage_percent:.1f}% of memory capacity",
                "action": "Downgrade to smaller node type (e.g., t3.small  t3.micro)",
            })
            return is_optimizable, optimization_score, priority, potential_savings, recommendations

        # No optimization opportunities found
        return is_optimizable, optimization_score, priority, potential_savings, recommendations

    async def scan_elasticache_clusters(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS ElastiCache Clusters for cost intelligence.

        This method scans ElastiCache clusters (Redis + Memcached) to provide cost
        visibility and optimization recommendations. Unlike orphan detection, this
        scans ALL clusters regardless of usage patterns.

        Args:
            region: AWS region to scan

        Returns:
            List of AllCloudResourceData objects for all ElastiCache clusters
        """
        resources: list[AllCloudResourceData] = []

        try:
            async with self.session.client("elasticache", region_name=region) as elasticache:
                async with self.session.client("cloudwatch", region_name=region) as cloudwatch:
                    # List all ElastiCache clusters (Redis + Memcached)
                    response = await elasticache.describe_cache_clusters()
                    clusters = response.get("CacheClusters", [])

                    for cluster in clusters:
                        try:
                            cluster_id = cluster.get("CacheClusterId", "")
                            cluster_status = cluster.get("CacheClusterStatus", "unknown")

                            # Skip non-available clusters
                            if cluster_status != "available":
                                continue

                            cluster_arn = cluster.get("ARN", "")
                            created_at = cluster.get("CacheClusterCreateTime")

                            # Extract cluster metadata
                            node_type = cluster.get("CacheNodeType", "cache.m5.large")
                            num_nodes = cluster.get("NumCacheNodes", 1)
                            engine = cluster.get("Engine", "redis")
                            engine_version = cluster.get("EngineVersion", "unknown")

                            # Get CloudWatch metrics (last 7 days)
                            end_time = datetime.utcnow()
                            start_time = end_time - timedelta(days=7)

                            # Get cache hits
                            cache_hits = 0.0
                            try:
                                hits_response = await cloudwatch.get_metric_statistics(
                                    Namespace="AWS/ElastiCache",
                                    MetricName="CacheHits",
                                    Dimensions=[
                                        {"Name": "CacheClusterId", "Value": cluster_id},
                                    ],
                                    StartTime=start_time,
                                    EndTime=end_time,
                                    Period=604800,  # 7 days
                                    Statistics=["Sum"],
                                )
                                datapoints = hits_response.get("Datapoints", [])
                                if datapoints:
                                    cache_hits = datapoints[0].get("Sum", 0.0)
                            except Exception:
                                pass

                            # Get cache misses
                            cache_misses = 0.0
                            try:
                                misses_response = await cloudwatch.get_metric_statistics(
                                    Namespace="AWS/ElastiCache",
                                    MetricName="CacheMisses",
                                    Dimensions=[
                                        {"Name": "CacheClusterId", "Value": cluster_id},
                                    ],
                                    StartTime=start_time,
                                    EndTime=end_time,
                                    Period=604800,  # 7 days
                                    Statistics=["Sum"],
                                )
                                datapoints = misses_response.get("Datapoints", [])
                                if datapoints:
                                    cache_misses = datapoints[0].get("Sum", 0.0)
                            except Exception:
                                pass

                            # Get current connections
                            curr_connections = 0.0
                            try:
                                conn_response = await cloudwatch.get_metric_statistics(
                                    Namespace="AWS/ElastiCache",
                                    MetricName="CurrConnections",
                                    Dimensions=[
                                        {"Name": "CacheClusterId", "Value": cluster_id},
                                    ],
                                    StartTime=start_time,
                                    EndTime=end_time,
                                    Period=604800,  # 7 days
                                    Statistics=["Average"],
                                )
                                datapoints = conn_response.get("Datapoints", [])
                                if datapoints:
                                    curr_connections = datapoints[0].get("Average", 0.0)
                            except Exception:
                                pass

                            # Get memory usage
                            memory_usage_percent = 0.0
                            try:
                                memory_response = await cloudwatch.get_metric_statistics(
                                    Namespace="AWS/ElastiCache",
                                    MetricName="DatabaseMemoryUsagePercentage",
                                    Dimensions=[
                                        {"Name": "CacheClusterId", "Value": cluster_id},
                                    ],
                                    StartTime=start_time,
                                    EndTime=end_time,
                                    Period=604800,  # 7 days
                                    Statistics=["Average"],
                                )
                                datapoints = memory_response.get("Datapoints", [])
                                if datapoints:
                                    memory_usage_percent = datapoints[0].get("Average", 0.0)
                            except Exception:
                                pass

                            # Calculate monthly cost
                            monthly_cost = self._calculate_elasticache_monthly_cost(
                                node_type=node_type,
                                num_nodes=num_nodes,
                                engine=engine,
                                region=region,
                            )

                            # Calculate optimization metrics
                            (
                                is_optimizable,
                                optimization_score,
                                optimization_priority,
                                potential_savings,
                                optimization_recommendations,
                            ) = self._calculate_elasticache_optimization(
                                cluster=cluster,
                                cache_hits=cache_hits,
                                cache_misses=cache_misses,
                                curr_connections=curr_connections,
                                memory_usage_percent=memory_usage_percent,
                                monthly_cost=monthly_cost,
                            )

                            # Calculate cache hit rate
                            total_requests = cache_hits + cache_misses
                            hit_rate = (cache_hits / total_requests * 100) if total_requests > 0 else 0.0

                            # Build metadata
                            metadata = {
                                "cluster_id": cluster_id,
                                "cluster_arn": cluster_arn,
                                "cluster_status": cluster_status,
                                "node_type": node_type,
                                "num_nodes": num_nodes,
                                "engine": engine,
                                "engine_version": engine_version,
                                "cache_hits": round(cache_hits, 2),
                                "cache_misses": round(cache_misses, 2),
                                "hit_rate_percent": round(hit_rate, 2),
                                "curr_connections": round(curr_connections, 2),
                                "memory_usage_percent": round(memory_usage_percent, 2),
                            }

                            # Create resource entry
                            resource = AllCloudResourceData(
                                resource_id=cluster_arn,
                                resource_type="elasticache_cluster",
                                resource_name=cluster_id,
                                region=region,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=metadata,
                                created_at_cloud=created_at,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=optimization_priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=optimization_recommendations,
                            )

                            resources.append(resource)

                        except Exception as e:
                            logger.error(
                                "elasticache.cluster_scan_failed",
                                cluster_id=cluster.get("CacheClusterId", "Unknown"),
                                region=region,
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "elasticache.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # =========================================================================
    # AWS Kinesis Stream - Cost Optimization Scanning
    # =========================================================================

    def _calculate_kinesis_monthly_cost(
        self,
        shard_count: int,
        retention_hours: int,
        enhanced_fanout_consumers: int,
        incoming_bytes_per_month: float,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS Kinesis Stream.

        Pricing model:
        - Shard cost: $0.015/hour = $10.80/month per shard
        - Data retention (3 tiers):
          * Free: First 24 hours (default)
          * Extended: 25-168 hours ($0.020/GB/month)
          * Long-term: >168 hours ($0.026/GB/month)
        - Enhanced fan-out: $0.015/hour = $10.95/month per consumer

        Args:
            shard_count: Number of shards
            retention_hours: Retention period (24-8760 hours)
            enhanced_fanout_consumers: Number of enhanced fan-out consumers
            incoming_bytes_per_month: Monthly incoming data volume (bytes)
            region: AWS region

        Returns:
            Estimated monthly cost in USD
        """
        # Shard cost
        shard_monthly_cost = self.PRICING.get("kinesis_shard", 10.80)
        total_shard_cost = shard_monthly_cost * shard_count

        # Retention cost (beyond free 24h)
        retention_cost = 0.0
        if retention_hours > 24:
            # Convert bytes to GB
            data_gb = incoming_bytes_per_month / (1024**3)

            if retention_hours <= 168:  # 25-168 hours (Extended)
                retention_rate = self.PRICING.get("kinesis_retention_extended_per_gb", 0.020)
                retention_cost = data_gb * retention_rate
            else:  # >168 hours (Long-term)
                retention_rate = self.PRICING.get("kinesis_retention_long_per_gb", 0.026)
                retention_cost = data_gb * retention_rate

        # Enhanced fan-out cost
        fanout_monthly_cost = self.PRICING.get("kinesis_enhanced_fanout_per_consumer", 10.95)
        total_fanout_cost = fanout_monthly_cost * enhanced_fanout_consumers

        total_cost = total_shard_cost + retention_cost + total_fanout_cost
        return total_cost

    def _calculate_kinesis_optimization(
        self,
        stream: dict[str, Any],
        incoming_records: float,
        incoming_bytes: float,
        get_records_count: float,
        iterator_age_ms: float,
        shard_count: int,
        retention_hours: int,
        enhanced_fanout_consumers: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Analyze AWS Kinesis Stream for cost optimization opportunities.

        Optimization scenarios (5):
        1. Completely inactive - No incoming records (critical)
        2. Written but never read - Data written, never consumed (high)
        3. Under-utilized - <1% throughput used (high)
        4. Excessive retention - >24h when not needed (medium)
        5. Unused enhanced fan-out - Consumers not reading data (medium)

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Completely inactive (no incoming records)
        if incoming_records == 0.0:
            is_optimizable = True
            optimization_score = 95
            priority = "critical"
            potential_savings = monthly_cost  # Full savings
            recommendations.append(
                {
                    "type": "delete_stream",
                    "title": "Delete Completely Inactive Kinesis Stream",
                    "description": f"Stream '{stream.get('StreamName', '')}' has received NO records in the last 7 days. "
                    f"This stream is not being used and costs ${monthly_cost:.2f}/month.",
                    "impact": "high",
                    "estimated_savings": f"${monthly_cost:.2f}/month",
                    "action": "Delete this unused Kinesis Stream",
                    "risk": "low",
                }
            )
            return (is_optimizable, optimization_score, priority, potential_savings, recommendations)

        # Scenario 2: HIGH - Written but never read (waste of shards)
        if incoming_records > 0 and get_records_count == 0.0:
            is_optimizable = True
            optimization_score = 85
            priority = "high"
            potential_savings = monthly_cost * 0.80  # 80% savings
            recommendations.append(
                {
                    "type": "unused_stream",
                    "title": "Stream Receives Data But Never Consumed",
                    "description": f"Stream '{stream.get('StreamName', '')}' receives {incoming_records:.0f} records/week "
                    f"but has ZERO GetRecords API calls. Data is written but never read.",
                    "impact": "high",
                    "estimated_savings": f"${potential_savings:.2f}/month",
                    "action": "Review if this stream is still needed, or implement consumers",
                    "risk": "medium",
                }
            )

        # Scenario 3: HIGH - Under-utilized (<1% throughput)
        # Kinesis shard capacity: 1 MB/s incoming, 2 MB/s outgoing, 1000 records/s
        # For 7 days, max capacity = 1 MB/s * 604800s = ~575 GB incoming
        max_incoming_bytes_7d = shard_count * 1_000_000 * 604800  # bytes
        utilization_percent = (incoming_bytes / max_incoming_bytes_7d) * 100 if max_incoming_bytes_7d > 0 else 0

        if utilization_percent < 1.0 and incoming_records > 0:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority != "critical" else priority

            # Estimate right-sized shard count
            recommended_shards = max(1, int(shard_count * utilization_percent / 100))
            shard_savings = (shard_count - recommended_shards) * self.PRICING.get("kinesis_shard", 10.80)
            potential_savings = max(potential_savings, shard_savings)

            recommendations.append(
                {
                    "type": "reduce_shards",
                    "title": f"Kinesis Stream Under-Utilized ({utilization_percent:.2f}% Capacity)",
                    "description": f"Stream '{stream.get('StreamName', '')}' uses only {utilization_percent:.2f}% "
                    f"of its {shard_count} shard(s) capacity. Recommend reducing to {recommended_shards} shard(s).",
                    "impact": "medium",
                    "estimated_savings": f"${shard_savings:.2f}/month",
                    "action": f"Reduce shard count from {shard_count} to {recommended_shards}",
                    "risk": "low",
                }
            )

        # Scenario 4: MEDIUM - Excessive retention (>24h when not needed)
        # If iterator age is low (<1h), consumers are reading data quickly
        # Extended retention (>24h) may be unnecessary
        if retention_hours > 24 and iterator_age_ms < 3_600_000:  # <1 hour lag
            is_optimizable = True
            optimization_score = max(optimization_score, 60)
            priority = "medium" if priority == "low" else priority

            # Calculate retention cost savings
            incoming_bytes_monthly = incoming_bytes * 4.33  # 7d  30d estimate
            data_gb = incoming_bytes_monthly / (1024**3)
            retention_rate = self.PRICING.get("kinesis_retention_extended_per_gb", 0.020)
            retention_savings = data_gb * retention_rate
            potential_savings = max(potential_savings, retention_savings)

            recommendations.append(
                {
                    "type": "reduce_retention",
                    "title": f"Excessive Data Retention ({retention_hours} Hours)",
                    "description": f"Stream '{stream.get('StreamName', '')}' retains data for {retention_hours} hours "
                    f"but consumers process data within 1 hour. Default 24h retention is sufficient.",
                    "impact": "low",
                    "estimated_savings": f"${retention_savings:.2f}/month",
                    "action": f"Reduce retention period from {retention_hours}h to 24h",
                    "risk": "low",
                }
            )

        # Scenario 5: MEDIUM - Unused enhanced fan-out (paying for consumers not reading)
        if enhanced_fanout_consumers > 0 and get_records_count == 0.0:
            is_optimizable = True
            optimization_score = max(optimization_score, 65)
            priority = "medium" if priority == "low" else priority

            fanout_cost = enhanced_fanout_consumers * self.PRICING.get("kinesis_enhanced_fanout_per_consumer", 10.95)
            potential_savings = max(potential_savings, fanout_cost)

            recommendations.append(
                {
                    "type": "remove_fanout",
                    "title": f"Unused Enhanced Fan-Out Consumers ({enhanced_fanout_consumers})",
                    "description": f"Stream '{stream.get('StreamName', '')}' has {enhanced_fanout_consumers} "
                    f"enhanced fan-out consumer(s) but zero GetRecords calls. These consumers are not reading data.",
                    "impact": "medium",
                    "estimated_savings": f"${fanout_cost:.2f}/month",
                    "action": "Remove unused enhanced fan-out consumers",
                    "risk": "low",
                }
            )

        return (is_optimizable, optimization_score, priority, potential_savings, recommendations)

    async def scan_kinesis_streams(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS Kinesis Streams in a region for cost intelligence.

        For each stream:
        - Calculate monthly cost (shards + retention + enhanced fan-out)
        - Fetch CloudWatch metrics (IncomingRecords, IncomingBytes, GetRecords, IteratorAge)
        - Analyze optimization opportunities (5 scenarios)

        Returns:
            List of AllCloudResourceData entries for all Kinesis Streams
        """
        resources: list[AllCloudResourceData] = []

        try:
            async with self.session.client("kinesis", region_name=region) as kinesis:
                async with self.session.client("cloudwatch", region_name=region) as cloudwatch:
                    # List all streams
                    paginator = kinesis.get_paginator("list_streams")
                    stream_names = []

                    async for page in paginator.paginate():
                        stream_names.extend(page.get("StreamNames", []))

                    logger.info(
                        "inventory.kinesis.streams_found",
                        region=region,
                        count=len(stream_names),
                    )

                    # Process each stream
                    for stream_name in stream_names:
                        try:
                            # Get stream details
                            stream_response = await kinesis.describe_stream(StreamName=stream_name)
                            stream_desc = stream_response.get("StreamDescription", {})

                            stream_arn = stream_desc.get("StreamARN", "")
                            stream_status = stream_desc.get("StreamStatus", "UNKNOWN")
                            shard_count = len(stream_desc.get("Shards", []))
                            retention_hours = stream_desc.get("RetentionPeriodHours", 24)
                            encryption_type = stream_desc.get("EncryptionType", "NONE")
                            created_timestamp = stream_desc.get("StreamCreationTimestamp")

                            # Get enhanced fan-out consumers
                            consumers_response = await kinesis.list_stream_consumers(StreamARN=stream_arn)
                            consumers = consumers_response.get("Consumers", [])
                            enhanced_fanout_consumers = len(consumers)

                            # Get CloudWatch metrics (7 days)
                            end_time = datetime.utcnow()
                            start_time = end_time - timedelta(days=7)

                            # Metric 1: IncomingRecords (sum)
                            incoming_records_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/Kinesis",
                                MetricName="IncomingRecords",
                                Dimensions=[{"Name": "StreamName", "Value": stream_name}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,  # 7 days
                                Statistics=["Sum"],
                            )
                            incoming_records = (
                                incoming_records_response["Datapoints"][0]["Sum"]
                                if incoming_records_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 2: IncomingBytes (sum)
                            incoming_bytes_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/Kinesis",
                                MetricName="IncomingBytes",
                                Dimensions=[{"Name": "StreamName", "Value": stream_name}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            incoming_bytes = (
                                incoming_bytes_response["Datapoints"][0]["Sum"]
                                if incoming_bytes_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 3: GetRecords.Records (sum) - Consumption activity
                            get_records_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/Kinesis",
                                MetricName="GetRecords.Records",
                                Dimensions=[{"Name": "StreamName", "Value": stream_name}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            get_records_count = (
                                get_records_response["Datapoints"][0]["Sum"]
                                if get_records_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 4: GetRecords.IteratorAgeMilliseconds (avg) - Consumer lag
                            iterator_age_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/Kinesis",
                                MetricName="GetRecords.IteratorAgeMilliseconds",
                                Dimensions=[{"Name": "StreamName", "Value": stream_name}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Average"],
                            )
                            iterator_age_ms = (
                                iterator_age_response["Datapoints"][0]["Average"]
                                if iterator_age_response.get("Datapoints")
                                else 0.0
                            )

                            # Calculate cost
                            incoming_bytes_monthly = incoming_bytes * 4.33  # 7d  30d estimate
                            monthly_cost = self._calculate_kinesis_monthly_cost(
                                shard_count=shard_count,
                                retention_hours=retention_hours,
                                enhanced_fanout_consumers=enhanced_fanout_consumers,
                                incoming_bytes_per_month=incoming_bytes_monthly,
                                region=region,
                            )

                            # Calculate optimization
                            (
                                is_optimizable,
                                optimization_score,
                                priority,
                                potential_savings,
                                recommendations,
                            ) = self._calculate_kinesis_optimization(
                                stream=stream_desc,
                                incoming_records=incoming_records,
                                incoming_bytes=incoming_bytes,
                                get_records_count=get_records_count,
                                iterator_age_ms=iterator_age_ms,
                                shard_count=shard_count,
                                retention_hours=retention_hours,
                                enhanced_fanout_consumers=enhanced_fanout_consumers,
                                monthly_cost=monthly_cost,
                            )

                            # Build metadata
                            resource_metadata = {
                                "stream_name": stream_name,
                                "stream_arn": stream_arn,
                                "stream_status": stream_status,
                                "shard_count": shard_count,
                                "retention_hours": retention_hours,
                                "encryption_type": encryption_type,
                                "enhanced_fanout_consumers": enhanced_fanout_consumers,
                                "created_timestamp": created_timestamp.isoformat() if created_timestamp else None,
                                "metrics": {
                                    "incoming_records_7d": incoming_records,
                                    "incoming_bytes_7d": incoming_bytes,
                                    "get_records_count_7d": get_records_count,
                                    "iterator_age_ms_avg": iterator_age_ms,
                                },
                            }

                            # Apply detection rules filtering
                            resource_age_days = None
                            if created_timestamp:
                                resource_age_days = (datetime.utcnow() - created_timestamp.replace(tzinfo=None)).days

                            if not self._should_include_resource("kinesis_stream", resource_age_days):
                                logger.debug("inventory.kinesis_filtered", stream=stream_name, age=resource_age_days)
                                continue

                            # Create resource entry
                            resource = AllCloudResourceData(
                                resource_id=stream_arn,
                                resource_type="kinesis_stream",
                                resource_name=stream_name,
                                region=region,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=resource_metadata,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                            )

                            resources.append(resource)

                            logger.info(
                                "inventory.kinesis.stream_scanned",
                                stream_name=stream_name,
                                region=region,
                                shard_count=shard_count,
                                monthly_cost=monthly_cost,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                            )

                        except Exception as e:
                            logger.error(
                                "inventory.kinesis.stream_scan_error",
                                stream_name=stream_name,
                                region=region,
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "kinesis.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # =========================================================================
    # AWS FSx File System - Cost Optimization Scanning
    # =========================================================================

    def _calculate_fsx_monthly_cost(
        self,
        file_system_type: str,
        storage_capacity_gb: int,
        throughput_capacity_mbps: int,
        backup_storage_gb: float,
        deployment_type: str,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS FSx File System.

        Pricing model (4 file system types):
        1. FSx for Lustre: $0.145/GB/month + $2.20/MB/s throughput
        2. FSx for Windows (SSD): $0.13/GB/month + $2.20/MB/s throughput
        3. FSx for Windows (HDD): $0.013/GB/month + $2.20/MB/s throughput
        4. FSx for ONTAP: $0.144/GB/month + $2.20/MB/s throughput
        5. FSx for OpenZFS: $0.0996/GB/month + $2.20/MB/s throughput
        6. Backup: $0.05/GB/month

        Args:
            file_system_type: Type (LUSTRE, WINDOWS, ONTAP, OPENZFS)
            storage_capacity_gb: Storage size in GB
            throughput_capacity_mbps: Throughput capacity in MB/s
            backup_storage_gb: Backup storage size in GB
            deployment_type: SINGLE_AZ_1, SINGLE_AZ_2, MULTI_AZ_1
            region: AWS region

        Returns:
            Estimated monthly cost in USD
        """
        # Storage cost
        storage_cost = 0.0
        if file_system_type == "LUSTRE":
            storage_rate = self.PRICING.get("fsx_lustre_per_gb", 0.145)
            storage_cost = storage_capacity_gb * storage_rate
        elif file_system_type == "WINDOWS":
            # Detect HDD vs SSD based on deployment_type or assume SSD
            storage_rate = self.PRICING.get("fsx_windows_per_gb", 0.13)
            storage_cost = storage_capacity_gb * storage_rate
        elif file_system_type == "ONTAP":
            storage_rate = self.PRICING.get("fsx_ontap_per_gb", 0.144)
            storage_cost = storage_capacity_gb * storage_rate
        elif file_system_type == "OPENZFS":
            storage_rate = self.PRICING.get("fsx_openzfs_per_gb", 0.0996)
            storage_cost = storage_capacity_gb * storage_rate
        else:
            # Fallback
            storage_cost = storage_capacity_gb * 0.13

        # Throughput cost (if applicable)
        throughput_cost = 0.0
        if throughput_capacity_mbps > 0:
            throughput_rate = self.PRICING.get("fsx_throughput_per_mbps", 2.20)
            throughput_cost = throughput_capacity_mbps * throughput_rate

        # Backup cost
        backup_rate = self.PRICING.get("fsx_backup_per_gb", 0.05)
        backup_cost = backup_storage_gb * backup_rate

        total_cost = storage_cost + throughput_cost + backup_cost
        return total_cost

    def _calculate_fsx_optimization(
        self,
        file_system: dict[str, Any],
        data_read_ops: float,
        data_write_ops: float,
        data_read_bytes: float,
        data_write_bytes: float,
        storage_capacity_gb: int,
        throughput_capacity_mbps: int,
        file_system_type: str,
        deployment_type: str,
        backup_storage_gb: float,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Analyze AWS FSx File System for cost optimization opportunities.

        Optimization scenarios (5):
        1. Completely inactive - No read/write operations (critical)
        2. Over-provisioned storage - >50% unused (high)
        3. Over-provisioned throughput - <20% utilization (high)
        4. Wrong storage type - Expensive SSD when HDD works (medium)
        5. Excessive backup retention - >90 days backup (medium)

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Completely inactive (no operations)
        if data_read_ops == 0.0 and data_write_ops == 0.0:
            is_optimizable = True
            optimization_score = 95
            priority = "critical"
            potential_savings = monthly_cost  # Full savings
            recommendations.append(
                {
                    "type": "delete_filesystem",
                    "title": "Delete Completely Inactive FSx File System",
                    "description": f"File system '{file_system.get('FileSystemId', '')}' has had NO read or write "
                    f"operations in the last 7 days. This file system is completely unused and costs ${monthly_cost:.2f}/month.",
                    "impact": "high",
                    "estimated_savings": f"${monthly_cost:.2f}/month",
                    "action": "Delete this unused FSx file system after verifying no critical data",
                    "risk": "low",
                }
            )
            return (is_optimizable, optimization_score, priority, potential_savings, recommendations)

        # Scenario 2: HIGH - Over-provisioned storage (>50% unused)
        # Estimate storage usage based on write operations
        total_ops = data_read_ops + data_write_ops
        if total_ops > 0:
            # Rough estimate: If very few write operations, storage likely under-utilized
            # This is a simplification - real storage usage requires detailed analysis
            write_ratio = data_write_ops / total_ops if total_ops > 0 else 0
            if write_ratio < 0.1 and storage_capacity_gb > 1024:  # <10% writes, >1TB storage
                is_optimizable = True
                optimization_score = max(optimization_score, 75)
                priority = "high" if priority != "critical" else priority

                # Estimate 40% reduction in storage
                recommended_storage_gb = int(storage_capacity_gb * 0.6)
                storage_rate = 0.13  # Default
                if file_system_type == "LUSTRE":
                    storage_rate = self.PRICING.get("fsx_lustre_per_gb", 0.145)
                elif file_system_type == "ONTAP":
                    storage_rate = self.PRICING.get("fsx_ontap_per_gb", 0.144)
                elif file_system_type == "OPENZFS":
                    storage_rate = self.PRICING.get("fsx_openzfs_per_gb", 0.0996)

                storage_savings = (storage_capacity_gb - recommended_storage_gb) * storage_rate
                potential_savings = max(potential_savings, storage_savings)

                recommendations.append(
                    {
                        "type": "reduce_storage",
                        "title": f"FSx File System Over-Provisioned (Low Write Activity)",
                        "description": f"File system '{file_system.get('FileSystemId', '')}' has only {write_ratio*100:.1f}% "
                        f"write operations, indicating potential over-provisioning of {storage_capacity_gb} GB storage.",
                        "impact": "medium",
                        "estimated_savings": f"${storage_savings:.2f}/month",
                        "action": f"Consider reducing storage capacity from {storage_capacity_gb} GB to {recommended_storage_gb} GB",
                        "risk": "medium",
                    }
                )

        # Scenario 3: HIGH - Over-provisioned throughput (<20% utilization)
        # Estimate throughput usage based on read/write bytes
        # FSx throughput capacity measured in MB/s, convert 7-day bytes to avg MB/s
        if throughput_capacity_mbps > 0:
            seven_days_seconds = 604800
            total_bytes_7d = data_read_bytes + data_write_bytes
            avg_throughput_mbps = (total_bytes_7d / seven_days_seconds) / (1024 * 1024)  # Bytes/s to MB/s
            utilization_percent = (avg_throughput_mbps / throughput_capacity_mbps) * 100

            if utilization_percent < 20.0 and throughput_capacity_mbps > 8:  # <20% utilization, >8 MB/s capacity
                is_optimizable = True
                optimization_score = max(optimization_score, 80)
                priority = "high" if priority != "critical" else priority

                # Estimate right-sized throughput
                recommended_throughput = max(8, int(throughput_capacity_mbps * 0.5))  # Min 8 MB/s
                throughput_rate = self.PRICING.get("fsx_throughput_per_mbps", 2.20)
                throughput_savings = (throughput_capacity_mbps - recommended_throughput) * throughput_rate
                potential_savings = max(potential_savings, throughput_savings)

                recommendations.append(
                    {
                        "type": "reduce_throughput",
                        "title": f"FSx Throughput Under-Utilized ({utilization_percent:.1f}% Used)",
                        "description": f"File system '{file_system.get('FileSystemId', '')}' uses only {utilization_percent:.1f}% "
                        f"of its {throughput_capacity_mbps} MB/s throughput capacity. Recommend reducing to {recommended_throughput} MB/s.",
                        "impact": "medium",
                        "estimated_savings": f"${throughput_savings:.2f}/month",
                        "action": f"Reduce throughput capacity from {throughput_capacity_mbps} to {recommended_throughput} MB/s",
                        "risk": "low",
                    }
                )

        # Scenario 4: MEDIUM - Wrong storage type (SSD when HDD works)
        # Windows File Server supports both SSD and HDD
        # If low IOPS requirements, HDD is 10x cheaper
        if file_system_type == "WINDOWS" and total_ops > 0:
            # If total operations per second < 100 IOPS, HDD is sufficient
            seven_days_seconds = 604800
            avg_iops = total_ops / seven_days_seconds

            if avg_iops < 100:  # Low IOPS workload
                is_optimizable = True
                optimization_score = max(optimization_score, 65)
                priority = "medium" if priority == "low" else priority

                # Calculate savings by switching SSD to HDD
                ssd_rate = self.PRICING.get("fsx_windows_per_gb", 0.13)
                hdd_rate = self.PRICING.get("fsx_windows_hdd_per_gb", 0.013)
                storage_savings = storage_capacity_gb * (ssd_rate - hdd_rate)
                potential_savings = max(potential_savings, storage_savings)

                recommendations.append(
                    {
                        "type": "switch_to_hdd",
                        "title": f"FSx Windows File Server - Switch SSD to HDD (Low IOPS)",
                        "description": f"File system '{file_system.get('FileSystemId', '')}' has only {avg_iops:.1f} IOPS "
                        f"on average. For low-IOPS workloads, HDD storage is 10x cheaper than SSD.",
                        "impact": "low",
                        "estimated_savings": f"${storage_savings:.2f}/month (90% cost reduction)",
                        "action": f"Migrate from SSD to HDD storage for {storage_capacity_gb} GB",
                        "risk": "low",
                    }
                )

        # Scenario 5: MEDIUM - Excessive backup retention (>90 days)
        # High backup storage relative to primary storage indicates long retention
        if backup_storage_gb > 0:
            backup_ratio = backup_storage_gb / storage_capacity_gb if storage_capacity_gb > 0 else 0

            if backup_ratio > 3.0:  # >3x primary storage (suggests 90+ days retention)
                is_optimizable = True
                optimization_score = max(optimization_score, 55)
                priority = "medium" if priority == "low" else priority

                # Estimate 50% reduction in backup storage
                recommended_backup_gb = backup_storage_gb * 0.5
                backup_rate = self.PRICING.get("fsx_backup_per_gb", 0.05)
                backup_savings = (backup_storage_gb - recommended_backup_gb) * backup_rate
                potential_savings = max(potential_savings, backup_savings)

                recommendations.append(
                    {
                        "type": "reduce_backup_retention",
                        "title": f"Excessive Backup Retention ({backup_ratio:.1f}x Primary Storage)",
                        "description": f"File system '{file_system.get('FileSystemId', '')}' has {backup_storage_gb:.0f} GB "
                        f"of backups, which is {backup_ratio:.1f}x the primary storage. Consider shorter retention period.",
                        "impact": "low",
                        "estimated_savings": f"${backup_savings:.2f}/month",
                        "action": f"Reduce backup retention to free up {backup_storage_gb - recommended_backup_gb:.0f} GB",
                        "risk": "low",
                    }
                )

        return (is_optimizable, optimization_score, priority, potential_savings, recommendations)

    async def scan_fsx_file_systems(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS FSx File Systems in a region for cost intelligence.

        Supports 4 FSx types:
        - FSx for Lustre (HPC, machine learning)
        - FSx for Windows File Server (SSD/HDD)
        - FSx for NetApp ONTAP (enterprise NAS)
        - FSx for OpenZFS (Linux workloads)

        For each file system:
        - Calculate monthly cost (storage + throughput + backup)
        - Fetch CloudWatch metrics (DataReadOperations, DataWriteOperations, DataReadBytes, DataWriteBytes)
        - Analyze optimization opportunities (5 scenarios)

        Returns:
            List of AllCloudResourceData entries for all FSx File Systems
        """
        resources: list[AllCloudResourceData] = []

        try:
            async with self.session.client("fsx", region_name=region) as fsx:
                async with self.session.client("cloudwatch", region_name=region) as cloudwatch:
                    # List all file systems
                    paginator = fsx.get_paginator("describe_file_systems")
                    file_systems = []

                    async for page in paginator.paginate():
                        file_systems.extend(page.get("FileSystems", []))

                    logger.info(
                        "inventory.fsx.file_systems_found",
                        region=region,
                        count=len(file_systems),
                    )

                    # Process each file system
                    for fs in file_systems:
                        try:
                            filesystem_id = fs.get("FileSystemId", "")
                            filesystem_type = fs.get("FileSystemType", "UNKNOWN")  # LUSTRE, WINDOWS, ONTAP, OPENZFS
                            lifecycle = fs.get("Lifecycle", "UNKNOWN")
                            storage_capacity_gb = fs.get("StorageCapacity", 0)
                            created_time = fs.get("CreationTime")

                            # Get type-specific details
                            deployment_type = "SINGLE_AZ_1"  # Default
                            throughput_capacity_mbps = 0

                            if filesystem_type == "WINDOWS":
                                windows_config = fs.get("WindowsConfiguration", {})
                                deployment_type = windows_config.get("DeploymentType", "SINGLE_AZ_1")
                                throughput_capacity_mbps = windows_config.get("ThroughputCapacity", 0)
                            elif filesystem_type == "LUSTRE":
                                lustre_config = fs.get("LustreConfiguration", {})
                                deployment_type = lustre_config.get("DeploymentType", "SCRATCH_1")
                                throughput_capacity_mbps = lustre_config.get("PerUnitStorageThroughput", 0) * storage_capacity_gb
                            elif filesystem_type == "ONTAP":
                                ontap_config = fs.get("OntapConfiguration", {})
                                deployment_type = ontap_config.get("DeploymentType", "SINGLE_AZ_1")
                                throughput_capacity_mbps = ontap_config.get("ThroughputCapacity", 0)
                            elif filesystem_type == "OPENZFS":
                                openzfs_config = fs.get("OpenZFSConfiguration", {})
                                deployment_type = openzfs_config.get("DeploymentType", "SINGLE_AZ_1")
                                throughput_capacity_mbps = openzfs_config.get("ThroughputCapacity", 0)

                            # Get backup information
                            # Note: BackupStorage is not directly available in describe_file_systems
                            # Would need to call describe_backups to get exact backup storage
                            # For now, estimate as 0 (can be enhanced later)
                            backup_storage_gb = 0.0

                            # Get CloudWatch metrics (7 days)
                            end_time = datetime.utcnow()
                            start_time = end_time - timedelta(days=7)

                            # Metric 1: DataReadOperations (sum)
                            data_read_ops_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/FSx",
                                MetricName="DataReadOperations",
                                Dimensions=[{"Name": "FileSystemId", "Value": filesystem_id}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,  # 7 days
                                Statistics=["Sum"],
                            )
                            data_read_ops = (
                                data_read_ops_response["Datapoints"][0]["Sum"]
                                if data_read_ops_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 2: DataWriteOperations (sum)
                            data_write_ops_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/FSx",
                                MetricName="DataWriteOperations",
                                Dimensions=[{"Name": "FileSystemId", "Value": filesystem_id}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            data_write_ops = (
                                data_write_ops_response["Datapoints"][0]["Sum"]
                                if data_write_ops_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 3: DataReadBytes (sum)
                            data_read_bytes_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/FSx",
                                MetricName="DataReadBytes",
                                Dimensions=[{"Name": "FileSystemId", "Value": filesystem_id}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            data_read_bytes = (
                                data_read_bytes_response["Datapoints"][0]["Sum"]
                                if data_read_bytes_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 4: DataWriteBytes (sum)
                            data_write_bytes_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/FSx",
                                MetricName="DataWriteBytes",
                                Dimensions=[{"Name": "FileSystemId", "Value": filesystem_id}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            data_write_bytes = (
                                data_write_bytes_response["Datapoints"][0]["Sum"]
                                if data_write_bytes_response.get("Datapoints")
                                else 0.0
                            )

                            # Calculate cost
                            monthly_cost = self._calculate_fsx_monthly_cost(
                                file_system_type=filesystem_type,
                                storage_capacity_gb=storage_capacity_gb,
                                throughput_capacity_mbps=throughput_capacity_mbps,
                                backup_storage_gb=backup_storage_gb,
                                deployment_type=deployment_type,
                                region=region,
                            )

                            # Calculate optimization
                            (
                                is_optimizable,
                                optimization_score,
                                priority,
                                potential_savings,
                                recommendations,
                            ) = self._calculate_fsx_optimization(
                                file_system=fs,
                                data_read_ops=data_read_ops,
                                data_write_ops=data_write_ops,
                                data_read_bytes=data_read_bytes,
                                data_write_bytes=data_write_bytes,
                                storage_capacity_gb=storage_capacity_gb,
                                throughput_capacity_mbps=throughput_capacity_mbps,
                                file_system_type=filesystem_type,
                                deployment_type=deployment_type,
                                backup_storage_gb=backup_storage_gb,
                                monthly_cost=monthly_cost,
                            )

                            # Build metadata
                            resource_metadata = {
                                "filesystem_id": filesystem_id,
                                "filesystem_type": filesystem_type,
                                "lifecycle": lifecycle,
                                "storage_capacity_gb": storage_capacity_gb,
                                "throughput_capacity_mbps": throughput_capacity_mbps,
                                "deployment_type": deployment_type,
                                "backup_storage_gb": backup_storage_gb,
                                "created_time": created_time.isoformat() if created_time else None,
                                "metrics": {
                                    "data_read_ops_7d": data_read_ops,
                                    "data_write_ops_7d": data_write_ops,
                                    "data_read_bytes_7d": data_read_bytes,
                                    "data_write_bytes_7d": data_write_bytes,
                                },
                            }

                            # Apply detection rules filtering
                            resource_age_days = None
                            if isinstance(created_time, datetime):
                                resource_age_days = (datetime.utcnow() - created_time.replace(tzinfo=None)).days

                            if not self._should_include_resource("fsx_file_system", resource_age_days):
                                logger.debug("inventory.fsx_filtered", filesystem_id=filesystem_id, age=resource_age_days)
                                continue

                            # Create resource entry
                            resource = AllCloudResourceData(
                                resource_id=filesystem_id,
                                resource_type="fsx_file_system",
                                resource_name=filesystem_id,  # FSx uses ID as name
                                region=region,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=resource_metadata,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                            )

                            resources.append(resource)

                            logger.info(
                                "inventory.fsx.filesystem_scanned",
                                filesystem_id=filesystem_id,
                                region=region,
                                filesystem_type=filesystem_type,
                                storage_capacity_gb=storage_capacity_gb,
                                monthly_cost=monthly_cost,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                            )

                        except Exception as e:
                            logger.error(
                                "inventory.fsx.filesystem_scan_error",
                                filesystem_id=fs.get("FileSystemId", "unknown"),
                                region=region,
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "fsx.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # =========================================================================
    # AWS OpenSearch Domain - Cost Optimization Scanning
    # =========================================================================

    def _calculate_opensearch_monthly_cost(
        self,
        instance_type: str,
        instance_count: int,
        storage_size_gb: int,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS OpenSearch Domain.

        Pricing model:
        - Instance cost: t3.small ($0.036/h) to r5.large ($0.228/h)  instance count
        - Storage cost: $0.10/GB/month (EBS GP2 SSD)

        Args:
            instance_type: Instance type (e.g., t3.small.search, m5.large.search)
            instance_count: Number of data nodes
            storage_size_gb: EBS storage size in GB
            region: AWS region

        Returns:
            Estimated monthly cost in USD
        """
        # Map instance type to pricing key
        # instance_type format: "t3.small.search"  pricing key: "opensearch_t3_small"
        pricing_key = f"opensearch_{instance_type.replace('.search', '').replace('.', '_')}"
        hourly_cost = self.PRICING.get(pricing_key, 0.161)  # Fallback to m5.large
        instance_monthly_cost = hourly_cost * 730 * instance_count

        # Storage cost
        storage_rate = self.PRICING.get("opensearch_storage_per_gb", 0.10)
        storage_monthly_cost = storage_size_gb * storage_rate

        total_cost = instance_monthly_cost + storage_monthly_cost
        return total_cost

    def _calculate_opensearch_optimization(
        self,
        domain: dict[str, Any],
        search_rate: float,
        indexing_rate: float,
        cpu_utilization: float,
        jvm_memory_pressure: float,
        free_storage_gb: float,
        total_storage_gb: int,
        instance_type: str,
        instance_count: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Analyze AWS OpenSearch Domain for cost optimization opportunities.

        Optimization scenarios (5):
        1. Completely idle - No search/indexing activity (critical)
        2. Very low usage - <100 requests/day (high)
        3. Over-provisioned instance - Low CPU/JVM (high)
        4. Over-provisioned storage - >50% free (medium)
        5. Wrong instance family - Memory vs compute optimized (medium)

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Completely idle (no activity)
        if search_rate == 0.0 and indexing_rate == 0.0:
            is_optimizable = True
            optimization_score = 95
            priority = "critical"
            potential_savings = monthly_cost  # Full savings
            recommendations.append(
                {
                    "type": "delete_domain",
                    "title": "Delete Completely Idle OpenSearch Domain",
                    "description": f"Domain '{domain.get('DomainName', '')}' has had NO search or indexing "
                    f"activity in the last 7 days. This domain is completely unused and costs ${monthly_cost:.2f}/month.",
                    "impact": "high",
                    "estimated_savings": f"${monthly_cost:.2f}/month",
                    "action": "Delete this unused OpenSearch domain after verifying no critical data",
                    "risk": "low",
                }
            )
            return (is_optimizable, optimization_score, priority, potential_savings, recommendations)

        # Scenario 2: HIGH - Very low usage (<100 requests/day)
        total_requests_7d = search_rate + indexing_rate
        requests_per_day = total_requests_7d / 7

        if requests_per_day > 0 and requests_per_day < 100:
            is_optimizable = True
            optimization_score = max(optimization_score, 80)
            priority = "high" if priority != "critical" else priority

            # Recommend downsizing to t3.small (dev/test tier)
            if "t3.small" not in instance_type:
                t3_hourly = self.PRICING.get("opensearch_t3_small", 0.036)
                t3_monthly_cost = t3_hourly * 730 * instance_count
                storage_cost = total_storage_gb * self.PRICING.get("opensearch_storage_per_gb", 0.10)
                new_total_cost = t3_monthly_cost + storage_cost
                savings = monthly_cost - new_total_cost
                potential_savings = max(potential_savings, savings)

                recommendations.append(
                    {
                        "type": "downsize_to_dev_tier",
                        "title": f"Very Low Usage ({requests_per_day:.0f} Requests/Day)",
                        "description": f"Domain '{domain.get('DomainName', '')}' has only {requests_per_day:.0f} requests/day. "
                        f"Consider downsizing from {instance_type} to t3.small.search (dev/test tier).",
                        "impact": "medium",
                        "estimated_savings": f"${savings:.2f}/month",
                        "action": f"Downsize instance type to t3.small.search ({instance_count} node(s))",
                        "risk": "low",
                    }
                )

        # Scenario 3: HIGH - Over-provisioned instance (low CPU + low JVM)
        if cpu_utilization > 0 and cpu_utilization < 20.0 and jvm_memory_pressure < 30.0:
            is_optimizable = True
            optimization_score = max(optimization_score, 75)
            priority = "high" if priority != "critical" else priority

            # Estimate 40% cost reduction by downsizing
            instance_savings = monthly_cost * 0.40
            potential_savings = max(potential_savings, instance_savings)

            recommendations.append(
                {
                    "type": "downsize_instance",
                    "title": f"Over-Provisioned Instance (CPU {cpu_utilization:.1f}%, JVM {jvm_memory_pressure:.1f}%)",
                    "description": f"Domain '{domain.get('DomainName', '')}' uses only {cpu_utilization:.1f}% CPU "
                    f"and {jvm_memory_pressure:.1f}% JVM memory. Recommend downsizing instance type.",
                    "impact": "medium",
                    "estimated_savings": f"${instance_savings:.2f}/month",
                    "action": f"Downsize from {instance_type} ({instance_count} nodes) to smaller instance",
                    "risk": "low",
                }
            )

        # Scenario 4: MEDIUM - Over-provisioned storage (>50% free)
        if total_storage_gb > 0 and free_storage_gb > 0:
            storage_utilization_percent = ((total_storage_gb - free_storage_gb) / total_storage_gb) * 100

            if storage_utilization_percent < 50.0:
                is_optimizable = True
                optimization_score = max(optimization_score, 60)
                priority = "medium" if priority == "low" else priority

                # Recommend reducing storage by 30%
                recommended_storage_gb = int(total_storage_gb * 0.7)
                storage_rate = self.PRICING.get("opensearch_storage_per_gb", 0.10)
                storage_savings = (total_storage_gb - recommended_storage_gb) * storage_rate
                potential_savings = max(potential_savings, storage_savings)

                recommendations.append(
                    {
                        "type": "reduce_storage",
                        "title": f"Over-Provisioned Storage ({storage_utilization_percent:.1f}% Used)",
                        "description": f"Domain '{domain.get('DomainName', '')}' uses only {storage_utilization_percent:.1f}% "
                        f"of its {total_storage_gb} GB storage. {free_storage_gb:.0f} GB is free.",
                        "impact": "low",
                        "estimated_savings": f"${storage_savings:.2f}/month",
                        "action": f"Reduce storage from {total_storage_gb} GB to {recommended_storage_gb} GB",
                        "risk": "low",
                    }
                )

        # Scenario 5: MEDIUM - Wrong instance family (r5 vs m5)
        # If JVM memory pressure is low (<50%), don't need memory-optimized (r5)
        # If JVM memory pressure is high (>75%), need memory-optimized (r5)
        if jvm_memory_pressure > 0:
            if "r5" in instance_type and jvm_memory_pressure < 50.0:
                # Using r5 (memory-optimized) but low memory usage  switch to m5 (general purpose)
                is_optimizable = True
                optimization_score = max(optimization_score, 65)
                priority = "medium" if priority == "low" else priority

                # Estimate 30% savings by switching r5  m5
                family_savings = monthly_cost * 0.30
                potential_savings = max(potential_savings, family_savings)

                recommendations.append(
                    {
                        "type": "switch_instance_family",
                        "title": f"Memory-Optimized Instance Unnecessary (JVM {jvm_memory_pressure:.1f}%)",
                        "description": f"Domain '{domain.get('DomainName', '')}' uses r5 (memory-optimized) instance "
                        f"but only {jvm_memory_pressure:.1f}% JVM memory. Switch to m5 (general purpose) to save 30%.",
                        "impact": "low",
                        "estimated_savings": f"${family_savings:.2f}/month",
                        "action": f"Switch from {instance_type} to m5 equivalent",
                        "risk": "low",
                    }
                )
            elif "m5" in instance_type and jvm_memory_pressure > 75.0:
                # Using m5 (general purpose) but high memory usage  recommend r5 (memory-optimized)
                # This is not a cost optimization, but a performance recommendation
                # We don't add it to potential_savings
                recommendations.append(
                    {
                        "type": "upgrade_instance_family_performance",
                        "title": f"Consider Memory-Optimized Instance (JVM {jvm_memory_pressure:.1f}%)",
                        "description": f"Domain '{domain.get('DomainName', '')}' uses m5 (general purpose) but has "
                        f"{jvm_memory_pressure:.1f}% JVM memory pressure. Consider r5 for better performance.",
                        "impact": "low",
                        "estimated_savings": "$0.00/month (performance recommendation, not cost savings)",
                        "action": f"Evaluate switching to r5 instance for better performance",
                        "risk": "low",
                    }
                )

        return (is_optimizable, optimization_score, priority, potential_savings, recommendations)

    async def scan_opensearch_domains(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS OpenSearch Domains in a region for cost intelligence.

        For each domain:
        - Calculate monthly cost (instance + storage)
        - Fetch CloudWatch metrics (SearchRate, IndexingRate, CPU, JVM, Storage)
        - Analyze optimization opportunities (5 scenarios)

        Returns:
            List of AllCloudResourceData entries for all OpenSearch Domains
        """
        resources: list[AllCloudResourceData] = []

        try:
            async with self.session.client("opensearch", region_name=region) as opensearch:
                async with self.session.client("cloudwatch", region_name=region) as cloudwatch:
                    # List all domains
                    response = await opensearch.list_domain_names()
                    domain_names = [d["DomainName"] for d in response.get("DomainNames", [])]

                    logger.info(
                        "inventory.opensearch.domains_found",
                        region=region,
                        count=len(domain_names),
                    )

                    # Process each domain
                    for domain_name in domain_names:
                        try:
                            # Get domain details
                            domain_response = await opensearch.describe_domain(DomainName=domain_name)
                            domain = domain_response.get("DomainStatus", {})

                            domain_arn = domain.get("ARN", "")
                            domain_status = domain.get("Processing", False)  # Processing = updating
                            created_at = domain.get("Created")

                            # Get cluster config
                            cluster_config = domain.get("ClusterConfig", {})
                            instance_type = cluster_config.get("InstanceType", "m5.large.search")
                            instance_count = cluster_config.get("InstanceCount", 1)

                            # Get EBS storage
                            ebs_options = domain.get("EBSOptions", {})
                            ebs_enabled = ebs_options.get("EBSEnabled", False)
                            storage_size_gb = ebs_options.get("VolumeSize", 0) if ebs_enabled else 0

                            # Get CloudWatch metrics (7 days)
                            end_time = datetime.utcnow()
                            start_time = end_time - timedelta(days=7)

                            # Metric 1: SearchRate (sum over 7 days)
                            search_rate_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/ES",
                                MetricName="SearchRate",
                                Dimensions=[
                                    {"Name": "DomainName", "Value": domain_name},
                                    {"Name": "ClientId", "Value": domain_arn.split(":")[4]},
                                ],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,  # 7 days
                                Statistics=["Sum"],
                            )
                            search_rate = (
                                search_rate_response["Datapoints"][0]["Sum"]
                                if search_rate_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 2: IndexingRate (sum over 7 days)
                            indexing_rate_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/ES",
                                MetricName="IndexingRate",
                                Dimensions=[
                                    {"Name": "DomainName", "Value": domain_name},
                                    {"Name": "ClientId", "Value": domain_arn.split(":")[4]},
                                ],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            indexing_rate = (
                                indexing_rate_response["Datapoints"][0]["Sum"]
                                if indexing_rate_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 3: CPUUtilization (average over 7 days)
                            cpu_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/ES",
                                MetricName="CPUUtilization",
                                Dimensions=[
                                    {"Name": "DomainName", "Value": domain_name},
                                    {"Name": "ClientId", "Value": domain_arn.split(":")[4]},
                                ],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Average"],
                            )
                            cpu_utilization = (
                                cpu_response["Datapoints"][0]["Average"]
                                if cpu_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 4: JVMMemoryPressure (average)
                            jvm_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/ES",
                                MetricName="JVMMemoryPressure",
                                Dimensions=[
                                    {"Name": "DomainName", "Value": domain_name},
                                    {"Name": "ClientId", "Value": domain_arn.split(":")[4]},
                                ],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Average"],
                            )
                            jvm_memory_pressure = (
                                jvm_response["Datapoints"][0]["Average"]
                                if jvm_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 5: FreeStorageSpace (minimum, to detect low storage)
                            storage_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/ES",
                                MetricName="FreeStorageSpace",
                                Dimensions=[
                                    {"Name": "DomainName", "Value": domain_name},
                                    {"Name": "ClientId", "Value": domain_arn.split(":")[4]},
                                ],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Average"],
                            )
                            free_storage_mb = (
                                storage_response["Datapoints"][0]["Average"]
                                if storage_response.get("Datapoints")
                                else 0.0
                            )
                            free_storage_gb = free_storage_mb / 1024  # Convert MB to GB

                            # Calculate cost
                            monthly_cost = self._calculate_opensearch_monthly_cost(
                                instance_type=instance_type,
                                instance_count=instance_count,
                                storage_size_gb=storage_size_gb,
                                region=region,
                            )

                            # Calculate optimization
                            (
                                is_optimizable,
                                optimization_score,
                                priority,
                                potential_savings,
                                recommendations,
                            ) = self._calculate_opensearch_optimization(
                                domain=domain,
                                search_rate=search_rate,
                                indexing_rate=indexing_rate,
                                cpu_utilization=cpu_utilization,
                                jvm_memory_pressure=jvm_memory_pressure,
                                free_storage_gb=free_storage_gb,
                                total_storage_gb=storage_size_gb,
                                instance_type=instance_type,
                                instance_count=instance_count,
                                monthly_cost=monthly_cost,
                            )

                            # Build metadata
                            resource_metadata = {
                                "domain_name": domain_name,
                                "domain_arn": domain_arn,
                                "instance_type": instance_type,
                                "instance_count": instance_count,
                                "storage_size_gb": storage_size_gb,
                                "engine_version": domain.get("EngineVersion", "Unknown"),
                                "processing": domain_status,
                                "created_at": created_at.isoformat() if created_at else None,
                                "metrics": {
                                    "search_rate_7d": search_rate,
                                    "indexing_rate_7d": indexing_rate,
                                    "cpu_utilization_avg": cpu_utilization,
                                    "jvm_memory_pressure_avg": jvm_memory_pressure,
                                    "free_storage_gb": free_storage_gb,
                                },
                            }

                            # Apply detection rules filtering
                            resource_age_days = None
                            if isinstance(created_at, datetime):
                                resource_age_days = (datetime.utcnow() - created_at.replace(tzinfo=None)).days

                            if not self._should_include_resource("opensearch_domain", resource_age_days):
                                logger.debug("inventory.opensearch_filtered", domain_name=domain_name, age=resource_age_days)
                                continue

                            # Create resource entry
                            resource = AllCloudResourceData(
                                resource_id=domain_arn,
                                resource_type="opensearch_domain",
                                resource_name=domain_name,
                                region=region,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=resource_metadata,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                            )

                            resources.append(resource)

                            logger.info(
                                "inventory.opensearch.domain_scanned",
                                domain_name=domain_name,
                                region=region,
                                instance_type=instance_type,
                                instance_count=instance_count,
                                monthly_cost=monthly_cost,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                            )

                        except Exception as e:
                            logger.error(
                                "inventory.opensearch.domain_scan_error",
                                domain_name=domain_name,
                                region=region,
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "opensearch.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # =========================================================================
    # AWS API Gateway - Cost Optimization Scanning
    # =========================================================================

    def _calculate_apigateway_monthly_cost(
        self,
        api_type: str,
        request_count_per_month: float,
        data_transfer_gb_per_month: float,
        cache_enabled: bool,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS API Gateway.

        Pricing model (3 API types):
        - REST API: $3.50 per million requests + $0.09/GB data transfer
        - HTTP API: $1.00 per million requests (70% cheaper, no caching)
        - WebSocket API: $1.00 per million messages + $0.25 per million connection minutes

        Args:
            api_type: API type (REST, HTTP, WEBSOCKET)
            request_count_per_month: Monthly request count (estimated from 7d)
            data_transfer_gb_per_month: Monthly data transfer in GB
            cache_enabled: Whether API cache is enabled (REST only)
            region: AWS region

        Returns:
            Estimated monthly cost in USD
        """
        # Request cost
        request_cost = 0.0
        if api_type == "REST":
            request_rate = self.PRICING.get("apigateway_rest_per_million", 3.50)
            request_cost = (request_count_per_month / 1_000_000) * request_rate
        elif api_type == "HTTP":
            request_rate = self.PRICING.get("apigateway_http_per_million", 1.00)
            request_cost = (request_count_per_month / 1_000_000) * request_rate
        elif api_type == "WEBSOCKET":
            message_rate = self.PRICING.get("apigateway_websocket_per_million", 1.00)
            request_cost = (request_count_per_month / 1_000_000) * message_rate
            # WebSocket also charges for connection minutes (not included here, would need additional metric)

        # Data transfer cost
        data_transfer_rate = self.PRICING.get("apigateway_data_transfer_per_gb", 0.09)
        data_transfer_cost = data_transfer_gb_per_month * data_transfer_rate

        # Cache cost (REST API only, if enabled)
        # Not included in basic pricing (would need cache size info)
        cache_cost = 0.0

        total_cost = request_cost + data_transfer_cost + cache_cost
        return total_cost

    def _calculate_apigateway_optimization(
        self,
        api: dict[str, Any],
        api_type: str,
        request_count: float,
        error_4xx_count: float,
        error_5xx_count: float,
        cache_hit_count: float,
        cache_miss_count: float,
        cache_enabled: bool,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Analyze AWS API Gateway for cost optimization opportunities.

        Optimization scenarios (6):
        1. Completely inactive - No requests (critical)
        2. Very low usage - <1000 req/day (high)
        3. High error rate - >10% 4XX/5XX (high)
        4. REST API when HTTP API sufficient - 70% savings (medium)
        5. Cache enabled but low hit rate - <50% (medium)
        6. WebSocket API with no active connections - Delete if unused (low)

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Completely inactive (no requests)
        if request_count == 0.0:
            is_optimizable = True
            optimization_score = 95
            priority = "critical"
            potential_savings = monthly_cost  # Full savings
            recommendations.append(
                {
                    "type": "delete_api",
                    "title": "Delete Completely Inactive API Gateway",
                    "description": f"API '{api.get('Name', api.get('name', ''))}' has received NO requests "
                    f"in the last 7 days. This API is completely unused and costs ${monthly_cost:.2f}/month.",
                    "impact": "high",
                    "estimated_savings": f"${monthly_cost:.2f}/month",
                    "action": "Delete this unused API Gateway after verifying no dependencies",
                    "risk": "low",
                }
            )
            return (is_optimizable, optimization_score, priority, potential_savings, recommendations)

        # Scenario 2: HIGH - Very low usage (<1000 requests/day)
        requests_per_day = request_count / 7

        if requests_per_day > 0 and requests_per_day < 1000:
            is_optimizable = True
            optimization_score = max(optimization_score, 80)
            priority = "high" if priority != "critical" else priority

            recommendations.append(
                {
                    "type": "low_usage",
                    "title": f"Very Low Usage ({requests_per_day:.0f} Requests/Day)",
                    "description": f"API '{api.get('Name', api.get('name', ''))}' receives only {requests_per_day:.0f} requests/day. "
                    f"Consider consolidating with other APIs or using serverless alternatives (AWS Lambda URL).",
                    "impact": "medium",
                    "estimated_savings": f"${monthly_cost * 0.50:.2f}/month (if consolidated)",
                    "action": "Review if this API can be consolidated or replaced with simpler alternatives",
                    "risk": "medium",
                }
            )
            potential_savings = max(potential_savings, monthly_cost * 0.50)

        # Scenario 3: HIGH - High error rate (>10% 4XX/5XX)
        total_errors = error_4xx_count + error_5xx_count
        if request_count > 0:
            error_rate = (total_errors / request_count) * 100

            if error_rate > 10.0:
                is_optimizable = True
                optimization_score = max(optimization_score, 85)
                priority = "high" if priority != "critical" else priority

                recommendations.append(
                    {
                        "type": "high_error_rate",
                        "title": f"High Error Rate ({error_rate:.1f}% Errors)",
                        "description": f"API '{api.get('Name', api.get('name', ''))}' has {error_rate:.1f}% error rate "
                        f"({error_4xx_count:.0f} 4XX + {error_5xx_count:.0f} 5XX errors). "
                        f"This indicates integration issues or broken endpoints.",
                        "impact": "high",
                        "estimated_savings": f"${monthly_cost * 0.30:.2f}/month (if errors indicate unused endpoints)",
                        "action": "Review and fix integration errors, or delete broken API if no longer needed",
                        "risk": "medium",
                    }
                )
                potential_savings = max(potential_savings, monthly_cost * 0.30)

        # Scenario 4: MEDIUM - REST API when HTTP API sufficient (70% savings)
        if api_type == "REST":
            is_optimizable = True
            optimization_score = max(optimization_score, 70)
            priority = "medium" if priority == "low" else priority

            # Calculate savings by migrating REST  HTTP API
            rest_rate = self.PRICING.get("apigateway_rest_per_million", 3.50)
            http_rate = self.PRICING.get("apigateway_http_per_million", 1.00)
            request_count_monthly = request_count * 4.33  # 7d  30d
            current_request_cost = (request_count_monthly / 1_000_000) * rest_rate
            new_request_cost = (request_count_monthly / 1_000_000) * http_rate
            migration_savings = current_request_cost - new_request_cost

            if migration_savings > 0:
                potential_savings = max(potential_savings, migration_savings)

                recommendations.append(
                    {
                        "type": "migrate_to_http_api",
                        "title": f"Migrate REST API to HTTP API (Save 70%)",
                        "description": f"API '{api.get('Name', '')}' uses REST API ($3.50/M requests). "
                        f"If you don't need API caching, request/response transformations, or usage plans, "
                        f"migrate to HTTP API ($1.00/M requests) to save 70%.",
                        "impact": "low",
                        "estimated_savings": f"${migration_savings:.2f}/month",
                        "action": "Evaluate migrating from REST API to HTTP API if features not needed",
                        "risk": "low",
                    }
                )

        # Scenario 5: MEDIUM - Cache enabled but low hit rate (<50%)
        if cache_enabled and (cache_hit_count + cache_miss_count) > 0:
            cache_total = cache_hit_count + cache_miss_count
            cache_hit_rate = (cache_hit_count / cache_total) * 100

            if cache_hit_rate < 50.0:
                is_optimizable = True
                optimization_score = max(optimization_score, 65)
                priority = "medium" if priority == "low" else priority

                # Estimate cache cost (varies by cache size, assume 0.5 GB = $0.020/hour = ~$14/month)
                estimated_cache_cost = 14.00
                potential_savings = max(potential_savings, estimated_cache_cost)

                recommendations.append(
                    {
                        "type": "disable_cache",
                        "title": f"Low Cache Hit Rate ({cache_hit_rate:.1f}%)",
                        "description": f"API '{api.get('Name', '')}' has caching enabled but only {cache_hit_rate:.1f}% hit rate. "
                        f"Cache is ineffective and costs ~$14/month. Consider disabling cache to save money.",
                        "impact": "low",
                        "estimated_savings": f"${estimated_cache_cost:.2f}/month",
                        "action": "Disable API Gateway cache if hit rate remains below 50%",
                        "risk": "low",
                    }
                )

        # Scenario 6: LOW - WebSocket API with no active connections
        # This would require additional metrics (WebSocket connection count)
        # For now, we detect via request_count = 0 (covered in Scenario 1)

        return (is_optimizable, optimization_score, priority, potential_savings, recommendations)

    async def scan_api_gateways(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS API Gateways in a region for cost intelligence.

        Scans 3 API types:
        1. REST API (apigateway client)
        2. HTTP API (apigatewayv2 client)
        3. WebSocket API (apigatewayv2 client)

        For each API:
        - Calculate monthly cost (requests + data transfer)
        - Fetch CloudWatch metrics (Count, 4XXError, 5XXError, Cache)
        - Analyze optimization opportunities (6 scenarios)

        Returns:
            List of AllCloudResourceData entries for all API Gateways
        """
        resources: list[AllCloudResourceData] = []

        try:
            # Scan REST APIs (apigateway client)
            async with self.session.client("apigateway", region_name=region) as apigw:
                async with self.session.client("cloudwatch", region_name=region) as cloudwatch:
                    # List REST APIs
                    rest_response = await apigw.get_rest_apis()
                    rest_apis = rest_response.get("items", [])

                    logger.info(
                        "inventory.apigateway.rest_apis_found",
                        region=region,
                        count=len(rest_apis),
                    )

                    for api in rest_apis:
                        try:
                            api_id = api.get("id", "")
                            api_name = api.get("name", "")
                            created_date = api.get("createdDate")

                            # Get CloudWatch metrics (7 days)
                            end_time = datetime.utcnow()
                            start_time = end_time - timedelta(days=7)

                            # Metric 1: Count (total requests)
                            count_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/ApiGateway",
                                MetricName="Count",
                                Dimensions=[{"Name": "ApiName", "Value": api_name}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            request_count = (
                                count_response["Datapoints"][0]["Sum"]
                                if count_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 2: 4XXError
                            error_4xx_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/ApiGateway",
                                MetricName="4XXError",
                                Dimensions=[{"Name": "ApiName", "Value": api_name}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            error_4xx_count = (
                                error_4xx_response["Datapoints"][0]["Sum"]
                                if error_4xx_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 3: 5XXError
                            error_5xx_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/ApiGateway",
                                MetricName="5XXError",
                                Dimensions=[{"Name": "ApiName", "Value": api_name}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            error_5xx_count = (
                                error_5xx_response["Datapoints"][0]["Sum"]
                                if error_5xx_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 4: CacheHitCount (if caching enabled)
                            cache_hit_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/ApiGateway",
                                MetricName="CacheHitCount",
                                Dimensions=[{"Name": "ApiName", "Value": api_name}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            cache_hit_count = (
                                cache_hit_response["Datapoints"][0]["Sum"]
                                if cache_hit_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 5: CacheMissCount
                            cache_miss_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/ApiGateway",
                                MetricName="CacheMissCount",
                                Dimensions=[{"Name": "ApiName", "Value": api_name}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            cache_miss_count = (
                                cache_miss_response["Datapoints"][0]["Sum"]
                                if cache_miss_response.get("Datapoints")
                                else 0.0
                            )

                            cache_enabled = (cache_hit_count + cache_miss_count) > 0

                            # Estimate monthly metrics from 7-day data
                            request_count_monthly = request_count * 4.33
                            data_transfer_gb_monthly = (request_count * 5) / (1024 ** 3)  # Assume 5KB avg response

                            # Calculate cost
                            monthly_cost = self._calculate_apigateway_monthly_cost(
                                api_type="REST",
                                request_count_per_month=request_count_monthly,
                                data_transfer_gb_per_month=data_transfer_gb_monthly,
                                cache_enabled=cache_enabled,
                                region=region,
                            )

                            # Calculate optimization
                            (
                                is_optimizable,
                                optimization_score,
                                priority,
                                potential_savings,
                                recommendations,
                            ) = self._calculate_apigateway_optimization(
                                api=api,
                                api_type="REST",
                                request_count=request_count,
                                error_4xx_count=error_4xx_count,
                                error_5xx_count=error_5xx_count,
                                cache_hit_count=cache_hit_count,
                                cache_miss_count=cache_miss_count,
                                cache_enabled=cache_enabled,
                                monthly_cost=monthly_cost,
                            )

                            # Build metadata
                            resource_metadata = {
                                "api_id": api_id,
                                "api_name": api_name,
                                "api_type": "REST",
                                "endpoint_type": api.get("endpointConfiguration", {}).get("types", ["REGIONAL"])[0],
                                "cache_enabled": cache_enabled,
                                "created_date": created_date.isoformat() if created_date else None,
                                "metrics": {
                                    "request_count_7d": request_count,
                                    "error_4xx_count_7d": error_4xx_count,
                                    "error_5xx_count_7d": error_5xx_count,
                                    "cache_hit_count_7d": cache_hit_count,
                                    "cache_miss_count_7d": cache_miss_count,
                                },
                            }

                            # Apply detection rules filtering
                            resource_age_days = None
                            if isinstance(created_date, datetime):
                                resource_age_days = (datetime.utcnow() - created_date.replace(tzinfo=None)).days

                            if not self._should_include_resource("api_gateway", resource_age_days):
                                logger.debug("inventory.apigateway_filtered", api_id=api_id, api_name=api_name, age=resource_age_days)
                                continue

                            # Create resource entry
                            resource = AllCloudResourceData(
                                resource_id=api_id,
                                resource_type="api_gateway",
                                resource_name=api_name,
                                region=region,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=resource_metadata,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                            )

                            resources.append(resource)

                            logger.info(
                                "inventory.apigateway.rest_api_scanned",
                                api_name=api_name,
                                region=region,
                                request_count=request_count,
                                monthly_cost=monthly_cost,
                                is_optimizable=is_optimizable,
                            )

                        except Exception as e:
                            logger.error(
                                "inventory.apigateway.rest_api_scan_error",
                                api_id=api.get("id", "unknown"),
                                region=region,
                                error=str(e),
                            )
                            continue

            # Scan HTTP + WebSocket APIs (apigatewayv2 client)
            async with self.session.client("apigatewayv2", region_name=region) as apigw2:
                async with self.session.client("cloudwatch", region_name=region) as cloudwatch:
                    # List HTTP + WebSocket APIs
                    v2_response = await apigw2.get_apis()
                    v2_apis = v2_response.get("Items", [])

                    logger.info(
                        "inventory.apigateway.v2_apis_found",
                        region=region,
                        count=len(v2_apis),
                    )

                    for api in v2_apis:
                        try:
                            api_id = api.get("ApiId", "")
                            api_name = api.get("Name", "")
                            api_protocol = api.get("ProtocolType", "HTTP")  # HTTP or WEBSOCKET
                            created_date = api.get("CreatedDate")

                            # CloudWatch metrics (note: different namespace for v2)
                            end_time = datetime.utcnow()
                            start_time = end_time - timedelta(days=7)

                            # For HTTP/WebSocket APIs, metrics use ApiId dimension
                            count_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/ApiGateway",
                                MetricName="Count",
                                Dimensions=[{"Name": "ApiId", "Value": api_id}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            request_count = (
                                count_response["Datapoints"][0]["Sum"]
                                if count_response.get("Datapoints")
                                else 0.0
                            )

                            # Errors
                            error_4xx_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/ApiGateway",
                                MetricName="4XXError",
                                Dimensions=[{"Name": "ApiId", "Value": api_id}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            error_4xx_count = (
                                error_4xx_response["Datapoints"][0]["Sum"]
                                if error_4xx_response.get("Datapoints")
                                else 0.0
                            )

                            error_5xx_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/ApiGateway",
                                MetricName="5XXError",
                                Dimensions=[{"Name": "ApiId", "Value": api_id}],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            error_5xx_count = (
                                error_5xx_response["Datapoints"][0]["Sum"]
                                if error_5xx_response.get("Datapoints")
                                else 0.0
                            )

                            # HTTP/WebSocket APIs don't support caching
                            cache_hit_count = 0.0
                            cache_miss_count = 0.0
                            cache_enabled = False

                            # Estimate monthly metrics
                            request_count_monthly = request_count * 4.33
                            data_transfer_gb_monthly = (request_count * 5) / (1024 ** 3)

                            # Calculate cost
                            monthly_cost = self._calculate_apigateway_monthly_cost(
                                api_type=api_protocol,
                                request_count_per_month=request_count_monthly,
                                data_transfer_gb_per_month=data_transfer_gb_monthly,
                                cache_enabled=False,
                                region=region,
                            )

                            # Calculate optimization
                            (
                                is_optimizable,
                                optimization_score,
                                priority,
                                potential_savings,
                                recommendations,
                            ) = self._calculate_apigateway_optimization(
                                api=api,
                                api_type=api_protocol,
                                request_count=request_count,
                                error_4xx_count=error_4xx_count,
                                error_5xx_count=error_5xx_count,
                                cache_hit_count=cache_hit_count,
                                cache_miss_count=cache_miss_count,
                                cache_enabled=cache_enabled,
                                monthly_cost=monthly_cost,
                            )

                            # Build metadata
                            resource_metadata = {
                                "api_id": api_id,
                                "api_name": api_name,
                                "api_type": api_protocol,
                                "api_endpoint": api.get("ApiEndpoint", ""),
                                "created_date": created_date.isoformat() if created_date else None,
                                "metrics": {
                                    "request_count_7d": request_count,
                                    "error_4xx_count_7d": error_4xx_count,
                                    "error_5xx_count_7d": error_5xx_count,
                                },
                            }

                            # Apply detection rules filtering (v2 APIs)
                            resource_age_days = None
                            if isinstance(created_date, datetime):
                                resource_age_days = (datetime.utcnow() - created_date.replace(tzinfo=None)).days

                            if not self._should_include_resource("api_gateway", resource_age_days):
                                logger.debug("inventory.apigateway_v2_filtered", api_id=api_id, api_name=api_name, age=resource_age_days)
                                continue

                            # Create resource entry
                            resource = AllCloudResourceData(
                                resource_id=api_id,
                                resource_type="api_gateway",
                                resource_name=api_name,
                                region=region,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=resource_metadata,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                            )

                            resources.append(resource)

                            logger.info(
                                "inventory.apigateway.v2_api_scanned",
                                api_name=api_name,
                                api_type=api_protocol,
                                region=region,
                                request_count=request_count,
                                monthly_cost=monthly_cost,
                                is_optimizable=is_optimizable,
                            )

                        except Exception as e:
                            logger.error(
                                "inventory.apigateway.v2_api_scan_error",
                                api_id=api.get("ApiId", "unknown"),
                                region=region,
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "apigateway.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # =========================================================================
    # AWS CloudFront Distribution - Cost Optimization Scanning (GLOBAL SERVICE)
    # =========================================================================

    def _calculate_cloudfront_monthly_cost(
        self,
        requests_per_month: float,
        data_transfer_gb_per_month: float,
        invalidations_per_month: int,
        price_class: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS CloudFront Distribution.

        Pricing model (varies by region/price class):
        - Data transfer out: $0.085/GB (US/Europe), $0.140/GB (Asia), $0.170/GB (Other)
        - HTTP requests: $0.0075 per 10,000 requests
        - HTTPS requests: $0.010 per 10,000 requests (assume 100% HTTPS)
        - Invalidations: First 1,000 paths free, $0.005 per path after

        Args:
            requests_per_month: Monthly request count (estimated from 7d)
            data_transfer_gb_per_month: Monthly data transfer out in GB
            invalidations_per_month: Monthly invalidation requests
            price_class: Price class (PriceClass_All, PriceClass_200, PriceClass_100)

        Returns:
            Estimated monthly cost in USD
        """
        # Data transfer cost (varies by price class)
        if price_class == "PriceClass_100":  # US, Canada, Europe
            data_transfer_rate = self.PRICING.get("cloudfront_data_transfer_us_europe_per_gb", 0.085)
        elif price_class == "PriceClass_200":  # US, Canada, Europe, Asia, Middle East, Africa
            data_transfer_rate = self.PRICING.get("cloudfront_data_transfer_asia_per_gb", 0.140)
        else:  # PriceClass_All (all edge locations)
            data_transfer_rate = self.PRICING.get("cloudfront_data_transfer_other_per_gb", 0.170)

        data_transfer_cost = data_transfer_gb_per_month * data_transfer_rate

        # Request cost (assume 100% HTTPS)
        https_rate = self.PRICING.get("cloudfront_https_requests_per_10k", 0.010)
        request_cost = (requests_per_month / 10_000) * https_rate

        # Invalidation cost (first 1000 free)
        invalidation_rate = self.PRICING.get("cloudfront_invalidation_per_path", 0.005)
        invalidation_cost = max(0, invalidations_per_month - 1000) * invalidation_rate

        total_cost = data_transfer_cost + request_cost + invalidation_cost
        return total_cost

    def _calculate_cloudfront_optimization(
        self,
        distribution: dict[str, Any],
        requests_per_month: float,
        data_transfer_gb: float,
        error_rate_4xx: float,
        error_rate_5xx: float,
        origin_latency_ms: float,
        price_class: str,
        invalidations_per_month: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list[dict[str, Any]]]:
        """
        Analyze AWS CloudFront Distribution for cost optimization opportunities.

        Optimization scenarios (5):
        1. Completely inactive - No requests (critical)
        2. Very low usage - <10K requests/month (high)
        3. High origin latency - >500ms avg, poor caching (high)
        4. Price class optimization - All Edges when NA/EU sufficient (medium)
        5. Excessive invalidations - >1000/month, use versioning (medium)

        Returns:
            (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        is_optimizable = False
        optimization_score = 0
        priority = "low"
        potential_savings = 0.0
        recommendations = []

        # Scenario 1: CRITICAL - Completely inactive (no requests)
        if requests_per_month == 0.0:
            is_optimizable = True
            optimization_score = 95
            priority = "critical"
            potential_savings = monthly_cost  # Full savings
            recommendations.append(
                {
                    "type": "delete_distribution",
                    "title": "Delete Completely Inactive CloudFront Distribution",
                    "description": f"Distribution '{distribution.get('Id', '')}' has received NO requests "
                    f"in the last 7 days. This distribution is completely unused and costs ${monthly_cost:.2f}/month.",
                    "impact": "high",
                    "estimated_savings": f"${monthly_cost:.2f}/month",
                    "action": "Delete this unused CloudFront distribution",
                    "risk": "low",
                }
            )
            return (is_optimizable, optimization_score, priority, potential_savings, recommendations)

        # Scenario 2: HIGH - Very low usage (<10K requests/month)
        if requests_per_month > 0 and requests_per_month < 10_000:
            is_optimizable = True
            optimization_score = max(optimization_score, 85)
            priority = "high" if priority != "critical" else priority

            # For very low traffic, direct S3 access may be cheaper
            recommendations.append(
                {
                    "type": "low_usage_consolidate",
                    "title": f"Very Low Usage ({requests_per_month:.0f} Requests/Month)",
                    "description": f"Distribution '{distribution.get('Id', '')}' receives only {requests_per_month:.0f} requests/month. "
                    f"For such low traffic, consider direct S3 access or consolidating with another distribution.",
                    "impact": "medium",
                    "estimated_savings": f"${monthly_cost * 0.70:.2f}/month (if using direct S3 access)",
                    "action": "Review if CloudFront is necessary for this low traffic volume",
                    "risk": "medium",
                }
            )
            potential_savings = max(potential_savings, monthly_cost * 0.70)

        # Scenario 3: HIGH - High origin latency (>500ms avg) indicates poor caching
        if origin_latency_ms > 500.0:
            is_optimizable = True
            optimization_score = max(optimization_score, 80)
            priority = "high" if priority != "critical" else priority

            # High latency = going to origin too often = cache not effective
            recommendations.append(
                {
                    "type": "optimize_caching",
                    "title": f"High Origin Latency ({origin_latency_ms:.0f}ms Average)",
                    "description": f"Distribution '{distribution.get('Id', '')}' has high origin latency ({origin_latency_ms:.0f}ms avg), "
                    f"indicating poor cache hit rate. Optimize Cache-Control headers or use Lambda@Edge for better caching.",
                    "impact": "high",
                    "estimated_savings": f"${monthly_cost * 0.30:.2f}/month (by improving cache hit rate)",
                    "action": "Review and optimize caching strategy (TTL, Cache-Control headers)",
                    "risk": "low",
                }
            )
            potential_savings = max(potential_savings, monthly_cost * 0.30)

        # Scenario 4: MEDIUM - Price class optimization
        # PriceClass_All = most expensive, PriceClass_100 = cheapest (US/Europe only)
        if price_class == "PriceClass_All":
            is_optimizable = True
            optimization_score = max(optimization_score, 65)
            priority = "medium" if priority == "low" else priority

            # Estimate savings by switching from All Edges to US/Europe only
            all_edges_rate = self.PRICING.get("cloudfront_data_transfer_other_per_gb", 0.170)
            us_europe_rate = self.PRICING.get("cloudfront_data_transfer_us_europe_per_gb", 0.085)
            data_transfer_savings = data_transfer_gb * (all_edges_rate - us_europe_rate)
            potential_savings = max(potential_savings, data_transfer_savings)

            recommendations.append(
                {
                    "type": "optimize_price_class",
                    "title": f"Price Class 'All Edges' May Be Overkill",
                    "description": f"Distribution '{distribution.get('Id', '')}' uses 'All Edges' price class. "
                    f"If most users are in North America/Europe, switch to 'Use Only US, Canada and Europe' to save 50% on data transfer.",
                    "impact": "low",
                    "estimated_savings": f"${data_transfer_savings:.2f}/month",
                    "action": "Change price class from 'All' to 'US/Europe' if traffic is mostly NA/EU",
                    "risk": "low",
                }
            )

        # Scenario 5: MEDIUM - Excessive invalidations (>1000/month)
        if invalidations_per_month > 1000:
            is_optimizable = True
            optimization_score = max(optimization_score, 60)
            priority = "medium" if priority == "low" else priority

            # Calculate invalidation cost savings
            invalidation_rate = self.PRICING.get("cloudfront_invalidation_per_path", 0.005)
            invalidation_cost = (invalidations_per_month - 1000) * invalidation_rate
            potential_savings = max(potential_savings, invalidation_cost)

            recommendations.append(
                {
                    "type": "reduce_invalidations",
                    "title": f"Excessive Invalidations ({invalidations_per_month} Paths/Month)",
                    "description": f"Distribution '{distribution.get('Id', '')}' has {invalidations_per_month} invalidation "
                    f"requests/month. After first 1,000 free, you're charged ${invalidation_cost:.2f}/month. "
                    f"Use versioned file names (e.g., app.v2.js) instead of invalidations.",
                    "impact": "low",
                    "estimated_savings": f"${invalidation_cost:.2f}/month",
                    "action": "Implement versioning strategy (file names with versions) to avoid invalidations",
                    "risk": "low",
                }
            )

        return (is_optimizable, optimization_score, priority, potential_savings, recommendations)

    async def scan_cloudfront_distributions(self) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS CloudFront Distributions for cost intelligence.

         IMPORTANT: CloudFront is a GLOBAL service (not regional).
        This method scans all distributions regardless of region.

        For each distribution:
        - Calculate monthly cost (requests + data transfer + invalidations)
        - Fetch CloudWatch metrics (Requests, BytesDownloaded, ErrorRate, OriginLatency)
        - Analyze optimization opportunities (5 scenarios)

        Returns:
            List of AllCloudResourceData entries for all CloudFront Distributions
        """
        resources: list[AllCloudResourceData] = []

        try:
            # CloudFront is a global service, client doesn't take region parameter
            async with self.session.client("cloudfront") as cloudfront:
                async with self.session.client("cloudwatch", region_name="us-east-1") as cloudwatch:
                    # CloudWatch metrics for CloudFront are ONLY in us-east-1

                    # List all distributions
                    paginator = cloudfront.get_paginator("list_distributions")
                    distributions = []

                    async for page in paginator.paginate():
                        dist_list = page.get("DistributionList", {})
                        distributions.extend(dist_list.get("Items", []))

                    logger.info(
                        "inventory.cloudfront.distributions_found",
                        count=len(distributions),
                    )

                    # Process each distribution
                    for dist in distributions:
                        try:
                            dist_id = dist.get("Id", "")
                            dist_domain_name = dist.get("DomainName", "")
                            dist_status = dist.get("Status", "")
                            dist_enabled = dist.get("Enabled", False)
                            price_class = dist.get("PriceClass", "PriceClass_All")

                            # Get CloudWatch metrics (7 days)
                            # NOTE: CloudFront metrics are in us-east-1 regardless of distribution location
                            end_time = datetime.utcnow()
                            start_time = end_time - timedelta(days=7)

                            # Metric 1: Requests (sum over 7 days)
                            requests_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/CloudFront",
                                MetricName="Requests",
                                Dimensions=[
                                    {"Name": "DistributionId", "Value": dist_id},
                                    {"Name": "Region", "Value": "Global"},
                                ],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,  # 7 days
                                Statistics=["Sum"],
                            )
                            requests_7d = (
                                requests_response["Datapoints"][0]["Sum"]
                                if requests_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 2: BytesDownloaded (sum)
                            bytes_downloaded_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/CloudFront",
                                MetricName="BytesDownloaded",
                                Dimensions=[
                                    {"Name": "DistributionId", "Value": dist_id},
                                    {"Name": "Region", "Value": "Global"},
                                ],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Sum"],
                            )
                            bytes_downloaded = (
                                bytes_downloaded_response["Datapoints"][0]["Sum"]
                                if bytes_downloaded_response.get("Datapoints")
                                else 0.0
                            )
                            data_transfer_gb_7d = bytes_downloaded / (1024 ** 3)  # Convert to GB

                            # Metric 3: 4xxErrorRate (average)
                            error_4xx_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/CloudFront",
                                MetricName="4xxErrorRate",
                                Dimensions=[
                                    {"Name": "DistributionId", "Value": dist_id},
                                    {"Name": "Region", "Value": "Global"},
                                ],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Average"],
                            )
                            error_rate_4xx = (
                                error_4xx_response["Datapoints"][0]["Average"]
                                if error_4xx_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 4: 5xxErrorRate (average)
                            error_5xx_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/CloudFront",
                                MetricName="5xxErrorRate",
                                Dimensions=[
                                    {"Name": "DistributionId", "Value": dist_id},
                                    {"Name": "Region", "Value": "Global"},
                                ],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Average"],
                            )
                            error_rate_5xx = (
                                error_5xx_response["Datapoints"][0]["Average"]
                                if error_5xx_response.get("Datapoints")
                                else 0.0
                            )

                            # Metric 5: OriginLatency (average)
                            origin_latency_response = await cloudwatch.get_metric_statistics(
                                Namespace="AWS/CloudFront",
                                MetricName="OriginLatency",
                                Dimensions=[
                                    {"Name": "DistributionId", "Value": dist_id},
                                    {"Name": "Region", "Value": "Global"},
                                ],
                                StartTime=start_time,
                                EndTime=end_time,
                                Period=604800,
                                Statistics=["Average"],
                            )
                            origin_latency_ms = (
                                origin_latency_response["Datapoints"][0]["Average"]
                                if origin_latency_response.get("Datapoints")
                                else 0.0
                            )

                            # Estimate monthly metrics from 7-day data
                            requests_per_month = requests_7d * 4.33
                            data_transfer_gb_per_month = data_transfer_gb_7d * 4.33

                            # Estimate invalidations (not available via CloudWatch, use placeholder)
                            # In real scenario, would need to track via CloudFront API or logs
                            invalidations_per_month = 0

                            # Calculate cost
                            monthly_cost = self._calculate_cloudfront_monthly_cost(
                                requests_per_month=requests_per_month,
                                data_transfer_gb_per_month=data_transfer_gb_per_month,
                                invalidations_per_month=invalidations_per_month,
                                price_class=price_class,
                            )

                            # Calculate optimization
                            (
                                is_optimizable,
                                optimization_score,
                                priority,
                                potential_savings,
                                recommendations,
                            ) = self._calculate_cloudfront_optimization(
                                distribution=dist,
                                requests_per_month=requests_per_month,
                                data_transfer_gb=data_transfer_gb_7d,
                                error_rate_4xx=error_rate_4xx,
                                error_rate_5xx=error_rate_5xx,
                                origin_latency_ms=origin_latency_ms,
                                price_class=price_class,
                                invalidations_per_month=invalidations_per_month,
                                monthly_cost=monthly_cost,
                            )

                            # Build metadata
                            resource_metadata = {
                                "distribution_id": dist_id,
                                "domain_name": dist_domain_name,
                                "status": dist_status,
                                "enabled": dist_enabled,
                                "price_class": price_class,
                                "metrics": {
                                    "requests_7d": requests_7d,
                                    "data_transfer_gb_7d": data_transfer_gb_7d,
                                    "error_rate_4xx_avg": error_rate_4xx,
                                    "error_rate_5xx_avg": error_rate_5xx,
                                    "origin_latency_ms_avg": origin_latency_ms,
                                },
                            }

                            # Apply detection rules filtering
                            resource_age_days = None
                            if isinstance(created_at, datetime):
                                resource_age_days = (datetime.utcnow() - created_at.replace(tzinfo=None)).days

                            if not self._should_include_resource("cloudfront_distribution", resource_age_days):
                                logger.debug("inventory.cloudfront_filtered", dist_id=dist_id, age=resource_age_days)
                                continue

                            # Create resource entry (region = "global" for CloudFront)
                            resource = AllCloudResourceData(
                                resource_id=dist_id,
                                resource_type="cloudfront_distribution",
                                resource_name=dist_domain_name,
                                region="global",  # CloudFront is global
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata=resource_metadata,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                                optimization_priority=priority,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=recommendations,
                            )

                            resources.append(resource)

                            logger.info(
                                "inventory.cloudfront.distribution_scanned",
                                distribution_id=dist_id,
                                domain_name=dist_domain_name,
                                requests_7d=requests_7d,
                                monthly_cost=monthly_cost,
                                is_optimizable=is_optimizable,
                                optimization_score=optimization_score,
                            )

                        except Exception as e:
                            logger.error(
                                "inventory.cloudfront.distribution_scan_error",
                                distribution_id=dist.get("Id", "unknown"),
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "cloudfront.scan_failed",
                error=str(e),
            )

        return resources

    # ============================================================================
    # AWS - ECS CLUSTER (Cost Intelligence / Inventory Mode)
    # ============================================================================

    def _calculate_ecs_cluster_monthly_cost(
        self,
        total_fargate_vcpu: float,
        total_fargate_memory_gb: float,
        total_ec2_instances: int,
        service_connect_enabled: bool,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS ECS Cluster.

        Cost structure:
        - ECS Control Plane: FREE (no cost for cluster itself)
        - Fargate compute: $0.04048/vCPU-hour + $0.004445/GB-hour
        - EC2 instances: Separate cost (instance pricing)
        - Service Connect (optional): $0.01/vCPU-hour

        Args:
            total_fargate_vcpu: Total Fargate vCPUs running in cluster
            total_fargate_memory_gb: Total Fargate memory GB running in cluster
            total_ec2_instances: Number of EC2 container instances
            service_connect_enabled: Whether Service Connect is enabled
            region: AWS region

        Returns:
            Estimated monthly cost for cluster (Fargate compute + Service Connect)
        """
        # Fargate compute cost (monthly = 730 hours)
        fargate_vcpu_cost = total_fargate_vcpu * 0.04048 * 730
        fargate_memory_cost = total_fargate_memory_gb * 0.004445 * 730
        fargate_total = fargate_vcpu_cost + fargate_memory_cost

        # Service Connect cost (if enabled)
        service_connect_cost = 0.0
        if service_connect_enabled and total_fargate_vcpu > 0:
            service_connect_rate = self.PRICING.get("ecs_service_connect_per_vcpu_hour", 0.01)
            service_connect_cost = total_fargate_vcpu * service_connect_rate * 730

        # Note: EC2 instance costs are tracked separately (not included here)
        total_cost = fargate_total + service_connect_cost

        return total_cost

    def _calculate_ecs_cluster_optimization(
        self,
        cluster_name: str,
        running_tasks_count: int,
        active_services_count: int,
        registered_container_instances: int,
        capacity_providers: list,
        avg_cpu_utilization: float,
        avg_memory_utilization: float,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list]:
        """
        Calculate optimization opportunities for ECS Cluster.

        Scenarios:
        1. CRITICAL (95): Empty cluster (0 tasks, 0 services)  Delete cluster
        2. HIGH (85): Low usage (<5 tasks, <3 services)  Consolidate to another cluster
        3. HIGH (80): Inactive services (services with desired count = 0)  Clean up
        4. MEDIUM (70): Mixed Fargate + EC2 (inefficient)  Pure Fargate or pure EC2
        5. MEDIUM (65): No auto-scaling configured  Enable ECS Service Auto Scaling

        Args:
            cluster_name: ECS cluster name
            running_tasks_count: Number of running tasks
            active_services_count: Number of active services
            registered_container_instances: Number of EC2 container instances
            capacity_providers: List of capacity providers (["FARGATE", "FARGATE_SPOT", "EC2", etc.])
            avg_cpu_utilization: Average CPU utilization (%)
            avg_memory_utilization: Average memory utilization (%)
            monthly_cost: Estimated monthly cost

        Returns:
            Tuple of (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        recommendations = []
        optimization_score = 0
        optimization_priority = "none"
        potential_savings = 0.0
        is_optimizable = False

        # Scenario 1: CRITICAL - Empty cluster (0 tasks, 0 services)
        if running_tasks_count == 0 and active_services_count == 0:
            optimization_score = 95
            optimization_priority = "critical"
            potential_savings = monthly_cost  # Save entire cluster cost
            is_optimizable = True
            recommendations.append({
                "scenario": "empty_cluster",
                "action": "delete_cluster",
                "priority": "critical",
                "estimated_savings": monthly_cost,
                "details": f"Cluster '{cluster_name}' is empty (0 tasks, 0 services). Delete to eliminate costs.",
                "recommendation": "Delete cluster - no workloads running"
            })

        # Scenario 2: HIGH - Low usage (<5 tasks, <3 services)
        if running_tasks_count > 0 and running_tasks_count < 5 and active_services_count < 3 and optimization_score < 85:
            optimization_score = max(optimization_score, 85)
            optimization_priority = "high" if optimization_priority == "none" else optimization_priority
            potential_savings = max(potential_savings, monthly_cost * 0.4)
            is_optimizable = True
            recommendations.append({
                "scenario": "low_usage",
                "action": "consolidate_cluster",
                "priority": "high",
                "estimated_savings": monthly_cost * 0.4,
                "details": f"Cluster '{cluster_name}' has only {running_tasks_count} tasks and {active_services_count} services. Consolidate to another cluster.",
                "recommendation": "Consolidate workloads into a single cluster to reduce overhead (40% savings)"
            })

        # Scenario 3: HIGH - Inactive services (detect via metadata if available)
        # Note: This requires service-level metadata, simplified here
        # In production, you'd fetch describe_services() to check desiredCount
        if active_services_count == 0 and running_tasks_count > 0 and optimization_score < 80:
            optimization_score = max(optimization_score, 80)
            optimization_priority = "high" if optimization_priority == "none" else optimization_priority
            is_optimizable = True
            recommendations.append({
                "scenario": "inactive_services",
                "action": "cleanup_services",
                "priority": "high",
                "estimated_savings": 0.0,
                "details": f"Cluster '{cluster_name}' has services with desired count = 0. Clean up unused service definitions.",
                "recommendation": "Delete inactive services (no cost impact, but improves visibility)"
            })

        # Scenario 4: MEDIUM - Mixed Fargate + EC2 (inefficient)
        has_fargate = any("FARGATE" in cp for cp in capacity_providers)
        has_ec2 = registered_container_instances > 0 or "EC2" in " ".join(capacity_providers)

        if has_fargate and has_ec2 and optimization_score < 70:
            optimization_score = max(optimization_score, 70)
            optimization_priority = "medium" if optimization_priority == "none" else optimization_priority
            potential_savings = max(potential_savings, monthly_cost * 0.2)
            is_optimizable = True
            recommendations.append({
                "scenario": "mixed_capacity_providers",
                "action": "standardize_launch_type",
                "priority": "medium",
                "estimated_savings": monthly_cost * 0.2,
                "details": f"Cluster '{cluster_name}' uses both Fargate and EC2 instances. Standardize to pure Fargate or pure EC2 for better cost efficiency.",
                "recommendation": "Use pure Fargate (easier management) or pure EC2 (lower cost for steady workloads)",
                "alternatives": [
                    {"name": "Pure Fargate", "savings": 0.0, "benefit": "Serverless, no instance management"},
                    {"name": "Pure EC2", "savings": monthly_cost * 0.2, "benefit": "20% cheaper for steady workloads"}
                ]
            })

        # Scenario 5: MEDIUM - No auto-scaling (heuristic: low CPU/memory utilization suggests over-provisioning)
        if (avg_cpu_utilization < 30 or avg_memory_utilization < 30) and running_tasks_count > 3 and optimization_score < 65:
            optimization_score = max(optimization_score, 65)
            optimization_priority = "medium" if optimization_priority == "none" else optimization_priority
            is_optimizable = True
            recommendations.append({
                "scenario": "no_auto_scaling",
                "action": "enable_auto_scaling",
                "priority": "medium",
                "estimated_savings": 0.0,
                "details": f"Cluster '{cluster_name}' has low CPU ({avg_cpu_utilization:.1f}%) or memory ({avg_memory_utilization:.1f}%) utilization. Enable ECS Service Auto Scaling to scale down during low demand.",
                "recommendation": "Configure ECS Service Auto Scaling (target CPU 70%, target memory 80%)"
            })

        return is_optimizable, optimization_score, optimization_priority, potential_savings, recommendations

    async def scan_ecs_clusters(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS ECS Clusters for cost intelligence.

        This method scans all ECS clusters to provide cost visibility and
        optimization recommendations for container orchestration.

        CloudWatch Metrics (AWS/ECS namespace):
        - CPUUtilization (cluster-level)
        - MemoryUtilization (cluster-level)

        Args:
            region: AWS region to scan

        Returns:
            List of AllCloudResourceData objects for all ECS clusters
        """
        resources: list[AllCloudResourceData] = []

        try:
            async with self.session.client("ecs", region_name=region) as ecs:
                async with self.session.client("cloudwatch", region_name=region) as cloudwatch:
                    # List all ECS clusters
                    list_response = await ecs.list_clusters()
                    cluster_arns = list_response.get("clusterArns", [])

                    if not cluster_arns:
                        logger.info("ecs.no_clusters_found", region=region)
                        return resources

                    # Describe all clusters
                    describe_response = await ecs.describe_clusters(clusters=cluster_arns)
                    clusters = describe_response.get("clusters", [])

                    for cluster in clusters:
                        try:
                            cluster_name = cluster.get("clusterName", "unknown")
                            cluster_arn = cluster.get("clusterArn", "")
                            status = cluster.get("status", "UNKNOWN")
                            running_tasks_count = cluster.get("runningTasksCount", 0)
                            pending_tasks_count = cluster.get("pendingTasksCount", 0)
                            active_services_count = cluster.get("activeServicesCount", 0)
                            registered_container_instances = cluster.get("registeredContainerInstancesCount", 0)
                            capacity_providers = cluster.get("capacityProviders", [])
                            default_capacity_provider_strategy = cluster.get("defaultCapacityProviderStrategy", [])

                            # Extract tags
                            tags = {}
                            for tag in cluster.get("tags", []):
                                tags[tag["Key"]] = tag["Value"]

                            # Get cluster settings (Service Connect, etc.)
                            settings = cluster.get("settings", [])
                            service_connect_enabled = any(
                                setting.get("name") == "containerInsights" and setting.get("value") == "enabled"
                                for setting in settings
                            )

                            # List all services in cluster
                            services_response = await ecs.list_services(cluster=cluster_arn)
                            service_arns = services_response.get("serviceArns", [])

                            # List all tasks in cluster
                            tasks_response = await ecs.list_tasks(cluster=cluster_arn)
                            task_arns = tasks_response.get("taskArns", [])

                            # Calculate total Fargate vCPU and memory (estimate from running tasks)
                            total_fargate_vcpu = 0.0
                            total_fargate_memory_gb = 0.0

                            if task_arns:
                                # Limit to first 100 tasks for performance
                                task_arns_sample = task_arns[:100]
                                tasks_describe_response = await ecs.describe_tasks(
                                    cluster=cluster_arn,
                                    tasks=task_arns_sample
                                )
                                tasks = tasks_describe_response.get("tasks", [])

                                for task in tasks:
                                    launch_type = task.get("launchType", "")
                                    if launch_type == "FARGATE":
                                        # Get task definition to extract vCPU and memory
                                        task_def_arn = task.get("taskDefinitionArn", "")
                                        if task_def_arn:
                                            try:
                                                task_def_response = await ecs.describe_task_definition(
                                                    taskDefinition=task_def_arn
                                                )
                                                task_definition = task_def_response["taskDefinition"]
                                                cpu_str = task_definition.get("cpu", "256")
                                                memory_str = task_definition.get("memory", "512")
                                                vcpu = float(cpu_str) / 1024
                                                memory_gb = float(memory_str) / 1024
                                                total_fargate_vcpu += vcpu
                                                total_fargate_memory_gb += memory_gb
                                            except Exception:
                                                pass

                            # Get CloudWatch metrics (last 7 days)
                            avg_cpu_utilization = 0.0
                            avg_memory_utilization = 0.0

                            try:
                                end_time = datetime.utcnow()
                                start_time = end_time - timedelta(days=7)

                                # CPU Utilization (cluster-level)
                                cpu_response = await cloudwatch.get_metric_statistics(
                                    Namespace="AWS/ECS",
                                    MetricName="CPUUtilization",
                                    Dimensions=[
                                        {"Name": "ClusterName", "Value": cluster_name}
                                    ],
                                    StartTime=start_time,
                                    EndTime=end_time,
                                    Period=604800,  # 7 days
                                    Statistics=["Average"],
                                )

                                datapoints = cpu_response.get("Datapoints", [])
                                if datapoints:
                                    avg_cpu_utilization = datapoints[0].get("Average", 0.0)
                            except Exception:
                                pass

                            try:
                                # Memory Utilization (cluster-level)
                                memory_response = await cloudwatch.get_metric_statistics(
                                    Namespace="AWS/ECS",
                                    MetricName="MemoryUtilization",
                                    Dimensions=[
                                        {"Name": "ClusterName", "Value": cluster_name}
                                    ],
                                    StartTime=start_time,
                                    EndTime=end_time,
                                    Period=604800,  # 7 days
                                    Statistics=["Average"],
                                )

                                datapoints = memory_response.get("Datapoints", [])
                                if datapoints:
                                    avg_memory_utilization = datapoints[0].get("Average", 0.0)
                            except Exception:
                                pass

                            # Calculate monthly cost
                            monthly_cost = self._calculate_ecs_cluster_monthly_cost(
                                total_fargate_vcpu=total_fargate_vcpu,
                                total_fargate_memory_gb=total_fargate_memory_gb,
                                total_ec2_instances=registered_container_instances,
                                service_connect_enabled=service_connect_enabled,
                                region=region,
                            )

                            # Calculate optimization metrics
                            (
                                is_optimizable,
                                optimization_score,
                                optimization_priority,
                                potential_savings,
                                optimization_recommendations,
                            ) = self._calculate_ecs_cluster_optimization(
                                cluster_name=cluster_name,
                                running_tasks_count=running_tasks_count,
                                active_services_count=active_services_count,
                                registered_container_instances=registered_container_instances,
                                capacity_providers=capacity_providers,
                                avg_cpu_utilization=avg_cpu_utilization,
                                avg_memory_utilization=avg_memory_utilization,
                                monthly_cost=monthly_cost,
                            )

                            # Determine utilization status
                            if running_tasks_count == 0 and active_services_count == 0:
                                utilization_status = "idle"
                            elif running_tasks_count < 5:
                                utilization_status = "low"
                            elif running_tasks_count < 20:
                                utilization_status = "medium"
                            else:
                                utilization_status = "high"

                            # Build metadata
                            metadata = {
                                "cluster_arn": cluster_arn,
                                "status": status,
                                "running_tasks_count": running_tasks_count,
                                "pending_tasks_count": pending_tasks_count,
                                "active_services_count": active_services_count,
                                "registered_container_instances_count": registered_container_instances,
                                "capacity_providers": capacity_providers,
                                "default_capacity_provider_strategy": default_capacity_provider_strategy,
                                "service_count": len(service_arns),
                                "task_count": len(task_arns),
                                "total_fargate_vcpu": total_fargate_vcpu,
                                "total_fargate_memory_gb": total_fargate_memory_gb,
                                "service_connect_enabled": service_connect_enabled,
                            }

                            # Apply detection rules filtering
                            # Note: ECS clusters don't have creation timestamp in API, so age is always None
                            resource_age_days = None

                            if not self._should_include_resource("ecs_cluster", resource_age_days):
                                logger.debug("inventory.ecs_filtered", cluster_name=cluster_name, age=resource_age_days)
                                continue

                            # Create resource
                            resource = AllCloudResourceData(
                                resource_type="ecs_cluster",
                                resource_id=cluster_name,
                                resource_name=cluster_name,
                                region=region,
                                estimated_monthly_cost=monthly_cost,
                                resource_metadata=metadata,
                                currency="USD",
                                utilization_status=utilization_status,
                                cpu_utilization_percent=avg_cpu_utilization,
                                memory_utilization_percent=avg_memory_utilization,
                                storage_utilization_percent=None,
                                network_utilization_mbps=None,
                                is_optimizable=is_optimizable,
                                optimization_priority=optimization_priority,
                                optimization_score=optimization_score,
                                potential_monthly_savings=potential_savings,
                                optimization_recommendations=optimization_recommendations,
                                tags=tags,
                                resource_status=status,
                                is_orphan=running_tasks_count == 0 and active_services_count == 0,
                                created_at_cloud=None,  # ECS clusters don't have creation timestamp in API
                                last_used_at=None,
                            )

                            resources.append(resource)

                        except Exception as e:
                            logger.error(
                                "ecs.cluster_scan_failed",
                                cluster_name=cluster.get("clusterName", "unknown"),
                                region=region,
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "ecs.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # ============================================================================
    # AWS - CLOUDWATCH LOG GROUP (Cost Intelligence / Inventory Mode)
    # ============================================================================

    def _calculate_cloudwatch_log_group_monthly_cost(
        self,
        stored_bytes: float,
        ingestion_bytes_per_month: float,
        retention_days: int,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS CloudWatch Log Group.

        Cost structure:
        - Ingestion: $0.50 per GB
        - Storage: $0.03 per GB/month (after retention period)
        - Vended Logs (VPC Flow, Lambda, RDS, etc.): FREE ingestion
        - Insights Queries: $0.005 per GB scanned (not included here)

        Args:
            stored_bytes: Total bytes stored in log group
            ingestion_bytes_per_month: Monthly ingestion in bytes
            retention_days: Log retention period in days
            region: AWS region

        Returns:
            Estimated monthly cost for log group
        """
        # Convert bytes to GB
        stored_gb = stored_bytes / (1024 ** 3)
        ingestion_gb_per_month = ingestion_bytes_per_month / (1024 ** 3)

        # Ingestion cost
        ingestion_rate = self.PRICING.get("cloudwatch_logs_ingestion_per_gb", 0.50)
        ingestion_cost = ingestion_gb_per_month * ingestion_rate

        # Storage cost (only charged after retention period)
        storage_rate = self.PRICING.get("cloudwatch_logs_storage_per_gb", 0.03)
        storage_cost = stored_gb * storage_rate

        total_cost = ingestion_cost + storage_cost

        return total_cost

    def _calculate_cloudwatch_log_group_optimization(
        self,
        log_group_name: str,
        stored_bytes: float,
        ingestion_bytes_per_month: float,
        retention_days: int,
        creation_time: int,
        metric_filter_count: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list]:
        """
        Calculate optimization opportunities for CloudWatch Log Group.

        Scenarios:
        1. CRITICAL (95): Empty log group (0 bytes, never used since 30+ days)  Delete
        2. HIGH (85): Excessive retention (365+ days, <1 GB stored)  Reduce to 30 days
        3. HIGH (80): Orphaned log group (resource source deleted)  Delete
        4. MEDIUM (70): Large size (>100 GB) + long retention (>90 days)  Archive to S3
        5. MEDIUM (65): High ingestion (>10 GB/day) without filters  Add subscription filters
        6. LOW (50): Critical logs without S3 export  Configure export

        Args:
            log_group_name: Log group name
            stored_bytes: Total bytes stored
            ingestion_bytes_per_month: Monthly ingestion in bytes
            retention_days: Retention period in days
            creation_time: Unix timestamp of log group creation
            metric_filter_count: Number of metric filters configured
            monthly_cost: Estimated monthly cost

        Returns:
            Tuple of (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        recommendations = []
        optimization_score = 0
        optimization_priority = "none"
        potential_savings = 0.0
        is_optimizable = False

        stored_gb = stored_bytes / (1024 ** 3)
        ingestion_gb_per_month = ingestion_bytes_per_month / (1024 ** 3)
        ingestion_gb_per_day = ingestion_gb_per_month / 30

        # Calculate age in days
        age_days = 0
        if creation_time:
            age_days = (datetime.utcnow().timestamp() - creation_time) / 86400

        # Scenario 1: CRITICAL - Empty log group (never used since 30+ days)
        if stored_bytes == 0 and ingestion_bytes_per_month == 0 and age_days > 30:
            optimization_score = 95
            optimization_priority = "critical"
            potential_savings = monthly_cost
            is_optimizable = True
            recommendations.append({
                "scenario": "empty_log_group",
                "action": "delete_log_group",
                "priority": "critical",
                "estimated_savings": monthly_cost,
                "details": f"Log group '{log_group_name}' is empty and unused for {age_days:.0f} days. Delete to eliminate costs.",
                "recommendation": "Delete log group - no data logged"
            })

        # Scenario 2: HIGH - Excessive retention (365+ days, <1 GB stored)
        if retention_days >= 365 and stored_gb < 1 and optimization_score < 85:
            optimization_score = max(optimization_score, 85)
            optimization_priority = "high" if optimization_priority == "none" else optimization_priority
            # Reducing retention from 365 to 30 days saves 90% storage cost
            storage_savings = monthly_cost * 0.9
            potential_savings = max(potential_savings, storage_savings)
            is_optimizable = True
            recommendations.append({
                "scenario": "excessive_retention",
                "action": "reduce_retention",
                "priority": "high",
                "estimated_savings": storage_savings,
                "details": f"Log group '{log_group_name}' has 365+ days retention but only {stored_gb:.2f} GB stored. Reduce retention to 30 days.",
                "recommendation": "Reduce retention to 30 days (90% storage savings)",
                "alternatives": [
                    {"name": "30 days", "savings": storage_savings, "benefit": "Standard retention"},
                    {"name": "90 days", "savings": storage_savings * 0.7, "benefit": "Compliance-friendly"}
                ]
            })

        # Scenario 3: HIGH - Orphaned log group (heuristic: contains "/aws/" in name but no recent ingestion)
        is_aws_service_log = "/aws/" in log_group_name or log_group_name.startswith("aws/")
        if is_aws_service_log and ingestion_bytes_per_month == 0 and age_days > 7 and optimization_score < 80:
            optimization_score = max(optimization_score, 80)
            optimization_priority = "high" if optimization_priority == "none" else optimization_priority
            potential_savings = max(potential_savings, monthly_cost)
            is_optimizable = True
            recommendations.append({
                "scenario": "orphaned_log_group",
                "action": "delete_orphaned",
                "priority": "high",
                "estimated_savings": monthly_cost,
                "details": f"Log group '{log_group_name}' appears orphaned (AWS service log with no ingestion for {age_days:.0f} days). Source resource may be deleted.",
                "recommendation": "Delete orphaned log group - source resource deleted"
            })

        # Scenario 4: MEDIUM - Large size (>100 GB) + long retention (>90 days)
        if stored_gb > 100 and retention_days > 90 and optimization_score < 70:
            optimization_score = max(optimization_score, 70)
            optimization_priority = "medium" if optimization_priority == "none" else optimization_priority
            # Archive to S3 saves 40% (S3 is cheaper than CloudWatch Logs storage)
            archive_savings = monthly_cost * 0.4
            potential_savings = max(potential_savings, archive_savings)
            is_optimizable = True
            recommendations.append({
                "scenario": "large_log_group",
                "action": "archive_to_s3",
                "priority": "medium",
                "estimated_savings": archive_savings,
                "details": f"Log group '{log_group_name}' has {stored_gb:.2f} GB stored with {retention_days} days retention. Archive older logs to S3.",
                "recommendation": "Export logs to S3 and reduce CloudWatch retention to 7-30 days (40% savings)"
            })

        # Scenario 5: MEDIUM - High ingestion (>10 GB/day) without filters
        if ingestion_gb_per_day > 10 and metric_filter_count == 0 and optimization_score < 65:
            optimization_score = max(optimization_score, 65)
            optimization_priority = "medium" if optimization_priority == "none" else optimization_priority
            # Subscription filters can reduce ingestion by 30%
            filter_savings = monthly_cost * 0.3
            potential_savings = max(potential_savings, filter_savings)
            is_optimizable = True
            recommendations.append({
                "scenario": "high_ingestion_no_filters",
                "action": "add_subscription_filters",
                "priority": "medium",
                "estimated_savings": filter_savings,
                "details": f"Log group '{log_group_name}' ingests {ingestion_gb_per_day:.2f} GB/day without subscription filters. Add filters to reduce noise.",
                "recommendation": "Configure subscription filters to drop verbose/debug logs (30% ingestion reduction)"
            })

        # Scenario 6: LOW - Critical logs without S3 export
        is_critical_log = any(keyword in log_group_name.lower() for keyword in ["prod", "production", "error", "critical"])
        if is_critical_log and stored_gb > 1 and optimization_score < 50:
            optimization_score = max(optimization_score, 50)
            optimization_priority = "low" if optimization_priority == "none" else optimization_priority
            is_optimizable = True
            recommendations.append({
                "scenario": "no_s3_export",
                "action": "configure_s3_export",
                "priority": "low",
                "estimated_savings": 0.0,
                "details": f"Log group '{log_group_name}' contains critical logs but no S3 export configured. Enable export for compliance/disaster recovery.",
                "recommendation": "Configure automated S3 export for long-term retention (no cost impact, but improves compliance)"
            })

        return is_optimizable, optimization_score, optimization_priority, potential_savings, recommendations

    async def scan_cloudwatch_log_groups(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS CloudWatch Log Groups for cost intelligence.

        This method scans all CloudWatch Log Groups to provide cost visibility
        and optimization recommendations for log management.

        CloudWatch Metrics (AWS/Logs namespace):
        - IncomingBytes (last 7 days)
        - IncomingLogEvents

        Args:
            region: AWS region to scan

        Returns:
            List of AllCloudResourceData objects for all CloudWatch Log Groups
        """
        resources: list[AllCloudResourceData] = []

        try:
            async with self.session.client("logs", region_name=region) as logs:
                async with self.session.client("cloudwatch", region_name=region) as cloudwatch:
                    # Paginate through all log groups
                    paginator = logs.get_paginator("describe_log_groups")

                    async for page in paginator.paginate():
                        log_groups = page.get("logGroups", [])

                        for log_group in log_groups:
                            try:
                                log_group_name = log_group.get("logGroupName", "unknown")
                                stored_bytes = log_group.get("storedBytes", 0)
                                retention_days = log_group.get("retentionInDays", 0)  # 0 = never expire
                                creation_time = log_group.get("creationTime", 0) / 1000  # Convert ms to seconds
                                kms_key_id = log_group.get("kmsKeyId")
                                metric_filter_count = log_group.get("metricFilterCount", 0)

                                # Get CloudWatch metrics for ingestion (last 30 days)
                                ingestion_bytes_per_month = 0.0

                                try:
                                    end_time = datetime.utcnow()
                                    start_time = end_time - timedelta(days=30)

                                    # IncomingBytes metric
                                    ingestion_response = await cloudwatch.get_metric_statistics(
                                        Namespace="AWS/Logs",
                                        MetricName="IncomingBytes",
                                        Dimensions=[
                                            {"Name": "LogGroupName", "Value": log_group_name}
                                        ],
                                        StartTime=start_time,
                                        EndTime=end_time,
                                        Period=2592000,  # 30 days
                                        Statistics=["Sum"],
                                    )

                                    datapoints = ingestion_response.get("Datapoints", [])
                                    if datapoints:
                                        ingestion_bytes_per_month = datapoints[0].get("Sum", 0.0)
                                except Exception:
                                    pass

                                # Calculate monthly cost
                                monthly_cost = self._calculate_cloudwatch_log_group_monthly_cost(
                                    stored_bytes=stored_bytes,
                                    ingestion_bytes_per_month=ingestion_bytes_per_month,
                                    retention_days=retention_days if retention_days > 0 else 365,  # Default to 365 if never expire
                                    region=region,
                                )

                                # Calculate optimization metrics
                                (
                                    is_optimizable,
                                    optimization_score,
                                    optimization_priority,
                                    potential_savings,
                                    optimization_recommendations,
                                ) = self._calculate_cloudwatch_log_group_optimization(
                                    log_group_name=log_group_name,
                                    stored_bytes=stored_bytes,
                                    ingestion_bytes_per_month=ingestion_bytes_per_month,
                                    retention_days=retention_days if retention_days > 0 else 365,
                                    creation_time=creation_time,
                                    metric_filter_count=metric_filter_count,
                                    monthly_cost=monthly_cost,
                                )

                                # Determine utilization status
                                stored_gb = stored_bytes / (1024 ** 3)
                                ingestion_gb_per_month = ingestion_bytes_per_month / (1024 ** 3)

                                if stored_bytes == 0 and ingestion_bytes_per_month == 0:
                                    utilization_status = "idle"
                                elif ingestion_gb_per_month < 1:  # <1 GB/month
                                    utilization_status = "low"
                                elif ingestion_gb_per_month < 10:  # <10 GB/month
                                    utilization_status = "medium"
                                else:
                                    utilization_status = "high"

                                # Build metadata
                                metadata = {
                                    "log_group_name": log_group_name,
                                    "stored_bytes": stored_bytes,
                                    "stored_gb": stored_gb,
                                    "retention_in_days": retention_days,
                                    "kms_key_id": kms_key_id,
                                    "creation_time": creation_time,
                                    "metric_filter_count": metric_filter_count,
                                    "ingestion_bytes_per_month": ingestion_bytes_per_month,
                                    "ingestion_gb_per_month": ingestion_gb_per_month,
                                    "ingestion_gb_per_day": ingestion_gb_per_month / 30,
                                }

                                # Apply detection rules filtering
                                resource_age_days = None
                                if creation_time > 0:
                                    resource_age_days = (datetime.utcnow() - datetime.fromtimestamp(creation_time)).days

                                if not self._should_include_resource("cloudwatch_log_group", resource_age_days):
                                    logger.debug("inventory.cloudwatch_log_filtered", log_group_name=log_group_name, age=resource_age_days)
                                    continue

                                # Create resource
                                resource = AllCloudResourceData(
                                    resource_type="cloudwatch_log_group",
                                    resource_id=log_group_name,
                                    resource_name=log_group_name,
                                    region=region,
                                    estimated_monthly_cost=monthly_cost,
                                    resource_metadata=metadata,
                                    currency="USD",
                                    utilization_status=utilization_status,
                                    cpu_utilization_percent=None,
                                    memory_utilization_percent=None,
                                    storage_utilization_percent=None,  # N/A for log groups
                                    network_utilization_mbps=None,
                                    is_optimizable=is_optimizable,
                                    optimization_priority=optimization_priority,
                                    optimization_score=optimization_score,
                                    potential_monthly_savings=potential_savings,
                                    optimization_recommendations=optimization_recommendations,
                                    tags={},  # Log groups don't support tags in API
                                    resource_status="ACTIVE",
                                    is_orphan=stored_bytes == 0 and ingestion_bytes_per_month == 0,
                                    created_at_cloud=datetime.fromtimestamp(creation_time) if creation_time else None,
                                    last_used_at=None,
                                )

                                resources.append(resource)

                            except Exception as e:
                                logger.error(
                                    "cloudwatch_logs.log_group_scan_failed",
                                    log_group_name=log_group.get("logGroupName", "unknown"),
                                    region=region,
                                    error=str(e),
                                )
                                continue

        except Exception as e:
            logger.error(
                "cloudwatch_logs.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # ============================================================================
    # AWS - VPC ENDPOINT (Cost Intelligence / Inventory Mode)
    # ============================================================================

    def _calculate_vpc_endpoint_monthly_cost(
        self,
        endpoint_type: str,
        availability_zone_count: int,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS VPC Endpoint.

        Cost structure:
        - Interface endpoint: $7.20/month per AZ
        - Gateway endpoint (S3, DynamoDB): FREE
        - Gateway Load Balancer endpoint: $7.20/month per AZ
        - Data transfer: $0.01/GB (not included here)

        Args:
            endpoint_type: Endpoint type (Interface, Gateway, GatewayLoadBalancer)
            availability_zone_count: Number of availability zones
            region: AWS region

        Returns:
            Estimated monthly cost for VPC endpoint
        """
        if endpoint_type == "Gateway":
            # Gateway endpoints (S3, DynamoDB) are FREE
            return 0.0

        # Interface and GatewayLoadBalancer endpoints cost $7.20/month per AZ
        base_cost = self.PRICING.get("vpc_endpoint", 7.20)
        total_cost = base_cost * max(availability_zone_count, 1)

        return total_cost

    def _calculate_vpc_endpoint_optimization(
        self,
        endpoint_id: str,
        endpoint_type: str,
        service_name: str,
        state: str,
        network_interface_count: int,
        availability_zone_count: int,
        monthly_cost: float,
    ) -> tuple[bool, int, str, float, list]:
        """
        Calculate optimization opportunities for VPC Endpoint.

        Scenarios:
        1. CRITICAL (95): Unused endpoint (0 network interfaces, state=available)  Delete
        2. HIGH (85): Interface endpoint for S3/DynamoDB  Use Gateway endpoint (FREE)
        3. MEDIUM (70): Multi-AZ for dev/test (unnecessary redundancy)  Single AZ
        4. MEDIUM (65): Gateway type available but using Interface  Migrate
        5. LOW (50): No private DNS enabled (security best practice)

        Args:
            endpoint_id: VPC endpoint ID
            endpoint_type: Endpoint type (Interface, Gateway, GatewayLoadBalancer)
            service_name: AWS service name (e.g., com.amazonaws.us-east-1.s3)
            state: Endpoint state (available, pending, deleting, etc.)
            network_interface_count: Number of network interfaces
            availability_zone_count: Number of AZs
            monthly_cost: Estimated monthly cost

        Returns:
            Tuple of (is_optimizable, optimization_score, priority, potential_savings, recommendations)
        """
        recommendations = []
        optimization_score = 0
        optimization_priority = "none"
        potential_savings = 0.0
        is_optimizable = False

        # Scenario 1: CRITICAL - Unused endpoint (0 network interfaces)
        if endpoint_type == "Interface" and network_interface_count == 0 and state == "available":
            optimization_score = 95
            optimization_priority = "critical"
            potential_savings = monthly_cost
            is_optimizable = True
            recommendations.append({
                "scenario": "unused_endpoint",
                "action": "delete_endpoint",
                "priority": "critical",
                "estimated_savings": monthly_cost,
                "details": f"VPC Endpoint '{endpoint_id}' has 0 network interfaces and is unused. Delete to eliminate costs.",
                "recommendation": "Delete VPC Endpoint - no active connections"
            })

        # Scenario 2: HIGH - Interface endpoint for S3/DynamoDB (Gateway is FREE)
        is_s3_or_dynamodb = ".s3" in service_name or ".dynamodb" in service_name
        if endpoint_type == "Interface" and is_s3_or_dynamodb and optimization_score < 85:
            optimization_score = max(optimization_score, 85)
            optimization_priority = "high" if optimization_priority == "none" else optimization_priority
            potential_savings = max(potential_savings, monthly_cost)
            is_optimizable = True
            service_type = "S3" if ".s3" in service_name else "DynamoDB"
            recommendations.append({
                "scenario": "interface_to_gateway",
                "action": "migrate_to_gateway",
                "priority": "high",
                "estimated_savings": monthly_cost,
                "details": f"VPC Endpoint '{endpoint_id}' uses Interface type for {service_type}. Gateway endpoints are FREE.",
                "recommendation": f"Migrate to Gateway endpoint for {service_type} (100% savings, from ${monthly_cost:.2f} to $0)"
            })

        # Scenario 3: MEDIUM - Multi-AZ for dev/test (unnecessary redundancy)
        # Heuristic: If endpoint has tags containing "dev", "test", "staging" with multi-AZ
        if endpoint_type == "Interface" and availability_zone_count > 1 and optimization_score < 70:
            optimization_score = max(optimization_score, 70)
            optimization_priority = "medium" if optimization_priority == "none" else optimization_priority
            # Reducing from multi-AZ to single AZ saves (az_count - 1) / az_count
            savings = monthly_cost * ((availability_zone_count - 1) / availability_zone_count)
            potential_savings = max(potential_savings, savings)
            is_optimizable = True
            recommendations.append({
                "scenario": "multi_az_dev_test",
                "action": "reduce_to_single_az",
                "priority": "medium",
                "estimated_savings": savings,
                "details": f"VPC Endpoint '{endpoint_id}' is deployed in {availability_zone_count} AZs. For dev/test, single AZ is sufficient.",
                "recommendation": f"Reduce to single AZ deployment ({((availability_zone_count - 1) / availability_zone_count) * 100:.0f}% savings)"
            })

        # Scenario 4: MEDIUM - Gateway type available but using Interface
        if endpoint_type == "Interface" and is_s3_or_dynamodb and optimization_score < 65:
            # This is similar to scenario 2, but with lower priority if already covered
            pass

        # Scenario 5: LOW - Security best practice (no cost impact)
        if endpoint_type == "Interface" and optimization_score < 50:
            optimization_score = max(optimization_score, 50)
            optimization_priority = "low" if optimization_priority == "none" else optimization_priority
            is_optimizable = True
            recommendations.append({
                "scenario": "no_private_dns",
                "action": "enable_private_dns",
                "priority": "low",
                "estimated_savings": 0.0,
                "details": f"VPC Endpoint '{endpoint_id}' should have private DNS enabled for security best practices.",
                "recommendation": "Enable private DNS for better security (no cost impact)"
            })

        return is_optimizable, optimization_score, optimization_priority, potential_savings, recommendations

    async def scan_vpc_endpoints(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS VPC Endpoints for cost intelligence.

        This method scans all VPC Endpoints to provide cost visibility and
        optimization recommendations.

        VPC Endpoint Types:
        - Interface: Elastic Network Interface in subnet ($7.20/AZ/month)
        - Gateway: Route table entry for S3/DynamoDB (FREE)
        - GatewayLoadBalancer: $7.20/AZ/month

        Note: VPC Endpoints do NOT have CloudWatch metrics. Detection based on:
        - Network interface count
        - Endpoint type
        - Service name
        - State

        Args:
            region: AWS region to scan

        Returns:
            List of AllCloudResourceData objects for all VPC Endpoints
        """
        resources: list[AllCloudResourceData] = []

        try:
            async with self.session.client("ec2", region_name=region) as ec2:
                # Describe all VPC endpoints
                response = await ec2.describe_vpc_endpoints()
                endpoints = response.get("VpcEndpoints", [])

                for endpoint in endpoints:
                    try:
                        endpoint_id = endpoint.get("VpcEndpointId", "unknown")
                        endpoint_type = endpoint.get("VpcEndpointType", "Interface")
                        service_name = endpoint.get("ServiceName", "")
                        state = endpoint.get("State", "unknown")
                        vpc_id = endpoint.get("VpcId", "unknown")
                        created_at = endpoint.get("CreationTimestamp")

                        # Extract tags
                        tags = {}
                        endpoint_name = None
                        for tag in endpoint.get("Tags", []):
                            tags[tag["Key"]] = tag["Value"]
                            if tag["Key"] == "Name":
                                endpoint_name = tag["Value"]

                        # Get network interfaces and subnets
                        network_interfaces = endpoint.get("NetworkInterfaceIds", [])
                        subnet_ids = endpoint.get("SubnetIds", [])
                        availability_zone_count = len(set(subnet_ids)) if subnet_ids else 1

                        # Calculate monthly cost
                        monthly_cost = self._calculate_vpc_endpoint_monthly_cost(
                            endpoint_type=endpoint_type,
                            availability_zone_count=availability_zone_count,
                            region=region,
                        )

                        # Calculate optimization metrics
                        (
                            is_optimizable,
                            optimization_score,
                            optimization_priority,
                            potential_savings,
                            optimization_recommendations,
                        ) = self._calculate_vpc_endpoint_optimization(
                            endpoint_id=endpoint_id,
                            endpoint_type=endpoint_type,
                            service_name=service_name,
                            state=state,
                            network_interface_count=len(network_interfaces),
                            availability_zone_count=availability_zone_count,
                            monthly_cost=monthly_cost,
                        )

                        # Determine utilization status
                        if len(network_interfaces) == 0:
                            utilization_status = "idle"
                        elif len(network_interfaces) < 2:
                            utilization_status = "low"
                        elif len(network_interfaces) < 5:
                            utilization_status = "medium"
                        else:
                            utilization_status = "high"

                        # Build metadata
                        metadata = {
                            "endpoint_id": endpoint_id,
                            "endpoint_type": endpoint_type,
                            "service_name": service_name,
                            "state": state,
                            "vpc_id": vpc_id,
                            "network_interface_count": len(network_interfaces),
                            "network_interface_ids": network_interfaces,
                            "subnet_ids": subnet_ids,
                            "availability_zone_count": availability_zone_count,
                            "private_dns_enabled": endpoint.get("PrivateDnsEnabled", False),
                            "route_table_ids": endpoint.get("RouteTableIds", []),
                        }

                        # Apply detection rules filtering
                        resource_age_days = None
                        if isinstance(created_at, datetime):
                            resource_age_days = (datetime.utcnow() - created_at.replace(tzinfo=None)).days

                        if not self._should_include_resource("vpc_endpoint", resource_age_days):
                            logger.debug("inventory.vpc_endpoint_filtered", endpoint_id=endpoint_id, age=resource_age_days)
                            continue

                        # Create resource
                        resource = AllCloudResourceData(
                            resource_type="vpc_endpoint",
                            resource_id=endpoint_id,
                            resource_name=endpoint_name or endpoint_id,
                            region=region,
                            estimated_monthly_cost=monthly_cost,
                            resource_metadata=metadata,
                            currency="USD",
                            utilization_status=utilization_status,
                            cpu_utilization_percent=None,
                            memory_utilization_percent=None,
                            storage_utilization_percent=None,
                            network_utilization_mbps=None,
                            is_optimizable=is_optimizable,
                            optimization_priority=optimization_priority,
                            optimization_score=optimization_score,
                            potential_monthly_savings=potential_savings,
                            optimization_recommendations=optimization_recommendations,
                            tags=tags,
                            resource_status=state,
                            is_orphan=len(network_interfaces) == 0 and state == "available",
                            created_at_cloud=created_at,
                            last_used_at=None,
                        )

                        resources.append(resource)

                    except Exception as e:
                        logger.error(
                            "vpc_endpoint.endpoint_scan_failed",
                            endpoint_id=endpoint.get("VpcEndpointId", "unknown"),
                            region=region,
                            error=str(e),
                        )
                        continue

        except Exception as e:
            logger.error(
                "vpc_endpoint.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # ============================================================
    # RESSOURCE 22 : NEPTUNE DATABASE (GRAPH DATABASE)
    # ============================================================

    def _calculate_neptune_monthly_cost(
        self,
        instance_type: str,
        instance_count: int,
        storage_gb: float,
        backup_storage_gb: float,
        estimated_io_requests_per_month: float,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS Neptune Database Cluster.

        Neptune is a fully managed graph database service compatible with Apache TinkerPop Gremlin
        and W3C's SPARQL. Neptune is optimized for storing billions of relationships and querying
        the graph with milliseconds latency.

        Cost Components:
        1. Instance cost (db.r5.large, db.r5.xlarge, db.r5.2xlarge, etc.)
        2. Storage cost ($0.10 per GB-month, billed per GB used)
        3. I/O cost ($0.20 per 1 million requests)
        4. Backup storage cost ($0.021 per GB-month, beyond cluster storage)

        Args:
            instance_type: Instance type (e.g., 'db.r5.large', 'db.r5.2xlarge')
            instance_count: Number of instances in cluster (writer + read replicas)
            storage_gb: Allocated storage in GB (Neptune auto-scales, no provisioning)
            backup_storage_gb: Backup storage in GB (beyond cluster storage)
            estimated_io_requests_per_month: Estimated I/O requests per month
            region: AWS region

        Returns:
            Estimated monthly cost in USD

        Example:
            Single-node cluster with db.r5.large, 100 GB storage, 10M I/O requests:
            - Instance: $0.348/hour * 730 hours = $254.04
            - Storage: 100 GB * $0.10 = $10.00
            - I/O: 10 million * $0.20 = $2.00
            - Total: $266.04/month
        """
        monthly_hours = 730

        # Instance cost (writer + read replicas)
        instance_rate_map = {
            "db.r5.large": 0.348,      # 2 vCPU, 16 GB RAM
            "db.r5.xlarge": 0.696,     # 4 vCPU, 32 GB RAM
            "db.r5.2xlarge": self.PRICING.get("neptune_db_r5_2xlarge", 1.392),  # 8 vCPU, 64 GB RAM
            "db.r5.4xlarge": self.PRICING.get("neptune_db_r5_4xlarge", 2.784),  # 16 vCPU, 128 GB RAM
            "db.r5.8xlarge": 5.568,    # 32 vCPU, 256 GB RAM
            "db.r5.12xlarge": 8.352,   # 48 vCPU, 384 GB RAM
            "db.r5.16xlarge": 11.136,  # 64 vCPU, 512 GB RAM
            "db.r5.24xlarge": 16.704,  # 96 vCPU, 768 GB RAM
            "db.t3.medium": 0.073,     # 2 vCPU, 4 GB RAM (dev/test)
        }
        instance_hourly_rate = instance_rate_map.get(instance_type, 0.348)  # Default to db.r5.large
        instance_cost = instance_hourly_rate * monthly_hours * instance_count

        # Storage cost (Neptune auto-scales, pay for what you use)
        storage_rate = self.PRICING.get("neptune_storage_per_gb", 0.10)
        storage_cost = storage_gb * storage_rate

        # I/O cost (per million requests)
        io_rate = self.PRICING.get("neptune_io_per_million", 0.20)
        io_cost = (estimated_io_requests_per_month / 1_000_000) * io_rate

        # Backup storage cost (beyond cluster storage)
        backup_rate = self.PRICING.get("neptune_backup_storage_per_gb", 0.021)
        backup_cost = backup_storage_gb * backup_rate

        total_cost = instance_cost + storage_cost + io_cost + backup_cost

        return round(total_cost, 2)

    def _calculate_neptune_optimization(
        self,
        cluster_identifier: str,
        instance_type: str,
        instance_count: int,
        storage_gb: float,
        backup_retention_days: int,
        multi_az: bool,
        avg_connections_7d: float,
        engine_version: str,
        status: str,
        tags: dict,
        region: str,
    ) -> list[OptimizationScenario]:
        """
        Calculate optimization scenarios for AWS Neptune Database Cluster.

        Neptune Optimization Scenarios:
        1. Zero Connections (CRITICAL) - No database connections for 7+ days  Delete cluster
        2. Low Connections (HIGH) - <5 avg connections on expensive instance (db.r5.2xlarge)  Downsize to db.r5.large
        3. Single Read Replica Dev/Test (MEDIUM) - Dev/test cluster with read replicas  Remove replicas
        4. Excessive Backup Retention (MEDIUM) - Backup retention >30 days for dev/test  Reduce to 7 days
        5. Single-AZ Production (LOW) - Production cluster without Multi-AZ  Enable Multi-AZ

        Args:
            cluster_identifier: Neptune cluster ID
            instance_type: Instance type (e.g., 'db.r5.large')
            instance_count: Number of instances (writer + replicas)
            storage_gb: Storage size in GB
            backup_retention_days: Backup retention period in days
            multi_az: Multi-AZ deployment enabled
            avg_connections_7d: Average connections over last 7 days
            engine_version: Neptune engine version
            status: Cluster status (available, stopped, etc.)
            tags: Resource tags
            region: AWS region

        Returns:
            List of optimization scenarios with priority, estimated savings, and recommendations
        """
        scenarios = []

        # Current cost
        current_cost = self._calculate_neptune_monthly_cost(
            instance_type=instance_type,
            instance_count=instance_count,
            storage_gb=storage_gb,
            backup_storage_gb=storage_gb * 0.1,  # Estimate 10% backup overhead
            estimated_io_requests_per_month=50_000_000,  # Default estimate
            region=region,
        )

        # Detect environment from tags
        env = tags.get("Environment", tags.get("environment", "")).lower()
        is_production = env in ["production", "prod", "prd"]

        # =============================================
        # SCENARIO 1: Zero Connections (CRITICAL)
        # =============================================
        if avg_connections_7d == 0.0 and status == "available":
            # No connections for 7+ days  Delete cluster
            potential_savings = current_cost
            scenarios.append(
                OptimizationScenario(
                    scenario_name="neptune_zero_connections",
                    priority="CRITICAL",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=0.0,
                    confidence_level="high",
                    description=(
                        f"Neptune cluster '{cluster_identifier}' has zero database connections "
                        f"over the last 7 days. This cluster is completely idle and should be deleted."
                    ),
                    recommendation=(
                        "Delete this Neptune cluster to eliminate costs:\n"
                        f"1. Take final snapshot: aws neptune create-db-cluster-snapshot\n"
                        f"2. Delete cluster: aws neptune delete-db-cluster --db-cluster-identifier {cluster_identifier}\n"
                        f"3. Estimated savings: ${potential_savings:.2f}/month (100%)"
                    ),
                    action_complexity="low",
                    risk_level="low",
                    implementation_time="5 minutes",
                )
            )

        # =============================================
        # SCENARIO 2: Low Connections + Expensive Instance (HIGH)
        # =============================================
        expensive_instances = ["db.r5.2xlarge", "db.r5.4xlarge", "db.r5.8xlarge", "db.r5.12xlarge"]
        if avg_connections_7d < 5.0 and instance_type in expensive_instances:
            # Low usage on expensive instance  Downsize to db.r5.large
            optimized_cost = self._calculate_neptune_monthly_cost(
                instance_type="db.r5.large",
                instance_count=instance_count,
                storage_gb=storage_gb,
                backup_storage_gb=storage_gb * 0.1,
                estimated_io_requests_per_month=50_000_000,
                region=region,
            )
            potential_savings = current_cost - optimized_cost

            scenarios.append(
                OptimizationScenario(
                    scenario_name="neptune_low_connections_expensive_instance",
                    priority="HIGH",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=optimized_cost,
                    confidence_level="high",
                    description=(
                        f"Neptune cluster '{cluster_identifier}' has low connection usage "
                        f"(avg {avg_connections_7d:.1f} connections) but uses expensive instance type '{instance_type}'. "
                        f"This is over-provisioned for the workload."
                    ),
                    recommendation=(
                        f"Downsize to db.r5.large to match workload:\n"
                        f"1. Modify cluster: aws neptune modify-db-cluster --db-cluster-identifier {cluster_identifier} --apply-immediately\n"
                        f"2. Modify instances: aws neptune modify-db-instance --db-instance-class db.r5.large\n"
                        f"3. Monitor performance after change\n"
                        f"4. Estimated savings: ${potential_savings:.2f}/month ({(potential_savings / current_cost * 100):.0f}%)"
                    ),
                    action_complexity="medium",
                    risk_level="low",
                    implementation_time="15 minutes",
                )
            )

        # =============================================
        # SCENARIO 3: Dev/Test with Read Replicas (MEDIUM)
        # =============================================
        if not is_production and instance_count > 1:
            # Dev/test cluster with read replicas  Remove replicas
            optimized_cost = self._calculate_neptune_monthly_cost(
                instance_type=instance_type,
                instance_count=1,  # Writer only
                storage_gb=storage_gb,
                backup_storage_gb=storage_gb * 0.1,
                estimated_io_requests_per_month=50_000_000,
                region=region,
            )
            potential_savings = current_cost - optimized_cost

            scenarios.append(
                OptimizationScenario(
                    scenario_name="neptune_dev_test_read_replicas",
                    priority="MEDIUM",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=optimized_cost,
                    confidence_level="high",
                    description=(
                        f"Neptune cluster '{cluster_identifier}' is tagged as dev/test but has "
                        f"{instance_count} instances (including read replicas). Read replicas are "
                        f"typically not required for non-production workloads."
                    ),
                    recommendation=(
                        f"Remove read replicas to reduce costs:\n"
                        f"1. Identify replica instances: aws neptune describe-db-instances --filters Name=db-cluster-id,Values={cluster_identifier}\n"
                        f"2. Delete replicas: aws neptune delete-db-instance --db-instance-identifier <replica-id>\n"
                        f"3. Keep only writer instance\n"
                        f"4. Estimated savings: ${potential_savings:.2f}/month ({(potential_savings / current_cost * 100):.0f}%)"
                    ),
                    action_complexity="medium",
                    risk_level="low",
                    implementation_time="10 minutes",
                )
            )

        # =============================================
        # SCENARIO 4: Excessive Backup Retention (MEDIUM)
        # =============================================
        if not is_production and backup_retention_days > 30:
            # Long backup retention for dev/test  Reduce to 7 days
            # Approximate backup storage savings
            backup_storage_gb = storage_gb * 0.1  # Estimate 10% backup overhead
            backup_rate = self.PRICING.get("neptune_backup_storage_per_gb", 0.021)
            backup_savings = backup_storage_gb * backup_rate * 0.5  # ~50% reduction

            scenarios.append(
                OptimizationScenario(
                    scenario_name="neptune_excessive_backup_retention",
                    priority="MEDIUM",
                    estimated_monthly_savings=backup_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=current_cost - backup_savings,
                    confidence_level="medium",
                    description=(
                        f"Neptune cluster '{cluster_identifier}' is tagged as dev/test but has "
                        f"backup retention set to {backup_retention_days} days. This is excessive "
                        f"for non-production workloads and increases backup storage costs."
                    ),
                    recommendation=(
                        f"Reduce backup retention to 7 days:\n"
                        f"1. Modify cluster: aws neptune modify-db-cluster --db-cluster-identifier {cluster_identifier} --backup-retention-period 7\n"
                        f"2. Old backups will be automatically deleted after 7 days\n"
                        f"3. Estimated savings: ${backup_savings:.2f}/month (backup storage reduction)"
                    ),
                    action_complexity="low",
                    risk_level="low",
                    implementation_time="5 minutes",
                )
            )

        # =============================================
        # SCENARIO 5: Single-AZ Production (LOW)
        # =============================================
        if is_production and not multi_az:
            # Production cluster without Multi-AZ  Enable Multi-AZ
            scenarios.append(
                OptimizationScenario(
                    scenario_name="neptune_single_az_production",
                    priority="LOW",
                    estimated_monthly_savings=0.0,  # This is a reliability improvement, not cost savings
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=current_cost,
                    confidence_level="high",
                    description=(
                        f"Neptune cluster '{cluster_identifier}' is tagged as production but does not have "
                        f"Multi-AZ deployment enabled. This creates a single point of failure and reduces "
                        f"availability during maintenance or AZ failures."
                    ),
                    recommendation=(
                        f"Enable Multi-AZ for high availability:\n"
                        f"1. Create read replica in different AZ: aws neptune create-db-instance --availability-zone <different-az>\n"
                        f"2. Configure automatic failover\n"
                        f"3. This improves reliability but increases cost (additional replica)\n"
                        f"4. Cost impact: +${(current_cost / instance_count):.2f}/month per replica"
                    ),
                    action_complexity="medium",
                    risk_level="low",
                    implementation_time="20 minutes",
                )
            )

        return scenarios

    async def scan_neptune_clusters(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS Neptune Database Clusters for cost intelligence.

        AWS Neptune is a fully managed graph database service that supports both Apache TinkerPop Gremlin
        and W3C's SPARQL query languages. Neptune is optimized for storing billions of relationships
        and querying the graph with milliseconds latency.

        Use Cases:
        - Knowledge graphs (fraud detection, recommendation engines)
        - Identity graphs (social networking, user profiles)
        - Network/IT operations (dependency tracking, impact analysis)

        CloudWatch Metrics Used:
        - DatabaseConnections (AWS/Neptune) - Active database connections

        Cost Optimization Scenarios:
        1. Zero Connections (CRITICAL) - No connections for 7+ days  Delete
        2. Low Connections + Expensive Instance (HIGH) - <5 connections on db.r5.2xlarge  Downsize
        3. Dev/Test with Read Replicas (MEDIUM) - Remove replicas for non-prod
        4. Excessive Backup Retention (MEDIUM) - >30 days for dev/test  Reduce to 7 days
        5. Single-AZ Production (LOW) - Enable Multi-AZ for reliability

        Args:
            region: AWS region to scan

        Returns:
            List of AllCloudResourceData for all Neptune clusters in region
        """
        resources = []

        try:
            async with self.session.client("neptune", region_name=region) as neptune:
                # Describe all Neptune clusters
                response = await neptune.describe_db_clusters()
                clusters = response.get("DBClusters", [])

                logger.info(
                    "neptune.clusters_found",
                    region=region,
                    cluster_count=len(clusters),
                )

                for cluster in clusters:
                    try:
                        cluster_identifier = cluster.get("DBClusterIdentifier", "unknown")
                        engine = cluster.get("Engine", "")

                        # Only process Neptune clusters (skip Aurora, RDS)
                        if engine != "neptune":
                            continue

                        # Extract cluster details
                        status = cluster.get("Status", "unknown")
                        instance_type = "db.r5.large"  # Default
                        instance_count = 0

                        # Get instance details from cluster members
                        members = cluster.get("DBClusterMembers", [])
                        instance_count = len(members)

                        # Get instance type from first member
                        if members:
                            first_member_id = members[0].get("DBInstanceIdentifier")
                            instances_response = await neptune.describe_db_instances(
                                DBInstanceIdentifier=first_member_id
                            )
                            instances = instances_response.get("DBInstances", [])
                            if instances:
                                instance_type = instances[0].get("DBInstanceClass", "db.r5.large")

                        storage_gb = cluster.get("AllocatedStorage", 0)  # Neptune auto-scales
                        backup_retention_days = cluster.get("BackupRetentionPeriod", 1)
                        multi_az = cluster.get("MultiAZ", False)
                        engine_version = cluster.get("EngineVersion", "unknown")

                        # Extract tags
                        tags_list = cluster.get("TagList", [])
                        tags = {tag["Key"]: tag["Value"] for tag in tags_list}

                        # ====================================
                        # CLOUDWATCH METRICS (7-day lookback)
                        # ====================================
                        end_time = datetime.now(timezone.utc)
                        start_time = end_time - timedelta(days=7)

                        # Metric: DatabaseConnections (active connections)
                        avg_connections_7d = await self._get_cloudwatch_metric_average(
                            region=region,
                            namespace="AWS/Neptune",
                            metric_name="DatabaseConnections",
                            dimensions=[
                                {"Name": "DBClusterIdentifier", "Value": cluster_identifier}
                            ],
                            start_time=start_time,
                            end_time=end_time,
                            period=3600,  # 1 hour
                            statistic="Average",
                        )

                        # ====================================
                        # COST CALCULATION
                        # ====================================
                        monthly_cost = self._calculate_neptune_monthly_cost(
                            instance_type=instance_type,
                            instance_count=instance_count,
                            storage_gb=storage_gb if storage_gb > 0 else 10.0,  # Min 10 GB
                            backup_storage_gb=(storage_gb if storage_gb > 0 else 10.0) * 0.1,
                            estimated_io_requests_per_month=50_000_000,  # Default estimate
                            region=region,
                        )

                        # ====================================
                        # OPTIMIZATION SCENARIOS
                        # ====================================
                        optimization_scenarios = self._calculate_neptune_optimization(
                            cluster_identifier=cluster_identifier,
                            instance_type=instance_type,
                            instance_count=instance_count,
                            storage_gb=storage_gb if storage_gb > 0 else 10.0,
                            backup_retention_days=backup_retention_days,
                            multi_az=multi_az,
                            avg_connections_7d=avg_connections_7d,
                            engine_version=engine_version,
                            status=status,
                            tags=tags,
                            region=region,
                        )

                        # ====================================
                        # CHECK IF ORPHAN (Zero connections for 7+ days)
                        # ====================================
                        is_orphan = avg_connections_7d == 0.0 and status == "available"

                        # ====================================
                        # CREATE RESOURCE DATA
                        # ====================================
                        resource = AllCloudResourceData(
                            resource_type="neptune_cluster",
                            resource_id=cluster.get("DbClusterResourceId", cluster_identifier),
                            resource_name=cluster_identifier,
                            region=region,
                            estimated_monthly_cost=monthly_cost,
                            currency="USD",
                            resource_metadata={
                                "cluster_identifier": cluster_identifier,
                                "engine": engine,
                                "engine_version": engine_version,
                                "status": status,
                                "instance_type": instance_type,
                                "instance_count": instance_count,
                                "storage_gb": storage_gb,
                                "backup_retention_days": backup_retention_days,
                                "multi_az": multi_az,
                                "endpoint": cluster.get("Endpoint", ""),
                                "reader_endpoint": cluster.get("ReaderEndpoint", ""),
                                "avg_connections_7d": round(avg_connections_7d, 2),
                                "tags": tags,
                            },
                            optimization_scenarios=optimization_scenarios,
                            is_orphan=is_orphan,
                            created_at_cloud=cluster.get("ClusterCreateTime"),
                        )

                        resources.append(resource)

                        logger.debug(
                            "neptune.cluster_scanned",
                            cluster_identifier=cluster_identifier,
                            instance_type=instance_type,
                            instance_count=instance_count,
                            monthly_cost=monthly_cost,
                            avg_connections_7d=round(avg_connections_7d, 2),
                            scenarios_count=len(optimization_scenarios),
                            is_orphan=is_orphan,
                        )

                    except Exception as e:
                        logger.error(
                            "neptune.cluster_scan_failed",
                            cluster_identifier=cluster.get("DBClusterIdentifier", "unknown"),
                            region=region,
                            error=str(e),
                        )
                        continue

        except Exception as e:
            logger.error(
                "neptune.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # ============================================================
    # RESSOURCE 23 : MSK (MANAGED STREAMING FOR APACHE KAFKA)
    # ============================================================

    def _calculate_msk_monthly_cost(
        self,
        broker_instance_type: str,
        broker_count: int,
        storage_per_broker_gb: int,
        estimated_data_transfer_gb: float,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS MSK (Managed Streaming for Apache Kafka) Cluster.

        MSK is a fully managed Apache Kafka service that makes it easy to build and run applications
        that use Apache Kafka to process streaming data. MSK manages Kafka cluster operations,
        patching, and monitoring.

        Cost Components:
        1. Broker instance cost (kafka.m5.large, kafka.m5.xlarge, kafka.m5.2xlarge, etc.)
        2. Storage cost ($0.10 per GB-month per broker)
        3. Data transfer cost ($0.01 per GB for cross-AZ/cross-region)

        Args:
            broker_instance_type: Broker instance type (e.g., 'kafka.m5.large', 'kafka.m5.2xlarge')
            broker_count: Number of broker nodes (typically 2-3 for HA)
            storage_per_broker_gb: Storage allocated per broker in GB
            estimated_data_transfer_gb: Estimated cross-AZ data transfer per month
            region: AWS region

        Returns:
            Estimated monthly cost in USD

        Example:
            3-broker cluster with kafka.m5.large, 1000 GB storage/broker, 500 GB transfer:
            - Brokers: $0.42/hour * 730 hours * 3 = $919.80
            - Storage: 1000 GB * $0.10 * 3 = $300.00
            - Data transfer: 500 GB * $0.01 = $5.00
            - Total: $1,224.80/month
        """
        monthly_hours = 730

        # Broker instance cost
        broker_rate_map = {
            "kafka.m5.large": 0.42,     # 2 vCPU, 8 GB RAM
            "kafka.m5.xlarge": 0.84,    # 4 vCPU, 16 GB RAM
            "kafka.m5.2xlarge": self.PRICING.get("msk_kafka_m5_2xlarge", 0.84),  # 8 vCPU, 32 GB RAM
            "kafka.m5.4xlarge": self.PRICING.get("msk_kafka_m5_4xlarge", 1.68),  # 16 vCPU, 64 GB RAM
            "kafka.m5.8xlarge": 3.36,   # 32 vCPU, 128 GB RAM
            "kafka.m5.12xlarge": 5.04,  # 48 vCPU, 192 GB RAM
            "kafka.m5.16xlarge": 6.72,  # 64 vCPU, 256 GB RAM
            "kafka.m5.24xlarge": 10.08, # 96 vCPU, 384 GB RAM
            "kafka.t3.small": 0.07,     # 2 vCPU, 2 GB RAM (dev/test)
        }
        broker_hourly_rate = broker_rate_map.get(broker_instance_type, 0.42)  # Default to kafka.m5.large
        broker_cost = broker_hourly_rate * monthly_hours * broker_count

        # Storage cost (per broker)
        storage_rate = self.PRICING.get("msk_storage_per_gb", 0.10)
        storage_cost = storage_per_broker_gb * storage_rate * broker_count

        # Data transfer cost (cross-AZ/cross-region)
        data_transfer_rate = self.PRICING.get("msk_data_transfer_per_gb", 0.01)
        data_transfer_cost = estimated_data_transfer_gb * data_transfer_rate

        total_cost = broker_cost + storage_cost + data_transfer_cost

        return round(total_cost, 2)

    def _calculate_msk_optimization(
        self,
        cluster_name: str,
        broker_instance_type: str,
        broker_count: int,
        storage_per_broker_gb: int,
        avg_bytes_in_per_sec_7d: float,
        avg_bytes_out_per_sec_7d: float,
        avg_cpu_percent_7d: float,
        kafka_version: str,
        state: str,
        tags: dict,
        region: str,
    ) -> list[OptimizationScenario]:
        """
        Calculate optimization scenarios for AWS MSK (Managed Streaming for Apache Kafka) Cluster.

        MSK Optimization Scenarios:
        1. Zero Throughput (CRITICAL) - No messages for 7+ days  Delete cluster
        2. Low Throughput + Expensive Brokers (HIGH) - Low throughput on expensive brokers (kafka.m5.2xlarge)  Downsize
        3. Over-provisioned Brokers (MEDIUM) - Low CPU (<20%) on expensive brokers  Downsize
        4. Excessive Storage (MEDIUM) - Storage usage <20% with large provisioned  Reduce storage
        5. Dev/Test with 3+ Brokers (MEDIUM) - Non-prod cluster with HA configuration  Reduce to 1-2 brokers
        6. No Auto-Scaling (LOW) - Manual storage provisioning  Enable auto-scaling

        Args:
            cluster_name: MSK cluster name
            broker_instance_type: Broker instance type (e.g., 'kafka.m5.large')
            broker_count: Number of broker nodes
            storage_per_broker_gb: Storage per broker in GB
            avg_bytes_in_per_sec_7d: Average incoming bytes/sec over 7 days
            avg_bytes_out_per_sec_7d: Average outgoing bytes/sec over 7 days
            avg_cpu_percent_7d: Average CPU utilization over 7 days
            kafka_version: Apache Kafka version
            state: Cluster state (ACTIVE, CREATING, DELETING, etc.)
            tags: Resource tags
            region: AWS region

        Returns:
            List of optimization scenarios with priority, estimated savings, and recommendations
        """
        scenarios = []

        # Current cost
        estimated_data_transfer_gb = (avg_bytes_out_per_sec_7d * 86400 * 30) / (1024 ** 3)  # Monthly estimate
        current_cost = self._calculate_msk_monthly_cost(
            broker_instance_type=broker_instance_type,
            broker_count=broker_count,
            storage_per_broker_gb=storage_per_broker_gb,
            estimated_data_transfer_gb=estimated_data_transfer_gb,
            region=region,
        )

        # Detect environment from tags
        env = tags.get("Environment", tags.get("environment", "")).lower()
        is_production = env in ["production", "prod", "prd"]

        # Calculate total throughput (bytes/sec)
        total_throughput_bps = avg_bytes_in_per_sec_7d + avg_bytes_out_per_sec_7d

        # =============================================
        # SCENARIO 1: Zero Throughput (CRITICAL)
        # =============================================
        if total_throughput_bps == 0.0 and state == "ACTIVE":
            # No messages for 7+ days  Delete cluster
            potential_savings = current_cost
            scenarios.append(
                OptimizationScenario(
                    scenario_name="msk_zero_throughput",
                    priority="CRITICAL",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=0.0,
                    confidence_level="high",
                    description=(
                        f"MSK cluster '{cluster_name}' has zero message throughput "
                        f"over the last 7 days. This cluster is completely idle and should be deleted."
                    ),
                    recommendation=(
                        "Delete this MSK cluster to eliminate costs:\n"
                        f"1. Backup configuration: aws kafka describe-cluster --cluster-arn <arn>\n"
                        f"2. Delete cluster: aws kafka delete-cluster --cluster-arn <arn>\n"
                        f"3. Estimated savings: ${potential_savings:.2f}/month (100%)"
                    ),
                    action_complexity="low",
                    risk_level="low",
                    implementation_time="5 minutes",
                )
            )

        # =============================================
        # SCENARIO 2: Low Throughput + Expensive Brokers (HIGH)
        # =============================================
        expensive_brokers = ["kafka.m5.2xlarge", "kafka.m5.4xlarge", "kafka.m5.8xlarge", "kafka.m5.12xlarge"]
        # Convert bytes/sec to MB/sec for readability
        throughput_mbps = total_throughput_bps / (1024 ** 2)

        if throughput_mbps < 10.0 and broker_instance_type in expensive_brokers:
            # Low throughput (<10 MB/s) on expensive brokers  Downsize to kafka.m5.large
            optimized_cost = self._calculate_msk_monthly_cost(
                broker_instance_type="kafka.m5.large",
                broker_count=broker_count,
                storage_per_broker_gb=storage_per_broker_gb,
                estimated_data_transfer_gb=estimated_data_transfer_gb,
                region=region,
            )
            potential_savings = current_cost - optimized_cost

            scenarios.append(
                OptimizationScenario(
                    scenario_name="msk_low_throughput_expensive_brokers",
                    priority="HIGH",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=optimized_cost,
                    confidence_level="high",
                    description=(
                        f"MSK cluster '{cluster_name}' has low throughput "
                        f"(avg {throughput_mbps:.2f} MB/s) but uses expensive broker type '{broker_instance_type}'. "
                        f"This is over-provisioned for the workload."
                    ),
                    recommendation=(
                        f"Downsize to kafka.m5.large to match workload:\n"
                        f"1. Create new configuration with kafka.m5.large brokers\n"
                        f"2. Update cluster: aws kafka update-broker-type --cluster-arn <arn> --target-instance-type kafka.m5.large\n"
                        f"3. Monitor performance after change\n"
                        f"4. Estimated savings: ${potential_savings:.2f}/month ({(potential_savings / current_cost * 100):.0f}%)"
                    ),
                    action_complexity="medium",
                    risk_level="low",
                    implementation_time="30 minutes",
                )
            )

        # =============================================
        # SCENARIO 3: Over-provisioned Brokers (MEDIUM)
        # =============================================
        if avg_cpu_percent_7d < 20.0 and broker_instance_type in expensive_brokers:
            # Low CPU (<20%) on expensive brokers  Downsize
            optimized_cost = self._calculate_msk_monthly_cost(
                broker_instance_type="kafka.m5.large",
                broker_count=broker_count,
                storage_per_broker_gb=storage_per_broker_gb,
                estimated_data_transfer_gb=estimated_data_transfer_gb,
                region=region,
            )
            potential_savings = current_cost - optimized_cost

            scenarios.append(
                OptimizationScenario(
                    scenario_name="msk_over_provisioned_brokers",
                    priority="MEDIUM",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=optimized_cost,
                    confidence_level="high",
                    description=(
                        f"MSK cluster '{cluster_name}' has low CPU utilization "
                        f"(avg {avg_cpu_percent_7d:.1f}%) on expensive broker type '{broker_instance_type}'. "
                        f"Brokers are under-utilized and can be downsized."
                    ),
                    recommendation=(
                        f"Downsize to kafka.m5.large based on CPU utilization:\n"
                        f"1. Review workload patterns over 30 days\n"
                        f"2. Update broker type: aws kafka update-broker-type --target-instance-type kafka.m5.large\n"
                        f"3. Monitor CPU and throughput metrics\n"
                        f"4. Estimated savings: ${potential_savings:.2f}/month ({(potential_savings / current_cost * 100):.0f}%)"
                    ),
                    action_complexity="medium",
                    risk_level="low",
                    implementation_time="30 minutes",
                )
            )

        # =============================================
        # SCENARIO 4: Excessive Storage (MEDIUM)
        # =============================================
        if storage_per_broker_gb >= 1000:
            # Large storage allocation (1 TB/broker)  Consider reducing if under-utilized
            # Estimate 50% reduction potential
            optimized_storage = int(storage_per_broker_gb * 0.5)
            optimized_cost = self._calculate_msk_monthly_cost(
                broker_instance_type=broker_instance_type,
                broker_count=broker_count,
                storage_per_broker_gb=optimized_storage,
                estimated_data_transfer_gb=estimated_data_transfer_gb,
                region=region,
            )
            potential_savings = current_cost - optimized_cost

            scenarios.append(
                OptimizationScenario(
                    scenario_name="msk_excessive_storage",
                    priority="MEDIUM",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=optimized_cost,
                    confidence_level="medium",
                    description=(
                        f"MSK cluster '{cluster_name}' has large storage allocation "
                        f"({storage_per_broker_gb} GB per broker). Review actual usage to "
                        f"determine if storage can be reduced."
                    ),
                    recommendation=(
                        f"Review storage usage and reduce if under-utilized:\n"
                        f"1. Check storage utilization: aws cloudwatch get-metric-statistics --namespace AWS/Kafka --metric-name KafkaDataLogsDiskUsed\n"
                        f"2. If usage <20%, reduce storage to {optimized_storage} GB per broker\n"
                        f"3. Enable storage auto-scaling for future growth\n"
                        f"4. Estimated savings: ${potential_savings:.2f}/month ({(potential_savings / current_cost * 100):.0f}%)"
                    ),
                    action_complexity="medium",
                    risk_level="medium",
                    implementation_time="20 minutes",
                )
            )

        # =============================================
        # SCENARIO 5: Dev/Test with 3+ Brokers (MEDIUM)
        # =============================================
        if not is_production and broker_count >= 3:
            # Dev/test cluster with high availability  Reduce to 1-2 brokers
            optimized_broker_count = 1
            optimized_cost = self._calculate_msk_monthly_cost(
                broker_instance_type=broker_instance_type,
                broker_count=optimized_broker_count,
                storage_per_broker_gb=storage_per_broker_gb,
                estimated_data_transfer_gb=estimated_data_transfer_gb,
                region=region,
            )
            potential_savings = current_cost - optimized_cost

            scenarios.append(
                OptimizationScenario(
                    scenario_name="msk_dev_test_high_availability",
                    priority="MEDIUM",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=optimized_cost,
                    confidence_level="high",
                    description=(
                        f"MSK cluster '{cluster_name}' is tagged as dev/test but has "
                        f"{broker_count} brokers (high availability configuration). "
                        f"Non-production workloads typically don't require HA."
                    ),
                    recommendation=(
                        f"Reduce broker count to 1 for dev/test:\n"
                        f"1.  MSK does not support in-place broker count changes\n"
                        f"2. Create new cluster with 1 broker: aws kafka create-cluster --number-of-broker-nodes 1\n"
                        f"3. Migrate topics and data from old cluster\n"
                        f"4. Delete old cluster\n"
                        f"5. Estimated savings: ${potential_savings:.2f}/month ({(potential_savings / current_cost * 100):.0f}%)"
                    ),
                    action_complexity="high",
                    risk_level="medium",
                    implementation_time="2 hours",
                )
            )

        # =============================================
        # SCENARIO 6: No Auto-Scaling (LOW)
        # =============================================
        # Note: MSK auto-scaling is determined by storage configuration, not exposed in describe-cluster
        # This scenario is informational/best-practice recommendation
        scenarios.append(
            OptimizationScenario(
                scenario_name="msk_no_auto_scaling",
                priority="LOW",
                estimated_monthly_savings=0.0,  # Prevents over-provisioning, not direct savings
                current_monthly_cost=current_cost,
                optimized_monthly_cost=current_cost,
                confidence_level="medium",
                description=(
                    f"MSK cluster '{cluster_name}' may not have storage auto-scaling enabled. "
                    f"Auto-scaling prevents storage exhaustion and reduces over-provisioning costs."
                ),
                recommendation=(
                    f"Enable storage auto-scaling for cost optimization:\n"
                    f"1. Check current configuration: aws kafka describe-cluster --cluster-arn <arn>\n"
                    f"2. Enable auto-scaling: aws kafka update-storage --cluster-arn <arn> --provisioned-throughput Enabled=true\n"
                    f"3. Set target utilization to 60-70%\n"
                    f"4. Benefit: Automatic storage adjustment based on usage"
                ),
                action_complexity="low",
                risk_level="low",
                implementation_time="10 minutes",
            )
        )

        return scenarios

    async def scan_msk_clusters(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS MSK (Managed Streaming for Apache Kafka) Clusters for cost intelligence.

        AWS MSK is a fully managed service for Apache Kafka that makes it easy to build and run
        applications that use Apache Kafka to process streaming data. MSK manages the Kafka cluster
        infrastructure, operations, and monitoring.

        Use Cases:
        - Real-time data pipelines (log aggregation, event sourcing)
        - Stream processing (fraud detection, clickstream analysis)
        - Microservices communication (event-driven architectures)

        CloudWatch Metrics Used:
        - BytesInPerSec (AWS/Kafka) - Incoming bytes per second
        - BytesOutPerSec (AWS/Kafka) - Outgoing bytes per second
        - CpuUser (AWS/Kafka) - CPU utilization (user space)

        Cost Optimization Scenarios:
        1. Zero Throughput (CRITICAL) - No messages for 7+ days  Delete
        2. Low Throughput + Expensive Brokers (HIGH) - <10 MB/s on kafka.m5.2xlarge  Downsize
        3. Over-provisioned Brokers (MEDIUM) - Low CPU (<20%) on expensive brokers  Downsize
        4. Excessive Storage (MEDIUM) - Large storage allocation (1 TB)  Review usage
        5. Dev/Test with 3+ Brokers (MEDIUM) - Non-prod with HA  Reduce to 1-2 brokers
        6. No Auto-Scaling (LOW) - Enable storage auto-scaling for cost efficiency

        Args:
            region: AWS region to scan

        Returns:
            List of AllCloudResourceData for all MSK clusters in region
        """
        resources = []

        try:
            async with self.session.client("kafka", region_name=region) as kafka:
                # List all MSK clusters
                response = await kafka.list_clusters_v2()
                cluster_info_list = response.get("ClusterInfoList", [])

                logger.info(
                    "msk.clusters_found",
                    region=region,
                    cluster_count=len(cluster_info_list),
                )

                for cluster_info in cluster_info_list:
                    try:
                        cluster_arn = cluster_info.get("ClusterArn", "")
                        cluster_name = cluster_info.get("ClusterName", "unknown")
                        state = cluster_info.get("State", "UNKNOWN")

                        # Get detailed cluster info
                        detail_response = await kafka.describe_cluster_v2(
                            ClusterArn=cluster_arn
                        )
                        cluster = detail_response.get("ClusterInfo", {})

                        # Extract cluster details
                        provisioned = cluster.get("Provisioned", {})
                        broker_node_group = provisioned.get("BrokerNodeGroupInfo", {})

                        broker_instance_type = broker_node_group.get("InstanceType", "kafka.m5.large")
                        broker_count = provisioned.get("NumberOfBrokerNodes", 0)

                        storage_info = broker_node_group.get("StorageInfo", {})
                        ebs_storage = storage_info.get("EbsStorageInfo", {})
                        storage_per_broker_gb = ebs_storage.get("VolumeSize", 0)

                        kafka_version = provisioned.get("KafkaVersion", "unknown")

                        # Extract tags
                        tags = cluster.get("Tags", {})

                        # ====================================
                        # CLOUDWATCH METRICS (7-day lookback)
                        # ====================================
                        end_time = datetime.now(timezone.utc)
                        start_time = end_time - timedelta(days=7)

                        # Metric: BytesInPerSec (incoming messages)
                        avg_bytes_in_per_sec_7d = await self._get_cloudwatch_metric_average(
                            region=region,
                            namespace="AWS/Kafka",
                            metric_name="BytesInPerSec",
                            dimensions=[
                                {"Name": "Cluster Name", "Value": cluster_name}
                            ],
                            start_time=start_time,
                            end_time=end_time,
                            period=3600,  # 1 hour
                            statistic="Average",
                        )

                        # Metric: BytesOutPerSec (outgoing messages)
                        avg_bytes_out_per_sec_7d = await self._get_cloudwatch_metric_average(
                            region=region,
                            namespace="AWS/Kafka",
                            metric_name="BytesOutPerSec",
                            dimensions=[
                                {"Name": "Cluster Name", "Value": cluster_name}
                            ],
                            start_time=start_time,
                            end_time=end_time,
                            period=3600,  # 1 hour
                            statistic="Average",
                        )

                        # Metric: CpuUser (CPU utilization)
                        avg_cpu_percent_7d = await self._get_cloudwatch_metric_average(
                            region=region,
                            namespace="AWS/Kafka",
                            metric_name="CpuUser",
                            dimensions=[
                                {"Name": "Cluster Name", "Value": cluster_name}
                            ],
                            start_time=start_time,
                            end_time=end_time,
                            period=3600,  # 1 hour
                            statistic="Average",
                        )

                        # ====================================
                        # COST CALCULATION
                        # ====================================
                        estimated_data_transfer_gb = (avg_bytes_out_per_sec_7d * 86400 * 30) / (1024 ** 3)
                        monthly_cost = self._calculate_msk_monthly_cost(
                            broker_instance_type=broker_instance_type,
                            broker_count=broker_count,
                            storage_per_broker_gb=storage_per_broker_gb,
                            estimated_data_transfer_gb=estimated_data_transfer_gb,
                            region=region,
                        )

                        # ====================================
                        # OPTIMIZATION SCENARIOS
                        # ====================================
                        optimization_scenarios = self._calculate_msk_optimization(
                            cluster_name=cluster_name,
                            broker_instance_type=broker_instance_type,
                            broker_count=broker_count,
                            storage_per_broker_gb=storage_per_broker_gb,
                            avg_bytes_in_per_sec_7d=avg_bytes_in_per_sec_7d,
                            avg_bytes_out_per_sec_7d=avg_bytes_out_per_sec_7d,
                            avg_cpu_percent_7d=avg_cpu_percent_7d,
                            kafka_version=kafka_version,
                            state=state,
                            tags=tags,
                            region=region,
                        )

                        # ====================================
                        # CHECK IF ORPHAN (Zero throughput for 7+ days)
                        # ====================================
                        total_throughput = avg_bytes_in_per_sec_7d + avg_bytes_out_per_sec_7d
                        is_orphan = total_throughput == 0.0 and state == "ACTIVE"

                        # ====================================
                        # CREATE RESOURCE DATA
                        # ====================================
                        resource = AllCloudResourceData(
                            resource_type="msk_cluster",
                            resource_id=cluster_arn,
                            resource_name=cluster_name,
                            region=region,
                            estimated_monthly_cost=monthly_cost,
                            currency="USD",
                            resource_metadata={
                                "cluster_name": cluster_name,
                                "cluster_arn": cluster_arn,
                                "state": state,
                                "broker_instance_type": broker_instance_type,
                                "broker_count": broker_count,
                                "storage_per_broker_gb": storage_per_broker_gb,
                                "kafka_version": kafka_version,
                                "avg_bytes_in_per_sec_7d": round(avg_bytes_in_per_sec_7d, 2),
                                "avg_bytes_out_per_sec_7d": round(avg_bytes_out_per_sec_7d, 2),
                                "avg_cpu_percent_7d": round(avg_cpu_percent_7d, 2),
                                "tags": tags,
                            },
                            optimization_scenarios=optimization_scenarios,
                            is_orphan=is_orphan,
                            created_at_cloud=cluster.get("CreationTime"),
                        )

                        resources.append(resource)

                        logger.debug(
                            "msk.cluster_scanned",
                            cluster_name=cluster_name,
                            broker_instance_type=broker_instance_type,
                            broker_count=broker_count,
                            monthly_cost=monthly_cost,
                            avg_throughput_mbps=round((avg_bytes_in_per_sec_7d + avg_bytes_out_per_sec_7d) / (1024 ** 2), 2),
                            scenarios_count=len(optimization_scenarios),
                            is_orphan=is_orphan,
                        )

                    except Exception as e:
                        logger.error(
                            "msk.cluster_scan_failed",
                            cluster_arn=cluster_info.get("ClusterArn", "unknown"),
                            region=region,
                            error=str(e),
                        )
                        continue

        except Exception as e:
            logger.error(
                "msk.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # ============================================================
    # RESSOURCE 24 : SAGEMAKER ENDPOINT (ML INFERENCE)
    # ============================================================

    def _calculate_sagemaker_endpoint_monthly_cost(
        self,
        instance_type: str,
        instance_count: int,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS SageMaker Endpoint.

        SageMaker Endpoints provide real-time inference for machine learning models. They host
        trained models and serve predictions with low latency. Endpoints run 24/7 and are billed
        per instance hour.

        Cost Components:
        1. Instance cost (ml.m5.large, ml.m5.xlarge, ml.m5.2xlarge, etc.)
        2. Data processing cost (included in instance price)

        Args:
            instance_type: ML instance type (e.g., 'ml.m5.large', 'ml.m5.2xlarge')
            instance_count: Number of instances behind endpoint
            region: AWS region

        Returns:
            Estimated monthly cost in USD

        Example:
            2-instance endpoint with ml.m5.large:
            - Instance: $0.115/hour * 730 hours * 2 = $167.90/month
        """
        monthly_hours = 730

        # Instance cost based on type
        instance_rate_map = {
            "ml.m5.large": self.PRICING.get("sagemaker_ml_m5_large", 0.115),      # 2 vCPU, 8 GB RAM
            "ml.m5.xlarge": self.PRICING.get("sagemaker_ml_m5_xlarge", 0.23),     # 4 vCPU, 16 GB RAM
            "ml.m5.2xlarge": self.PRICING.get("sagemaker_ml_m5_2xlarge", 0.46),   # 8 vCPU, 32 GB RAM
            "ml.m5.4xlarge": self.PRICING.get("sagemaker_ml_m5_4xlarge", 0.92),   # 16 vCPU, 64 GB RAM
            "ml.t3.medium": self.PRICING.get("sagemaker_ml_t3_medium", 0.056),    # 2 vCPU, 4 GB RAM (dev/test)
        }

        # Extract base instance type (ml.m5.large, ml.m5.xlarge, etc.)
        instance_hourly_rate = instance_rate_map.get(instance_type, 0.115)  # Default to ml.m5.large

        instance_cost = instance_hourly_rate * monthly_hours * instance_count

        return round(instance_cost, 2)

    def _calculate_sagemaker_endpoint_optimization(
        self,
        endpoint_name: str,
        instance_type: str,
        instance_count: int,
        total_invocations_7d: int,
        avg_model_latency_ms: float,
        status: str,
        tags: dict,
        region: str,
    ) -> list[OptimizationScenario]:
        """
        Calculate optimization scenarios for AWS SageMaker Endpoint.

        SageMaker Endpoint Optimization Scenarios:
        1. Zero Invocations (CRITICAL) - No invocations for 7+ days  Delete endpoint
        2. Low Invocations + Expensive Instance (HIGH) - <100 invocations/day on ml.m5.2xlarge  Downsize
        3. Low Latency Requirements (MEDIUM) - Avg latency <100ms + multi-instance  Reduce to 1 instance
        4. Dev/Test with Production Instance (MEDIUM) - Dev/test endpoint with expensive instance  Downsize to ml.t3.medium
        5. No Auto-Scaling (LOW) - Manual instance count  Enable auto-scaling policy

        Args:
            endpoint_name: Endpoint name
            instance_type: ML instance type (e.g., 'ml.m5.large')
            instance_count: Number of instances
            total_invocations_7d: Total invocations over last 7 days
            avg_model_latency_ms: Average model latency in milliseconds
            status: Endpoint status (InService, Creating, etc.)
            tags: Resource tags
            region: AWS region

        Returns:
            List of optimization scenarios with priority, estimated savings, and recommendations
        """
        scenarios = []

        # Current cost
        current_cost = self._calculate_sagemaker_endpoint_monthly_cost(
            instance_type=instance_type,
            instance_count=instance_count,
            region=region,
        )

        # Detect environment from tags
        env = tags.get("Environment", tags.get("environment", "")).lower()
        is_production = env in ["production", "prod", "prd"]

        # Calculate invocations per day
        invocations_per_day = total_invocations_7d / 7.0

        # =============================================
        # SCENARIO 1: Zero Invocations (CRITICAL)
        # =============================================
        if total_invocations_7d == 0 and status == "InService":
            # No invocations for 7+ days  Delete endpoint
            potential_savings = current_cost
            scenarios.append(
                OptimizationScenario(
                    scenario_name="sagemaker_endpoint_zero_invocations",
                    priority="CRITICAL",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=0.0,
                    confidence_level="high",
                    description=(
                        f"SageMaker Endpoint '{endpoint_name}' has zero invocations "
                        f"over the last 7 days. This endpoint is completely idle and should be deleted."
                    ),
                    recommendation=(
                        "Delete this SageMaker Endpoint to eliminate costs:\n"
                        f"1. Delete endpoint: aws sagemaker delete-endpoint --endpoint-name {endpoint_name}\n"
                        f"2. Delete endpoint config: aws sagemaker delete-endpoint-config --endpoint-config-name <config-name>\n"
                        f"3. Estimated savings: ${potential_savings:.2f}/month (100%)"
                    ),
                    action_complexity="low",
                    risk_level="low",
                    implementation_time="5 minutes",
                )
            )

        # =============================================
        # SCENARIO 2: Low Invocations + Expensive Instance (HIGH)
        # =============================================
        expensive_instances = ["ml.m5.2xlarge", "ml.m5.4xlarge", "ml.m5.8xlarge", "ml.m5.12xlarge"]
        if invocations_per_day < 100 and instance_type in expensive_instances:
            # Low invocations (<100/day) on expensive instance  Downsize to ml.m5.large
            optimized_cost = self._calculate_sagemaker_endpoint_monthly_cost(
                instance_type="ml.m5.large",
                instance_count=instance_count,
                region=region,
            )
            potential_savings = current_cost - optimized_cost

            scenarios.append(
                OptimizationScenario(
                    scenario_name="sagemaker_endpoint_low_invocations_expensive_instance",
                    priority="HIGH",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=optimized_cost,
                    confidence_level="high",
                    description=(
                        f"SageMaker Endpoint '{endpoint_name}' has low invocation rate "
                        f"(avg {invocations_per_day:.1f} invocations/day) but uses expensive instance type '{instance_type}'. "
                        f"This is over-provisioned for the workload."
                    ),
                    recommendation=(
                        f"Downsize to ml.m5.large to match workload:\n"
                        f"1. Create new endpoint configuration with ml.m5.large: aws sagemaker create-endpoint-config\n"
                        f"2. Update endpoint: aws sagemaker update-endpoint --endpoint-name {endpoint_name}\n"
                        f"3. Monitor latency and error rate after change\n"
                        f"4. Estimated savings: ${potential_savings:.2f}/month ({(potential_savings / current_cost * 100):.0f}%)"
                    ),
                    action_complexity="medium",
                    risk_level="low",
                    implementation_time="20 minutes",
                )
            )

        # =============================================
        # SCENARIO 3: Low Latency Requirements + Multi-Instance (MEDIUM)
        # =============================================
        if avg_model_latency_ms < 100 and instance_count > 1:
            # Low latency (<100ms) + multi-instance  Reduce to 1 instance
            optimized_cost = self._calculate_sagemaker_endpoint_monthly_cost(
                instance_type=instance_type,
                instance_count=1,  # Single instance
                region=region,
            )
            potential_savings = current_cost - optimized_cost

            scenarios.append(
                OptimizationScenario(
                    scenario_name="sagemaker_endpoint_low_latency_multi_instance",
                    priority="MEDIUM",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=optimized_cost,
                    confidence_level="high",
                    description=(
                        f"SageMaker Endpoint '{endpoint_name}' has low latency "
                        f"(avg {avg_model_latency_ms:.1f}ms) but uses {instance_count} instances. "
                        f"A single instance may be sufficient for this workload."
                    ),
                    recommendation=(
                        f"Reduce to 1 instance for cost efficiency:\n"
                        f"1. Create new endpoint configuration with InitialInstanceCount=1\n"
                        f"2. Update endpoint: aws sagemaker update-endpoint --endpoint-name {endpoint_name}\n"
                        f"3. Monitor latency and throughput after change\n"
                        f"4. Estimated savings: ${potential_savings:.2f}/month ({(potential_savings / current_cost * 100):.0f}%)"
                    ),
                    action_complexity="medium",
                    risk_level="low",
                    implementation_time="20 minutes",
                )
            )

        # =============================================
        # SCENARIO 4: Dev/Test with Production Instance (MEDIUM)
        # =============================================
        production_instances = ["ml.m5.large", "ml.m5.xlarge", "ml.m5.2xlarge", "ml.m5.4xlarge"]
        if not is_production and instance_type in production_instances:
            # Dev/test endpoint with production instance  Downsize to ml.t3.medium
            optimized_cost = self._calculate_sagemaker_endpoint_monthly_cost(
                instance_type="ml.t3.medium",
                instance_count=instance_count,
                region=region,
            )
            potential_savings = current_cost - optimized_cost

            scenarios.append(
                OptimizationScenario(
                    scenario_name="sagemaker_endpoint_dev_test_production_instance",
                    priority="MEDIUM",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=optimized_cost,
                    confidence_level="high",
                    description=(
                        f"SageMaker Endpoint '{endpoint_name}' is tagged as dev/test but uses "
                        f"production instance type '{instance_type}'. Dev/test workloads can use "
                        f"cheaper burstable instances (ml.t3.medium)."
                    ),
                    recommendation=(
                        f"Downsize to ml.t3.medium for dev/test:\n"
                        f"1. Create new endpoint configuration with ml.t3.medium\n"
                        f"2. Update endpoint: aws sagemaker update-endpoint --endpoint-name {endpoint_name}\n"
                        f"3. ml.t3.medium provides sufficient performance for dev/test workloads\n"
                        f"4. Estimated savings: ${potential_savings:.2f}/month ({(potential_savings / current_cost * 100):.0f}%)"
                    ),
                    action_complexity="low",
                    risk_level="low",
                    implementation_time="15 minutes",
                )
            )

        # =============================================
        # SCENARIO 5: No Auto-Scaling (LOW)
        # =============================================
        # Note: Auto-scaling is determined by application auto-scaling policies, not exposed in endpoint details
        # This scenario is informational/best-practice recommendation
        scenarios.append(
            OptimizationScenario(
                scenario_name="sagemaker_endpoint_no_auto_scaling",
                priority="LOW",
                estimated_monthly_savings=0.0,  # Prevents over-provisioning, not direct savings
                current_monthly_cost=current_cost,
                optimized_monthly_cost=current_cost,
                confidence_level="medium",
                description=(
                    f"SageMaker Endpoint '{endpoint_name}' may not have auto-scaling enabled. "
                    f"Auto-scaling adjusts instance count based on invocation rate, reducing costs "
                    f"during low-traffic periods."
                ),
                recommendation=(
                    f"Enable auto-scaling for cost efficiency:\n"
                    f"1. Register scalable target: aws application-autoscaling register-scalable-target --service-namespace sagemaker\n"
                    f"2. Create scaling policy based on SageMakerVariantInvocationsPerInstance metric\n"
                    f"3. Set min instances to 1, max instances to {instance_count * 2}\n"
                    f"4. Benefit: Automatic scaling based on workload (reduce costs during off-peak)"
                ),
                action_complexity="medium",
                risk_level="low",
                implementation_time="30 minutes",
            )
        )

        return scenarios

    async def scan_sagemaker_endpoints(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS SageMaker Endpoints for cost intelligence.

        AWS SageMaker Endpoints provide real-time inference for machine learning models. They host
        trained models on dedicated compute instances (ml.m5.large, ml.m5.xlarge, etc.) and serve
        predictions with low latency. Endpoints run 24/7 and are billed per instance hour.

        Use Cases:
        - Real-time inference (fraud detection, recommendation engines)
        - Batch transform (offline predictions)
        - A/B testing (multiple model variants)

        CloudWatch Metrics Used:
        - ModelInvocations (AWS/SageMaker) - Total invocations over 7 days
        - ModelLatency (AWS/SageMaker) - Average inference latency

        Cost Optimization Scenarios:
        1. Zero Invocations (CRITICAL) - No invocations for 7+ days  Delete
        2. Low Invocations + Expensive Instance (HIGH) - <100 invocations/day on ml.m5.2xlarge  Downsize
        3. Low Latency Requirements (MEDIUM) - Avg latency <100ms + multi-instance  Reduce to 1 instance
        4. Dev/Test with Production Instance (MEDIUM) - Dev/test with production instance  Downsize to ml.t3.medium
        5. No Auto-Scaling (LOW) - Enable auto-scaling for cost efficiency

        Args:
            region: AWS region to scan

        Returns:
            List of AllCloudResourceData for all SageMaker Endpoints in region
        """
        resources = []

        try:
            async with self.session.client("sagemaker", region_name=region) as sagemaker:
                # List all SageMaker endpoints
                response = await sagemaker.list_endpoints()
                endpoints = response.get("Endpoints", [])

                logger.info(
                    "sagemaker.endpoints_found",
                    region=region,
                    endpoint_count=len(endpoints),
                )

                for endpoint in endpoints:
                    try:
                        endpoint_name = endpoint.get("EndpointName", "unknown")
                        status = endpoint.get("EndpointStatus", "Unknown")

                        # Describe endpoint to get detailed configuration
                        detail_response = await sagemaker.describe_endpoint(
                            EndpointName=endpoint_name
                        )

                        endpoint_config_name = detail_response.get("EndpointConfigName", "")

                        # Get endpoint configuration details
                        config_response = await sagemaker.describe_endpoint_config(
                            EndpointConfigName=endpoint_config_name
                        )

                        # Extract instance type and count from production variants
                        instance_type = "ml.m5.large"  # Default
                        instance_count = 1
                        production_variants = config_response.get("ProductionVariants", [])

                        if production_variants:
                            first_variant = production_variants[0]
                            instance_type = first_variant.get("InstanceType", "ml.m5.large")
                            instance_count = first_variant.get("InitialInstanceCount", 1)

                        # Extract tags
                        tags_response = await sagemaker.list_tags(
                            ResourceArn=detail_response.get("EndpointArn", "")
                        )
                        tags_list = tags_response.get("Tags", [])
                        tags = {tag["Key"]: tag["Value"] for tag in tags_list}

                        # ====================================
                        # CLOUDWATCH METRICS (7-day lookback)
                        # ====================================
                        end_time = datetime.now(timezone.utc)
                        start_time = end_time - timedelta(days=7)

                        # Metric: ModelInvocations (total invocations)
                        total_invocations_7d = 0
                        for variant in production_variants:
                            variant_name = variant.get("VariantName", "AllTraffic")
                            invocations = await self._get_cloudwatch_metric_sum(
                                region=region,
                                namespace="AWS/SageMaker",
                                metric_name="ModelInvocations",
                                dimensions=[
                                    {"Name": "EndpointName", "Value": endpoint_name},
                                    {"Name": "VariantName", "Value": variant_name},
                                ],
                                start_time=start_time,
                                end_time=end_time,
                                period=3600,  # 1 hour
                            )
                            total_invocations_7d += int(invocations)

                        # Metric: ModelLatency (average latency in microseconds)
                        avg_model_latency_us = 0.0
                        for variant in production_variants:
                            variant_name = variant.get("VariantName", "AllTraffic")
                            latency_us = await self._get_cloudwatch_metric_average(
                                region=region,
                                namespace="AWS/SageMaker",
                                metric_name="ModelLatency",
                                dimensions=[
                                    {"Name": "EndpointName", "Value": endpoint_name},
                                    {"Name": "VariantName", "Value": variant_name},
                                ],
                                start_time=start_time,
                                end_time=end_time,
                                period=3600,  # 1 hour
                                statistic="Average",
                            )
                            avg_model_latency_us += latency_us

                        # Convert to milliseconds
                        avg_model_latency_ms = avg_model_latency_us / 1000.0 if avg_model_latency_us > 0 else 0.0

                        # ====================================
                        # COST CALCULATION
                        # ====================================
                        monthly_cost = self._calculate_sagemaker_endpoint_monthly_cost(
                            instance_type=instance_type,
                            instance_count=instance_count,
                            region=region,
                        )

                        # ====================================
                        # OPTIMIZATION SCENARIOS
                        # ====================================
                        optimization_scenarios = self._calculate_sagemaker_endpoint_optimization(
                            endpoint_name=endpoint_name,
                            instance_type=instance_type,
                            instance_count=instance_count,
                            total_invocations_7d=total_invocations_7d,
                            avg_model_latency_ms=avg_model_latency_ms,
                            status=status,
                            tags=tags,
                            region=region,
                        )

                        # ====================================
                        # CHECK IF ORPHAN (Zero invocations for 7+ days)
                        # ====================================
                        is_orphan = total_invocations_7d == 0 and status == "InService"

                        # ====================================
                        # CREATE RESOURCE DATA
                        # ====================================
                        resource = AllCloudResourceData(
                            resource_type="sagemaker_endpoint",
                            resource_id=endpoint_name,
                            resource_name=endpoint_name,
                            region=region,
                            estimated_monthly_cost=monthly_cost,
                            currency="USD",
                            resource_metadata={
                                "endpoint_name": endpoint_name,
                                "status": status,
                                "instance_type": instance_type,
                                "instance_count": instance_count,
                                "endpoint_config_name": endpoint_config_name,
                                "total_invocations_7d": total_invocations_7d,
                                "avg_model_latency_ms": round(avg_model_latency_ms, 2),
                                "tags": tags,
                            },
                            optimization_scenarios=optimization_scenarios,
                            is_orphan=is_orphan,
                            created_at_cloud=endpoint.get("CreationTime"),
                        )

                        resources.append(resource)

                        logger.debug(
                            "sagemaker.endpoint_scanned",
                            endpoint_name=endpoint_name,
                            instance_type=instance_type,
                            instance_count=instance_count,
                            monthly_cost=monthly_cost,
                            total_invocations_7d=total_invocations_7d,
                            scenarios_count=len(optimization_scenarios),
                            is_orphan=is_orphan,
                        )

                    except Exception as e:
                        logger.error(
                            "sagemaker.endpoint_scan_failed",
                            endpoint_name=endpoint.get("EndpointName", "unknown"),
                            region=region,
                            error=str(e),
                        )
                        continue

        except Exception as e:
            logger.error(
                "sagemaker.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # ============================================================
    # RESSOURCE 25 : REDSHIFT CLUSTER (DATA WAREHOUSE)
    # ============================================================

    def _calculate_redshift_monthly_cost(
        self,
        node_type: str,
        number_of_nodes: int,
        storage_gb: float,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS Redshift Cluster.

        Redshift is a fully managed data warehouse service optimized for analytics workloads.
        It uses columnar storage and parallel query execution for fast query performance.

        Cost Components:
        1. Node cost (dc2.large, dc2.xlarge, ra3.xlplus, ra3.4xlarge, etc.)
        2. Storage cost (for RA3 nodes only - managed storage)
        3. Backup storage cost (not included - typically minimal)

        Args:
            node_type: Node type (e.g., 'dc2.large', 'ra3.xlplus')
            number_of_nodes: Number of nodes in cluster
            storage_gb: Storage size in GB (for RA3 only)
            region: AWS region

        Returns:
            Estimated monthly cost in USD

        Example:
            2-node cluster with dc2.large:
            - Nodes: $0.25/hour * 730 hours * 2 = $365/month
        """
        monthly_hours = 730

        # Node cost based on type
        node_rate_map = {
            "dc2.large": self.PRICING.get("redshift_dc2_large", 0.25),         # 2 vCPU, 15 GB RAM, 160 GB SSD
            "dc2.xlarge": self.PRICING.get("redshift_dc2_xlarge", 1.00),       # 4 vCPU, 31 GB RAM, 2.56 TB SSD
            "dc2.8xlarge": self.PRICING.get("redshift_dc2_8xlarge", 4.80),     # 32 vCPU, 244 GB RAM, 2.56 TB SSD
            "ra3.xlplus": self.PRICING.get("redshift_ra3_xlplus", 1.086),      # 4 vCPU, 32 GB RAM, managed storage
            "ra3.4xlarge": self.PRICING.get("redshift_ra3_4xlarge", 3.26),     # 12 vCPU, 96 GB RAM, managed storage
            "ra3.16xlarge": 13.04,   # 48 vCPU, 384 GB RAM, managed storage
        }

        node_hourly_rate = node_rate_map.get(node_type, 0.25)  # Default to dc2.large
        node_cost = node_hourly_rate * monthly_hours * number_of_nodes

        # Storage cost (only for RA3 nodes with managed storage)
        storage_cost = 0.0
        if "ra3" in node_type and storage_gb > 0:
            storage_rate = self.PRICING.get("redshift_storage_per_gb", 0.024)
            storage_cost = storage_gb * storage_rate

        total_cost = node_cost + storage_cost

        return round(total_cost, 2)

    def _calculate_redshift_optimization(
        self,
        cluster_identifier: str,
        node_type: str,
        number_of_nodes: int,
        storage_gb: float,
        avg_connections_7d: float,
        avg_cpu_percent_7d: float,
        avg_disk_usage_percent: float,
        cluster_status: str,
        tags: dict,
        region: str,
    ) -> list[OptimizationScenario]:
        """
        Calculate optimization scenarios for AWS Redshift Cluster.

        Redshift Optimization Scenarios:
        1. Zero Connections (CRITICAL) - No database connections for 7+ days  Pause/Delete cluster
        2. Low Query Volume + Expensive Nodes (HIGH) - Low usage on expensive nodes (dc2.8xlarge)  Downsize or migrate to Aurora
        3. Over-provisioned Nodes (MEDIUM) - Low CPU (<30%) + multi-node cluster  Reduce node count
        4. Dev/Test Multi-Node (MEDIUM) - Dev/test cluster with >1 node  Single node cluster
        5. No Concurrency Scaling (LOW) - Manual scaling  Enable concurrency scaling

        Args:
            cluster_identifier: Cluster identifier
            node_type: Node type (e.g., 'dc2.large')
            number_of_nodes: Number of nodes
            storage_gb: Storage size in GB
            avg_connections_7d: Average connections over 7 days
            avg_cpu_percent_7d: Average CPU utilization over 7 days
            avg_disk_usage_percent: Average disk usage percentage
            cluster_status: Cluster status (available, paused, etc.)
            tags: Resource tags
            region: AWS region

        Returns:
            List of optimization scenarios with priority, estimated savings, and recommendations
        """
        scenarios = []

        # Current cost
        current_cost = self._calculate_redshift_monthly_cost(
            node_type=node_type,
            number_of_nodes=number_of_nodes,
            storage_gb=storage_gb,
            region=region,
        )

        # Detect environment from tags
        env = tags.get("Environment", tags.get("environment", "")).lower()
        is_production = env in ["production", "prod", "prd"]

        # =============================================
        # SCENARIO 1: Zero Connections (CRITICAL)
        # =============================================
        if avg_connections_7d < 0.1 and cluster_status == "available":
            # No connections for 7+ days  Pause or Delete cluster
            potential_savings = current_cost
            scenarios.append(
                OptimizationScenario(
                    scenario_name="redshift_zero_connections",
                    priority="CRITICAL",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=0.0,
                    confidence_level="high",
                    description=(
                        f"Redshift cluster '{cluster_identifier}' has zero database connections "
                        f"over the last 7 days. This cluster is completely idle."
                    ),
                    recommendation=(
                        "Pause or delete this Redshift cluster:\n"
                        f"1. Pause cluster (retains data): aws redshift pause-cluster --cluster-identifier {cluster_identifier}\n"
                        f"2. Or delete cluster: aws redshift delete-cluster --cluster-identifier {cluster_identifier} --final-cluster-snapshot-identifier final-snapshot\n"
                        f"3. Paused clusters only incur storage costs (~10% of compute cost)\n"
                        f"4. Estimated savings: ${potential_savings:.2f}/month (100% if deleted, ~90% if paused)"
                    ),
                    action_complexity="low",
                    risk_level="low",
                    implementation_time="5 minutes",
                )
            )

        # =============================================
        # SCENARIO 2: Low Query Volume + Expensive Nodes (HIGH)
        # =============================================
        expensive_nodes = ["dc2.8xlarge", "ra3.4xlarge", "ra3.16xlarge"]
        if avg_connections_7d < 10.0 and node_type in expensive_nodes:
            # Low connections (<10 avg) on expensive nodes  Downsize to dc2.large or migrate
            optimized_cost = self._calculate_redshift_monthly_cost(
                node_type="dc2.large",
                number_of_nodes=number_of_nodes,
                storage_gb=0,  # dc2 has local storage
                region=region,
            )
            potential_savings = current_cost - optimized_cost

            scenarios.append(
                OptimizationScenario(
                    scenario_name="redshift_low_query_volume_expensive_nodes",
                    priority="HIGH",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=optimized_cost,
                    confidence_level="high",
                    description=(
                        f"Redshift cluster '{cluster_identifier}' has low connection rate "
                        f"(avg {avg_connections_7d:.1f} connections) but uses expensive node type '{node_type}'. "
                        f"Consider downsizing to dc2.large or migrating to Aurora PostgreSQL for OLTP workloads."
                    ),
                    recommendation=(
                        f"Downsize to dc2.large or migrate to Aurora:\n"
                        f"1. If OLAP workload: Resize cluster to dc2.large: aws redshift modify-cluster --node-type dc2.large\n"
                        f"2. If OLTP workload: Migrate to Aurora PostgreSQL (lower cost for transactional queries)\n"
                        f"3. Take snapshot before resize for rollback option\n"
                        f"4. Estimated savings: ${potential_savings:.2f}/month ({(potential_savings / current_cost * 100):.0f}%)"
                    ),
                    action_complexity="medium",
                    risk_level="medium",
                    implementation_time="1 hour",
                )
            )

        # =============================================
        # SCENARIO 3: Over-provisioned Nodes (MEDIUM)
        # =============================================
        if avg_cpu_percent_7d < 30.0 and number_of_nodes > 1:
            # Low CPU (<30%) + multi-node  Reduce node count
            optimized_nodes = max(1, number_of_nodes // 2)
            optimized_cost = self._calculate_redshift_monthly_cost(
                node_type=node_type,
                number_of_nodes=optimized_nodes,
                storage_gb=storage_gb if "ra3" in node_type else 0,
                region=region,
            )
            potential_savings = current_cost - optimized_cost

            scenarios.append(
                OptimizationScenario(
                    scenario_name="redshift_over_provisioned_nodes",
                    priority="MEDIUM",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=optimized_cost,
                    confidence_level="high",
                    description=(
                        f"Redshift cluster '{cluster_identifier}' has low CPU utilization "
                        f"(avg {avg_cpu_percent_7d:.1f}%) with {number_of_nodes} nodes. "
                        f"The cluster is over-provisioned and can be downsized."
                    ),
                    recommendation=(
                        f"Reduce node count to {optimized_nodes} nodes:\n"
                        f"1. Resize cluster: aws redshift modify-cluster --cluster-identifier {cluster_identifier} --number-of-nodes {optimized_nodes}\n"
                        f"2. Monitor query performance after resize\n"
                        f"3. Redshift will redistribute data automatically\n"
                        f"4. Estimated savings: ${potential_savings:.2f}/month ({(potential_savings / current_cost * 100):.0f}%)"
                    ),
                    action_complexity="medium",
                    risk_level="low",
                    implementation_time="30 minutes",
                )
            )

        # =============================================
        # SCENARIO 4: Dev/Test Multi-Node (MEDIUM)
        # =============================================
        if not is_production and number_of_nodes > 1:
            # Dev/test cluster with multi-node  Single node
            optimized_cost = self._calculate_redshift_monthly_cost(
                node_type=node_type,
                number_of_nodes=1,
                storage_gb=storage_gb if "ra3" in node_type else 0,
                region=region,
            )
            potential_savings = current_cost - optimized_cost

            scenarios.append(
                OptimizationScenario(
                    scenario_name="redshift_dev_test_multi_node",
                    priority="MEDIUM",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=optimized_cost,
                    confidence_level="high",
                    description=(
                        f"Redshift cluster '{cluster_identifier}' is tagged as dev/test but has "
                        f"{number_of_nodes} nodes. Dev/test workloads typically don't require "
                        f"multi-node clusters for parallel processing."
                    ),
                    recommendation=(
                        f"Reduce to single-node cluster for dev/test:\n"
                        f"1. Resize cluster: aws redshift modify-cluster --cluster-identifier {cluster_identifier} --number-of-nodes 1\n"
                        f"2. Single node is sufficient for most dev/test workloads\n"
                        f"3. Use production cluster for load testing\n"
                        f"4. Estimated savings: ${potential_savings:.2f}/month ({(potential_savings / current_cost * 100):.0f}%)"
                    ),
                    action_complexity="low",
                    risk_level="low",
                    implementation_time="20 minutes",
                )
            )

        # =============================================
        # SCENARIO 5: No Concurrency Scaling (LOW)
        # =============================================
        # Note: Concurrency scaling is determined by cluster settings, not exposed in describe-clusters
        # This scenario is informational/best-practice recommendation
        scenarios.append(
            OptimizationScenario(
                scenario_name="redshift_no_concurrency_scaling",
                priority="LOW",
                estimated_monthly_savings=0.0,  # Variable savings based on workload spikes
                current_monthly_cost=current_cost,
                optimized_monthly_cost=current_cost,
                confidence_level="medium",
                description=(
                    f"Redshift cluster '{cluster_identifier}' may not have concurrency scaling enabled. "
                    f"Concurrency scaling automatically adds compute capacity during query spikes, "
                    f"preventing queue delays without over-provisioning base cluster."
                ),
                recommendation=(
                    f"Enable concurrency scaling for cost efficiency:\n"
                    f"1. Enable in cluster parameter group: concurrency_scaling_mode = auto\n"
                    f"2. Set max_concurrency_scaling_clusters (default: 10)\n"
                    f"3. Free for up to 1 hour/day of usage\n"
                    f"4. Benefit: Handle query spikes without over-provisioning base cluster"
                ),
                action_complexity="low",
                risk_level="low",
                implementation_time="10 minutes",
            )
        )

        return scenarios

    async def scan_redshift_clusters(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS Redshift Clusters for cost intelligence.

        AWS Redshift is a fully managed data warehouse service optimized for analytics workloads.
        It uses columnar storage and parallel query execution for fast performance on large datasets.

        Use Cases:
        - Business intelligence (BI) and analytics
        - Data warehousing (ETL pipelines)
        - Log analysis and event data processing

        CloudWatch Metrics Used:
        - DatabaseConnections (AWS/Redshift) - Average database connections
        - CPUUtilization (AWS/Redshift) - CPU usage percentage
        - PercentageDiskSpaceUsed (AWS/Redshift) - Disk usage percentage

        Cost Optimization Scenarios:
        1. Zero Connections (CRITICAL) - No connections for 7+ days  Pause/Delete
        2. Low Query Volume + Expensive Nodes (HIGH) - <10 connections on expensive nodes  Downsize
        3. Over-provisioned Nodes (MEDIUM) - Low CPU (<30%) + multi-node  Reduce nodes
        4. Dev/Test Multi-Node (MEDIUM) - Dev/test with >1 node  Single node
        5. No Concurrency Scaling (LOW) - Enable concurrency scaling for spike handling

        Args:
            region: AWS region to scan

        Returns:
            List of AllCloudResourceData for all Redshift clusters in region
        """
        resources = []

        try:
            async with self.session.client("redshift", region_name=region) as redshift:
                # Describe all Redshift clusters
                response = await redshift.describe_clusters()
                clusters = response.get("Clusters", [])

                logger.info(
                    "redshift.clusters_found",
                    region=region,
                    cluster_count=len(clusters),
                )

                for cluster in clusters:
                    try:
                        cluster_identifier = cluster.get("ClusterIdentifier", "unknown")
                        cluster_status = cluster.get("ClusterStatus", "unknown")

                        # Extract cluster details
                        node_type = cluster.get("NodeType", "dc2.large")
                        number_of_nodes = cluster.get("NumberOfNodes", 1)

                        # Calculate storage (for RA3 nodes, use TotalStorageCapacity; for DC2, estimate)
                        storage_gb = 0.0
                        if "ra3" in node_type:
                            storage_gb = cluster.get("TotalStorageCapacity", 0) / (1024 ** 3)  # Convert bytes to GB

                        # Extract tags
                        tags_list = cluster.get("Tags", [])
                        tags = {tag["Key"]: tag["Value"] for tag in tags_list}

                        # ====================================
                        # CLOUDWATCH METRICS (7-day lookback)
                        # ====================================
                        end_time = datetime.now(timezone.utc)
                        start_time = end_time - timedelta(days=7)

                        # Metric: DatabaseConnections (average connections)
                        avg_connections_7d = await self._get_cloudwatch_metric_average(
                            region=region,
                            namespace="AWS/Redshift",
                            metric_name="DatabaseConnections",
                            dimensions=[
                                {"Name": "ClusterIdentifier", "Value": cluster_identifier}
                            ],
                            start_time=start_time,
                            end_time=end_time,
                            period=3600,  # 1 hour
                            statistic="Average",
                        )

                        # Metric: CPUUtilization (CPU usage)
                        avg_cpu_percent_7d = await self._get_cloudwatch_metric_average(
                            region=region,
                            namespace="AWS/Redshift",
                            metric_name="CPUUtilization",
                            dimensions=[
                                {"Name": "ClusterIdentifier", "Value": cluster_identifier}
                            ],
                            start_time=start_time,
                            end_time=end_time,
                            period=3600,  # 1 hour
                            statistic="Average",
                        )

                        # Metric: PercentageDiskSpaceUsed (disk usage)
                        avg_disk_usage_percent = await self._get_cloudwatch_metric_average(
                            region=region,
                            namespace="AWS/Redshift",
                            metric_name="PercentageDiskSpaceUsed",
                            dimensions=[
                                {"Name": "ClusterIdentifier", "Value": cluster_identifier}
                            ],
                            start_time=start_time,
                            end_time=end_time,
                            period=3600,  # 1 hour
                            statistic="Average",
                        )

                        # ====================================
                        # COST CALCULATION
                        # ====================================
                        monthly_cost = self._calculate_redshift_monthly_cost(
                            node_type=node_type,
                            number_of_nodes=number_of_nodes,
                            storage_gb=storage_gb,
                            region=region,
                        )

                        # ====================================
                        # OPTIMIZATION SCENARIOS
                        # ====================================
                        optimization_scenarios = self._calculate_redshift_optimization(
                            cluster_identifier=cluster_identifier,
                            node_type=node_type,
                            number_of_nodes=number_of_nodes,
                            storage_gb=storage_gb,
                            avg_connections_7d=avg_connections_7d,
                            avg_cpu_percent_7d=avg_cpu_percent_7d,
                            avg_disk_usage_percent=avg_disk_usage_percent,
                            cluster_status=cluster_status,
                            tags=tags,
                            region=region,
                        )

                        # ====================================
                        # CHECK IF ORPHAN (Zero connections for 7+ days)
                        # ====================================
                        is_orphan = avg_connections_7d < 0.1 and cluster_status == "available"

                        # ====================================
                        # CREATE RESOURCE DATA
                        # ====================================
                        resource = AllCloudResourceData(
                            resource_type="redshift_cluster",
                            resource_id=cluster_identifier,
                            resource_name=cluster_identifier,
                            region=region,
                            estimated_monthly_cost=monthly_cost,
                            currency="USD",
                            resource_metadata={
                                "cluster_identifier": cluster_identifier,
                                "cluster_status": cluster_status,
                                "node_type": node_type,
                                "number_of_nodes": number_of_nodes,
                                "storage_gb": round(storage_gb, 2) if storage_gb > 0 else 0,
                                "endpoint": cluster.get("Endpoint", {}).get("Address", ""),
                                "avg_connections_7d": round(avg_connections_7d, 2),
                                "avg_cpu_percent_7d": round(avg_cpu_percent_7d, 2),
                                "avg_disk_usage_percent": round(avg_disk_usage_percent, 2),
                                "tags": tags,
                            },
                            optimization_scenarios=optimization_scenarios,
                            is_orphan=is_orphan,
                            created_at_cloud=cluster.get("ClusterCreateTime"),
                        )

                        resources.append(resource)

                        logger.debug(
                            "redshift.cluster_scanned",
                            cluster_identifier=cluster_identifier,
                            node_type=node_type,
                            number_of_nodes=number_of_nodes,
                            monthly_cost=monthly_cost,
                            avg_connections_7d=round(avg_connections_7d, 2),
                            scenarios_count=len(optimization_scenarios),
                            is_orphan=is_orphan,
                        )

                    except Exception as e:
                        logger.error(
                            "redshift.cluster_scan_failed",
                            cluster_identifier=cluster.get("ClusterIdentifier", "unknown"),
                            region=region,
                            error=str(e),
                        )
                        continue

        except Exception as e:
            logger.error(
                "redshift.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # ============================================================
    # RESSOURCE 26 : VPN CONNECTION (SITE-TO-SITE VPN)
    # ============================================================

    def _calculate_vpn_connection_monthly_cost(
        self,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS VPN Connection.

        VPN Connections provide secure connectivity between on-premises networks and AWS VPCs
        using IPsec tunnels. Each connection has 2 tunnels for redundancy.

        Cost Components:
        1. Connection cost ($36/month flat rate per connection)
        2. Data transfer cost (not included - varies by usage)

        Args:
            region: AWS region

        Returns:
            Estimated monthly cost in USD (flat rate)

        Example:
            Single VPN Connection:
            - Connection: $36/month (flat rate)
        """
        # VPN Connection is a flat rate ($36/month)
        vpn_cost = self.PRICING.get("vpn_connection", 36.00)

        return round(vpn_cost, 2)

    def _calculate_vpn_connection_optimization(
        self,
        vpn_id: str,
        total_bytes_in_30d: float,
        total_bytes_out_30d: float,
        tunnel_1_state: str,
        tunnel_2_state: str,
        vpn_state: str,
        tags: dict,
        region: str,
    ) -> list[OptimizationScenario]:
        """
        Calculate optimization scenarios for AWS VPN Connection.

        VPN Connection Optimization Scenarios:
        1. Zero Data Transfer (CRITICAL) - No data transfer for 30+ days  Delete connection
        2. Low Data Transfer (HIGH) - <100 MB/month + available state  Delete or consolidate
        3. Single Tunnel Active (MEDIUM) - Only 1 of 2 tunnels UP  Check redundancy requirements
        4. Replaced by Transit Gateway (MEDIUM) - Transit Gateway exists  Migrate to TGW
        5. Dev/Test VPN (LOW) - Dev/test connection always up  Delete when not needed

        Args:
            vpn_id: VPN Connection ID
            total_bytes_in_30d: Total incoming bytes over 30 days
            total_bytes_out_30d: Total outgoing bytes over 30 days
            tunnel_1_state: Tunnel 1 state (UP/DOWN)
            tunnel_2_state: Tunnel 2 state (UP/DOWN)
            vpn_state: VPN state (available, deleted, etc.)
            tags: Resource tags
            region: AWS region

        Returns:
            List of optimization scenarios with priority, estimated savings, and recommendations
        """
        scenarios = []

        # Current cost (flat rate)
        current_cost = self._calculate_vpn_connection_monthly_cost(region=region)

        # Detect environment from tags
        env = tags.get("Environment", tags.get("environment", "")).lower()
        is_production = env in ["production", "prod", "prd"]

        # Calculate total data transfer
        total_bytes_transferred = total_bytes_in_30d + total_bytes_out_30d
        total_mb_transferred = total_bytes_transferred / (1024 ** 2)

        # =============================================
        # SCENARIO 1: Zero Data Transfer (CRITICAL)
        # =============================================
        if total_bytes_transferred == 0 and vpn_state == "available":
            # No data transfer for 30+ days  Delete connection
            potential_savings = current_cost
            scenarios.append(
                OptimizationScenario(
                    scenario_name="vpn_zero_data_transfer",
                    priority="CRITICAL",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=0.0,
                    confidence_level="high",
                    description=(
                        f"VPN Connection '{vpn_id}' has zero data transfer "
                        f"over the last 30 days. This connection is completely idle and should be deleted."
                    ),
                    recommendation=(
                        "Delete this VPN Connection to eliminate costs:\n"
                        f"1. Verify no active routes pointing to this VPN\n"
                        f"2. Delete VPN connection: aws ec2 delete-vpn-connection --vpn-connection-id {vpn_id}\n"
                        f"3. Estimated savings: ${potential_savings:.2f}/month (100%)"
                    ),
                    action_complexity="low",
                    risk_level="low",
                    implementation_time="5 minutes",
                )
            )

        # =============================================
        # SCENARIO 2: Low Data Transfer (HIGH)
        # =============================================
        if total_mb_transferred < 100 and total_mb_transferred > 0 and vpn_state == "available":
            # Low data transfer (<100 MB/month)  Delete or consolidate
            potential_savings = current_cost
            scenarios.append(
                OptimizationScenario(
                    scenario_name="vpn_low_data_transfer",
                    priority="HIGH",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=0.0,
                    confidence_level="high",
                    description=(
                        f"VPN Connection '{vpn_id}' has very low data transfer "
                        f"({total_mb_transferred:.2f} MB over 30 days). This suggests minimal usage "
                        f"and the connection may no longer be needed."
                    ),
                    recommendation=(
                        f"Delete or consolidate VPN connection:\n"
                        f"1. Investigate if workload can be migrated to AWS or consolidated with another VPN\n"
                        f"2. If no longer needed: aws ec2 delete-vpn-connection --vpn-connection-id {vpn_id}\n"
                        f"3. Estimated savings: ${potential_savings:.2f}/month (100%)"
                    ),
                    action_complexity="medium",
                    risk_level="medium",
                    implementation_time="15 minutes",
                )
            )

        # =============================================
        # SCENARIO 3: Single Tunnel Active (MEDIUM)
        # =============================================
        if vpn_state == "available" and ((tunnel_1_state == "UP" and tunnel_2_state == "DOWN") or
                                          (tunnel_1_state == "DOWN" and tunnel_2_state == "UP")):
            # Only one tunnel UP  Check redundancy
            scenarios.append(
                OptimizationScenario(
                    scenario_name="vpn_single_tunnel_active",
                    priority="MEDIUM",
                    estimated_monthly_savings=0.0,  # Reliability issue, not cost savings
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=current_cost,
                    confidence_level="high",
                    description=(
                        f"VPN Connection '{vpn_id}' has only one tunnel UP (Tunnel 1: {tunnel_1_state}, "
                        f"Tunnel 2: {tunnel_2_state}). VPN connections should have both tunnels UP "
                        f"for redundancy and high availability."
                    ),
                    recommendation=(
                        f"Investigate and restore second tunnel:\n"
                        f"1. Check on-premises VPN device configuration for both tunnels\n"
                        f"2. Verify IPsec settings match AWS configuration\n"
                        f"3. Review CloudWatch VPN tunnel status metrics\n"
                        f"4. Both tunnels should be UP for redundancy (no cost impact)"
                    ),
                    action_complexity="medium",
                    risk_level="high",
                    implementation_time="30 minutes",
                )
            )

        # =============================================
        # SCENARIO 4: Replaced by Transit Gateway (MEDIUM)
        # =============================================
        # Note: This requires checking if Transit Gateway exists in same VPC/region
        # This is a heuristic recommendation - actual migration requires analysis
        scenarios.append(
            OptimizationScenario(
                scenario_name="vpn_replaced_by_transit_gateway",
                priority="MEDIUM",
                estimated_monthly_savings=0.0,  # TGW may be more cost-effective at scale
                current_monthly_cost=current_cost,
                optimized_monthly_cost=current_cost,
                confidence_level="low",
                description=(
                    f"VPN Connection '{vpn_id}' may be a candidate for Transit Gateway migration. "
                    f"Transit Gateway provides centralized connectivity management and can be more "
                    f"cost-effective for multiple VPN connections or VPCs."
                ),
                recommendation=(
                    f"Consider migrating to Transit Gateway if you have:\n"
                    f"1. Multiple VPN connections (3+ VPNs)\n"
                    f"2. Multiple VPC peering connections\n"
                    f"3. Centralized network architecture requirement\n"
                    f"4. Note: TGW has hourly charge ($0.05/hour) but simplifies management at scale"
                ),
                action_complexity="high",
                risk_level="medium",
                implementation_time="2 hours",
            )
        )

        # =============================================
        # SCENARIO 5: Dev/Test VPN Always Up (LOW)
        # =============================================
        if not is_production and vpn_state == "available":
            # Dev/test VPN always up  Delete when not needed
            potential_savings = current_cost * 0.5  # Assume 50% time not needed
            scenarios.append(
                OptimizationScenario(
                    scenario_name="vpn_dev_test_always_up",
                    priority="LOW",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=current_cost - potential_savings,
                    confidence_level="medium",
                    description=(
                        f"VPN Connection '{vpn_id}' is tagged as dev/test but remains UP continuously. "
                        f"Dev/test VPNs are typically only needed during business hours or active development periods."
                    ),
                    recommendation=(
                        f"Delete VPN when not needed for dev/test:\n"
                        f"1. Delete VPN outside business hours: aws ec2 delete-vpn-connection --vpn-connection-id {vpn_id}\n"
                        f"2. Recreate when needed (configuration can be saved as template)\n"
                        f"3. Or use Lambda to automate start/stop based on schedule\n"
                        f"4. Estimated savings: ~${potential_savings:.2f}/month (50% time savings)"
                    ),
                    action_complexity="medium",
                    risk_level="low",
                    implementation_time="20 minutes",
                )
            )

        return scenarios

    async def scan_vpn_connections(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS VPN Connections for cost intelligence.

        AWS VPN Connections provide secure connectivity between on-premises networks and AWS VPCs
        using IPsec tunnels. Each connection has 2 tunnels for redundancy and high availability.

        Use Cases:
        - Hybrid cloud connectivity (on-premises to AWS)
        - Disaster recovery (backup connectivity)
        - Secure remote access (site-to-site VPN)

        CloudWatch Metrics Used:
        - TunnelDataIn (AWS/VPN) - Incoming bytes over 30 days
        - TunnelDataOut (AWS/VPN) - Outgoing bytes over 30 days
        - TunnelState (AWS/VPN) - Tunnel status (UP/DOWN)

        Cost Optimization Scenarios:
        1. Zero Data Transfer (CRITICAL) - No data transfer for 30+ days  Delete
        2. Low Data Transfer (HIGH) - <100 MB/month  Delete or consolidate
        3. Single Tunnel Active (MEDIUM) - Only 1 of 2 tunnels UP  Fix redundancy
        4. Replaced by Transit Gateway (MEDIUM) - Migrate to TGW for scale
        5. Dev/Test VPN Always Up (LOW) - Delete when not needed

        Args:
            region: AWS region to scan

        Returns:
            List of AllCloudResourceData for all VPN Connections in region
        """
        resources = []

        try:
            async with self.session.client("ec2", region_name=region) as ec2:
                # Describe all VPN connections
                response = await ec2.describe_vpn_connections()
                vpn_connections = response.get("VpnConnections", [])

                logger.info(
                    "vpn.connections_found",
                    region=region,
                    vpn_count=len(vpn_connections),
                )

                for vpn in vpn_connections:
                    try:
                        vpn_id = vpn.get("VpnConnectionId", "unknown")
                        vpn_state = vpn.get("State", "unknown")

                        # Skip deleted/deleting VPN connections
                        if vpn_state in ["deleted", "deleting"]:
                            continue

                        # Extract VPN details
                        vpn_type = vpn.get("Type", "ipsec.1")
                        customer_gateway_id = vpn.get("CustomerGatewayId", "")
                        vpn_gateway_id = vpn.get("VpnGatewayId", "")

                        # Extract tunnel states
                        vgw_telemetry = vpn.get("VgwTelemetry", [])
                        tunnel_1_state = "DOWN"
                        tunnel_2_state = "DOWN"

                        if len(vgw_telemetry) >= 1:
                            tunnel_1_state = vgw_telemetry[0].get("Status", "DOWN")
                        if len(vgw_telemetry) >= 2:
                            tunnel_2_state = vgw_telemetry[1].get("Status", "DOWN")

                        # Extract tags
                        tags_list = vpn.get("Tags", [])
                        tags = {tag["Key"]: tag["Value"] for tag in tags_list}

                        # ====================================
                        # CLOUDWATCH METRICS (30-day lookback)
                        # ====================================
                        end_time = datetime.now(timezone.utc)
                        start_time = end_time - timedelta(days=30)

                        # Metric: TunnelDataIn (incoming bytes)
                        total_bytes_in_30d = await self._get_cloudwatch_metric_sum(
                            region=region,
                            namespace="AWS/VPN",
                            metric_name="TunnelDataIn",
                            dimensions=[
                                {"Name": "VpnId", "Value": vpn_id}
                            ],
                            start_time=start_time,
                            end_time=end_time,
                            period=86400,  # 1 day
                        )

                        # Metric: TunnelDataOut (outgoing bytes)
                        total_bytes_out_30d = await self._get_cloudwatch_metric_sum(
                            region=region,
                            namespace="AWS/VPN",
                            metric_name="TunnelDataOut",
                            dimensions=[
                                {"Name": "VpnId", "Value": vpn_id}
                            ],
                            start_time=start_time,
                            end_time=end_time,
                            period=86400,  # 1 day
                        )

                        # ====================================
                        # COST CALCULATION
                        # ====================================
                        monthly_cost = self._calculate_vpn_connection_monthly_cost(region=region)

                        # ====================================
                        # OPTIMIZATION SCENARIOS
                        # ====================================
                        optimization_scenarios = self._calculate_vpn_connection_optimization(
                            vpn_id=vpn_id,
                            total_bytes_in_30d=total_bytes_in_30d,
                            total_bytes_out_30d=total_bytes_out_30d,
                            tunnel_1_state=tunnel_1_state,
                            tunnel_2_state=tunnel_2_state,
                            vpn_state=vpn_state,
                            tags=tags,
                            region=region,
                        )

                        # ====================================
                        # CHECK IF ORPHAN (Zero data transfer for 30+ days)
                        # ====================================
                        total_bytes_transferred = total_bytes_in_30d + total_bytes_out_30d
                        is_orphan = total_bytes_transferred == 0 and vpn_state == "available"

                        # ====================================
                        # CREATE RESOURCE DATA
                        # ====================================
                        resource = AllCloudResourceData(
                            resource_type="vpn_connection",
                            resource_id=vpn_id,
                            resource_name=tags.get("Name", vpn_id),
                            region=region,
                            estimated_monthly_cost=monthly_cost,
                            currency="USD",
                            resource_metadata={
                                "vpn_id": vpn_id,
                                "vpn_state": vpn_state,
                                "vpn_type": vpn_type,
                                "customer_gateway_id": customer_gateway_id,
                                "vpn_gateway_id": vpn_gateway_id,
                                "tunnel_1_state": tunnel_1_state,
                                "tunnel_2_state": tunnel_2_state,
                                "total_bytes_in_30d": int(total_bytes_in_30d),
                                "total_bytes_out_30d": int(total_bytes_out_30d),
                                "total_mb_transferred_30d": round((total_bytes_in_30d + total_bytes_out_30d) / (1024 ** 2), 2),
                                "tags": tags,
                            },
                            optimization_scenarios=optimization_scenarios,
                            is_orphan=is_orphan,
                            created_at_cloud=None,  # VPN connections don't have creation time in API
                        )

                        resources.append(resource)

                        logger.debug(
                            "vpn.connection_scanned",
                            vpn_id=vpn_id,
                            vpn_state=vpn_state,
                            monthly_cost=monthly_cost,
                            total_mb_transferred_30d=round((total_bytes_in_30d + total_bytes_out_30d) / (1024 ** 2), 2),
                            scenarios_count=len(optimization_scenarios),
                            is_orphan=is_orphan,
                        )

                    except Exception as e:
                        logger.error(
                            "vpn.connection_scan_failed",
                            vpn_id=vpn.get("VpnConnectionId", "unknown"),
                            region=region,
                            error=str(e),
                        )
                        continue

        except Exception as e:
            logger.error(
                "vpn.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # ============================================================
    # RESSOURCE 27 : TRANSIT GATEWAY ATTACHMENT
    # ============================================================

    def _calculate_transit_gateway_monthly_cost(
        self,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS Transit Gateway Attachment.

        Transit Gateway Attachments connect VPCs, VPNs, and on-premises networks
        to a central Transit Gateway for simplified network topology.

        Cost Components:
        1. Attachment cost ($36/month flat rate per attachment)
        2. Data processing cost (not included - varies by usage)

        Args:
            region: AWS region

        Returns:
            Estimated monthly cost in USD (flat rate)

        Example:
            Single TGW Attachment:
            - Attachment: $36/month (flat rate)
        """
        # Transit Gateway Attachment is a flat rate ($36/month)
        tgw_attachment_cost = self.PRICING.get("transit_gateway_attachment", 36.00)

        return round(tgw_attachment_cost, 2)

    def _calculate_transit_gateway_optimization(
        self,
        attachment_id: str,
        total_bytes_in_30d: float,
        total_bytes_out_30d: float,
        packet_drop_blackhole: float,
        attachment_state: str,
        resource_type: str,
        tags: dict,
        region: str,
    ) -> list[OptimizationScenario]:
        """
        Calculate optimization scenarios for AWS Transit Gateway Attachment.

        Transit Gateway Attachment Optimization Scenarios:
        1. Zero Data Transfer (CRITICAL) - No data transfer for 30+ days  Delete attachment
        2. Very Low Data Transfer (HIGH) - <100 MB/month  Delete or consolidate
        3. Blackhole Packet Drops (MEDIUM) - Route invalid  Fix routing tables
        4. Duplicate Attachment (MEDIUM) - Same VPC + TGW  Remove duplicates
        5. Dev/Test Attachment (LOW) - Dev/test attachment always on  Delete when not needed

        Args:
            attachment_id: Transit Gateway Attachment ID
            total_bytes_in_30d: Total incoming bytes over 30 days
            total_bytes_out_30d: Total outgoing bytes over 30 days
            packet_drop_blackhole: Packet drops due to blackhole routes over 7 days
            attachment_state: Attachment state (available, deleted, etc.)
            resource_type: Attachment resource type (vpc, vpn, etc.)
            tags: Resource tags
            region: AWS region

        Returns:
            List of optimization scenarios with priority, estimated savings, and recommendations
        """
        scenarios = []

        # Current cost (flat rate)
        current_cost = self._calculate_transit_gateway_monthly_cost(region=region)

        # Detect environment from tags
        env = tags.get("Environment", tags.get("environment", "")).lower()
        is_production = env in ["production", "prod", "prd"]

        # Calculate total data transfer
        total_bytes_transferred = total_bytes_in_30d + total_bytes_out_30d
        total_mb_transferred = total_bytes_transferred / (1024 ** 2)

        # =============================================
        # SCENARIO 1: Zero Data Transfer (CRITICAL)
        # =============================================
        if total_bytes_transferred == 0 and attachment_state == "available":
            # No data transfer for 30+ days  Delete attachment
            potential_savings = current_cost
            scenarios.append(
                OptimizationScenario(
                    scenario_name="tgw_zero_data_transfer",
                    priority="CRITICAL",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=0.0,
                    confidence_level="high",
                    description=(
                        f"Transit Gateway Attachment '{attachment_id}' has zero data transfer "
                        f"over the last 30 days. This attachment is completely idle and should be deleted."
                    ),
                    recommendation=(
                        "Delete this Transit Gateway Attachment to eliminate costs:\n"
                        "1. Verify no critical traffic is routed through this attachment\n"
                        "2. aws ec2 delete-transit-gateway-attachment --transit-gateway-attachment-id "
                        f"{attachment_id} --region {region}\n"
                        "3. Update route tables if necessary\n\n"
                        f" Monthly Savings: ${potential_savings:.2f}"
                    ),
                    optimization_score=95,
                )
            )

        # =============================================
        # SCENARIO 2: Very Low Data Transfer (HIGH)
        # =============================================
        elif 0 < total_mb_transferred < 100 and attachment_state == "available":
            # <100 MB/month  Delete or consolidate
            potential_savings = current_cost
            scenarios.append(
                OptimizationScenario(
                    scenario_name="tgw_low_data_transfer",
                    priority="HIGH",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=0.0,
                    confidence_level="high",
                    description=(
                        f"Transit Gateway Attachment '{attachment_id}' has very low data transfer "
                        f"({total_mb_transferred:.2f} MB over 30 days). Consider consolidating traffic "
                        f"through other attachments or deleting if not critical."
                    ),
                    recommendation=(
                        "Delete or consolidate this Transit Gateway Attachment:\n"
                        f"1. Current data transfer: {total_mb_transferred:.2f} MB/month\n"
                        "2. Consider consolidating traffic through other attachments\n"
                        "3. If not critical, delete to save costs\n"
                        "4. aws ec2 delete-transit-gateway-attachment --transit-gateway-attachment-id "
                        f"{attachment_id} --region {region}\n\n"
                        f" Monthly Savings: ${potential_savings:.2f}"
                    ),
                    optimization_score=85,
                )
            )

        # =============================================
        # SCENARIO 3: Blackhole Packet Drops (MEDIUM)
        # =============================================
        if packet_drop_blackhole > 1000:
            # Route invalid  Fix routing tables
            potential_savings = 0.0  # Opportunity cost
            scenarios.append(
                OptimizationScenario(
                    scenario_name="tgw_blackhole_packet_drops",
                    priority="MEDIUM",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=current_cost,
                    confidence_level="high",
                    description=(
                        f"Transit Gateway Attachment '{attachment_id}' has {int(packet_drop_blackhole)} "
                        f"blackhole packet drops over 7 days. This indicates invalid routing configuration. "
                        f"The attachment is configured but not functioning properly."
                    ),
                    recommendation=(
                        "Fix routing configuration for this Transit Gateway Attachment:\n"
                        f"1. Blackhole packet drops: {int(packet_drop_blackhole)} (last 7 days)\n"
                        "2. Check Transit Gateway route tables: aws ec2 describe-transit-gateway-route-tables\n"
                        "3. Verify VPC/VPN route tables for valid routes\n"
                        "4. Fix routes or delete attachment if not needed\n\n"
                        " No direct cost savings, but improves network reliability"
                    ),
                    optimization_score=70,
                )
            )

        # =============================================
        # SCENARIO 4: Duplicate Attachment (MEDIUM)
        # =============================================
        # Note: This scenario requires cross-attachment analysis, which is complex.
        # For simplicity, we'll check if there are tags indicating duplication.
        duplicate_tag = tags.get("Duplicate", tags.get("duplicate", "")).lower()
        if duplicate_tag in ["true", "yes", "1"]:
            # Same VPC + TGW  Remove duplicates
            potential_savings = current_cost
            scenarios.append(
                OptimizationScenario(
                    scenario_name="tgw_duplicate_attachment",
                    priority="MEDIUM",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=0.0,
                    confidence_level="medium",
                    description=(
                        f"Transit Gateway Attachment '{attachment_id}' appears to be a duplicate "
                        f"(based on tags). Multiple attachments for the same VPC/Transit Gateway "
                        f"can be consolidated."
                    ),
                    recommendation=(
                        "Remove duplicate Transit Gateway Attachment:\n"
                        "1. List all attachments: aws ec2 describe-transit-gateway-attachments\n"
                        "2. Identify duplicate attachments for same VPC/TGW\n"
                        "3. Delete duplicate: aws ec2 delete-transit-gateway-attachment "
                        f"--transit-gateway-attachment-id {attachment_id} --region {region}\n\n"
                        f" Monthly Savings: ${potential_savings:.2f}"
                    ),
                    optimization_score=65,
                )
            )

        # =============================================
        # SCENARIO 5: Dev/Test Attachment (LOW)
        # =============================================
        if not is_production and env in ["dev", "test", "staging", "development"]:
            # Dev/test attachment always on  Delete when not needed
            potential_savings = current_cost
            scenarios.append(
                OptimizationScenario(
                    scenario_name="tgw_dev_test_always_on",
                    priority="LOW",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=0.0,
                    confidence_level="medium",
                    description=(
                        f"Transit Gateway Attachment '{attachment_id}' is tagged as '{env}' environment "
                        f"but remains always on. Dev/test attachments should be deleted when not in use."
                    ),
                    recommendation=(
                        "Delete dev/test Transit Gateway Attachment when not needed:\n"
                        f"1. Environment: {env}\n"
                        "2. Delete during off-hours or non-testing periods\n"
                        "3. Recreate when needed (IaC: Terraform/CloudFormation)\n"
                        "4. aws ec2 delete-transit-gateway-attachment --transit-gateway-attachment-id "
                        f"{attachment_id} --region {region}\n\n"
                        f" Monthly Savings: ${potential_savings:.2f}"
                    ),
                    optimization_score=50,
                )
            )

        return scenarios

    async def scan_transit_gateway_attachments(
        self, region: str
    ) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS Transit Gateway Attachments for cost intelligence.

        Transit Gateway Attachments connect VPCs, VPNs, Direct Connect gateways,
        and peering connections to a central Transit Gateway hub.

        CloudWatch Metrics Used:
        - BytesIn (AWS/TransitGateway) - Incoming bytes over 30 days
        - BytesOut (AWS/TransitGateway) - Outgoing bytes over 30 days
        - PacketDropCountBlackhole (AWS/TransitGateway) - Packet drops due to invalid routes

        Cost Optimization Scenarios:
        1. Zero Data Transfer (CRITICAL) - No data transfer for 30+ days  Delete
        2. Very Low Data Transfer (HIGH) - <100 MB/month  Delete or consolidate
        3. Blackhole Packet Drops (MEDIUM) - Route invalid  Fix routing tables
        4. Duplicate Attachment (MEDIUM) - Same VPC + TGW  Remove duplicates
        5. Dev/Test Attachment (LOW) - Dev/test attachment always on  Delete when not needed

        Args:
            region: AWS region to scan

        Returns:
            List of all Transit Gateway Attachments with optimization recommendations
        """
        import structlog

        logger = structlog.get_logger()
        resources = []

        try:
            async with self.provider.session.client("ec2", region_name=region) as ec2:
                async with self.provider.session.client(
                    "cloudwatch", region_name=region
                ) as cw:
                    # Describe all Transit Gateway attachments
                    response = await ec2.describe_transit_gateway_attachments()

                    for attachment in response.get("TransitGatewayAttachments", []):
                        try:
                            attachment_id = attachment["TransitGatewayAttachmentId"]
                            attachment_state = attachment.get("State", "unknown")
                            resource_type = attachment.get("ResourceType", "unknown")

                            # Skip deleted/deleting/failed attachments
                            if attachment_state in ["deleted", "deleting", "failed"]:
                                continue

                            # Extract tags
                            tags = {}
                            for tag in attachment.get("Tags", []):
                                tags[tag["Key"]] = tag["Value"]

                            # ====================================
                            # CLOUDWATCH METRICS (30-day lookback)
                            # ====================================
                            now = datetime.now(timezone.utc)
                            start_time = now - timedelta(days=30)

                            # BytesIn metric (30 days)
                            bytes_in_response = await cw.get_metric_statistics(
                                Namespace="AWS/TransitGateway",
                                MetricName="BytesIn",
                                Dimensions=[
                                    {
                                        "Name": "TransitGatewayAttachment",
                                        "Value": attachment_id,
                                    },
                                ],
                                StartTime=start_time,
                                EndTime=now,
                                Period=86400,  # 1 day
                                Statistics=["Sum"],
                            )
                            total_bytes_in_30d = sum(
                                dp["Sum"]
                                for dp in bytes_in_response.get("Datapoints", [])
                            )

                            # BytesOut metric (30 days)
                            bytes_out_response = await cw.get_metric_statistics(
                                Namespace="AWS/TransitGateway",
                                MetricName="BytesOut",
                                Dimensions=[
                                    {
                                        "Name": "TransitGatewayAttachment",
                                        "Value": attachment_id,
                                    },
                                ],
                                StartTime=start_time,
                                EndTime=now,
                                Period=86400,  # 1 day
                                Statistics=["Sum"],
                            )
                            total_bytes_out_30d = sum(
                                dp["Sum"]
                                for dp in bytes_out_response.get("Datapoints", [])
                            )

                            # PacketDropCountBlackhole metric (7 days)
                            start_time_7d = now - timedelta(days=7)
                            blackhole_response = await cw.get_metric_statistics(
                                Namespace="AWS/TransitGateway",
                                MetricName="PacketDropCountBlackhole",
                                Dimensions=[
                                    {
                                        "Name": "TransitGatewayAttachment",
                                        "Value": attachment_id,
                                    },
                                ],
                                StartTime=start_time_7d,
                                EndTime=now,
                                Period=3600,  # 1 hour
                                Statistics=["Sum"],
                            )
                            packet_drop_blackhole = sum(
                                dp["Sum"]
                                for dp in blackhole_response.get("Datapoints", [])
                            )

                            # ====================================
                            # COST CALCULATION
                            # ====================================
                            monthly_cost = self._calculate_transit_gateway_monthly_cost(
                                region=region
                            )

                            # ====================================
                            # OPTIMIZATION SCENARIOS
                            # ====================================
                            optimization_scenarios = (
                                self._calculate_transit_gateway_optimization(
                                    attachment_id=attachment_id,
                                    total_bytes_in_30d=total_bytes_in_30d,
                                    total_bytes_out_30d=total_bytes_out_30d,
                                    packet_drop_blackhole=packet_drop_blackhole,
                                    attachment_state=attachment_state,
                                    resource_type=resource_type,
                                    tags=tags,
                                    region=region,
                                )
                            )

                            # ====================================
                            # ORPHAN DETECTION
                            # ====================================
                            total_bytes_transferred = (
                                total_bytes_in_30d + total_bytes_out_30d
                            )
                            is_orphan = (
                                total_bytes_transferred == 0
                                and attachment_state == "available"
                            )

                            # Apply detection rules filtering
                            creation_time = attachment.get("CreationTime")
                            resource_age_days = None
                            if isinstance(creation_time, datetime):
                                resource_age_days = (datetime.utcnow() - creation_time.replace(tzinfo=None)).days

                            if not self._should_include_resource("transit_gateway_attachment", resource_age_days):
                                logger.debug("inventory.transit_gateway_filtered", attachment_id=attachment_id, age=resource_age_days)
                                continue

                            # ====================================
                            # CREATE RESOURCE DATA
                            # ====================================
                            resource = AllCloudResourceData(
                                resource_type="transit_gateway_attachment",
                                resource_id=attachment_id,
                                resource_name=tags.get("Name", attachment_id),
                                region=region,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata={
                                    "attachment_id": attachment_id,
                                    "attachment_state": attachment_state,
                                    "resource_type": resource_type,
                                    "transit_gateway_id": attachment.get(
                                        "TransitGatewayId", "unknown"
                                    ),
                                    "resource_id": attachment.get(
                                        "ResourceId", "unknown"
                                    ),
                                    "resource_owner_id": attachment.get(
                                        "ResourceOwnerId", "unknown"
                                    ),
                                    "total_bytes_in_30d": int(total_bytes_in_30d),
                                    "total_bytes_out_30d": int(total_bytes_out_30d),
                                    "total_mb_transferred_30d": round(
                                        (total_bytes_in_30d + total_bytes_out_30d)
                                        / (1024**2),
                                        2,
                                    ),
                                    "packet_drop_blackhole_7d": int(
                                        packet_drop_blackhole
                                    ),
                                    "tags": tags,
                                },
                                optimization_scenarios=optimization_scenarios,
                                is_orphan=is_orphan,
                                created_at_cloud=attachment.get("CreationTime"),
                            )

                            resources.append(resource)

                            logger.debug(
                                "tgw.attachment_scanned",
                                attachment_id=attachment_id,
                                attachment_state=attachment_state,
                                monthly_cost=monthly_cost,
                                total_mb_transferred_30d=round(
                                    (total_bytes_in_30d + total_bytes_out_30d)
                                    / (1024**2),
                                    2,
                                ),
                                scenarios_count=len(optimization_scenarios),
                                is_orphan=is_orphan,
                            )

                        except Exception as e:
                            logger.error(
                                "tgw.attachment_scan_failed",
                                attachment_id=attachment.get(
                                    "TransitGatewayAttachmentId", "unknown"
                                ),
                                region=region,
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "tgw.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # ============================================================
    # RESSOURCE 28 : GLOBAL ACCELERATOR (CDN)
    # ============================================================

    def _calculate_global_accelerator_monthly_cost(
        self,
        region: str = "global",
    ) -> float:
        """
        Calculate estimated monthly cost for AWS Global Accelerator.

        Global Accelerator improves application availability and performance by routing traffic
        through the AWS global network to optimal endpoints.

        Cost Components:
        1. Fixed fee ($18/month per accelerator)
        2. Data transfer cost (not included - $0.025/GB varies by usage)

        Args:
            region: AWS region (always "global" for Global Accelerator)

        Returns:
            Estimated monthly cost in USD (fixed fee only)

        Example:
            Single Global Accelerator:
            - Fixed fee: $18/month
            - Data transfer: Variable (not included in base cost)
        """
        # Global Accelerator fixed fee ($18/month)
        ga_cost = self.PRICING.get("global_accelerator", 18.00)

        return round(ga_cost, 2)

    def _calculate_global_accelerator_optimization(
        self,
        accelerator_arn: str,
        accelerator_name: str,
        endpoint_count: int,
        total_bytes_in_30d: float,
        active_flow_count_7d: float,
        enabled: bool,
        tags: dict,
        region: str = "global",
    ) -> list[OptimizationScenario]:
        """
        Calculate optimization scenarios for AWS Global Accelerator.

        Global Accelerator Optimization Scenarios:
        1. Zero Endpoints Configured (CRITICAL) - No endpoints  Delete accelerator
        2. Zero Traffic (HIGH) - No traffic for 30+ days  Delete accelerator
        3. Very Low Traffic (MEDIUM) - <1 GB/month  Use CloudFront instead
        4. No Active Flows (MEDIUM) - No active connections for 7+ days  Delete
        5. Disabled Accelerator (LOW) - Accelerator disabled but still charged  Delete

        Args:
            accelerator_arn: Global Accelerator ARN
            accelerator_name: Accelerator name
            endpoint_count: Number of endpoints configured
            total_bytes_in_30d: Total processed bytes over 30 days
            active_flow_count_7d: Active flow count over 7 days
            enabled: Whether accelerator is enabled
            tags: Resource tags
            region: AWS region (always "global")

        Returns:
            List of optimization scenarios with priority, estimated savings, and recommendations
        """
        scenarios = []

        # Current cost (fixed fee only, data transfer not included)
        current_cost = self._calculate_global_accelerator_monthly_cost(region=region)

        # Detect environment from tags
        env = tags.get("Environment", tags.get("environment", "")).lower()
        is_production = env in ["production", "prod", "prd"]

        # Calculate data transfer in GB
        total_gb_transferred = total_bytes_in_30d / (1024**3)

        # =============================================
        # SCENARIO 1: Zero Endpoints Configured (CRITICAL)
        # =============================================
        if endpoint_count == 0:
            # No endpoints  Delete accelerator
            potential_savings = current_cost
            scenarios.append(
                OptimizationScenario(
                    scenario_name="ga_zero_endpoints",
                    priority="CRITICAL",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=0.0,
                    confidence_level="high",
                    description=(
                        f"Global Accelerator '{accelerator_name}' has zero endpoints configured. "
                        f"Without endpoints, the accelerator serves no purpose and should be deleted."
                    ),
                    recommendation=(
                        "Delete Global Accelerator with no endpoints:\n"
                        "1. Verify no endpoints are configured: aws globalaccelerator list-accelerators\n"
                        f"2. Delete accelerator: aws globalaccelerator delete-accelerator --accelerator-arn {accelerator_arn}\n"
                        "3. Wait for deletion to complete (can take a few minutes)\n\n"
                        f" Monthly Savings: ${potential_savings:.2f}"
                    ),
                    optimization_score=95,
                )
            )

        # =============================================
        # SCENARIO 2: Zero Traffic (HIGH)
        # =============================================
        elif total_bytes_in_30d == 0 and endpoint_count > 0:
            # No traffic for 30+ days  Delete accelerator
            potential_savings = current_cost
            scenarios.append(
                OptimizationScenario(
                    scenario_name="ga_zero_traffic",
                    priority="HIGH",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=0.0,
                    confidence_level="high",
                    description=(
                        f"Global Accelerator '{accelerator_name}' has {endpoint_count} endpoint(s) "
                        f"but zero traffic over the last 30 days. The accelerator is not being used."
                    ),
                    recommendation=(
                        "Delete Global Accelerator with zero traffic:\n"
                        "1. Verify no traffic: Check CloudWatch ProcessedBytesIn metric\n"
                        f"2. Delete accelerator: aws globalaccelerator delete-accelerator --accelerator-arn {accelerator_arn}\n"
                        "3. Remove DNS records pointing to accelerator\n\n"
                        f" Monthly Savings: ${potential_savings:.2f}"
                    ),
                    optimization_score=85,
                )
            )

        # =============================================
        # SCENARIO 3: Very Low Traffic (MEDIUM)
        # =============================================
        elif 0 < total_gb_transferred < 1:
            # <1 GB/month  Use CloudFront instead
            potential_savings = current_cost  # CloudFront has no fixed fee
            scenarios.append(
                OptimizationScenario(
                    scenario_name="ga_low_traffic",
                    priority="MEDIUM",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=0.0,
                    confidence_level="medium",
                    description=(
                        f"Global Accelerator '{accelerator_name}' has very low traffic "
                        f"({total_gb_transferred:.2f} GB over 30 days). For low traffic volumes, "
                        f"CloudFront is more cost-effective (no fixed fee, $0.085/GB)."
                    ),
                    recommendation=(
                        "Migrate from Global Accelerator to CloudFront for low traffic:\n"
                        f"1. Current traffic: {total_gb_transferred:.2f} GB/month\n"
                        f"2. Global Accelerator cost: ${current_cost}/month + ${total_gb_transferred * 0.025:.2f} (data transfer)\n"
                        f"3. CloudFront cost: ${total_gb_transferred * 0.085:.2f} (no fixed fee)\n"
                        "4. Migrate to CloudFront distribution\n"
                        f"5. Delete accelerator: aws globalaccelerator delete-accelerator --accelerator-arn {accelerator_arn}\n\n"
                        f" Monthly Savings: ${potential_savings:.2f} (fixed fee)"
                    ),
                    optimization_score=70,
                )
            )

        # =============================================
        # SCENARIO 4: No Active Flows (MEDIUM)
        # =============================================
        if active_flow_count_7d == 0 and endpoint_count > 0:
            # No active connections for 7+ days  Delete
            potential_savings = current_cost
            scenarios.append(
                OptimizationScenario(
                    scenario_name="ga_no_active_flows",
                    priority="MEDIUM",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=0.0,
                    confidence_level="high",
                    description=(
                        f"Global Accelerator '{accelerator_name}' has no active flows over the last 7 days. "
                        f"No clients are actively using the accelerator."
                    ),
                    recommendation=(
                        "Delete Global Accelerator with no active connections:\n"
                        "1. Verify no active flows: Check CloudWatch ActiveFlowCount metric\n"
                        f"2. Delete accelerator: aws globalaccelerator delete-accelerator --accelerator-arn {accelerator_arn}\n"
                        "3. Update DNS records if necessary\n\n"
                        f" Monthly Savings: ${potential_savings:.2f}"
                    ),
                    optimization_score=65,
                )
            )

        # =============================================
        # SCENARIO 5: Disabled Accelerator (LOW)
        # =============================================
        if not enabled:
            # Accelerator disabled but still charged  Delete
            potential_savings = current_cost
            scenarios.append(
                OptimizationScenario(
                    scenario_name="ga_disabled_accelerator",
                    priority="LOW",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=0.0,
                    confidence_level="high",
                    description=(
                        f"Global Accelerator '{accelerator_name}' is disabled but still incurs charges "
                        f"(${current_cost}/month). Disabled accelerators should be deleted, not just disabled."
                    ),
                    recommendation=(
                        "Delete disabled Global Accelerator (still charged):\n"
                        "1. Accelerator is disabled but still costs money\n"
                        f"2. Delete accelerator: aws globalaccelerator delete-accelerator --accelerator-arn {accelerator_arn}\n"
                        "3. Recreate when needed if necessary\n\n"
                        f" Monthly Savings: ${potential_savings:.2f}"
                    ),
                    optimization_score=50,
                )
            )

        return scenarios

    async def scan_global_accelerators(
        self, region: str
    ) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS Global Accelerators for cost intelligence.

        Global Accelerator is a global service that improves application availability
        and performance by routing traffic through AWS's global network.

        IMPORTANT: Global Accelerator is a global service. This scan should only
        be run in us-west-2 region to avoid duplicate results.

        CloudWatch Metrics Used:
        - ProcessedBytesIn (AWS/GlobalAccelerator) - Incoming bytes over 30 days
        - ActiveFlowCount (AWS/GlobalAccelerator) - Active connections over 7 days

        Cost Optimization Scenarios:
        1. Zero Endpoints Configured (CRITICAL) - No endpoints  Delete accelerator
        2. Zero Traffic (HIGH) - No traffic for 30+ days  Delete accelerator
        3. Very Low Traffic (MEDIUM) - <1 GB/month  Use CloudFront instead
        4. No Active Flows (MEDIUM) - No active connections for 7+ days  Delete
        5. Disabled Accelerator (LOW) - Accelerator disabled but still charged  Delete

        Args:
            region: AWS region to scan (must be us-west-2 for Global Accelerator)

        Returns:
            List of all Global Accelerators with optimization recommendations
        """
        import structlog

        logger = structlog.get_logger()
        resources = []

        # Global Accelerator is a global service, only check in us-west-2
        if region != "us-west-2":
            logger.debug("ga.skip_region", region=region, reason="Global Accelerator is only scanned in us-west-2")
            return resources

        try:
            async with self.provider.session.client(
                "globalaccelerator", region_name="us-west-2"
            ) as ga:
                async with self.provider.session.client(
                    "cloudwatch", region_name="us-west-2"
                ) as cw:
                    # List all Global Accelerators
                    response = await ga.list_accelerators()

                    for accelerator in response.get("Accelerators", []):
                        try:
                            accelerator_arn = accelerator["AcceleratorArn"]
                            accelerator_name = accelerator.get(
                                "Name", accelerator_arn.split("/")[-1]
                            )
                            enabled = accelerator.get("Enabled", False)
                            status = accelerator.get("Status", "unknown")

                            # Skip deleted accelerators
                            if status == "IN_PROGRESS":
                                continue

                            # Count endpoints
                            endpoint_count = 0
                            try:
                                listeners_response = await ga.list_listeners(
                                    AcceleratorArn=accelerator_arn
                                )
                                for listener in listeners_response.get("Listeners", []):
                                    listener_arn = listener["ListenerArn"]
                                    endpoint_groups = await ga.list_endpoint_groups(
                                        ListenerArn=listener_arn
                                    )
                                    for eg in endpoint_groups.get("EndpointGroups", []):
                                        endpoint_count += len(
                                            eg.get("EndpointDescriptions", [])
                                        )
                            except Exception as e:
                                logger.warning(
                                    "ga.endpoint_count_failed",
                                    accelerator_arn=accelerator_arn,
                                    error=str(e),
                                )

                            # Extract tags
                            tags = {}
                            try:
                                tags_response = await ga.list_tags_for_resource(
                                    ResourceArn=accelerator_arn
                                )
                                for tag in tags_response.get("Tags", []):
                                    tags[tag["Key"]] = tag["Value"]
                            except Exception as e:
                                logger.warning(
                                    "ga.tags_failed",
                                    accelerator_arn=accelerator_arn,
                                    error=str(e),
                                )

                            # ====================================
                            # CLOUDWATCH METRICS
                            # ====================================
                            now = datetime.now(timezone.utc)

                            # ProcessedBytesIn metric (30 days)
                            start_time_30d = now - timedelta(days=30)
                            try:
                                bytes_in_response = await cw.get_metric_statistics(
                                    Namespace="AWS/GlobalAccelerator",
                                    MetricName="ProcessedBytesIn",
                                    Dimensions=[
                                        {"Name": "Accelerator", "Value": accelerator_arn},
                                    ],
                                    StartTime=start_time_30d,
                                    EndTime=now,
                                    Period=86400,  # 1 day
                                    Statistics=["Sum"],
                                )
                                total_bytes_in_30d = sum(
                                    dp["Sum"]
                                    for dp in bytes_in_response.get("Datapoints", [])
                                )
                            except Exception as e:
                                logger.warning(
                                    "ga.bytes_in_metric_failed",
                                    accelerator_arn=accelerator_arn,
                                    error=str(e),
                                )
                                total_bytes_in_30d = 0

                            # ActiveFlowCount metric (7 days)
                            start_time_7d = now - timedelta(days=7)
                            try:
                                flow_count_response = await cw.get_metric_statistics(
                                    Namespace="AWS/GlobalAccelerator",
                                    MetricName="ActiveFlowCount",
                                    Dimensions=[
                                        {"Name": "Accelerator", "Value": accelerator_arn},
                                    ],
                                    StartTime=start_time_7d,
                                    EndTime=now,
                                    Period=3600,  # 1 hour
                                    Statistics=["Average"],
                                )
                                active_flow_count_7d = sum(
                                    dp["Average"]
                                    for dp in flow_count_response.get("Datapoints", [])
                                )
                            except Exception as e:
                                logger.warning(
                                    "ga.flow_count_metric_failed",
                                    accelerator_arn=accelerator_arn,
                                    error=str(e),
                                )
                                active_flow_count_7d = 0

                            # ====================================
                            # COST CALCULATION
                            # ====================================
                            monthly_cost = self._calculate_global_accelerator_monthly_cost(
                                region="global"
                            )

                            # ====================================
                            # OPTIMIZATION SCENARIOS
                            # ====================================
                            optimization_scenarios = (
                                self._calculate_global_accelerator_optimization(
                                    accelerator_arn=accelerator_arn,
                                    accelerator_name=accelerator_name,
                                    endpoint_count=endpoint_count,
                                    total_bytes_in_30d=total_bytes_in_30d,
                                    active_flow_count_7d=active_flow_count_7d,
                                    enabled=enabled,
                                    tags=tags,
                                    region="global",
                                )
                            )

                            # ====================================
                            # ORPHAN DETECTION
                            # ====================================
                            is_orphan = (
                                endpoint_count == 0
                                or (total_bytes_in_30d == 0 and endpoint_count > 0)
                            )

                            # Apply detection rules filtering
                            created_time = accelerator.get("CreatedTime")
                            resource_age_days = None
                            if isinstance(created_time, datetime):
                                resource_age_days = (datetime.utcnow() - created_time.replace(tzinfo=None)).days

                            if not self._should_include_resource("global_accelerator", resource_age_days):
                                logger.debug("inventory.global_accelerator_filtered", accelerator_arn=accelerator_arn, age=resource_age_days)
                                continue

                            # ====================================
                            # CREATE RESOURCE DATA
                            # ====================================
                            resource = AllCloudResourceData(
                                resource_type="global_accelerator",
                                resource_id=accelerator_arn,
                                resource_name=accelerator_name,
                                region="global",
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata={
                                    "accelerator_arn": accelerator_arn,
                                    "accelerator_name": accelerator_name,
                                    "enabled": enabled,
                                    "status": status,
                                    "endpoint_count": endpoint_count,
                                    "ip_addresses": accelerator.get("IpSets", []),
                                    "dns_name": accelerator.get("DnsName", ""),
                                    "total_bytes_in_30d": int(total_bytes_in_30d),
                                    "total_gb_transferred_30d": round(
                                        total_bytes_in_30d / (1024**3), 2
                                    ),
                                    "active_flow_count_7d": round(active_flow_count_7d, 2),
                                    "tags": tags,
                                },
                                optimization_scenarios=optimization_scenarios,
                                is_orphan=is_orphan,
                                created_at_cloud=accelerator.get("CreatedTime"),
                            )

                            resources.append(resource)

                            logger.debug(
                                "ga.accelerator_scanned",
                                accelerator_name=accelerator_name,
                                enabled=enabled,
                                endpoint_count=endpoint_count,
                                monthly_cost=monthly_cost,
                                total_gb_transferred_30d=round(
                                    total_bytes_in_30d / (1024**3), 2
                                ),
                                scenarios_count=len(optimization_scenarios),
                                is_orphan=is_orphan,
                            )

                        except Exception as e:
                            logger.error(
                                "ga.accelerator_scan_failed",
                                accelerator_arn=accelerator.get("AcceleratorArn", "unknown"),
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "ga.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # ============================================================
    # RESSOURCE 29 : DOCUMENTDB CLUSTER (MONGODB-COMPATIBLE DATABASE)
    # ============================================================

    def _calculate_documentdb_monthly_cost(
        self,
        instance_count: int,
        instance_class: str,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS DocumentDB Cluster.

        DocumentDB is a fully managed MongoDB-compatible database service
        with support for multi-instance clusters.

        Cost Components:
        1. Instance cost (per instance per hour)
        2. Storage cost (billed separately, not included here)
        3. I/O cost (billed separately, not included here)

        Args:
            instance_count: Number of instances in the cluster
            instance_class: Instance class (e.g., db.r5.large)
            region: AWS region

        Returns:
            Estimated monthly cost in USD (instances only)

        Example:
            Single db.r5.large instance:
            - Instance: $0.277/hour  730 hours = ~$199/month
        """
        monthly_hours = 730

        # Instance pricing (only db.r5.large for MVP, default for others)
        instance_hourly_rate = self.PRICING.get("documentdb_r5_large", 0.277)

        # Calculate total cost for all instances
        total_cost = instance_hourly_rate * monthly_hours * instance_count

        return round(total_cost, 2)

    def _calculate_documentdb_optimization(
        self,
        cluster_id: str,
        instance_count: int,
        instance_class: str,
        avg_connections_7d: float,
        avg_cpu_7d: float,
        cluster_status: str,
        tags: dict,
        region: str,
    ) -> list[OptimizationScenario]:
        """
        Calculate optimization scenarios for AWS DocumentDB Cluster.

        DocumentDB Cluster Optimization Scenarios:
        1. Zero Connections (CRITICAL) - No connections for 7+ days  Delete cluster
        2. Very Low Connections + Multi-Instance (HIGH) - <5 connections/day + >1 instance  Reduce instances
        3. Low CPU + Multi-Instance (MEDIUM) - <20% CPU + >1 instance  Reduce instances
        4. Stopped Cluster (MEDIUM) - Status = stopped + age > 7 days  Delete or snapshot
        5. Oversized Instance (LOW) - Dev/test with production instance  Downsize

        Args:
            cluster_id: DocumentDB cluster identifier
            instance_count: Number of instances in the cluster
            instance_class: Instance class (e.g., db.r5.large)
            avg_connections_7d: Average database connections over 7 days
            avg_cpu_7d: Average CPU utilization over 7 days
            cluster_status: Cluster status (available, stopped, etc.)
            tags: Resource tags
            region: AWS region

        Returns:
            List of optimization scenarios with priority, estimated savings, and recommendations
        """
        scenarios = []

        # Current cost (per instance)
        current_cost = self._calculate_documentdb_monthly_cost(
            instance_count=instance_count,
            instance_class=instance_class,
            region=region,
        )

        # Cost per instance
        cost_per_instance = current_cost / max(instance_count, 1)

        # Detect environment from tags
        env = tags.get("Environment", tags.get("environment", "")).lower()
        is_production = env in ["production", "prod", "prd"]

        # =============================================
        # SCENARIO 1: Zero Connections (CRITICAL)
        # =============================================
        if avg_connections_7d == 0 and cluster_status == "available":
            # No connections for 7+ days  Delete cluster
            potential_savings = current_cost
            scenarios.append(
                OptimizationScenario(
                    scenario_name="documentdb_zero_connections",
                    priority="CRITICAL",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=0.0,
                    confidence_level="high",
                    description=(
                        f"DocumentDB Cluster '{cluster_id}' has zero database connections "
                        f"over the last 7 days ({instance_count} instance(s), ${current_cost}/month). "
                        f"The cluster is completely idle and should be deleted."
                    ),
                    recommendation=(
                        "Delete DocumentDB Cluster with zero connections:\n"
                        "1. Verify no applications are using this cluster\n"
                        f"2. Create final snapshot (optional): aws docdb create-db-cluster-snapshot --db-cluster-identifier {cluster_id} --db-cluster-snapshot-identifier final-snapshot-{cluster_id}\n"
                        f"3. Delete cluster: aws docdb delete-db-cluster --db-cluster-identifier {cluster_id} --skip-final-snapshot --region {region}\n\n"
                        f" Monthly Savings: ${potential_savings:.2f}"
                    ),
                    optimization_score=95,
                )
            )

        # =============================================
        # SCENARIO 2: Very Low Connections + Multi-Instance (HIGH)
        # =============================================
        elif avg_connections_7d < 5 and instance_count > 1 and cluster_status == "available":
            # <5 connections/day + >1 instance  Reduce to 1 instance
            instances_to_remove = instance_count - 1
            potential_savings = cost_per_instance * instances_to_remove
            scenarios.append(
                OptimizationScenario(
                    scenario_name="documentdb_low_connections_multi_instance",
                    priority="HIGH",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=cost_per_instance,
                    confidence_level="high",
                    description=(
                        f"DocumentDB Cluster '{cluster_id}' has very low connection volume "
                        f"({avg_connections_7d:.1f} connections over 7 days) but runs {instance_count} instances. "
                        f"Reduce to 1 instance for dev/test workloads."
                    ),
                    recommendation=(
                        "Reduce DocumentDB Cluster to 1 instance:\n"
                        f"1. Current: {instance_count} instances, avg {avg_connections_7d:.1f} connections\n"
                        f"2. Delete {instances_to_remove} instance(s): aws docdb delete-db-instance --db-instance-identifier <instance-id> --region {region}\n"
                        "3. Keep only primary instance for low workloads\n\n"
                        f" Monthly Savings: ${potential_savings:.2f} ({instances_to_remove} instance(s))"
                    ),
                    optimization_score=85,
                )
            )

        # =============================================
        # SCENARIO 3: Low CPU + Multi-Instance (MEDIUM)
        # =============================================
        if avg_cpu_7d < 20 and instance_count > 1 and cluster_status == "available":
            # <20% CPU + >1 instance  Reduce instances or downsize
            instances_to_remove = instance_count - 1
            potential_savings = cost_per_instance * instances_to_remove
            scenarios.append(
                OptimizationScenario(
                    scenario_name="documentdb_low_cpu_multi_instance",
                    priority="MEDIUM",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=cost_per_instance,
                    confidence_level="medium",
                    description=(
                        f"DocumentDB Cluster '{cluster_id}' has low CPU utilization "
                        f"({avg_cpu_7d:.1f}% over 7 days) with {instance_count} instances. "
                        f"The cluster is over-provisioned and can be scaled down."
                    ),
                    recommendation=(
                        "Reduce DocumentDB Cluster instances or downsize:\n"
                        f"1. Current: {instance_count} instances, avg {avg_cpu_7d:.1f}% CPU\n"
                        f"2. Option A: Delete {instances_to_remove} instance(s) (save ${potential_savings:.2f}/month)\n"
                        f"3. Option B: Downsize instance class (e.g., {instance_class}  db.t3.medium)\n"
                        "4. Monitor performance after change\n\n"
                        f" Monthly Savings: ${potential_savings:.2f} (remove instances)"
                    ),
                    optimization_score=70,
                )
            )

        # =============================================
        # SCENARIO 4: Stopped Cluster (MEDIUM)
        # =============================================
        if cluster_status == "stopped":
            # Status = stopped  Delete or snapshot
            potential_savings = current_cost  # Stopped clusters don't incur instance costs, but storage still costs
            scenarios.append(
                OptimizationScenario(
                    scenario_name="documentdb_stopped_cluster",
                    priority="MEDIUM",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=0.0,
                    confidence_level="high",
                    description=(
                        f"DocumentDB Cluster '{cluster_id}' is stopped ({instance_count} instance(s)). "
                        f"Stopped clusters still incur storage costs. Consider creating a snapshot and deleting."
                    ),
                    recommendation=(
                        "Delete stopped DocumentDB Cluster (storage still charged):\n"
                        "1. Cluster is stopped but storage is still billed\n"
                        f"2. Create final snapshot: aws docdb create-db-cluster-snapshot --db-cluster-identifier {cluster_id} --db-cluster-snapshot-identifier final-snapshot-{cluster_id}\n"
                        f"3. Delete cluster: aws docdb delete-db-cluster --db-cluster-identifier {cluster_id} --skip-final-snapshot --region {region}\n"
                        "4. Restore from snapshot when needed\n\n"
                        f" Monthly Savings: ${potential_savings:.2f} (instance cost) + storage cost"
                    ),
                    optimization_score=65,
                )
            )

        # =============================================
        # SCENARIO 5: Oversized Instance (LOW)
        # =============================================
        if not is_production and env in ["dev", "test", "staging", "development"]:
            # Dev/test with production instance  Downsize
            # Estimate 50% savings by downsizing to db.t3.medium
            potential_savings = current_cost * 0.5
            scenarios.append(
                OptimizationScenario(
                    scenario_name="documentdb_oversized_instance_dev",
                    priority="LOW",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=current_cost * 0.5,
                    confidence_level="medium",
                    description=(
                        f"DocumentDB Cluster '{cluster_id}' is tagged as '{env}' environment "
                        f"but uses production-grade instance class '{instance_class}'. "
                        f"Dev/test environments should use smaller instance types."
                    ),
                    recommendation=(
                        "Downsize DocumentDB Cluster for dev/test:\n"
                        f"1. Environment: {env}\n"
                        f"2. Current: {instance_class} ({instance_count} instance(s))\n"
                        "3. Recommended: db.t3.medium or db.t4g.medium for dev/test\n"
                        f"4. Modify instance class: aws docdb modify-db-instance --db-instance-identifier <instance-id> --db-instance-class db.t3.medium --apply-immediately --region {region}\n\n"
                        f" Monthly Savings: ~${potential_savings:.2f} (50% reduction)"
                    ),
                    optimization_score=50,
                )
            )

        return scenarios

    async def scan_documentdb_clusters(
        self, region: str
    ) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS DocumentDB Clusters for cost intelligence.

        DocumentDB is a fully managed MongoDB-compatible database service
        designed for JSON data storage and querying.

        CloudWatch Metrics Used:
        - DatabaseConnections (AWS/DocDB) - Average connections over 7 days
        - CPUUtilization (AWS/DocDB) - Average CPU % over 7 days
        - FreeableMemory (AWS/DocDB) - Available memory

        Cost Optimization Scenarios:
        1. Zero Connections (CRITICAL) - No connections for 7+ days  Delete cluster
        2. Very Low Connections + Multi-Instance (HIGH) - <5 connections/day + >1 instance  Reduce instances
        3. Low CPU + Multi-Instance (MEDIUM) - <20% CPU + >1 instance  Reduce instances
        4. Stopped Cluster (MEDIUM) - Status = stopped + age > 7 days  Delete or snapshot
        5. Oversized Instance (LOW) - Dev/test with production instance  Downsize

        Args:
            region: AWS region to scan

        Returns:
            List of all DocumentDB Clusters with optimization recommendations
        """
        import structlog

        logger = structlog.get_logger()
        resources = []

        try:
            async with self.provider.session.client("docdb", region_name=region) as docdb:
                async with self.provider.session.client(
                    "cloudwatch", region_name=region
                ) as cw:
                    # Describe all DocumentDB clusters
                    response = await docdb.describe_db_clusters()

                    for cluster in response.get("DBClusters", []):
                        try:
                            cluster_id = cluster["DBClusterIdentifier"]
                            cluster_status = cluster.get("Status", "unknown")
                            engine = cluster.get("Engine", "docdb")
                            engine_version = cluster.get("EngineVersion", "unknown")

                            # Get cluster members (instances)
                            cluster_members = cluster.get("DBClusterMembers", [])
                            instance_count = len(cluster_members)

                            # Get instance class (from first instance)
                            instance_class = "db.r5.large"  # Default
                            if instance_count > 0:
                                try:
                                    first_instance_id = cluster_members[0].get(
                                        "DBInstanceIdentifier"
                                    )
                                    if first_instance_id:
                                        instances_response = (
                                            await docdb.describe_db_instances(
                                                DBInstanceIdentifier=first_instance_id
                                            )
                                        )
                                        if instances_response.get("DBInstances"):
                                            instance_class = instances_response[
                                                "DBInstances"
                                            ][0].get("DBInstanceClass", "db.r5.large")
                                except Exception as e:
                                    logger.warning(
                                        "documentdb.instance_class_failed",
                                        cluster_id=cluster_id,
                                        error=str(e),
                                    )

                            # Extract tags
                            tags = {}
                            for tag in cluster.get("TagList", []):
                                tags[tag["Key"]] = tag["Value"]

                            # ====================================
                            # CLOUDWATCH METRICS (7-day lookback)
                            # ====================================
                            now = datetime.now(timezone.utc)
                            start_time = now - timedelta(days=7)

                            # DatabaseConnections metric (7 days)
                            try:
                                connections_response = await cw.get_metric_statistics(
                                    Namespace="AWS/DocDB",
                                    MetricName="DatabaseConnections",
                                    Dimensions=[
                                        {"Name": "DBClusterIdentifier", "Value": cluster_id},
                                    ],
                                    StartTime=start_time,
                                    EndTime=now,
                                    Period=3600,  # 1 hour
                                    Statistics=["Average"],
                                )
                                datapoints = connections_response.get("Datapoints", [])
                                avg_connections_7d = (
                                    sum(dp["Average"] for dp in datapoints) / len(datapoints)
                                    if datapoints
                                    else 0
                                )
                            except Exception as e:
                                logger.warning(
                                    "documentdb.connections_metric_failed",
                                    cluster_id=cluster_id,
                                    error=str(e),
                                )
                                avg_connections_7d = 0

                            # CPUUtilization metric (7 days)
                            try:
                                cpu_response = await cw.get_metric_statistics(
                                    Namespace="AWS/DocDB",
                                    MetricName="CPUUtilization",
                                    Dimensions=[
                                        {"Name": "DBClusterIdentifier", "Value": cluster_id},
                                    ],
                                    StartTime=start_time,
                                    EndTime=now,
                                    Period=3600,  # 1 hour
                                    Statistics=["Average"],
                                )
                                datapoints = cpu_response.get("Datapoints", [])
                                avg_cpu_7d = (
                                    sum(dp["Average"] for dp in datapoints) / len(datapoints)
                                    if datapoints
                                    else 0
                                )
                            except Exception as e:
                                logger.warning(
                                    "documentdb.cpu_metric_failed",
                                    cluster_id=cluster_id,
                                    error=str(e),
                                )
                                avg_cpu_7d = 0

                            # ====================================
                            # COST CALCULATION
                            # ====================================
                            monthly_cost = self._calculate_documentdb_monthly_cost(
                                instance_count=max(instance_count, 1),
                                instance_class=instance_class,
                                region=region,
                            )

                            # ====================================
                            # OPTIMIZATION SCENARIOS
                            # ====================================
                            optimization_scenarios = self._calculate_documentdb_optimization(
                                cluster_id=cluster_id,
                                instance_count=instance_count,
                                instance_class=instance_class,
                                avg_connections_7d=avg_connections_7d,
                                avg_cpu_7d=avg_cpu_7d,
                                cluster_status=cluster_status,
                                tags=tags,
                                region=region,
                            )

                            # ====================================
                            # ORPHAN DETECTION
                            # ====================================
                            is_orphan = avg_connections_7d < 0.1 and cluster_status == "available"

                            # Apply detection rules filtering
                            cluster_create_time = cluster.get("ClusterCreateTime")
                            resource_age_days = None
                            if isinstance(cluster_create_time, datetime):
                                resource_age_days = (datetime.utcnow() - cluster_create_time.replace(tzinfo=None)).days

                            if not self._should_include_resource("documentdb_cluster", resource_age_days):
                                logger.debug("inventory.documentdb_filtered", cluster_id=cluster_id, age=resource_age_days)
                                continue

                            # ====================================
                            # CREATE RESOURCE DATA
                            # ====================================
                            resource = AllCloudResourceData(
                                resource_type="documentdb_cluster",
                                resource_id=cluster_id,
                                resource_name=cluster_id,
                                region=region,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata={
                                    "cluster_id": cluster_id,
                                    "cluster_status": cluster_status,
                                    "engine": engine,
                                    "engine_version": engine_version,
                                    "instance_count": instance_count,
                                    "instance_class": instance_class,
                                    "cluster_endpoint": cluster.get("Endpoint", ""),
                                    "reader_endpoint": cluster.get("ReaderEndpoint", ""),
                                    "port": cluster.get("Port", 27017),
                                    "multi_az": cluster.get("MultiAZ", False),
                                    "storage_encrypted": cluster.get("StorageEncrypted", False),
                                    "avg_connections_7d": round(avg_connections_7d, 2),
                                    "avg_cpu_7d": round(avg_cpu_7d, 2),
                                    "tags": tags,
                                },
                                optimization_scenarios=optimization_scenarios,
                                is_orphan=is_orphan,
                                created_at_cloud=cluster.get("ClusterCreateTime"),
                            )

                            resources.append(resource)

                            logger.debug(
                                "documentdb.cluster_scanned",
                                cluster_id=cluster_id,
                                cluster_status=cluster_status,
                                instance_count=instance_count,
                                monthly_cost=monthly_cost,
                                avg_connections_7d=round(avg_connections_7d, 2),
                                avg_cpu_7d=round(avg_cpu_7d, 2),
                                scenarios_count=len(optimization_scenarios),
                                is_orphan=is_orphan,
                            )

                        except Exception as e:
                            logger.error(
                                "documentdb.cluster_scan_failed",
                                cluster_id=cluster.get("DBClusterIdentifier", "unknown"),
                                region=region,
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "documentdb.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # ============================================================
    # RESSOURCE 30 : ECR REPOSITORY (ELASTIC CONTAINER REGISTRY)
    # ============================================================

    def _calculate_ecr_monthly_cost(
        self,
        storage_gb: float,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS ECR Repository.

        ECR (Elastic Container Registry) is a fully managed Docker container registry
        that stores, manages, and deploys Docker container images.

        Cost Components:
        1. Storage cost ($0.10 per GB/month)
        2. Data transfer cost (not included - standard AWS rates)

        Args:
            storage_gb: Storage size in GB
            region: AWS region

        Returns:
            Estimated monthly cost in USD (storage only)

        Example:
            Repository with 10 GB of images:
            - Storage: 10 GB  $0.10/GB = $1.00/month
        """
        # Storage cost ($0.10 per GB/month)
        storage_cost_per_gb = self.PRICING.get("ecr_storage_per_gb", 0.10)
        storage_cost = storage_gb * storage_cost_per_gb

        return round(storage_cost, 2)

    def _calculate_ecr_optimization(
        self,
        repository_name: str,
        image_count: int,
        storage_gb: float,
        oldest_image_days: int,
        newest_image_days: int,
        untagged_image_count: int,
        pull_count_90d: int,
        has_lifecycle_policy: bool,
        tags: dict,
        region: str,
    ) -> list[OptimizationScenario]:
        """
        Calculate optimization scenarios for AWS ECR Repository.

        ECR Repository Optimization Scenarios:
        1. Empty Repository (CRITICAL) - 0 images + age > 90 days  Delete repository
        2. All Images Very Old (HIGH) - All images > 365 days old  Delete or archive
        3. No Image Pulls (MEDIUM) - 0 pulls for 90 days  Archive or delete
        4. Untagged Images (MEDIUM) - >50% untagged images  Cleanup untagged
        5. No Lifecycle Policy (LOW) - No lifecycle policy  Enable lifecycle policy

        Args:
            repository_name: ECR repository name
            image_count: Total number of images in repository
            storage_gb: Total storage size in GB
            oldest_image_days: Age of oldest image in days
            newest_image_days: Age of newest image in days
            untagged_image_count: Number of untagged images
            pull_count_90d: Number of image pulls over 90 days
            has_lifecycle_policy: Whether lifecycle policy is configured
            tags: Resource tags
            region: AWS region

        Returns:
            List of optimization scenarios with priority, estimated savings, and recommendations
        """
        scenarios = []

        # Current cost (storage only)
        current_cost = self._calculate_ecr_monthly_cost(
            storage_gb=storage_gb,
            region=region,
        )

        # Detect environment from tags
        env = tags.get("Environment", tags.get("environment", "")).lower()
        is_production = env in ["production", "prod", "prd"]

        # =============================================
        # SCENARIO 1: Empty Repository (CRITICAL)
        # =============================================
        if image_count == 0 and newest_image_days > 90:
            # 0 images + age > 90 days  Delete repository
            potential_savings = current_cost
            scenarios.append(
                OptimizationScenario(
                    scenario_name="ecr_empty_repository",
                    priority="CRITICAL",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=0.0,
                    confidence_level="high",
                    description=(
                        f"ECR Repository '{repository_name}' has zero images and has not been used "
                        f"for {newest_image_days} days. The repository serves no purpose and should be deleted."
                    ),
                    recommendation=(
                        "Delete empty ECR Repository:\n"
                        "1. Verify no images are stored: aws ecr describe-images "
                        f"--repository-name {repository_name} --region {region}\n"
                        f"2. Delete repository: aws ecr delete-repository --repository-name {repository_name} "
                        f"--force --region {region}\n\n"
                        f" Monthly Savings: ${potential_savings:.2f}"
                    ),
                    optimization_score=95,
                )
            )

        # =============================================
        # SCENARIO 2: All Images Very Old (HIGH)
        # =============================================
        elif image_count > 0 and newest_image_days > 365:
            # All images > 365 days old  Delete or archive
            potential_savings = current_cost
            scenarios.append(
                OptimizationScenario(
                    scenario_name="ecr_all_images_old",
                    priority="HIGH",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=0.0,
                    confidence_level="high",
                    description=(
                        f"ECR Repository '{repository_name}' contains {image_count} image(s), "
                        f"but ALL images are over 365 days old (newest image: {newest_image_days} days). "
                        f"This indicates the repository is no longer actively maintained."
                    ),
                    recommendation=(
                        "Delete or archive ECR Repository with very old images:\n"
                        f"1. List all images: aws ecr describe-images --repository-name {repository_name} --region {region}\n"
                        f"2. All images are over {newest_image_days} days old\n"
                        "3. Option A: Delete repository if not needed\n"
                        "4. Option B: Export critical images to S3, then delete repository\n\n"
                        f" Monthly Savings: ${potential_savings:.2f}"
                    ),
                    optimization_score=85,
                )
            )

        # =============================================
        # SCENARIO 3: No Image Pulls (MEDIUM)
        # =============================================
        if pull_count_90d == 0 and image_count > 0:
            # 0 pulls for 90 days  Archive or delete
            potential_savings = current_cost
            scenarios.append(
                OptimizationScenario(
                    scenario_name="ecr_no_pulls",
                    priority="MEDIUM",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=0.0,
                    confidence_level="high",
                    description=(
                        f"ECR Repository '{repository_name}' has {image_count} image(s) "
                        f"({storage_gb:.2f} GB) but zero image pulls over the last 90 days. "
                        f"The repository is not being used by any deployments."
                    ),
                    recommendation=(
                        "Archive or delete ECR Repository with no pulls:\n"
                        "1. Check CloudWatch RepositoryPullCount metric\n"
                        f"2. No pulls in 90 days for repository '{repository_name}'\n"
                        "3. Verify no active deployments using these images\n"
                        "4. Delete repository: aws ecr delete-repository --repository-name "
                        f"{repository_name} --force --region {region}\n\n"
                        f" Monthly Savings: ${potential_savings:.2f} (storage)"
                    ),
                    optimization_score=70,
                )
            )

        # =============================================
        # SCENARIO 4: Untagged Images (MEDIUM)
        # =============================================
        if untagged_image_count > 0 and (untagged_image_count / max(image_count, 1)) > 0.5:
            # >50% untagged images  Cleanup untagged
            # Estimate untagged images = 50% of storage
            untagged_storage_gb = storage_gb * (untagged_image_count / max(image_count, 1))
            potential_savings = self._calculate_ecr_monthly_cost(
                storage_gb=untagged_storage_gb,
                region=region,
            )
            scenarios.append(
                OptimizationScenario(
                    scenario_name="ecr_untagged_images",
                    priority="MEDIUM",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=current_cost - potential_savings,
                    confidence_level="medium",
                    description=(
                        f"ECR Repository '{repository_name}' has {untagged_image_count} untagged images "
                        f"out of {image_count} total images ({untagged_image_count / max(image_count, 1) * 100:.1f}%). "
                        f"Untagged images consume ~{untagged_storage_gb:.2f} GB of storage."
                    ),
                    recommendation=(
                        "Cleanup untagged images in ECR Repository:\n"
                        f"1. List untagged images: aws ecr list-images --repository-name {repository_name} "
                        f"--filter tagStatus=UNTAGGED --region {region}\n"
                        f"2. Delete untagged images: aws ecr batch-delete-image --repository-name {repository_name} "
                        f"--image-ids imageDigest=<digest> --region {region}\n"
                        "3. Or enable lifecycle policy to auto-cleanup untagged images\n\n"
                        f" Monthly Savings: ${potential_savings:.2f} (~{untagged_storage_gb:.2f} GB)"
                    ),
                    optimization_score=65,
                )
            )

        # =============================================
        # SCENARIO 5: No Lifecycle Policy (LOW)
        # =============================================
        if not has_lifecycle_policy and image_count > 0:
            # No lifecycle policy  Enable lifecycle policy
            # Estimate 20% savings from lifecycle cleanup
            potential_savings = current_cost * 0.2
            scenarios.append(
                OptimizationScenario(
                    scenario_name="ecr_no_lifecycle_policy",
                    priority="LOW",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=current_cost * 0.8,
                    confidence_level="medium",
                    description=(
                        f"ECR Repository '{repository_name}' has no lifecycle policy configured. "
                        f"Without lifecycle policies, old and untagged images accumulate unnecessarily, "
                        f"increasing storage costs over time."
                    ),
                    recommendation=(
                        "Enable lifecycle policy for ECR Repository:\n"
                        f"1. Create lifecycle policy for repository '{repository_name}'\n"
                        "2. Recommended rules:\n"
                        "   - Delete untagged images after 14 days\n"
                        "   - Keep only 10 most recent tagged images\n"
                        "   - Delete images older than 180 days\n"
                        "3. aws ecr put-lifecycle-policy --repository-name "
                        f"{repository_name} --lifecycle-policy-text file://policy.json --region {region}\n\n"
                        f" Monthly Savings: ~${potential_savings:.2f} (20% reduction)"
                    ),
                    optimization_score=50,
                )
            )

        return scenarios

    async def scan_ecr_repositories(
        self, region: str
    ) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS ECR Repositories for cost intelligence.

        ECR (Elastic Container Registry) is a fully managed Docker container registry
        that stores, manages, and deploys Docker container images.

        CloudWatch Metrics Used:
        - RepositoryPullCount (AWS/ECR) - Image pulls over 90 days

        Cost Optimization Scenarios:
        1. Empty Repository (CRITICAL) - 0 images + age > 90 days  Delete repository
        2. All Images Very Old (HIGH) - All images > 365 days old  Delete or archive
        3. No Image Pulls (MEDIUM) - 0 pulls for 90 days  Archive or delete
        4. Untagged Images (MEDIUM) - >50% untagged images  Cleanup untagged
        5. No Lifecycle Policy (LOW) - No lifecycle policy  Enable lifecycle policy

        Args:
            region: AWS region to scan

        Returns:
            List of all ECR Repositories with optimization recommendations
        """
        import structlog

        logger = structlog.get_logger()
        resources = []

        try:
            async with self.provider.session.client("ecr", region_name=region) as ecr:
                async with self.provider.session.client(
                    "cloudwatch", region_name=region
                ) as cw:
                    # Describe all ECR repositories
                    response = await ecr.describe_repositories()

                    for repository in response.get("repositories", []):
                        try:
                            repository_name = repository["repositoryName"]
                            repository_arn = repository["repositoryArn"]
                            repository_uri = repository["repositoryUri"]
                            created_at = repository.get("createdAt")

                            # Get image count and storage size
                            try:
                                images_response = await ecr.describe_images(
                                    repositoryName=repository_name
                                )
                                images = images_response.get("imageDetails", [])
                                image_count = len(images)

                                # Calculate total storage size
                                storage_bytes = sum(
                                    img.get("imageSizeInBytes", 0) for img in images
                                )
                                storage_gb = storage_bytes / (1024**3)

                                # Find oldest and newest images
                                if images:
                                    image_pushed_dates = [
                                        img.get("imagePushedAt")
                                        for img in images
                                        if img.get("imagePushedAt")
                                    ]
                                    if image_pushed_dates:
                                        oldest_image_date = min(image_pushed_dates)
                                        newest_image_date = max(image_pushed_dates)
                                        now = datetime.now(timezone.utc)
                                        oldest_image_days = (
                                            now - oldest_image_date
                                        ).days
                                        newest_image_days = (
                                            now - newest_image_date
                                        ).days
                                    else:
                                        oldest_image_days = 0
                                        newest_image_days = 0
                                else:
                                    oldest_image_days = 0
                                    newest_image_days = (
                                        datetime.now(timezone.utc) - created_at
                                    ).days if created_at else 365

                                # Count untagged images
                                untagged_image_count = sum(
                                    1
                                    for img in images
                                    if not img.get("imageTags")
                                )

                            except Exception as e:
                                logger.warning(
                                    "ecr.images_failed",
                                    repository_name=repository_name,
                                    error=str(e),
                                )
                                image_count = 0
                                storage_gb = 0.0
                                oldest_image_days = 0
                                newest_image_days = 0
                                untagged_image_count = 0

                            # Check lifecycle policy
                            has_lifecycle_policy = False
                            try:
                                await ecr.get_lifecycle_policy(
                                    repositoryName=repository_name
                                )
                                has_lifecycle_policy = True
                            except Exception:
                                # No lifecycle policy configured
                                pass

                            # Extract tags
                            tags = {}
                            try:
                                tags_response = await ecr.list_tags_for_resource(
                                    resourceArn=repository_arn
                                )
                                for tag in tags_response.get("tags", []):
                                    tags[tag["Key"]] = tag["Value"]
                            except Exception as e:
                                logger.warning(
                                    "ecr.tags_failed",
                                    repository_name=repository_name,
                                    error=str(e),
                                )

                            # ====================================
                            # CLOUDWATCH METRICS (90-day lookback)
                            # ====================================
                            now = datetime.now(timezone.utc)
                            start_time = now - timedelta(days=90)

                            # RepositoryPullCount metric (90 days)
                            try:
                                pull_count_response = await cw.get_metric_statistics(
                                    Namespace="AWS/ECR",
                                    MetricName="RepositoryPullCount",
                                    Dimensions=[
                                        {"Name": "RepositoryName", "Value": repository_name},
                                    ],
                                    StartTime=start_time,
                                    EndTime=now,
                                    Period=86400,  # 1 day
                                    Statistics=["Sum"],
                                )
                                pull_count_90d = int(
                                    sum(
                                        dp["Sum"]
                                        for dp in pull_count_response.get("Datapoints", [])
                                    )
                                )
                            except Exception as e:
                                logger.warning(
                                    "ecr.pull_count_metric_failed",
                                    repository_name=repository_name,
                                    error=str(e),
                                )
                                pull_count_90d = 0

                            # ====================================
                            # COST CALCULATION
                            # ====================================
                            monthly_cost = self._calculate_ecr_monthly_cost(
                                storage_gb=storage_gb,
                                region=region,
                            )

                            # ====================================
                            # OPTIMIZATION SCENARIOS
                            # ====================================
                            optimization_scenarios = self._calculate_ecr_optimization(
                                repository_name=repository_name,
                                image_count=image_count,
                                storage_gb=storage_gb,
                                oldest_image_days=oldest_image_days,
                                newest_image_days=newest_image_days,
                                untagged_image_count=untagged_image_count,
                                pull_count_90d=pull_count_90d,
                                has_lifecycle_policy=has_lifecycle_policy,
                                tags=tags,
                                region=region,
                            )

                            # ====================================
                            # ORPHAN DETECTION
                            # ====================================
                            is_orphan = (
                                image_count == 0 and newest_image_days > 90
                            ) or (pull_count_90d == 0 and image_count > 0)

                            # Apply detection rules filtering
                            resource_age_days = None
                            if isinstance(created_at, datetime):
                                resource_age_days = (datetime.utcnow() - created_at.replace(tzinfo=None)).days

                            if not self._should_include_resource("ecr_repository", resource_age_days):
                                logger.debug("inventory.ecr_filtered", repository_name=repository_name, age=resource_age_days)
                                continue

                            # ====================================
                            # CREATE RESOURCE DATA
                            # ====================================
                            resource = AllCloudResourceData(
                                resource_type="ecr_repository",
                                resource_id=repository_arn,
                                resource_name=repository_name,
                                region=region,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata={
                                    "repository_name": repository_name,
                                    "repository_arn": repository_arn,
                                    "repository_uri": repository_uri,
                                    "image_count": image_count,
                                    "storage_gb": round(storage_gb, 2),
                                    "untagged_image_count": untagged_image_count,
                                    "oldest_image_days": oldest_image_days,
                                    "newest_image_days": newest_image_days,
                                    "pull_count_90d": pull_count_90d,
                                    "has_lifecycle_policy": has_lifecycle_policy,
                                    "image_scanning_enabled": repository.get(
                                        "imageScanningConfiguration", {}
                                    ).get("scanOnPush", False),
                                    "encryption_type": repository.get(
                                        "encryptionConfiguration", {}
                                    ).get("encryptionType", "AES256"),
                                    "tags": tags,
                                },
                                optimization_scenarios=optimization_scenarios,
                                is_orphan=is_orphan,
                                created_at_cloud=created_at,
                            )

                            resources.append(resource)

                            logger.debug(
                                "ecr.repository_scanned",
                                repository_name=repository_name,
                                image_count=image_count,
                                storage_gb=round(storage_gb, 2),
                                monthly_cost=monthly_cost,
                                pull_count_90d=pull_count_90d,
                                scenarios_count=len(optimization_scenarios),
                                is_orphan=is_orphan,
                            )

                        except Exception as e:
                            logger.error(
                                "ecr.repository_scan_failed",
                                repository_name=repository.get("repositoryName", "unknown"),
                                region=region,
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "ecr.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # ============================================================
    # RESSOURCE 31 : SNS TOPIC (SIMPLE NOTIFICATION SERVICE)
    # ============================================================

    def _calculate_sns_monthly_cost(
        self,
        topic_type: str,
        publishes_30d: int,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS SNS Topic.

        SNS (Simple Notification Service) enables pub/sub messaging for distributed systems.

        Cost Components:
        1. Publishes cost (after 1M free tier per month)
        2. Subscription cost (not included - SMS/email charged separately)

        Args:
            topic_type: Topic type ('standard' or 'fifo')
            publishes_30d: Number of publishes in last 30 days
            region: AWS region

        Returns:
            Estimated monthly cost in USD

        Example:
            Standard topic with 5M publishes:
            - First 1M: $0 (free tier)
            - Next 4M: 4  $0.50 = $2.00/month
        """
        free_tier = 1_000_000  # 1M free publishes per month
        billable_publishes = max(0, publishes_30d - free_tier)

        if topic_type == "fifo":
            rate_per_million = self.PRICING.get("sns_fifo_per_million", 0.60)
        else:  # standard
            rate_per_million = self.PRICING.get("sns_standard_per_million", 0.50)

        cost = (billable_publishes / 1_000_000) * rate_per_million

        return round(cost, 2)

    def _calculate_sns_optimization(
        self,
        topic_name: str,
        topic_arn: str,
        topic_type: str,
        subscription_count: int,
        publishes_30d: int,
        delivered_30d: int,
        failed_30d: int,
        tags: dict,
        region: str,
    ) -> list[OptimizationScenario]:
        """
        Calculate optimization scenarios for AWS SNS Topic.

        SNS Topic Optimization Scenarios:
        1. Zero Subscriptions (CRITICAL) - 0 subscriptions + age > 30 days  Delete topic
        2. Zero Publishes (HIGH) - 0 publishes for 30 days  Delete topic
        3. All Subscriptions Failed (MEDIUM) - >95% delivery failures  Fix or delete
        4. Redundant FIFO Topic (MEDIUM) - FIFO with low volume  Migrate to Standard
        5. Dev/Test Topic Always Active (LOW) - Dev/test topic  Delete when not needed

        Args:
            topic_name: SNS topic name
            topic_arn: Topic ARN
            topic_type: Topic type ('standard' or 'fifo')
            subscription_count: Number of subscriptions
            publishes_30d: Number of publishes over 30 days
            delivered_30d: Number of notifications delivered
            failed_30d: Number of notifications failed
            tags: Resource tags
            region: AWS region

        Returns:
            List of optimization scenarios
        """
        scenarios = []

        current_cost = self._calculate_sns_monthly_cost(
            topic_type=topic_type,
            publishes_30d=publishes_30d,
            region=region,
        )

        env = tags.get("Environment", tags.get("environment", "")).lower()
        is_production = env in ["production", "prod", "prd"]

        # SCENARIO 1: Zero Subscriptions (CRITICAL)
        if subscription_count == 0:
            potential_savings = current_cost
            scenarios.append(
                OptimizationScenario(
                    scenario_name="sns_zero_subscriptions",
                    priority="CRITICAL",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=0.0,
                    confidence_level="high",
                    description=(
                        f"SNS Topic '{topic_name}' has zero subscriptions. "
                        f"Without subscriptions, the topic serves no purpose."
                    ),
                    recommendation=(
                        f"Delete SNS Topic with no subscriptions:\n"
                        f"1. aws sns delete-topic --topic-arn {topic_arn} --region {region}\n\n"
                        f" Monthly Savings: ${potential_savings:.2f}"
                    ),
                    optimization_score=95,
                )
            )

        # SCENARIO 2: Zero Publishes (HIGH)
        elif publishes_30d == 0:
            potential_savings = current_cost
            scenarios.append(
                OptimizationScenario(
                    scenario_name="sns_zero_publishes",
                    priority="HIGH",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=0.0,
                    confidence_level="high",
                    description=(
                        f"SNS Topic '{topic_name}' has zero publishes over 30 days "
                        f"({subscription_count} subscription(s)). The topic is not being used."
                    ),
                    recommendation=(
                        f"Delete inactive SNS Topic:\n"
                        f"1. aws sns delete-topic --topic-arn {topic_arn} --region {region}\n\n"
                        f" Monthly Savings: ${potential_savings:.2f}"
                    ),
                    optimization_score=85,
                )
            )

        # SCENARIO 3: All Subscriptions Failed (MEDIUM)
        if delivered_30d > 0 and failed_30d > 0:
            failure_rate = failed_30d / (delivered_30d + failed_30d)
            if failure_rate > 0.95:
                potential_savings = current_cost * 0.5  # Partial savings
                scenarios.append(
                    OptimizationScenario(
                        scenario_name="sns_high_failure_rate",
                        priority="MEDIUM",
                        estimated_monthly_savings=potential_savings,
                        current_monthly_cost=current_cost,
                        optimized_monthly_cost=current_cost * 0.5,
                        confidence_level="high",
                        description=(
                            f"SNS Topic '{topic_name}' has {failure_rate * 100:.1f}% delivery failure rate "
                            f"({failed_30d} failed, {delivered_30d} delivered). Subscriptions are broken."
                        ),
                        recommendation=(
                            f"Fix or delete SNS Topic with high failures:\n"
                            f"1. List subscriptions: aws sns list-subscriptions-by-topic --topic-arn {topic_arn}\n"
                            f"2. Fix broken endpoints or delete failed subscriptions\n\n"
                            f" Monthly Savings: ${potential_savings:.2f} (if deleted)"
                        ),
                        optimization_score=70,
                    )
                )

        # SCENARIO 4: Redundant FIFO Topic (MEDIUM)
        if topic_type == "fifo" and publishes_30d < 300:  # <10/day
            # FIFO costs $0.10 more per million than Standard
            potential_savings = (publishes_30d / 1_000_000) * 0.10
            scenarios.append(
                OptimizationScenario(
                    scenario_name="sns_redundant_fifo",
                    priority="MEDIUM",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=current_cost - potential_savings,
                    confidence_level="medium",
                    description=(
                        f"SNS Topic '{topic_name}' is FIFO type but has very low volume "
                        f"({publishes_30d} publishes in 30 days). FIFO costs 20% more than Standard."
                    ),
                    recommendation=(
                        f"Migrate FIFO topic to Standard for low volume:\n"
                        f"1. Create Standard topic with same name (without .fifo suffix)\n"
                        f"2. Update publishers to use new topic ARN\n"
                        f"3. Delete FIFO topic when migration complete\n\n"
                        f" Monthly Savings: ${potential_savings:.2f} (FIFO premium)"
                    ),
                    optimization_score=65,
                )
            )

        # SCENARIO 5: Dev/Test Topic Always Active (LOW)
        if not is_production and env in ["dev", "test", "staging", "development"]:
            potential_savings = current_cost
            scenarios.append(
                OptimizationScenario(
                    scenario_name="sns_dev_test_always_on",
                    priority="LOW",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=0.0,
                    confidence_level="medium",
                    description=(
                        f"SNS Topic '{topic_name}' is tagged as '{env}' environment. "
                        f"Dev/test topics should be deleted when not in use."
                    ),
                    recommendation=(
                        f"Delete dev/test SNS Topic when not needed:\n"
                        f"1. aws sns delete-topic --topic-arn {topic_arn} --region {region}\n"
                        f"2. Recreate when needed (IaC: Terraform/CloudFormation)\n\n"
                        f" Monthly Savings: ${potential_savings:.2f}"
                    ),
                    optimization_score=50,
                )
            )

        return scenarios

    async def scan_sns_topics(
        self, region: str
    ) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS SNS Topics for cost intelligence.

        SNS (Simple Notification Service) enables pub/sub messaging.

        CloudWatch Metrics Used:
        - NumberOfMessagesPublished (AWS/SNS) - Publishes over 30 days
        - NumberOfNotificationsDelivered (AWS/SNS) - Delivered over 30 days
        - NumberOfNotificationsFailed (AWS/SNS) - Failed over 30 days

        Cost Optimization Scenarios:
        1. Zero Subscriptions (CRITICAL) - 0 subscriptions  Delete topic
        2. Zero Publishes (HIGH) - 0 publishes for 30 days  Delete topic
        3. All Subscriptions Failed (MEDIUM) - >95% delivery failures  Fix or delete
        4. Redundant FIFO Topic (MEDIUM) - FIFO with low volume  Migrate to Standard
        5. Dev/Test Topic Always Active (LOW) - Dev/test topic  Delete when not needed

        Args:
            region: AWS region to scan

        Returns:
            List of all SNS Topics with optimization recommendations
        """
        import structlog

        logger = structlog.get_logger()
        resources = []

        try:
            async with self.provider.session.client("sns", region_name=region) as sns:
                async with self.provider.session.client(
                    "cloudwatch", region_name=region
                ) as cw:
                    # List all SNS topics
                    response = await sns.list_topics()

                    for topic in response.get("Topics", []):
                        try:
                            topic_arn = topic["TopicArn"]
                            topic_name = topic_arn.split(":")[-1]

                            # Get topic attributes
                            attrs_response = await sns.get_topic_attributes(TopicArn=topic_arn)
                            attributes = attrs_response.get("Attributes", {})

                            # Determine topic type (FIFO or Standard)
                            topic_type = "fifo" if topic_name.endswith(".fifo") else "standard"

                            # Count subscriptions
                            subs_response = await sns.list_subscriptions_by_topic(TopicArn=topic_arn)
                            subscription_count = len(subs_response.get("Subscriptions", []))

                            # Extract tags
                            tags = {}
                            try:
                                tags_response = await sns.list_tags_for_resource(ResourceArn=topic_arn)
                                for tag in tags_response.get("Tags", []):
                                    tags[tag["Key"]] = tag["Value"]
                            except Exception:
                                pass

                            # CloudWatch Metrics (30-day lookback)
                            now = datetime.now(timezone.utc)
                            start_time = now - timedelta(days=30)

                            # NumberOfMessagesPublished metric
                            try:
                                publishes_response = await cw.get_metric_statistics(
                                    Namespace="AWS/SNS",
                                    MetricName="NumberOfMessagesPublished",
                                    Dimensions=[{"Name": "TopicName", "Value": topic_name}],
                                    StartTime=start_time,
                                    EndTime=now,
                                    Period=86400,
                                    Statistics=["Sum"],
                                )
                                publishes_30d = int(sum(dp["Sum"] for dp in publishes_response.get("Datapoints", [])))
                            except Exception:
                                publishes_30d = 0

                            # NumberOfNotificationsDelivered metric
                            try:
                                delivered_response = await cw.get_metric_statistics(
                                    Namespace="AWS/SNS",
                                    MetricName="NumberOfNotificationsDelivered",
                                    Dimensions=[{"Name": "TopicName", "Value": topic_name}],
                                    StartTime=start_time,
                                    EndTime=now,
                                    Period=86400,
                                    Statistics=["Sum"],
                                )
                                delivered_30d = int(sum(dp["Sum"] for dp in delivered_response.get("Datapoints", [])))
                            except Exception:
                                delivered_30d = 0

                            # NumberOfNotificationsFailed metric
                            try:
                                failed_response = await cw.get_metric_statistics(
                                    Namespace="AWS/SNS",
                                    MetricName="NumberOfNotificationsFailed",
                                    Dimensions=[{"Name": "TopicName", "Value": topic_name}],
                                    StartTime=start_time,
                                    EndTime=now,
                                    Period=86400,
                                    Statistics=["Sum"],
                                )
                                failed_30d = int(sum(dp["Sum"] for dp in failed_response.get("Datapoints", [])))
                            except Exception:
                                failed_30d = 0

                            # Cost calculation
                            monthly_cost = self._calculate_sns_monthly_cost(
                                topic_type=topic_type,
                                publishes_30d=publishes_30d,
                                region=region,
                            )

                            # Optimization scenarios
                            optimization_scenarios = self._calculate_sns_optimization(
                                topic_name=topic_name,
                                topic_arn=topic_arn,
                                topic_type=topic_type,
                                subscription_count=subscription_count,
                                publishes_30d=publishes_30d,
                                delivered_30d=delivered_30d,
                                failed_30d=failed_30d,
                                tags=tags,
                                region=region,
                            )

                            # Orphan detection
                            is_orphan = subscription_count == 0 or publishes_30d == 0

                            # Apply detection rules filtering
                            # Note: SNS topics don't have creation timestamp in API, so age is always None
                            resource_age_days = None

                            if not self._should_include_resource("sns_topic", resource_age_days):
                                logger.debug("inventory.sns_filtered", topic_arn=topic_arn, age=resource_age_days)
                                continue

                            # Create resource data
                            resource = AllCloudResourceData(
                                resource_type="sns_topic",
                                resource_id=topic_arn,
                                resource_name=topic_name,
                                region=region,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata={
                                    "topic_name": topic_name,
                                    "topic_arn": topic_arn,
                                    "topic_type": topic_type,
                                    "subscription_count": subscription_count,
                                    "publishes_30d": publishes_30d,
                                    "delivered_30d": delivered_30d,
                                    "failed_30d": failed_30d,
                                    "failure_rate": round(failed_30d / (delivered_30d + failed_30d), 3) if (delivered_30d + failed_30d) > 0 else 0,
                                    "tags": tags,
                                },
                                optimization_scenarios=optimization_scenarios,
                                is_orphan=is_orphan,
                                created_at_cloud=None,
                            )

                            resources.append(resource)

                            logger.debug(
                                "sns.topic_scanned",
                                topic_name=topic_name,
                                subscription_count=subscription_count,
                                publishes_30d=publishes_30d,
                                monthly_cost=monthly_cost,
                                scenarios_count=len(optimization_scenarios),
                                is_orphan=is_orphan,
                            )

                        except Exception as e:
                            logger.error(
                                "sns.topic_scan_failed",
                                topic_arn=topic.get("TopicArn", "unknown"),
                                region=region,
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "sns.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # ============================================================
    # RESSOURCE 32 : SQS QUEUE (SIMPLE QUEUE SERVICE)
    # ============================================================

    def _calculate_sqs_monthly_cost(
        self,
        queue_type: str,
        requests_30d: int,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS SQS Queue.

        SQS (Simple Queue Service) enables message queueing for distributed applications.

        Cost Components:
        1. Request cost (after 1M free tier per month)
        2. No charge for empty queues

        Args:
            queue_type: Queue type ('standard' or 'fifo')
            requests_30d: Number of requests in last 30 days
            region: AWS region

        Returns:
            Estimated monthly cost in USD

        Example:
            Standard queue with 5M requests:
            - First 1M: $0 (free tier)
            - Next 4M: 4  $0.40 = $1.60/month
        """
        free_tier = 1_000_000  # 1M free requests per month
        billable_requests = max(0, requests_30d - free_tier)

        if queue_type == "fifo":
            rate_per_million = self.PRICING.get("sqs_fifo_per_million", 0.50)
        else:  # standard
            rate_per_million = self.PRICING.get("sqs_standard_per_million", 0.40)

        cost = (billable_requests / 1_000_000) * rate_per_million

        return round(cost, 2)

    def _calculate_sqs_optimization(
        self,
        queue_name: str,
        queue_url: str,
        queue_type: str,
        messages_sent_30d: int,
        messages_received_30d: int,
        messages_visible: int,
        oldest_message_age_seconds: int,
        is_dead_letter_queue: bool,
        has_redrive_policy: bool,
        tags: dict,
        region: str,
    ) -> list[OptimizationScenario]:
        """
        Calculate optimization scenarios for AWS SQS Queue.

        SQS Queue Optimization Scenarios:
        1. Zero Messages (CRITICAL) - 0 messages for 30 days  Delete queue
        2. High Message Age (HIGH) - Messages > 14 days old  Process or purge
        3. Dead Letter Queue Not Processed (MEDIUM) - DLQ with no redrive  Process
        4. Redundant FIFO Queue (MEDIUM) - FIFO with low volume  Migrate to Standard
        5. No Redrive Policy (LOW) - Queue without DLQ  Configure DLQ

        Args:
            queue_name: SQS queue name
            queue_url: Queue URL
            queue_type: Queue type ('standard' or 'fifo')
            messages_sent_30d: Messages sent over 30 days
            messages_received_30d: Messages received over 30 days
            messages_visible: Current visible messages
            oldest_message_age_seconds: Age of oldest message in seconds
            is_dead_letter_queue: Whether queue is a DLQ
            has_redrive_policy: Whether queue has redrive policy
            tags: Resource tags
            region: AWS region

        Returns:
            List of optimization scenarios
        """
        scenarios = []

        total_requests_30d = messages_sent_30d + messages_received_30d
        current_cost = self._calculate_sqs_monthly_cost(
            queue_type=queue_type,
            requests_30d=total_requests_30d,
            region=region,
        )

        env = tags.get("Environment", tags.get("environment", "")).lower()
        is_production = env in ["production", "prod", "prd"]

        oldest_message_age_days = oldest_message_age_seconds / 86400

        # SCENARIO 1: Zero Messages (CRITICAL)
        if messages_sent_30d == 0 and messages_received_30d == 0:
            potential_savings = current_cost
            scenarios.append(
                OptimizationScenario(
                    scenario_name="sqs_zero_messages",
                    priority="CRITICAL",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=0.0,
                    confidence_level="high",
                    description=(
                        f"SQS Queue '{queue_name}' has zero messages sent or received "
                        f"over 30 days. The queue is not being used."
                    ),
                    recommendation=(
                        f"Delete inactive SQS Queue:\n"
                        f"1. aws sqs delete-queue --queue-url {queue_url} --region {region}\n\n"
                        f" Monthly Savings: ${potential_savings:.2f}"
                    ),
                    optimization_score=95,
                )
            )

        # SCENARIO 2: High Message Age (HIGH)
        if oldest_message_age_days > 14 and messages_visible > 0:
            potential_savings = current_cost * 0.8  # Partial savings
            scenarios.append(
                OptimizationScenario(
                    scenario_name="sqs_high_message_age",
                    priority="HIGH",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=current_cost * 0.2,
                    confidence_level="high",
                    description=(
                        f"SQS Queue '{queue_name}' has {messages_visible} messages with oldest message "
                        f"{int(oldest_message_age_days)} days old. Messages are stuck in the queue."
                    ),
                    recommendation=(
                        f"Process or purge old messages in SQS Queue:\n"
                        f"1. Investigate why messages are not being processed\n"
                        f"2. Fix consumer application or delete stuck messages\n"
                        f"3. Purge queue: aws sqs purge-queue --queue-url {queue_url} --region {region}\n\n"
                        f" Monthly Savings: ${potential_savings:.2f} (if queue deleted)"
                    ),
                    optimization_score=85,
                )
            )

        # SCENARIO 3: Dead Letter Queue Not Processed (MEDIUM)
        if is_dead_letter_queue and messages_visible > 0 and messages_received_30d < 10:
            potential_savings = current_cost * 0.6
            scenarios.append(
                OptimizationScenario(
                    scenario_name="sqs_dlq_not_processed",
                    priority="MEDIUM",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=current_cost * 0.4,
                    confidence_level="high",
                    description=(
                        f"SQS Queue '{queue_name}' is a Dead Letter Queue with {messages_visible} messages "
                        f"but only {messages_received_30d} messages processed in 30 days. "
                        f"Failed messages are not being redriven."
                    ),
                    recommendation=(
                        f"Process Dead Letter Queue messages:\n"
                        f"1. Inspect failed messages: aws sqs receive-message --queue-url {queue_url}\n"
                        f"2. Redrive messages to source queue or fix and reprocess\n"
                        f"3. Purge after processing: aws sqs purge-queue --queue-url {queue_url}\n\n"
                        f" Monthly Savings: ${potential_savings:.2f} (if processed/deleted)"
                    ),
                    optimization_score=70,
                )
            )

        # SCENARIO 4: Redundant FIFO Queue (MEDIUM)
        if queue_type == "fifo" and total_requests_30d < 3000:  # <100/day
            # FIFO costs $0.10 more per million than Standard
            potential_savings = (total_requests_30d / 1_000_000) * 0.10
            scenarios.append(
                OptimizationScenario(
                    scenario_name="sqs_redundant_fifo",
                    priority="MEDIUM",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=current_cost - potential_savings,
                    confidence_level="medium",
                    description=(
                        f"SQS Queue '{queue_name}' is FIFO type but has very low volume "
                        f"({total_requests_30d} requests in 30 days). FIFO costs 25% more than Standard."
                    ),
                    recommendation=(
                        f"Migrate FIFO queue to Standard for low volume:\n"
                        f"1. Create Standard queue with same name (without .fifo suffix)\n"
                        f"2. Update producers/consumers to use new queue URL\n"
                        f"3. Delete FIFO queue when migration complete\n\n"
                        f" Monthly Savings: ${potential_savings:.2f} (FIFO premium)"
                    ),
                    optimization_score=65,
                )
            )

        # SCENARIO 5: No Redrive Policy (LOW)
        if not has_redrive_policy and not is_dead_letter_queue and messages_sent_30d > 0:
            potential_savings = 0.0  # Best practice, no direct savings
            scenarios.append(
                OptimizationScenario(
                    scenario_name="sqs_no_redrive_policy",
                    priority="LOW",
                    estimated_monthly_savings=potential_savings,
                    current_monthly_cost=current_cost,
                    optimized_monthly_cost=current_cost,
                    confidence_level="medium",
                    description=(
                        f"SQS Queue '{queue_name}' has no Dead Letter Queue configured. "
                        f"Failed messages will be lost without a DLQ redrive policy."
                    ),
                    recommendation=(
                        f"Configure Dead Letter Queue for '{queue_name}':\n"
                        f"1. Create DLQ: aws sqs create-queue --queue-name {queue_name}-dlq\n"
                        f"2. Set redrive policy with maxReceiveCount=3-5\n"
                        f"3. Monitor DLQ for failed messages\n\n"
                        f" No direct cost savings, but improves reliability"
                    ),
                    optimization_score=50,
                )
            )

        return scenarios

    async def scan_sqs_queues(
        self, region: str
    ) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS SQS Queues for cost intelligence.

        SQS (Simple Queue Service) enables message queueing for distributed applications.

        CloudWatch Metrics Used:
        - NumberOfMessagesSent (AWS/SQS) - Messages sent over 30 days
        - NumberOfMessagesReceived (AWS/SQS) - Messages received over 30 days
        - ApproximateNumberOfMessagesVisible (Queue Attribute) - Current message count
        - ApproximateAgeOfOldestMessage (Queue Attribute) - Oldest message age

        Cost Optimization Scenarios:
        1. Zero Messages (CRITICAL) - 0 messages for 30 days  Delete queue
        2. High Message Age (HIGH) - Messages > 14 days old  Process or purge
        3. Dead Letter Queue Not Processed (MEDIUM) - DLQ with no redrive  Process
        4. Redundant FIFO Queue (MEDIUM) - FIFO with low volume  Migrate to Standard
        5. No Redrive Policy (LOW) - Queue without DLQ  Configure DLQ

        Args:
            region: AWS region to scan

        Returns:
            List of all SQS Queues with optimization recommendations
        """
        import structlog

        logger = structlog.get_logger()
        resources = []

        try:
            async with self.provider.session.client("sqs", region_name=region) as sqs:
                async with self.provider.session.client(
                    "cloudwatch", region_name=region
                ) as cw:
                    # List all SQS queues
                    response = await sqs.list_queues()
                    queue_urls = response.get("QueueUrls", [])

                    for queue_url in queue_urls:
                        try:
                            queue_name = queue_url.split("/")[-1]

                            # Get queue attributes
                            attrs_response = await sqs.get_queue_attributes(
                                QueueUrl=queue_url,
                                AttributeNames=["All"]
                            )
                            attributes = attrs_response.get("Attributes", {})

                            # Determine queue type
                            queue_type = "fifo" if attributes.get("FifoQueue") == "true" else "standard"

                            # Get queue metadata
                            messages_visible = int(attributes.get("ApproximateNumberOfMessagesVisible", 0))
                            oldest_message_age_seconds = int(attributes.get("ApproximateAgeOfOldestMessage", 0))
                            has_redrive_policy = "RedrivePolicy" in attributes

                            # Check if this is a DLQ (has -dlq suffix or is target of redrive)
                            is_dead_letter_queue = queue_name.endswith("-dlq") or queue_name.endswith("_dlq")

                            # Extract tags
                            tags = {}
                            try:
                                tags_response = await sqs.list_queue_tags(QueueUrl=queue_url)
                                tags = tags_response.get("Tags", {})
                            except Exception:
                                pass

                            # CloudWatch Metrics (30-day lookback)
                            now = datetime.now(timezone.utc)
                            start_time = now - timedelta(days=30)

                            # NumberOfMessagesSent metric
                            try:
                                sent_response = await cw.get_metric_statistics(
                                    Namespace="AWS/SQS",
                                    MetricName="NumberOfMessagesSent",
                                    Dimensions=[{"Name": "QueueName", "Value": queue_name}],
                                    StartTime=start_time,
                                    EndTime=now,
                                    Period=86400,
                                    Statistics=["Sum"],
                                )
                                messages_sent_30d = int(sum(dp["Sum"] for dp in sent_response.get("Datapoints", [])))
                            except Exception:
                                messages_sent_30d = 0

                            # NumberOfMessagesReceived metric
                            try:
                                received_response = await cw.get_metric_statistics(
                                    Namespace="AWS/SQS",
                                    MetricName="NumberOfMessagesReceived",
                                    Dimensions=[{"Name": "QueueName", "Value": queue_name}],
                                    StartTime=start_time,
                                    EndTime=now,
                                    Period=86400,
                                    Statistics=["Sum"],
                                )
                                messages_received_30d = int(sum(dp["Sum"] for dp in received_response.get("Datapoints", [])))
                            except Exception:
                                messages_received_30d = 0

                            # Cost calculation
                            total_requests_30d = messages_sent_30d + messages_received_30d
                            monthly_cost = self._calculate_sqs_monthly_cost(
                                queue_type=queue_type,
                                requests_30d=total_requests_30d,
                                region=region,
                            )

                            # Optimization scenarios
                            optimization_scenarios = self._calculate_sqs_optimization(
                                queue_name=queue_name,
                                queue_url=queue_url,
                                queue_type=queue_type,
                                messages_sent_30d=messages_sent_30d,
                                messages_received_30d=messages_received_30d,
                                messages_visible=messages_visible,
                                oldest_message_age_seconds=oldest_message_age_seconds,
                                is_dead_letter_queue=is_dead_letter_queue,
                                has_redrive_policy=has_redrive_policy,
                                tags=tags,
                                region=region,
                            )

                            # Orphan detection
                            is_orphan = messages_sent_30d == 0 and messages_received_30d == 0

                            # Apply detection rules filtering
                            # Note: SQS queues don't have creation timestamp in API, so age is always None
                            resource_age_days = None

                            if not self._should_include_resource("sqs_queue", resource_age_days):
                                logger.debug("inventory.sqs_filtered", queue_url=queue_url, age=resource_age_days)
                                continue

                            # Create resource data
                            resource = AllCloudResourceData(
                                resource_type="sqs_queue",
                                resource_id=queue_url,
                                resource_name=queue_name,
                                region=region,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata={
                                    "queue_name": queue_name,
                                    "queue_url": queue_url,
                                    "queue_type": queue_type,
                                    "messages_sent_30d": messages_sent_30d,
                                    "messages_received_30d": messages_received_30d,
                                    "total_requests_30d": total_requests_30d,
                                    "messages_visible": messages_visible,
                                    "oldest_message_age_seconds": oldest_message_age_seconds,
                                    "oldest_message_age_days": round(oldest_message_age_seconds / 86400, 1),
                                    "is_dead_letter_queue": is_dead_letter_queue,
                                    "has_redrive_policy": has_redrive_policy,
                                    "tags": tags,
                                },
                                optimization_scenarios=optimization_scenarios,
                                is_orphan=is_orphan,
                                created_at_cloud=None,
                            )

                            resources.append(resource)

                            logger.debug(
                                "sqs.queue_scanned",
                                queue_name=queue_name,
                                queue_type=queue_type,
                                messages_visible=messages_visible,
                                monthly_cost=monthly_cost,
                                scenarios_count=len(optimization_scenarios),
                                is_orphan=is_orphan,
                            )

                        except Exception as e:
                            logger.error(
                                "sqs.queue_scan_failed",
                                queue_url=queue_url,
                                region=region,
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "sqs.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # ============================================================
    # RESSOURCE 33 : SECRETS MANAGER
    # ============================================================

    def _calculate_secrets_manager_monthly_cost(self, region: str) -> float:
        """
        Calculate estimated monthly cost for AWS Secrets Manager secret.

        Secrets Manager stores and rotates secrets securely.

        Cost Components:
        1. Storage cost: $0.40 per secret per month
        2. API call cost: $0.05 per 10,000 API calls (beyond free tier)

        Args:
            region: AWS region

        Returns:
            Estimated monthly cost in USD per secret

        Example:
            Single secret stored for 1 month:
            - Storage: $0.40/month
            - API calls (100K): 10  $0.05 = $0.50/month
            - Total: $0.90/month
        """
        # Secrets Manager charges per secret stored
        secret_cost = self.PRICING.get("secrets_manager_per_secret", 0.40)

        return round(secret_cost, 2)

    def _calculate_secrets_manager_optimization(
        self,
        secret_name: str,
        secret_arn: str,
        last_accessed_date: datetime | None,
        last_rotated_date: datetime | None,
        rotation_enabled: bool,
        created_date: datetime,
        tags: dict,
        region: str,
    ) -> list[OptimizationScenario]:
        """
        Calculate optimization scenarios for AWS Secrets Manager secret.

        Secrets Manager Optimization Scenarios:
        1. Secret Never Used (CRITICAL) - 0 accesses in 90 days  Delete secret
        2. Secret Not Rotated (HIGH) - No rotation in 365+ days  Security risk + cost
        3. Duplicate Secrets (MEDIUM) - Same secret in multiple regions  Consolidate
        4. Secret Inactive (MEDIUM) - < 10 accesses/month  Delete if dev/test
        5. No Rotation Configured (LOW) - Manual rotation only  Enable auto-rotation

        Args:
            secret_name: Secret name
            secret_arn: Secret ARN
            last_accessed_date: Last time secret was accessed (can be None)
            last_rotated_date: Last rotation date (can be None)
            rotation_enabled: Whether automatic rotation is enabled
            created_date: Secret creation date
            tags: Resource tags
            region: AWS region

        Returns:
            List of optimization scenarios
        """
        scenarios = []

        current_cost = self._calculate_secrets_manager_monthly_cost(region=region)

        # Calculate age
        age_days = (datetime.now(timezone.utc) - created_date.replace(tzinfo=timezone.utc)).days

        # Calculate days since last access
        days_since_access = None
        if last_accessed_date:
            days_since_access = (
                datetime.now(timezone.utc) - last_accessed_date.replace(tzinfo=timezone.utc)
            ).days

        # Calculate days since last rotation
        days_since_rotation = None
        if last_rotated_date:
            days_since_rotation = (
                datetime.now(timezone.utc) - last_rotated_date.replace(tzinfo=timezone.utc)
            ).days

        env = tags.get("Environment", tags.get("environment", "")).lower()
        is_production = env in ["production", "prod", "prd"]

        # SCENARIO 1: Secret Never Used (CRITICAL)
        if days_since_access and days_since_access >= 90:
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Secret Never Used",
                    priority="CRITICAL",
                    confidence_score=95,
                    estimated_monthly_savings=current_cost,
                    recommendation=(
                        f"Secret has not been accessed in {days_since_access} days. "
                        f"Delete if no longer needed or archive for compliance."
                    ),
                    implementation_steps=[
                        "Verify secret is not referenced in code or infrastructure",
                        "Archive secret value if needed for compliance",
                        "Delete secret via AWS CLI: aws secretsmanager delete-secret --secret-id ...",
                        "Monitor for any access attempts after deletion"
                    ],
                    risk_level="LOW",
                    effort="LOW",
                )
            )

        # SCENARIO 2: Secret Not Rotated (HIGH)
        if days_since_rotation and days_since_rotation >= 365 and is_production:
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Secret Not Rotated (Security Risk)",
                    priority="HIGH",
                    confidence_score=85,
                    estimated_monthly_savings=current_cost if not rotation_enabled else 0.0,
                    recommendation=(
                        f"Secret has not been rotated in {days_since_rotation} days (>1 year). "
                        f"Security best practice is to rotate secrets every 30-90 days. "
                        f"Enable automatic rotation or rotate manually."
                    ),
                    implementation_steps=[
                        "Enable automatic rotation for supported secret types (RDS, Redshift, etc.)",
                        "For custom secrets, configure Lambda rotation function",
                        "Set rotation schedule (recommended: every 30-90 days)",
                        "Test rotation does not break applications"
                    ],
                    risk_level="MEDIUM",
                    effort="MEDIUM",
                )
            )

        # SCENARIO 3: Duplicate Secrets (MEDIUM)
        # Note: This requires cross-region comparison, detected by checking name patterns
        if "copy" in secret_name.lower() or "duplicate" in secret_name.lower():
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Potential Duplicate Secret",
                    priority="MEDIUM",
                    confidence_score=70,
                    estimated_monthly_savings=current_cost,
                    recommendation=(
                        f"Secret name suggests it may be a duplicate ('{secret_name}'). "
                        f"Consolidate secrets to reduce cost and management overhead."
                    ),
                    implementation_steps=[
                        "Search for similar secrets across all regions",
                        "Compare secret values (if not sensitive)",
                        "Update applications to use single secret",
                        "Delete duplicate secrets"
                    ],
                    risk_level="LOW",
                    effort="MEDIUM",
                )
            )

        # SCENARIO 4: Secret Inactive (MEDIUM)
        if (
            days_since_access
            and 7 <= days_since_access < 90
            and not is_production
        ):
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Secret Inactive (Dev/Test)",
                    priority="MEDIUM",
                    confidence_score=65,
                    estimated_monthly_savings=current_cost,
                    recommendation=(
                        f"Secret has low activity ({days_since_access} days since last access) "
                        f"in non-production environment. Consider deleting if no longer needed."
                    ),
                    implementation_steps=[
                        "Confirm secret is not needed for dev/test workflows",
                        "Delete secret or move to hardcoded config for dev",
                        "Document decision in infrastructure-as-code"
                    ],
                    risk_level="LOW",
                    effort="LOW",
                )
            )

        # SCENARIO 5: No Rotation Configured (LOW)
        if not rotation_enabled and is_production and age_days >= 30:
            scenarios.append(
                OptimizationScenario(
                    scenario_name="No Automatic Rotation Configured",
                    priority="LOW",
                    confidence_score=50,
                    estimated_monthly_savings=0.0,  # Security best practice, not cost savings
                    recommendation=(
                        f"Automatic rotation is not enabled. Enable rotation for security best practice. "
                        f"This does not reduce cost but improves security posture."
                    ),
                    implementation_steps=[
                        "Enable automatic rotation if supported secret type",
                        "Configure Lambda rotation function for custom secrets",
                        "Set rotation schedule (30-90 days recommended)",
                        "Test rotation process in non-production first"
                    ],
                    risk_level="MEDIUM",
                    effort="MEDIUM",
                )
            )

        return scenarios

    async def scan_secrets_manager_secrets(
        self, region: str
    ) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS Secrets Manager secrets for cost intelligence.

        Secrets Manager stores sensitive data like passwords, API keys, database credentials.

        CloudWatch Metrics Used:
        - N/A (Secrets Manager doesn't publish CloudWatch metrics for secret access)
        - LastAccessedDate field from describe_secret API

        API Calls:
        - list_secrets() - List all secrets
        - describe_secret() - Get secret details (LastAccessedDate, RotationEnabled)

        Cost Optimization Scenarios:
        1. Secret Never Used (CRITICAL) - 0 accesses in 90 days  Delete secret
        2. Secret Not Rotated (HIGH) - No rotation in 365+ days  Security risk
        3. Duplicate Secrets (MEDIUM) - Same secret in multiple regions  Consolidate
        4. Secret Inactive (MEDIUM) - < 10 accesses/month  Delete if dev/test
        5. No Rotation Configured (LOW) - Manual rotation only  Enable auto-rotation

        Returns:
            List of AllCloudResourceData representing ALL secrets in the region
        """
        resources = []

        try:
            async with self.session.client("secretsmanager", region_name=region) as sm:
                # List all secrets
                paginator = sm.get_paginator("list_secrets")
                async for page in paginator.paginate():
                    for secret in page.get("SecretList", []):
                        try:
                            secret_arn = secret.get("ARN", "")
                            secret_name = secret.get("Name", "")

                            # Get detailed secret information
                            try:
                                secret_details = await sm.describe_secret(SecretId=secret_arn)
                            except Exception as e:
                                logger.warning(
                                    "secrets_manager.describe_failed",
                                    secret_name=secret_name,
                                    error=str(e),
                                )
                                continue

                            # Extract metadata
                            last_accessed_date = secret_details.get("LastAccessedDate")
                            last_rotated_date = secret_details.get("LastRotatedDate")
                            rotation_enabled = secret_details.get("RotationEnabled", False)
                            created_date = secret_details.get("CreatedDate")
                            tags_list = secret_details.get("Tags", [])
                            tags = {tag["Key"]: tag["Value"] for tag in tags_list}

                            # Calculate monthly cost
                            monthly_cost = self._calculate_secrets_manager_monthly_cost(
                                region=region
                            )

                            # Calculate optimization scenarios
                            optimization_scenarios = self._calculate_secrets_manager_optimization(
                                secret_name=secret_name,
                                secret_arn=secret_arn,
                                last_accessed_date=last_accessed_date,
                                last_rotated_date=last_rotated_date,
                                rotation_enabled=rotation_enabled,
                                created_date=created_date,
                                tags=tags,
                                region=region,
                            )

                            # Apply detection rules filtering
                            resource_age_days = None
                            if isinstance(created_date, datetime):
                                resource_age_days = (datetime.utcnow() - created_date.replace(tzinfo=None)).days

                            if not self._should_include_resource("secrets_manager_secret", resource_age_days):
                                logger.debug("inventory.secrets_manager_filtered", secret_name=secret_name, age=resource_age_days)
                                continue

                            # Prepare resource data
                            resource_data = AllCloudResourceData(
                                resource_type="secrets_manager_secret",
                                resource_id=secret_arn,
                                resource_name=secret_name,
                                region=region,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata={
                                    "last_accessed_date": last_accessed_date.isoformat()
                                    if last_accessed_date
                                    else None,
                                    "last_rotated_date": last_rotated_date.isoformat()
                                    if last_rotated_date
                                    else None,
                                    "rotation_enabled": rotation_enabled,
                                    "created_date": created_date.isoformat()
                                    if created_date
                                    else None,
                                    "tags": tags,
                                },
                                optimization_scenarios=optimization_scenarios,
                            )

                            resources.append(resource_data)

                            logger.debug(
                                "secrets_manager.secret_scanned",
                                secret_name=secret_name,
                                region=region,
                                scenarios=len(optimization_scenarios),
                            )

                        except Exception as e:
                            logger.warning(
                                "secrets_manager.secret_scan_error",
                                secret_name=secret.get("Name", "unknown"),
                                region=region,
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "secrets_manager.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # ============================================================
    # RESSOURCE 34 : AWS BACKUP VAULT
    # ============================================================

    def _calculate_backup_vault_monthly_cost(
        self, storage_gb: float, storage_tier: str, region: str
    ) -> float:
        """
        Calculate estimated monthly cost for AWS Backup Vault storage.

        AWS Backup centralizes backup management across AWS services.

        Cost Components:
        1. Warm backup storage: $0.05 per GB/month
        2. Cold backup storage: $0.01 per GB/month (archive tier)
        3. Continuous backup (PITR): $0.20 per GB/month (not tracked here)

        Args:
            storage_gb: Total backup storage in GB
            storage_tier: Storage tier ('warm' or 'cold')
            region: AWS region

        Returns:
            Estimated monthly cost in USD

        Example:
            100 GB warm backup storage:
            - Cost: 100  $0.05 = $5.00/month

            100 GB cold backup storage:
            - Cost: 100  $0.01 = $1.00/month
        """
        if storage_tier == "cold":
            rate_per_gb = self.PRICING.get("backup_cold_storage_per_gb", 0.01)
        else:  # warm (default)
            rate_per_gb = self.PRICING.get("backup_warm_storage_per_gb", 0.05)

        cost = storage_gb * rate_per_gb

        return round(cost, 2)

    def _calculate_backup_vault_optimization(
        self,
        vault_name: str,
        vault_arn: str,
        recovery_points_count: int,
        total_storage_gb: float,
        oldest_recovery_point_age_days: int,
        last_restore_time: datetime | None,
        backup_jobs_30d: int,
        restore_jobs_30d: int,
        tags: dict,
        region: str,
    ) -> list[OptimizationScenario]:
        """
        Calculate optimization scenarios for AWS Backup Vault.

        AWS Backup Vault Optimization Scenarios:
        1. Empty Vault (CRITICAL) - 0 recovery points  Delete vault
        2. Old Recovery Points (HIGH) - Recovery points >365 days  Delete expired backups
        3. Never Tested (MEDIUM) - No restore jobs ever  Test backup viability
        4. Duplicate Backups (MEDIUM) - Same source in multiple vaults  Consolidate
        5. Warm Storage for Archives (LOW) - >180 days  Migrate to Cold storage

        Args:
            vault_name: Backup vault name
            vault_arn: Vault ARN
            recovery_points_count: Number of recovery points in vault
            total_storage_gb: Total storage used (GB)
            oldest_recovery_point_age_days: Age of oldest recovery point
            last_restore_time: Last time a restore was performed (can be None)
            backup_jobs_30d: Number of backup jobs in last 30 days
            restore_jobs_30d: Number of restore jobs in last 30 days
            tags: Resource tags
            region: AWS region

        Returns:
            List of optimization scenarios
        """
        scenarios = []

        current_cost = self._calculate_backup_vault_monthly_cost(
            storage_gb=total_storage_gb,
            storage_tier="warm",  # Assume warm by default
            region=region,
        )

        env = tags.get("Environment", tags.get("environment", "")).lower()
        is_production = env in ["production", "prod", "prd"]

        # Calculate days since last restore
        days_since_restore = None
        if last_restore_time:
            days_since_restore = (
                datetime.now(timezone.utc) - last_restore_time.replace(tzinfo=timezone.utc)
            ).days

        # SCENARIO 1: Empty Vault (CRITICAL)
        if recovery_points_count == 0:
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Empty Backup Vault",
                    priority="CRITICAL",
                    confidence_score=95,
                    estimated_monthly_savings=0.0,  # No storage cost if empty
                    recommendation=(
                        f"Backup vault '{vault_name}' has no recovery points. "
                        f"Delete vault if no longer needed to simplify management."
                    ),
                    implementation_steps=[
                        "Verify vault is not referenced in backup plans",
                        "Check for any scheduled backup jobs targeting this vault",
                        "Delete vault via AWS CLI: aws backup delete-backup-vault --backup-vault-name ...",
                        "Update infrastructure-as-code to remove vault"
                    ],
                    risk_level="LOW",
                    effort="LOW",
                )
            )

        # SCENARIO 2: Old Recovery Points (HIGH)
        if oldest_recovery_point_age_days > 365:
            # Estimate cost of old backups (assume 50% of storage)
            old_backup_cost = current_cost * 0.5
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Recovery Points Exceed Retention Policy",
                    priority="HIGH",
                    confidence_score=85,
                    estimated_monthly_savings=old_backup_cost,
                    recommendation=(
                        f"Oldest recovery point is {oldest_recovery_point_age_days} days old (>1 year). "
                        f"Review retention policy and delete expired backups. "
                        f"Estimated savings: ${old_backup_cost:.2f}/month (50% of storage)."
                    ),
                    implementation_steps=[
                        "Review backup lifecycle policy for vault",
                        "Identify recovery points exceeding retention requirements",
                        "Delete expired recovery points manually or via lifecycle policy",
                        "Update backup plan with appropriate retention (e.g., 90 days for warm, 365 for cold)"
                    ],
                    risk_level="LOW",
                    effort="LOW",
                )
            )

        # SCENARIO 3: Never Tested (MEDIUM)
        if recovery_points_count > 0 and restore_jobs_30d == 0 and (
            days_since_restore is None or days_since_restore > 180
        ):
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Backups Never Tested (Reliability Risk)",
                    priority="MEDIUM",
                    confidence_score=70,
                    estimated_monthly_savings=0.0,  # Reliability issue, not cost
                    recommendation=(
                        f"Vault has {recovery_points_count} recovery points but no restore jobs "
                        f"in the last 180 days. Test restore process to verify backup viability."
                    ),
                    implementation_steps=[
                        "Select a recent recovery point for testing",
                        "Perform test restore to verify backup integrity",
                        "Document restore procedure for disaster recovery",
                        "Schedule regular restore tests (quarterly recommended)"
                    ],
                    risk_level="HIGH",
                    effort="MEDIUM",
                )
            )

        # SCENARIO 4: Duplicate Backups (MEDIUM)
        # Detected by name patterns suggesting duplication
        if "copy" in vault_name.lower() or "backup" in vault_name.lower() and "-2" in vault_name:
            duplicate_savings = current_cost * 0.5  # Assume 50% duplication
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Potential Duplicate Backup Vault",
                    priority="MEDIUM",
                    confidence_score=65,
                    estimated_monthly_savings=duplicate_savings,
                    recommendation=(
                        f"Vault name suggests potential duplication ('{vault_name}'). "
                        f"Consolidate backups to single vault to reduce cost. "
                        f"Estimated savings: ${duplicate_savings:.2f}/month."
                    ),
                    implementation_steps=[
                        "Review all backup vaults and identify duplicates",
                        "Verify if backups cover same resources",
                        "Update backup plans to use single vault",
                        "Migrate or copy recovery points if needed",
                        "Delete duplicate vault after migration"
                    ],
                    risk_level="MEDIUM",
                    effort="MEDIUM",
                )
            )

        # SCENARIO 5: Warm Storage for Archives (LOW)
        if oldest_recovery_point_age_days > 180 and total_storage_gb > 10:
            # Calculate savings from migrating to cold storage
            warm_cost = self._calculate_backup_vault_monthly_cost(
                storage_gb=total_storage_gb,
                storage_tier="warm",
                region=region,
            )
            cold_cost = self._calculate_backup_vault_monthly_cost(
                storage_gb=total_storage_gb,
                storage_tier="cold",
                region=region,
            )
            savings = warm_cost - cold_cost

            scenarios.append(
                OptimizationScenario(
                    scenario_name="Use Cold Storage for Old Backups",
                    priority="LOW",
                    confidence_score=50,
                    estimated_monthly_savings=savings,
                    recommendation=(
                        f"Vault has {total_storage_gb:.1f} GB of backups with oldest > 180 days. "
                        f"Migrate to cold storage tier to save ${savings:.2f}/month "
                        f"(${warm_cost:.2f} warm  ${cold_cost:.2f} cold)."
                    ),
                    implementation_steps=[
                        "Update backup plan lifecycle to use cold storage after 90 days",
                        "AWS Backup automatically transitions to cold after lifecycle threshold",
                        "Note: Cold storage has longer restore times (hours vs minutes)",
                        "Suitable for compliance/archive backups not needed for quick recovery"
                    ],
                    risk_level="LOW",
                    effort="LOW",
                )
            )

        return scenarios

    async def scan_backup_vaults_aws(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS Backup Vaults for cost intelligence.

        AWS Backup provides centralized backup management for AWS services
        (EC2, RDS, DynamoDB, EFS, S3, etc.).

        CloudWatch Metrics Used:
        - NumberOfBackupJobs (AWS/Backup) - Backup jobs created in last 30 days
        - NumberOfRestoreJobs (AWS/Backup) - Restore jobs created in last 30 days

        API Calls:
        - list_backup_vaults() - List all vaults
        - list_recovery_points_by_backup_vault() - Get recovery points and storage

        Cost Optimization Scenarios:
        1. Empty Vault (CRITICAL) - 0 recovery points  Delete vault
        2. Old Recovery Points (HIGH) - Recovery points >365 days  Delete expired
        3. Never Tested (MEDIUM) - No restore jobs ever  Test viability
        4. Duplicate Backups (MEDIUM) - Same source in multiple vaults  Consolidate
        5. Warm Storage for Archives (LOW) - >180 days  Migrate to Cold

        Returns:
            List of AllCloudResourceData representing ALL backup vaults in the region
        """
        resources = []

        try:
            async with self.session.client("backup", region_name=region) as backup:
                # List all backup vaults
                paginator = backup.get_paginator("list_backup_vaults")
                async for page in paginator.paginate():
                    for vault in page.get("BackupVaultList", []):
                        try:
                            vault_name = vault.get("BackupVaultName", "")
                            vault_arn = vault.get("BackupVaultArn", "")

                            # Get recovery points for this vault
                            recovery_points = []
                            total_storage_bytes = 0
                            oldest_recovery_point_date = None

                            try:
                                rp_paginator = backup.get_paginator(
                                    "list_recovery_points_by_backup_vault"
                                )
                                async for rp_page in rp_paginator.paginate(
                                    BackupVaultName=vault_name
                                ):
                                    for rp in rp_page.get("RecoveryPoints", []):
                                        recovery_points.append(rp)
                                        # Sum storage
                                        backup_size_bytes = rp.get("BackupSizeInBytes", 0)
                                        total_storage_bytes += backup_size_bytes

                                        # Track oldest recovery point
                                        creation_date = rp.get("CreationDate")
                                        if creation_date:
                                            if (
                                                oldest_recovery_point_date is None
                                                or creation_date < oldest_recovery_point_date
                                            ):
                                                oldest_recovery_point_date = creation_date

                            except Exception as e:
                                logger.warning(
                                    "backup_vault.list_recovery_points_failed",
                                    vault_name=vault_name,
                                    error=str(e),
                                )

                            total_storage_gb = total_storage_bytes / (1024**3)
                            recovery_points_count = len(recovery_points)

                            # Calculate age of oldest recovery point
                            oldest_recovery_point_age_days = 0
                            if oldest_recovery_point_date:
                                oldest_recovery_point_age_days = (
                                    datetime.now(timezone.utc)
                                    - oldest_recovery_point_date.replace(tzinfo=timezone.utc)
                                ).days

                            # Get CloudWatch metrics for backup/restore jobs
                            backup_jobs_30d = 0
                            restore_jobs_30d = 0
                            last_restore_time = None

                            try:
                                async with self.session.client(
                                    "cloudwatch", region_name=region
                                ) as cw:
                                    # NumberOfBackupJobs metric
                                    backup_jobs_response = await cw.get_metric_statistics(
                                        Namespace="AWS/Backup",
                                        MetricName="NumberOfBackupJobs",
                                        Dimensions=[
                                            {"Name": "BackupVaultName", "Value": vault_name}
                                        ],
                                        StartTime=datetime.now(timezone.utc) - timedelta(days=30),
                                        EndTime=datetime.now(timezone.utc),
                                        Period=86400,  # 1 day
                                        Statistics=["Sum"],
                                    )
                                    backup_jobs_30d = sum(
                                        dp["Sum"]
                                        for dp in backup_jobs_response.get("Datapoints", [])
                                    )

                                    # NumberOfRestoreJobs metric
                                    restore_jobs_response = await cw.get_metric_statistics(
                                        Namespace="AWS/Backup",
                                        MetricName="NumberOfRestoreJobs",
                                        Dimensions=[
                                            {"Name": "BackupVaultName", "Value": vault_name}
                                        ],
                                        StartTime=datetime.now(timezone.utc) - timedelta(days=30),
                                        EndTime=datetime.now(timezone.utc),
                                        Period=86400,
                                        Statistics=["Sum"],
                                    )
                                    datapoints = restore_jobs_response.get("Datapoints", [])
                                    restore_jobs_30d = sum(dp["Sum"] for dp in datapoints)

                                    # Get last restore time
                                    if datapoints:
                                        sorted_datapoints = sorted(
                                            datapoints, key=lambda x: x["Timestamp"], reverse=True
                                        )
                                        if sorted_datapoints[0]["Sum"] > 0:
                                            last_restore_time = sorted_datapoints[0]["Timestamp"]

                            except Exception as e:
                                logger.warning(
                                    "backup_vault.cloudwatch_metrics_failed",
                                    vault_name=vault_name,
                                    error=str(e),
                                )

                            # Get tags
                            tags = {}
                            try:
                                tags_response = await backup.list_tags(ResourceArn=vault_arn)
                                tags = tags_response.get("Tags", {})
                            except Exception:
                                pass  # Tags not critical

                            # Calculate monthly cost
                            monthly_cost = self._calculate_backup_vault_monthly_cost(
                                storage_gb=total_storage_gb,
                                storage_tier="warm",  # Assume warm by default
                                region=region,
                            )

                            # Calculate optimization scenarios
                            optimization_scenarios = self._calculate_backup_vault_optimization(
                                vault_name=vault_name,
                                vault_arn=vault_arn,
                                recovery_points_count=recovery_points_count,
                                total_storage_gb=total_storage_gb,
                                oldest_recovery_point_age_days=oldest_recovery_point_age_days,
                                last_restore_time=last_restore_time,
                                backup_jobs_30d=int(backup_jobs_30d),
                                restore_jobs_30d=int(restore_jobs_30d),
                                tags=tags,
                                region=region,
                            )

                            # Apply detection rules filtering
                            vault_creation_date = vault.get("CreationDate")
                            resource_age_days = None
                            if isinstance(vault_creation_date, datetime):
                                resource_age_days = (datetime.utcnow() - vault_creation_date.replace(tzinfo=None)).days

                            if not self._should_include_resource("backup_vault", resource_age_days):
                                logger.debug("inventory.backup_vault_filtered", vault_name=vault_name, age=resource_age_days)
                                continue

                            # Prepare resource data
                            resource_data = AllCloudResourceData(
                                resource_type="backup_vault",
                                resource_id=vault_arn,
                                resource_name=vault_name,
                                region=region,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata={
                                    "recovery_points_count": recovery_points_count,
                                    "total_storage_gb": round(total_storage_gb, 2),
                                    "oldest_recovery_point_age_days": oldest_recovery_point_age_days,
                                    "backup_jobs_30d": int(backup_jobs_30d),
                                    "restore_jobs_30d": int(restore_jobs_30d),
                                    "last_restore_time": last_restore_time.isoformat()
                                    if last_restore_time
                                    else None,
                                    "tags": tags,
                                },
                                optimization_scenarios=optimization_scenarios,
                            )

                            resources.append(resource_data)

                            logger.debug(
                                "backup_vault.scanned",
                                vault_name=vault_name,
                                region=region,
                                recovery_points=recovery_points_count,
                                scenarios=len(optimization_scenarios),
                            )

                        except Exception as e:
                            logger.warning(
                                "backup_vault.scan_error",
                                vault_name=vault.get("BackupVaultName", "unknown"),
                                region=region,
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "backup_vault.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # ============================================================
    # RESSOURCE 35 : APP RUNNER SERVICE
    # ============================================================

    def _calculate_app_runner_monthly_cost(
        self,
        vcpu: int,
        memory_gb: int,
        active_hours: int,
        provisioned_hours: int,
        region: str,
    ) -> float:
        """
        Calculate estimated monthly cost for AWS App Runner service.

        App Runner is a fully managed service for containerized web apps and APIs.

        Cost Components:
        1. Active compute: vCPU + Memory (when processing requests)
        2. Provisioned compute: vCPU + Memory (idle state, ready to serve)

        Args:
            vcpu: Number of vCPUs (typically 1)
            memory_gb: Memory in GB (typically 2GB)
            active_hours: Hours in active state (processing requests)
            provisioned_hours: Hours in provisioned state (idle)
            region: AWS region

        Returns:
            Estimated monthly cost in USD

        Example:
            1 vCPU, 2GB, always active (730h/month):
            - Active: (1  $0.064 + 2  $0.007)  730 = $56.94/month
            - Provisioned: (1  $0.0064 + 2  $0.0007)  730 = $5.69/month
            - Total: $62.63/month
        """
        vcpu_active_rate = self.PRICING.get("app_runner_vcpu_active", 0.064)
        vcpu_provisioned_rate = self.PRICING.get("app_runner_vcpu_provisioned", 0.0064)
        memory_active_rate = self.PRICING.get("app_runner_memory_active", 0.007)
        memory_provisioned_rate = self.PRICING.get("app_runner_memory_provisioned", 0.0007)

        # Calculate active cost
        active_cost = (vcpu * vcpu_active_rate + memory_gb * memory_active_rate) * active_hours

        # Calculate provisioned cost
        provisioned_cost = (
            vcpu * vcpu_provisioned_rate + memory_gb * memory_provisioned_rate
        ) * provisioned_hours

        total_cost = active_cost + provisioned_cost

        return round(total_cost, 2)

    def _calculate_app_runner_optimization(
        self,
        service_name: str,
        service_arn: str,
        service_status: str,
        vcpu: int,
        memory_gb: int,
        requests_30d: int,
        active_instances: int,
        cpu_utilization_avg: float,
        memory_utilization_avg: float,
        success_responses_30d: int,
        error_responses_30d: int,
        tags: dict,
        region: str,
    ) -> list[OptimizationScenario]:
        """
        Calculate optimization scenarios for AWS App Runner service.

        App Runner Optimization Scenarios:
        1. Zero Requests (CRITICAL) - 0 requests in 30 days  Delete service
        2. Service Paused (HIGH) - Paused but still provisioned  Resume or delete
        3. Low Traffic (MEDIUM) - < 1000 requests/month  Migrate to Lambda
        4. Oversized Resources (MEDIUM) - Low CPU/Memory utilization  Downgrade
        5. Dev/Test Always Running (LOW) - Non-prod always active  Enable auto-pause

        Args:
            service_name: App Runner service name
            service_arn: Service ARN
            service_status: Service status (RUNNING, PAUSED, etc.)
            vcpu: Number of vCPUs configured
            memory_gb: Memory in GB
            requests_30d: Total requests in last 30 days
            active_instances: Number of active instances
            cpu_utilization_avg: Average CPU utilization (%)
            memory_utilization_avg: Average memory utilization (%)
            success_responses_30d: Successful responses (2xx) in 30 days
            error_responses_30d: Error responses (4xx/5xx) in 30 days
            tags: Resource tags
            region: AWS region

        Returns:
            List of optimization scenarios
        """
        scenarios = []

        # Calculate current monthly cost (assuming always active)
        monthly_hours = 730
        current_cost = self._calculate_app_runner_monthly_cost(
            vcpu=vcpu,
            memory_gb=memory_gb,
            active_hours=monthly_hours,  # Assume always active
            provisioned_hours=0,
            region=region,
        )

        env = tags.get("Environment", tags.get("environment", "")).lower()
        is_production = env in ["production", "prod", "prd"]

        # SCENARIO 1: Zero Requests (CRITICAL)
        if requests_30d == 0:
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Zero Traffic (No Requests)",
                    priority="CRITICAL",
                    confidence_score=95,
                    estimated_monthly_savings=current_cost,
                    recommendation=(
                        f"App Runner service has received 0 requests in the last 30 days. "
                        f"Delete service if no longer needed. "
                        f"Savings: ${current_cost:.2f}/month."
                    ),
                    implementation_steps=[
                        "Verify service is not used in production",
                        "Check DNS records and application code for references",
                        "Pause service first to test impact",
                        "Delete service via AWS CLI: aws apprunner delete-service --service-arn ..."
                    ],
                    risk_level="LOW",
                    effort="LOW",
                )
            )

        # SCENARIO 2: Service Paused (HIGH)
        if service_status.upper() == "PAUSED":
            # When paused, only pay provisioned rate (much cheaper)
            paused_cost = self._calculate_app_runner_monthly_cost(
                vcpu=vcpu,
                memory_gb=memory_gb,
                active_hours=0,
                provisioned_hours=monthly_hours,
                region=region,
            )
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Service Paused but Still Charged",
                    priority="HIGH",
                    confidence_score=85,
                    estimated_monthly_savings=paused_cost,
                    recommendation=(
                        f"Service is paused but still incurring provisioned compute costs "
                        f"(${paused_cost:.2f}/month). Resume if needed or delete to save cost."
                    ),
                    implementation_steps=[
                        "If service no longer needed, delete it completely",
                        "If temporarily unused, keep paused (low cost)",
                        "If needed, resume service to serve traffic",
                        "Consider migrating to Lambda if infrequent use"
                    ],
                    risk_level="LOW",
                    effort="LOW",
                )
            )

        # SCENARIO 3: Low Traffic (MEDIUM)
        if 0 < requests_30d < 1000 and not is_production:
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Low Traffic (Migrate to Lambda)",
                    priority="MEDIUM",
                    confidence_score=70,
                    estimated_monthly_savings=current_cost,
                    recommendation=(
                        f"Service has low traffic ({requests_30d} requests/month). "
                        f"Consider migrating to AWS Lambda for serverless cost model. "
                        f"Lambda free tier: 1M requests/month + 400,000 GB-seconds."
                    ),
                    implementation_steps=[
                        "Evaluate if workload is suitable for Lambda (< 15min execution, stateless)",
                        "Package application as Lambda function",
                        "Use API Gateway or Lambda Function URLs for HTTP access",
                        "Test Lambda version thoroughly before switching traffic",
                        "Delete App Runner service after migration"
                    ],
                    risk_level="MEDIUM",
                    effort="HIGH",
                )
            )

        # SCENARIO 4: Oversized Resources (MEDIUM)
        if cpu_utilization_avg < 20 and memory_utilization_avg < 20 and requests_30d > 0:
            # Estimate cost with half resources
            downgraded_vcpu = max(0.25, vcpu / 2)  # Min 0.25 vCPU
            downgraded_memory = max(0.5, memory_gb / 2)  # Min 0.5 GB
            downgraded_cost = self._calculate_app_runner_monthly_cost(
                vcpu=int(downgraded_vcpu) if downgraded_vcpu >= 1 else 1,
                memory_gb=int(downgraded_memory) if downgraded_memory >= 1 else 2,
                active_hours=monthly_hours,
                provisioned_hours=0,
                region=region,
            )
            savings = current_cost - downgraded_cost

            scenarios.append(
                OptimizationScenario(
                    scenario_name="Oversized Resources (Low Utilization)",
                    priority="MEDIUM",
                    confidence_score=65,
                    estimated_monthly_savings=savings,
                    recommendation=(
                        f"Service has low resource utilization (CPU: {cpu_utilization_avg:.1f}%, "
                        f"Memory: {memory_utilization_avg:.1f}%). Downgrade to smaller instance size. "
                        f"Current: {vcpu} vCPU, {memory_gb}GB  Suggested: {downgraded_vcpu} vCPU, {downgraded_memory}GB. "
                        f"Savings: ${savings:.2f}/month."
                    ),
                    implementation_steps=[
                        "Update App Runner service configuration to reduce CPU/memory",
                        "Test with smaller instance size in non-production first",
                        "Monitor performance metrics after downgrade",
                        "Adjust configuration if performance degrades"
                    ],
                    risk_level="LOW",
                    effort="LOW",
                )
            )

        # SCENARIO 5: Dev/Test Always Running (LOW)
        if not is_production and requests_30d > 0 and service_status == "RUNNING":
            # Estimate cost savings with auto-pause (assume 50% uptime for dev/test)
            auto_pause_cost = self._calculate_app_runner_monthly_cost(
                vcpu=vcpu,
                memory_gb=memory_gb,
                active_hours=monthly_hours * 0.5,
                provisioned_hours=monthly_hours * 0.5,
                region=region,
            )
            savings = current_cost - auto_pause_cost

            scenarios.append(
                OptimizationScenario(
                    scenario_name="Dev/Test Always Running (Enable Auto-Pause)",
                    priority="LOW",
                    confidence_score=50,
                    estimated_monthly_savings=savings,
                    recommendation=(
                        f"Non-production service is always running. Enable auto-pause for dev/test "
                        f"to reduce costs during idle periods. "
                        f"Estimated savings: ${savings:.2f}/month (50% reduction with auto-pause)."
                    ),
                    implementation_steps=[
                        "App Runner doesn't have native auto-pause (unlike Lambda)",
                        "Consider using EventBridge + Lambda to pause/resume on schedule",
                        "Pause during nights/weekends if not needed",
                        "Alternative: Migrate to Lambda for true serverless model"
                    ],
                    risk_level="LOW",
                    effort="MEDIUM",
                )
            )

        return scenarios

    async def scan_app_runner_services(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS App Runner services for cost intelligence.

        App Runner is a fully managed service for deploying containerized web apps.

        CloudWatch Metrics Used:
        - Requests (AWS/AppRunner) - Total requests in last 30 days
        - ActiveInstances (AWS/AppRunner) - Number of active instances (7 days)
        - 2xxStatusResponses (AWS/AppRunner) - Successful responses (30 days)
        - 4xxStatusResponses (AWS/AppRunner) - Client errors (30 days)
        - CPUUtilization (AWS/AppRunner) - CPU usage % (7 days average)
        - MemoryUtilization (AWS/AppRunner) - Memory usage % (7 days average)

        API Calls:
        - list_services() - List all App Runner services
        - describe_service() - Get service details (CPU, memory, status)

        Cost Optimization Scenarios:
        1. Zero Requests (CRITICAL) - 0 requests in 30 days  Delete service
        2. Service Paused (HIGH) - Paused but still provisioned  Resume or delete
        3. Low Traffic (MEDIUM) - < 1000 requests/month  Migrate to Lambda
        4. Oversized Resources (MEDIUM) - Low CPU/Memory utilization  Downgrade
        5. Dev/Test Always Running (LOW) - Non-prod always active  Enable auto-pause

        Returns:
            List of AllCloudResourceData representing ALL App Runner services in the region
        """
        resources = []

        try:
            async with self.session.client("apprunner", region_name=region) as apprunner:
                # List all App Runner services
                paginator = apprunner.get_paginator("list_services")
                async for page in paginator.paginate():
                    for service_summary in page.get("ServiceSummaryList", []):
                        try:
                            service_arn = service_summary.get("ServiceArn", "")
                            service_name = service_summary.get("ServiceName", "")

                            # Get detailed service information
                            try:
                                service_details = await apprunner.describe_service(
                                    ServiceArn=service_arn
                                )
                                service = service_details.get("Service", {})
                            except Exception as e:
                                logger.warning(
                                    "app_runner.describe_failed",
                                    service_name=service_name,
                                    error=str(e),
                                )
                                continue

                            # Extract configuration
                            service_status = service.get("Status", "UNKNOWN")
                            instance_config = service.get("InstanceConfiguration", {})
                            vcpu = int(instance_config.get("Cpu", "1 vCPU").split()[0])
                            memory_str = instance_config.get("Memory", "2 GB")
                            memory_gb = int(memory_str.split()[0])

                            # Get tags
                            tags_list = service_summary.get("Tags", [])
                            tags = {tag["Key"]: tag["Value"] for tag in tags_list}

                            # Get CloudWatch metrics
                            requests_30d = 0
                            active_instances = 0
                            cpu_utilization_avg = 0.0
                            memory_utilization_avg = 0.0
                            success_responses_30d = 0
                            error_responses_30d = 0

                            try:
                                async with self.session.client(
                                    "cloudwatch", region_name=region
                                ) as cw:
                                    # Requests metric (30 days)
                                    requests_response = await cw.get_metric_statistics(
                                        Namespace="AWS/AppRunner",
                                        MetricName="Requests",
                                        Dimensions=[
                                            {"Name": "ServiceName", "Value": service_name}
                                        ],
                                        StartTime=datetime.now(timezone.utc) - timedelta(days=30),
                                        EndTime=datetime.now(timezone.utc),
                                        Period=86400,
                                        Statistics=["Sum"],
                                    )
                                    requests_30d = sum(
                                        dp["Sum"]
                                        for dp in requests_response.get("Datapoints", [])
                                    )

                                    # ActiveInstances metric (7 days average)
                                    instances_response = await cw.get_metric_statistics(
                                        Namespace="AWS/AppRunner",
                                        MetricName="ActiveInstances",
                                        Dimensions=[
                                            {"Name": "ServiceName", "Value": service_name}
                                        ],
                                        StartTime=datetime.now(timezone.utc) - timedelta(days=7),
                                        EndTime=datetime.now(timezone.utc),
                                        Period=3600,
                                        Statistics=["Average"],
                                    )
                                    instances_datapoints = instances_response.get("Datapoints", [])
                                    if instances_datapoints:
                                        active_instances = sum(
                                            dp["Average"] for dp in instances_datapoints
                                        ) / len(instances_datapoints)

                                    # CPUUtilization metric (7 days average)
                                    cpu_response = await cw.get_metric_statistics(
                                        Namespace="AWS/AppRunner",
                                        MetricName="CPUUtilization",
                                        Dimensions=[
                                            {"Name": "ServiceName", "Value": service_name}
                                        ],
                                        StartTime=datetime.now(timezone.utc) - timedelta(days=7),
                                        EndTime=datetime.now(timezone.utc),
                                        Period=3600,
                                        Statistics=["Average"],
                                    )
                                    cpu_datapoints = cpu_response.get("Datapoints", [])
                                    if cpu_datapoints:
                                        cpu_utilization_avg = sum(
                                            dp["Average"] for dp in cpu_datapoints
                                        ) / len(cpu_datapoints)

                                    # MemoryUtilization metric (7 days average)
                                    memory_response = await cw.get_metric_statistics(
                                        Namespace="AWS/AppRunner",
                                        MetricName="MemoryUtilization",
                                        Dimensions=[
                                            {"Name": "ServiceName", "Value": service_name}
                                        ],
                                        StartTime=datetime.now(timezone.utc) - timedelta(days=7),
                                        EndTime=datetime.now(timezone.utc),
                                        Period=3600,
                                        Statistics=["Average"],
                                    )
                                    memory_datapoints = memory_response.get("Datapoints", [])
                                    if memory_datapoints:
                                        memory_utilization_avg = sum(
                                            dp["Average"] for dp in memory_datapoints
                                        ) / len(memory_datapoints)

                                    # 2xxStatusResponses metric (30 days)
                                    success_response = await cw.get_metric_statistics(
                                        Namespace="AWS/AppRunner",
                                        MetricName="2xxStatusResponses",
                                        Dimensions=[
                                            {"Name": "ServiceName", "Value": service_name}
                                        ],
                                        StartTime=datetime.now(timezone.utc) - timedelta(days=30),
                                        EndTime=datetime.now(timezone.utc),
                                        Period=86400,
                                        Statistics=["Sum"],
                                    )
                                    success_responses_30d = sum(
                                        dp["Sum"]
                                        for dp in success_response.get("Datapoints", [])
                                    )

                                    # 4xxStatusResponses metric (30 days)
                                    error_4xx_response = await cw.get_metric_statistics(
                                        Namespace="AWS/AppRunner",
                                        MetricName="4xxStatusResponses",
                                        Dimensions=[
                                            {"Name": "ServiceName", "Value": service_name}
                                        ],
                                        StartTime=datetime.now(timezone.utc) - timedelta(days=30),
                                        EndTime=datetime.now(timezone.utc),
                                        Period=86400,
                                        Statistics=["Sum"],
                                    )
                                    error_responses_30d += sum(
                                        dp["Sum"]
                                        for dp in error_4xx_response.get("Datapoints", [])
                                    )

                            except Exception as e:
                                logger.warning(
                                    "app_runner.cloudwatch_metrics_failed",
                                    service_name=service_name,
                                    error=str(e),
                                )

                            # Calculate monthly cost (assume always active)
                            monthly_cost = self._calculate_app_runner_monthly_cost(
                                vcpu=vcpu,
                                memory_gb=memory_gb,
                                active_hours=730,  # Full month
                                provisioned_hours=0,
                                region=region,
                            )

                            # Calculate optimization scenarios
                            optimization_scenarios = self._calculate_app_runner_optimization(
                                service_name=service_name,
                                service_arn=service_arn,
                                service_status=service_status,
                                vcpu=vcpu,
                                memory_gb=memory_gb,
                                requests_30d=int(requests_30d),
                                active_instances=int(active_instances),
                                cpu_utilization_avg=cpu_utilization_avg,
                                memory_utilization_avg=memory_utilization_avg,
                                success_responses_30d=int(success_responses_30d),
                                error_responses_30d=int(error_responses_30d),
                                tags=tags,
                                region=region,
                            )

                            # Apply detection rules filtering
                            resource_age_days = None
                            if isinstance(creation_time, datetime):
                                resource_age_days = (datetime.utcnow() - creation_time.replace(tzinfo=None)).days

                            if not self._should_include_resource("app_runner_service", resource_age_days):
                                logger.debug("inventory.app_runner_filtered", service_name=service_name, age=resource_age_days)
                                continue

                            # Prepare resource data
                            resource_data = AllCloudResourceData(
                                resource_type="app_runner_service",
                                resource_id=service_arn,
                                resource_name=service_name,
                                region=region,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata={
                                    "service_status": service_status,
                                    "vcpu": vcpu,
                                    "memory_gb": memory_gb,
                                    "requests_30d": int(requests_30d),
                                    "active_instances": int(active_instances),
                                    "cpu_utilization_avg": round(cpu_utilization_avg, 2),
                                    "memory_utilization_avg": round(memory_utilization_avg, 2),
                                    "success_responses_30d": int(success_responses_30d),
                                    "error_responses_30d": int(error_responses_30d),
                                    "tags": tags,
                                },
                                optimization_scenarios=optimization_scenarios,
                            )

                            resources.append(resource_data)

                            logger.debug(
                                "app_runner.service_scanned",
                                service_name=service_name,
                                region=region,
                                scenarios=len(optimization_scenarios),
                            )

                        except Exception as e:
                            logger.warning(
                                "app_runner.service_scan_error",
                                service_name=service_summary.get("ServiceName", "unknown"),
                                region=region,
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "app_runner.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # ============================================================
    # RESSOURCE 36 : LIGHTSAIL INSTANCE
    # ============================================================

    def _calculate_lightsail_monthly_cost(self, bundle_id: str, region: str) -> float:
        """
        Calculate estimated monthly cost for AWS Lightsail instance.

        Lightsail offers simple, fixed-price VPS instances with predictable pricing.

        Cost Components:
        - Fixed monthly price based on bundle (nano, micro, small, medium, large)
        - Includes: compute, storage (SSD), data transfer (1TB-7TB depending on plan)

        Args:
            bundle_id: Bundle identifier (e.g., 'nano_2_0', 'micro_2_0', 'small_2_0')
            region: AWS region

        Returns:
            Estimated monthly cost in USD

        Example:
            Micro bundle (1GB RAM, 1 vCPU): $5.00/month
            Medium bundle (4GB RAM, 2 vCPU): $20.00/month
        """
        # Extract bundle size from bundle_id (e.g., 'nano_2_0'  'nano')
        bundle_size = bundle_id.split("_")[0] if "_" in bundle_id else bundle_id.lower()

        # Map bundle to pricing
        pricing_map = {
            "nano": self.PRICING.get("lightsail_nano", 3.50),
            "micro": self.PRICING.get("lightsail_micro", 5.00),
            "small": self.PRICING.get("lightsail_small", 10.00),
            "medium": self.PRICING.get("lightsail_medium", 20.00),
            "large": self.PRICING.get("lightsail_large", 40.00),
            "xlarge": 80.00,  # Not in PRICING yet
            "2xlarge": 160.00,  # Not in PRICING yet
        }

        cost = pricing_map.get(bundle_size, 10.00)  # Default to small if unknown

        return round(cost, 2)

    def _calculate_lightsail_optimization(
        self,
        instance_name: str,
        instance_arn: str,
        bundle_id: str,
        instance_state: str,
        created_at: datetime,
        cpu_utilization_avg: float,
        network_in_30d: float,
        network_out_30d: float,
        tags: dict,
        region: str,
    ) -> list[OptimizationScenario]:
        """
        Calculate optimization scenarios for AWS Lightsail instance.

        Lightsail Optimization Scenarios:
        1. Instance Stopped Long-Term (CRITICAL) - Stopped >30 days  Still charged!
        2. Very Low CPU (<5% for 7 days) (HIGH) - Unused instance  Delete or downgrade
        3. Oversized Bundle (MEDIUM) - <20% CPU+RAM usage  Downgrade bundle
        4. No Network Traffic (MEDIUM) - 0 bytes in/out for 30 days  Delete unused
        5. Dev/Test Not Needed 24/7 (LOW) - Non-prod  Migrate to EC2 with scheduling

        Args:
            instance_name: Lightsail instance name
            instance_arn: Instance ARN
            bundle_id: Bundle type (nano, micro, small, etc.)
            instance_state: Instance state (running, stopped, pending, etc.)
            created_at: Instance creation time
            cpu_utilization_avg: Average CPU utilization (%) over 7 days
            network_in_30d: Network bytes received in last 30 days
            network_out_30d: Network bytes sent in last 30 days
            tags: Resource tags
            region: AWS region

        Returns:
            List of optimization scenarios
        """
        scenarios = []

        current_cost = self._calculate_lightsail_monthly_cost(
            bundle_id=bundle_id, region=region
        )

        # Calculate age
        age_days = (datetime.now(timezone.utc) - created_at.replace(tzinfo=timezone.utc)).days

        env = tags.get("Environment", tags.get("environment", "")).lower()
        is_production = env in ["production", "prod", "prd"]

        # SCENARIO 1: Instance Stopped Long-Term (CRITICAL)
        # IMPORTANT: Lightsail charges even when stopped (unlike EC2)
        if instance_state.lower() == "stopped" and age_days >= 30:
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Instance Stopped but Still Charged",
                    priority="CRITICAL",
                    confidence_score=95,
                    estimated_monthly_savings=current_cost,
                    recommendation=(
                        f"Lightsail instance '{instance_name}' has been stopped for {age_days} days "
                        f"but is still charged ${current_cost}/month. Lightsail charges even when stopped. "
                        f"Delete instance if no longer needed."
                    ),
                    implementation_steps=[
                        "Take snapshot of instance for backup if needed",
                        "Verify instance is not needed for production",
                        "Delete instance via AWS CLI: aws lightsail delete-instance --instance-name ...",
                        "Consider EC2 if need stop/start functionality without charges"
                    ],
                    risk_level="LOW",
                    effort="LOW",
                )
            )

        # SCENARIO 2: Very Low CPU Utilization (HIGH)
        if cpu_utilization_avg < 5 and instance_state.lower() == "running":
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Very Low CPU Utilization",
                    priority="HIGH",
                    confidence_score=85,
                    estimated_monthly_savings=current_cost,
                    recommendation=(
                        f"Instance has very low CPU utilization ({cpu_utilization_avg:.1f}% over 7 days). "
                        f"Consider deleting if unused or downgrading to smaller bundle. "
                        f"Savings: ${current_cost:.2f}/month if deleted."
                    ),
                    implementation_steps=[
                        "Verify application is not running or has minimal load",
                        "Check if instance can be deleted or consolidated",
                        "If needed, downgrade to nano bundle ($3.50/month)",
                        "Delete instance if completely unused"
                    ],
                    risk_level="LOW",
                    effort="LOW",
                )
            )

        # SCENARIO 3: Oversized Bundle (MEDIUM)
        if (
            cpu_utilization_avg < 20
            and instance_state.lower() == "running"
            and "large" in bundle_id.lower()
        ):
            # Estimate savings from downgrading (e.g., large  medium or small)
            downgrade_cost = current_cost * 0.5  # Assume 50% cost reduction
            savings = current_cost - downgrade_cost

            scenarios.append(
                OptimizationScenario(
                    scenario_name="Oversized Bundle (Underutilized)",
                    priority="MEDIUM",
                    confidence_score=70,
                    estimated_monthly_savings=savings,
                    recommendation=(
                        f"Instance is on '{bundle_id}' bundle (${current_cost}/month) "
                        f"but has low CPU utilization ({cpu_utilization_avg:.1f}%). "
                        f"Downgrade to smaller bundle to save ~${savings:.2f}/month."
                    ),
                    implementation_steps=[
                        "Create snapshot of current instance",
                        "Create new instance with smaller bundle",
                        "Migrate data and applications",
                        "Update DNS/load balancer to point to new instance",
                        "Delete old instance after verification"
                    ],
                    risk_level="MEDIUM",
                    effort="MEDIUM",
                )
            )

        # SCENARIO 4: No Network Traffic (MEDIUM)
        if (
            network_in_30d == 0
            and network_out_30d == 0
            and instance_state.lower() == "running"
        ):
            scenarios.append(
                OptimizationScenario(
                    scenario_name="No Network Traffic (Unused Service)",
                    priority="MEDIUM",
                    confidence_score=65,
                    estimated_monthly_savings=current_cost,
                    recommendation=(
                        f"Instance has 0 bytes network traffic in/out over last 30 days. "
                        f"Service appears unused. Delete instance to save ${current_cost:.2f}/month."
                    ),
                    implementation_steps=[
                        "Verify no services are actively using this instance",
                        "Check application logs for activity",
                        "Take snapshot for backup before deletion",
                        "Delete instance if truly unused"
                    ],
                    risk_level="LOW",
                    effort="LOW",
                )
            )

        # SCENARIO 5: Dev/Test Not Needed 24/7 (LOW)
        if not is_production and instance_state.lower() == "running" and age_days >= 7:
            # Lightsail doesn't support stop/start without charges
            # Suggest migration to EC2 for better cost control
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Dev/Test Instance Always Running",
                    priority="LOW",
                    confidence_score=50,
                    estimated_monthly_savings=current_cost * 0.5,  # Estimate 50% savings with scheduling
                    recommendation=(
                        f"Non-production instance running 24/7 (${current_cost}/month). "
                        f"Lightsail charges even when stopped. Consider migrating to EC2 "
                        f"for stop/start scheduling to save ~50% on nights/weekends."
                    ),
                    implementation_steps=[
                        "Evaluate if workload is suitable for EC2 migration",
                        "Create EC2 instance with equivalent specs",
                        "Set up EventBridge + Lambda for start/stop scheduling",
                        "Migrate application to EC2 instance",
                        "Delete Lightsail instance after migration"
                    ],
                    risk_level="MEDIUM",
                    effort="HIGH",
                )
            )

        return scenarios

    async def scan_lightsail_instances(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS Lightsail instances for cost intelligence.

        Lightsail provides simple, fixed-price VPS instances for developers.

        CloudWatch Metrics Used:
        - CPUUtilization (Lightsail) - CPU usage % over 7 days
        - NetworkIn (Lightsail) - Bytes received over 30 days
        - NetworkOut (Lightsail) - Bytes sent over 30 days

        API Calls:
        - get_instances() - List all Lightsail instances
        - get_instance_metric_data() - Get CloudWatch metrics via Lightsail API

        Cost Optimization Scenarios:
        1. Instance Stopped Long-Term (CRITICAL) - Stopped >30 days  Still charged!
        2. Very Low CPU (<5%) (HIGH) - Unused instance  Delete or downgrade
        3. Oversized Bundle (MEDIUM) - <20% CPU usage  Downgrade bundle
        4. No Network Traffic (MEDIUM) - 0 bytes in/out for 30 days  Delete
        5. Dev/Test Always Running (LOW) - Non-prod  Migrate to EC2 with scheduling

        Returns:
            List of AllCloudResourceData representing ALL Lightsail instances in the region
        """
        resources = []

        try:
            async with self.session.client("lightsail", region_name=region) as lightsail:
                # Get all Lightsail instances
                response = await lightsail.get_instances()
                instances = response.get("instances", [])

                for instance in instances:
                    try:
                        instance_name = instance.get("name", "")
                        instance_arn = instance.get("arn", "")
                        bundle_id = instance.get("bundleId", "")
                        instance_state = instance.get("state", {}).get("name", "unknown")
                        created_at = instance.get("createdAt")
                        tags_list = instance.get("tags", [])
                        tags = {tag["key"]: tag.get("value", "") for tag in tags_list}

                        # Get CloudWatch metrics
                        cpu_utilization_avg = 0.0
                        network_in_30d = 0.0
                        network_out_30d = 0.0

                        try:
                            # CPUUtilization metric (7 days average)
                            cpu_response = await lightsail.get_instance_metric_data(
                                instanceName=instance_name,
                                metricName="CPUUtilization",
                                period=3600,  # 1 hour
                                startTime=datetime.now(timezone.utc) - timedelta(days=7),
                                endTime=datetime.now(timezone.utc),
                                unit="Percent",
                                statistics=["Average"],
                            )
                            cpu_datapoints = cpu_response.get("metricData", [])
                            if cpu_datapoints:
                                cpu_utilization_avg = sum(
                                    dp["average"] for dp in cpu_datapoints
                                ) / len(cpu_datapoints)

                            # NetworkIn metric (30 days total)
                            network_in_response = await lightsail.get_instance_metric_data(
                                instanceName=instance_name,
                                metricName="NetworkIn",
                                period=86400,  # 1 day
                                startTime=datetime.now(timezone.utc) - timedelta(days=30),
                                endTime=datetime.now(timezone.utc),
                                unit="Bytes",
                                statistics=["Sum"],
                            )
                            network_in_datapoints = network_in_response.get("metricData", [])
                            network_in_30d = sum(dp["sum"] for dp in network_in_datapoints)

                            # NetworkOut metric (30 days total)
                            network_out_response = await lightsail.get_instance_metric_data(
                                instanceName=instance_name,
                                metricName="NetworkOut",
                                period=86400,
                                startTime=datetime.now(timezone.utc) - timedelta(days=30),
                                endTime=datetime.now(timezone.utc),
                                unit="Bytes",
                                statistics=["Sum"],
                            )
                            network_out_datapoints = network_out_response.get("metricData", [])
                            network_out_30d = sum(dp["sum"] for dp in network_out_datapoints)

                        except Exception as e:
                            logger.warning(
                                "lightsail.metrics_failed",
                                instance_name=instance_name,
                                error=str(e),
                            )

                        # Calculate monthly cost
                        monthly_cost = self._calculate_lightsail_monthly_cost(
                            bundle_id=bundle_id, region=region
                        )

                        # Calculate optimization scenarios
                        optimization_scenarios = self._calculate_lightsail_optimization(
                            instance_name=instance_name,
                            instance_arn=instance_arn,
                            bundle_id=bundle_id,
                            instance_state=instance_state,
                            created_at=created_at,
                            cpu_utilization_avg=cpu_utilization_avg,
                            network_in_30d=network_in_30d,
                            network_out_30d=network_out_30d,
                            tags=tags,
                            region=region,
                        )

                        # Prepare resource data
                        resource_data = AllCloudResourceData(
                            resource_type="lightsail_instance",
                            resource_id=instance_arn,
                            resource_name=instance_name,
                            region=region,
                            estimated_monthly_cost=monthly_cost,
                            currency="USD",
                            resource_metadata={
                                "bundle_id": bundle_id,
                                "instance_state": instance_state,
                                "created_at": created_at.isoformat() if created_at else None,
                                "cpu_utilization_avg": round(cpu_utilization_avg, 2),
                                "network_in_30d_gb": round(network_in_30d / (1024**3), 2),
                                "network_out_30d_gb": round(network_out_30d / (1024**3), 2),
                                "tags": tags,
                            },
                            optimization_scenarios=optimization_scenarios,
                        )

                        resources.append(resource_data)

                        logger.debug(
                            "lightsail.instance_scanned",
                            instance_name=instance_name,
                            region=region,
                            scenarios=len(optimization_scenarios),
                        )

                    except Exception as e:
                        logger.warning(
                            "lightsail.instance_scan_error",
                            instance_name=instance.get("name", "unknown"),
                            region=region,
                            error=str(e),
                        )
                        continue

        except Exception as e:
            logger.error(
                "lightsail.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    # ============================================================
    # RESSOURCE 37 : WORKSPACES (DESKTOP AS A SERVICE)
    # ============================================================

    def _calculate_workspaces_monthly_cost(
        self, compute_type: str, running_mode: str, region: str
    ) -> float:
        """
        Calculate estimated monthly cost for Amazon WorkSpaces.

        WorkSpaces provides managed, secure cloud desktops (Desktop-as-a-Service).

        Cost Components:
        - Always-On mode: Fixed monthly fee (VALUE: $25, STANDARD: $35, PERFORMANCE: $60, POWER: $80)
        - AutoStop mode: Monthly fee + hourly usage fee (less predictable)

        Args:
            compute_type: WorkSpace compute type (VALUE, STANDARD, PERFORMANCE, POWER)
            running_mode: Running mode (ALWAYS_ON or AUTO_STOP)
            region: AWS region

        Returns:
            Estimated monthly cost in USD (Always-On pricing)

        Example:
            STANDARD Always-On: $35/month
            POWER Always-On: $80/month
        """
        # Map compute type to Always-On pricing
        pricing_map = {
            "VALUE": self.PRICING.get("workspaces_value", 25.00),
            "STANDARD": self.PRICING.get("workspaces_standard", 35.00),
            "PERFORMANCE": self.PRICING.get("workspaces_performance", 60.00),
            "POWER": self.PRICING.get("workspaces_power", 80.00),
            "POWERPRO": 120.00,  # Not in PRICING yet
            "GRAPHICS": 145.00,  # Not in PRICING yet
        }

        cost = pricing_map.get(compute_type.upper(), 35.00)  # Default to STANDARD

        # Note: AutoStop pricing is complex (base fee + hourly), we estimate Always-On
        # For optimization, we suggest AutoStop when appropriate

        return round(cost, 2)

    def _calculate_workspaces_optimization(
        self,
        workspace_id: str,
        workspace_state: str,
        compute_type: str,
        running_mode: str,
        last_known_user_connection_timestamp: datetime | None,
        user_connected_time_hours: float,
        created_time: datetime,
        tags: dict,
        region: str,
    ) -> list[OptimizationScenario]:
        """
        Calculate optimization scenarios for Amazon WorkSpaces.

        WorkSpaces Optimization Scenarios:
        1. WorkSpace Stopped Long-Term (CRITICAL) - Stopped >30 days  Still charged (Always-On)
        2. Never Connected (HIGH) - 0 sessions in 30 days  Delete unused WorkSpace
        3. Low Usage AutoStop Better (MEDIUM) - <10h/month  AutoStop cheaper
        4. High Usage Always-On Optimal (MEDIUM) - >40h/week  Confirm Always-On needed
        5. Oversized Compute Type (LOW) - VALUE sufficient  Downgrade

        Args:
            workspace_id: WorkSpace ID
            workspace_state: State (AVAILABLE, STOPPED, ERROR, etc.)
            compute_type: Compute type (VALUE, STANDARD, PERFORMANCE, POWER)
            running_mode: Running mode (ALWAYS_ON or AUTO_STOP)
            last_known_user_connection_timestamp: Last user connection time
            user_connected_time_hours: Total connected hours in last 30 days
            created_time: WorkSpace creation time
            tags: Resource tags
            region: AWS region

        Returns:
            List of optimization scenarios
        """
        scenarios = []

        current_cost = self._calculate_workspaces_monthly_cost(
            compute_type=compute_type, running_mode=running_mode, region=region
        )

        # Calculate days since creation
        age_days = (datetime.now(timezone.utc) - created_time.replace(tzinfo=timezone.utc)).days

        # Calculate days since last connection
        days_since_connection = None
        if last_known_user_connection_timestamp:
            days_since_connection = (
                datetime.now(timezone.utc)
                - last_known_user_connection_timestamp.replace(tzinfo=timezone.utc)
            ).days

        env = tags.get("Environment", tags.get("environment", "")).lower()
        is_production = env in ["production", "prod", "prd"]

        # SCENARIO 1: WorkSpace Stopped Long-Term (CRITICAL)
        if workspace_state == "STOPPED" and age_days >= 30 and running_mode == "ALWAYS_ON":
            scenarios.append(
                OptimizationScenario(
                    scenario_name="WorkSpace Stopped but Still Charged",
                    priority="CRITICAL",
                    confidence_score=95,
                    estimated_monthly_savings=current_cost,
                    recommendation=(
                        f"WorkSpace '{workspace_id}' has been stopped for {age_days} days "
                        f"but Always-On mode still charges ${current_cost}/month. "
                        f"Delete WorkSpace if no longer needed."
                    ),
                    implementation_steps=[
                        "Verify WorkSpace is not needed by user",
                        "Contact user to confirm before deletion",
                        "Take snapshot/image if data preservation needed",
                        "Delete WorkSpace via AWS CLI: aws workspaces terminate-workspaces --workspace-ids ..."
                    ],
                    risk_level="LOW",
                    effort="LOW",
                )
            )

        # SCENARIO 2: Never Connected (HIGH)
        if days_since_connection and days_since_connection >= 30:
            scenarios.append(
                OptimizationScenario(
                    scenario_name="WorkSpace Never Used (No Connections)",
                    priority="HIGH",
                    confidence_score=85,
                    estimated_monthly_savings=current_cost,
                    recommendation=(
                        f"WorkSpace has not been connected in {days_since_connection} days. "
                        f"User may no longer need this desktop. "
                        f"Savings: ${current_cost:.2f}/month if deleted."
                    ),
                    implementation_steps=[
                        "Contact user to verify if WorkSpace still needed",
                        "If user left company, delete immediately",
                        "If user doesn't remember, delete after 7-day warning",
                        "Terminate WorkSpace to stop charges"
                    ],
                    risk_level="LOW",
                    effort="LOW",
                )
            )

        # SCENARIO 3: Low Usage - AutoStop Better (MEDIUM)
        if (
            running_mode == "ALWAYS_ON"
            and user_connected_time_hours < 10
            and age_days >= 7
        ):
            # AutoStop pricing example (STANDARD): $9.75/month + $0.36/hour
            # 10h usage: $9.75 + (10  $0.36) = $13.35 vs $35 Always-On
            autostop_estimate = current_cost * 0.4  # Rough estimate (40% of Always-On)
            savings = current_cost - autostop_estimate

            scenarios.append(
                OptimizationScenario(
                    scenario_name="Low Usage (AutoStop Mode Better)",
                    priority="MEDIUM",
                    confidence_score=70,
                    estimated_monthly_savings=savings,
                    recommendation=(
                        f"WorkSpace has low usage ({user_connected_time_hours:.1f}h in last 30 days). "
                        f"Switching to AutoStop mode would save ~${savings:.2f}/month. "
                        f"AutoStop recommended for <40h/month usage."
                    ),
                    implementation_steps=[
                        "Review user's typical usage pattern (is low usage temporary?)",
                        "Switch WorkSpace to AutoStop mode if usage remains low",
                        "Notify user that WorkSpace will auto-stop after inactivity",
                        "Monitor costs after switching to verify savings"
                    ],
                    risk_level="LOW",
                    effort="LOW",
                )
            )

        # SCENARIO 4: High Usage - Always-On Optimal (MEDIUM)
        if (
            running_mode == "ALWAYS_ON"
            and user_connected_time_hours > 160
            and not is_production
        ):
            # 160h/month = 40h/week (full-time usage)
            # This is expected for production, but verify for non-production
            scenarios.append(
                OptimizationScenario(
                    scenario_name="High Usage Non-Production (Verify Need)",
                    priority="MEDIUM",
                    confidence_score=65,
                    estimated_monthly_savings=0.0,  # Informational only
                    recommendation=(
                        f"Non-production WorkSpace has high usage ({user_connected_time_hours:.1f}h/month). "
                        f"Always-On mode (${current_cost}/month) is optimal for this usage. "
                        f"Verify if WorkSpace is still needed for dev/test purposes."
                    ),
                    implementation_steps=[
                        "Confirm WorkSpace is actively used for development/testing",
                        "Consider if user needs WorkSpace 24/7 or only during work hours",
                        "If only business hours, AutoStop may still save money",
                        "No action needed if usage is legitimate"
                    ],
                    risk_level="LOW",
                    effort="LOW",
                )
            )

        # SCENARIO 5: Oversized Compute Type (LOW)
        if compute_type in ["PERFORMANCE", "POWER", "POWERPRO"] and user_connected_time_hours < 40:
            # High-performance bundles for low usage users
            value_cost = self._calculate_workspaces_monthly_cost(
                compute_type="VALUE", running_mode=running_mode, region=region
            )
            savings = current_cost - value_cost

            scenarios.append(
                OptimizationScenario(
                    scenario_name="Oversized Compute Type (Downgrade)",
                    priority="LOW",
                    confidence_score=50,
                    estimated_monthly_savings=savings,
                    recommendation=(
                        f"WorkSpace uses high-performance bundle ({compute_type}, ${current_cost}/month) "
                        f"but has low usage ({user_connected_time_hours:.1f}h/month). "
                        f"Downgrade to VALUE bundle to save ${savings:.2f}/month."
                    ),
                    implementation_steps=[
                        "Verify user's workload requirements (CPU, RAM needs)",
                        "If user only needs web browsing/Office, VALUE bundle sufficient",
                        "Create new WorkSpace with VALUE bundle",
                        "Migrate user data and applications",
                        "Delete old WorkSpace after user confirms satisfaction"
                    ],
                    risk_level="MEDIUM",
                    effort="MEDIUM",
                )
            )

        return scenarios

    async def scan_workspaces(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL Amazon WorkSpaces for cost intelligence.

        WorkSpaces provides managed, secure cloud desktops (Desktop-as-a-Service).

        CloudWatch Metrics Used:
        - UserConnected (AWS/WorkSpaces) - User connection sessions (30 days)
        - N/A for detailed usage metrics (requires WorkSpaces API)

        API Calls:
        - describe_workspaces() - List all WorkSpaces
        - describe_workspaces_connection_status() - Last connection timestamp

        Cost Optimization Scenarios:
        1. WorkSpace Stopped Long-Term (CRITICAL) - Stopped >30 days  Still charged
        2. Never Connected (HIGH) - 0 sessions in 30 days  Delete unused
        3. Low Usage AutoStop Better (MEDIUM) - <10h/month  AutoStop cheaper
        4. High Usage Always-On Optimal (MEDIUM) - >40h/week  Confirm needed
        5. Oversized Compute Type (LOW) - VALUE sufficient  Downgrade

        Returns:
            List of AllCloudResourceData representing ALL WorkSpaces in the region
        """
        resources = []

        try:
            async with self.session.client("workspaces", region_name=region) as ws:
                # List all WorkSpaces
                paginator = ws.get_paginator("describe_workspaces")
                async for page in paginator.paginate():
                    for workspace in page.get("Workspaces", []):
                        try:
                            workspace_id = workspace.get("WorkspaceId", "")
                            workspace_state = workspace.get("State", "UNKNOWN")
                            compute_type = workspace.get("WorkspaceProperties", {}).get(
                                "ComputeTypeName", "STANDARD"
                            )
                            running_mode = workspace.get("WorkspaceProperties", {}).get(
                                "RunningMode", "ALWAYS_ON"
                            )
                            user_name = workspace.get("UserName", "")
                            bundle_id = workspace.get("BundleId", "")

                            # Get creation time (use directory creation as proxy if not available)
                            created_time = workspace.get("WorkspaceProperties", {}).get(
                                "LastKnownUserConnectionTimestamp"
                            )
                            if not created_time:
                                created_time = datetime.now(timezone.utc) - timedelta(days=30)

                            # Get tags
                            tags = {}
                            if "Tags" in workspace:
                                tags = {tag["Key"]: tag.get("Value", "") for tag in workspace["Tags"]}

                            # Get connection status
                            last_known_user_connection_timestamp = None
                            user_connected_time_hours = 0.0

                            try:
                                conn_status_response = await ws.describe_workspaces_connection_status(
                                    WorkspaceIds=[workspace_id]
                                )
                                for conn_status in conn_status_response.get(
                                    "WorkspacesConnectionStatus", []
                                ):
                                    last_known_user_connection_timestamp = conn_status.get(
                                        "LastKnownUserConnectionTimestamp"
                                    )
                                    # Note: ConnectionState only shows current state, not historical usage
                                    # Would need CloudWatch metrics for accurate usage hours

                            except Exception as e:
                                logger.warning(
                                    "workspaces.connection_status_failed",
                                    workspace_id=workspace_id,
                                    error=str(e),
                                )

                            # Get CloudWatch metrics for user connected time
                            try:
                                async with self.session.client(
                                    "cloudwatch", region_name=region
                                ) as cw:
                                    # UserConnected metric (30 days)
                                    # Note: This metric may not be available for all WorkSpaces
                                    user_connected_response = await cw.get_metric_statistics(
                                        Namespace="AWS/WorkSpaces",
                                        MetricName="UserConnected",
                                        Dimensions=[
                                            {"Name": "WorkspaceId", "Value": workspace_id}
                                        ],
                                        StartTime=datetime.now(timezone.utc) - timedelta(days=30),
                                        EndTime=datetime.now(timezone.utc),
                                        Period=3600,  # 1 hour
                                        Statistics=["Sum"],
                                    )
                                    datapoints = user_connected_response.get("Datapoints", [])
                                    # Sum hours where user was connected
                                    user_connected_time_hours = sum(
                                        dp["Sum"] for dp in datapoints if dp["Sum"] > 0
                                    )

                            except Exception as e:
                                logger.warning(
                                    "workspaces.cloudwatch_metrics_failed",
                                    workspace_id=workspace_id,
                                    error=str(e),
                                )

                            # Calculate monthly cost
                            monthly_cost = self._calculate_workspaces_monthly_cost(
                                compute_type=compute_type,
                                running_mode=running_mode,
                                region=region,
                            )

                            # Calculate optimization scenarios
                            optimization_scenarios = self._calculate_workspaces_optimization(
                                workspace_id=workspace_id,
                                workspace_state=workspace_state,
                                compute_type=compute_type,
                                running_mode=running_mode,
                                last_known_user_connection_timestamp=last_known_user_connection_timestamp,
                                user_connected_time_hours=user_connected_time_hours,
                                created_time=created_time,
                                tags=tags,
                                region=region,
                            )

                            # Prepare resource data
                            resource_data = AllCloudResourceData(
                                resource_type="workspaces_workspace",
                                resource_id=workspace_id,
                                resource_name=f"{user_name}-{workspace_id}",
                                region=region,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata={
                                    "workspace_state": workspace_state,
                                    "compute_type": compute_type,
                                    "running_mode": running_mode,
                                    "user_name": user_name,
                                    "bundle_id": bundle_id,
                                    "last_connection": last_known_user_connection_timestamp.isoformat()
                                    if last_known_user_connection_timestamp
                                    else None,
                                    "user_connected_time_hours": round(
                                        user_connected_time_hours, 2
                                    ),
                                    "tags": tags,
                                },
                                optimization_scenarios=optimization_scenarios,
                            )

                            resources.append(resource_data)

                            logger.debug(
                                "workspaces.scanned",
                                workspace_id=workspace_id,
                                region=region,
                                scenarios=len(optimization_scenarios),
                            )

                        except Exception as e:
                            logger.warning(
                                "workspaces.scan_error",
                                workspace_id=workspace.get("WorkspaceId", "unknown"),
                                region=region,
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "workspaces.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    def _calculate_emr_cluster_monthly_cost(
        self, instance_type: str, instance_count: int, running_hours: int, region: str
    ) -> float:
        """
        Calculate estimated monthly cost for AWS EMR Cluster.

        EMR pricing = EC2 instance cost + EMR per-instance-hour surcharge.

        Args:
            instance_type: EC2 instance type (e.g., "m5.xlarge")
            instance_count: Number of instances in cluster (core + task nodes)
            running_hours: Hours running per month (~730 for 24/7)
            region: AWS region

        Returns:
            Estimated monthly cost in USD
        """
        # EMR surcharge rates (approximate, varies by region)
        if instance_type.startswith(("t3.", "t2.", "m5.large")):
            emr_surcharge = self.PRICING.get("emr_surcharge_small", 0.03)
        elif instance_type.startswith(("m5.xlarge", "c5.xlarge", "m5.2xlarge")):
            emr_surcharge = self.PRICING.get("emr_surcharge_medium", 0.096)
        else:  # Large instances (r5.4xlarge+, etc.)
            emr_surcharge = self.PRICING.get("emr_surcharge_large", 0.27)

        # Simplified EC2 cost estimation (actual cost varies by instance type)
        # This is a rough estimate - real implementation would query AWS Pricing API
        ec2_cost_per_hour = {
            "m5.large": 0.096,
            "m5.xlarge": 0.192,
            "m5.2xlarge": 0.384,
            "c5.xlarge": 0.17,
            "r5.xlarge": 0.252,
            "r5.4xlarge": 1.008,
        }.get(instance_type, 0.192)  # Default to m5.xlarge

        total_hourly_cost = (ec2_cost_per_hour + emr_surcharge) * instance_count
        monthly_cost = total_hourly_cost * running_hours

        return round(monthly_cost, 2)

    def _calculate_emr_cluster_optimization(
        self,
        cluster_id: str,
        cluster_name: str,
        state: str,
        instance_type: str,
        instance_count: int,
        running_hours: int,
        is_idle: bool,
        apps_running: int,
        apps_completed_30d: int,
        hdfs_utilization_pct: float,
        creation_time: datetime,
        region: str,
    ) -> list[OptimizationScenario]:
        """
        Calculate optimization scenarios for EMR Cluster.

        Scenarios:
        1. Cluster Idle Long-Term (CRITICAL) - IsIdle=1 for 7+ days
        2. Cluster Never Used (HIGH) - 0 apps completed in 30 days
        3. Long-Running Dev/Test Cluster (MEDIUM) - >168h/month non-production
        4. Oversized Cluster Low Utilization (MEDIUM) - <20% HDFS usage
        5. Cluster Running 24/7 Non-Production (LOW) - Auto-terminate schedule

        Args:
            cluster_id: EMR cluster ID
            cluster_name: Cluster name/tag
            state: Cluster state (RUNNING, WAITING, TERMINATED, etc.)
            instance_type: EC2 instance type
            instance_count: Number of instances
            running_hours: Hours running this month
            is_idle: Whether cluster is idle (IsIdle metric)
            apps_running: Number of apps currently running
            apps_completed_30d: Apps completed in last 30 days
            hdfs_utilization_pct: HDFS utilization percentage
            creation_time: Cluster creation timestamp
            region: AWS region

        Returns:
            List of optimization scenarios
        """
        scenarios = []
        age_days = (datetime.now(timezone.utc) - creation_time).days
        monthly_cost = self._calculate_emr_cluster_monthly_cost(
            instance_type, instance_count, running_hours, region
        )

        # Scenario 1: Cluster Idle Long-Term (CRITICAL - 95)
        if is_idle and age_days >= 7:
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Cluster Idle Long-Term",
                    priority_level=95,
                    description=(
                        f"EMR cluster '{cluster_name}' (ID: {cluster_id}) has been idle for {age_days} days. "
                        f"IsIdle metric shows cluster is not processing any jobs. "
                        f"Consider terminating cluster to save ~${monthly_cost:.2f}/month."
                    ),
                    estimated_monthly_savings=monthly_cost,
                    recommended_action=(
                        "Terminate idle EMR cluster. If needed in future, create new cluster with "
                        "auto-termination enabled or use EMR Serverless for on-demand processing."
                    ),
                    confidence_level="critical",
                )
            )

        # Scenario 2: Cluster Never Used (HIGH - 85)
        if apps_completed_30d == 0 and age_days >= 30:
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Cluster Never Used",
                    priority_level=85,
                    description=(
                        f"EMR cluster '{cluster_name}' (ID: {cluster_id}) has completed 0 applications "
                        f"in the last 30 days (age: {age_days} days). Cluster is running but unused. "
                        f"Current monthly cost: ~${monthly_cost:.2f}/month."
                    ),
                    estimated_monthly_savings=monthly_cost,
                    recommended_action=(
                        "Terminate unused EMR cluster. Verify cluster is not needed for scheduled jobs. "
                        "Consider EMR Serverless or on-demand cluster creation for future needs."
                    ),
                    confidence_level="high",
                )
            )

        # Scenario 3: Long-Running Dev/Test Cluster (MEDIUM - 70)
        if running_hours > 168 and ("dev" in cluster_name.lower() or "test" in cluster_name.lower()):
            # 168 hours = 1 week continuous runtime
            # Estimate 50% savings by using spot instances or auto-termination
            estimated_savings = monthly_cost * 0.50
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Long-Running Dev/Test Cluster",
                    priority_level=70,
                    description=(
                        f"EMR cluster '{cluster_name}' appears to be a dev/test cluster running {running_hours}h "
                        f"this month ({running_hours / 730 * 100:.1f}% uptime). Dev/test clusters should use "
                        f"spot instances or auto-termination. Current cost: ~${monthly_cost:.2f}/month."
                    ),
                    estimated_monthly_savings=estimated_savings,
                    recommended_action=(
                        "Migrate to EMR with spot instances (70-90% savings on EC2 cost) or enable "
                        "auto-termination after idle period. Consider EMR Serverless for dev/test workloads."
                    ),
                    confidence_level="medium",
                )
            )

        # Scenario 4: Oversized Cluster Low Utilization (MEDIUM - 65)
        if hdfs_utilization_pct > 0 and hdfs_utilization_pct < 20 and instance_count >= 3:
            # Low HDFS utilization suggests oversized cluster
            # Estimate 40% savings by reducing cluster size
            estimated_savings = monthly_cost * 0.40
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Oversized Cluster Low Utilization",
                    priority_level=65,
                    description=(
                        f"EMR cluster '{cluster_name}' has low HDFS utilization ({hdfs_utilization_pct:.1f}%) "
                        f"with {instance_count} instances. Cluster may be oversized for current workload. "
                        f"Current cost: ~${monthly_cost:.2f}/month."
                    ),
                    estimated_monthly_savings=estimated_savings,
                    recommended_action=(
                        f"Reduce cluster size by ~40% (from {instance_count} to {int(instance_count * 0.6)} instances) "
                        f"or downgrade instance type. Monitor performance after optimization."
                    ),
                    confidence_level="medium",
                )
            )

        # Scenario 5: Cluster Running 24/7 Non-Production (LOW - 50)
        if (
            running_hours >= 700  # ~95% uptime (730h/month * 0.95 = 693.5h)
            and apps_completed_30d < 100  # Low job frequency
            and "prod" not in cluster_name.lower()
        ):
            # Estimate 60% savings by scheduling cluster runtime
            estimated_savings = monthly_cost * 0.60
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Cluster Running 24/7 Non-Production",
                    priority_level=50,
                    description=(
                        f"EMR cluster '{cluster_name}' is running 24/7 ({running_hours}h this month) "
                        f"but completed only {apps_completed_30d} jobs in 30 days. Non-production clusters "
                        f"should use auto-termination schedules. Current cost: ~${monthly_cost:.2f}/month."
                    ),
                    estimated_monthly_savings=estimated_savings,
                    recommended_action=(
                        "Implement auto-termination schedule (e.g., terminate after 8h business hours). "
                        "Use EMR managed scaling and auto-termination policies. Consider step execution mode "
                        "where cluster terminates automatically after job completion."
                    ),
                    confidence_level="low",
                )
            )

        return scenarios

    async def scan_emr_clusters(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS EMR (Elastic MapReduce) clusters for cost intelligence.

        EMR is a managed Big Data platform for processing large datasets using
        frameworks like Apache Hadoop, Spark, Presto, and HBase.

        CloudWatch Metrics Used:
        - IsIdle (AWS/ElasticMapReduce) - Cluster idle status (7 days)
        - CoreNodesRunning (AWS/ElasticMapReduce) - Number of core nodes (7 days)
        - AppsCompleted (AWS/ElasticMapReduce) - Applications completed (30 days)
        - HDFSUtilization (AWS/ElasticMapReduce) - HDFS storage used % (7 days)

        API Calls:
        - list_clusters() - List all clusters
        - describe_cluster() - Get cluster details

        Cost Optimization Scenarios:
        1. Cluster Idle Long-Term (CRITICAL) - IsIdle=1 for 7+ days  Terminate
        2. Cluster Never Used (HIGH) - 0 apps completed 30 days  Delete unused
        3. Long-Running Dev/Test (MEDIUM) - >168h/month  Spot instances
        4. Oversized Low Utilization (MEDIUM) - <20% HDFS  Downsize 40%
        5. 24/7 Non-Production (LOW) - Always on  Auto-terminate schedule

        Args:
            region: AWS region to scan

        Returns:
            List of all EMR clusters with optimization recommendations
        """
        resources = []

        try:
            async with self.session.client("emr", region_name=region) as emr:
                # List all EMR clusters (active and recently terminated)
                cluster_states = ["RUNNING", "WAITING", "TERMINATING", "TERMINATED"]
                paginator = emr.get_paginator("list_clusters")

                async for page in paginator.paginate(
                    ClusterStates=cluster_states,
                    CreatedAfter=datetime.now(timezone.utc) - timedelta(days=90),
                ):
                    for cluster_summary in page.get("Clusters", []):
                        try:
                            cluster_id = cluster_summary.get("Id")
                            cluster_name = cluster_summary.get("Name", "Unnamed")
                            state = cluster_summary["Status"]["State"]

                            # Get detailed cluster info
                            cluster_response = await emr.describe_cluster(ClusterId=cluster_id)
                            cluster = cluster_response["Cluster"]

                            creation_time = cluster["Status"]["Timeline"]["CreationDateTime"]
                            instance_count = cluster.get("NormalizedInstanceHours", 0)

                            # Get instance fleet/group info for cost calculation
                            instance_type = "m5.xlarge"  # Default
                            total_instances = 0

                            if "InstanceGroups" in cluster:
                                for group in cluster.get("InstanceGroups", []):
                                    instance_type = group.get("InstanceType", instance_type)
                                    total_instances += group.get("RequestedInstanceCount", 0)
                            elif "InstanceFleets" in cluster:
                                for fleet in cluster.get("InstanceFleets", []):
                                    if fleet.get("InstanceFleetType") == "CORE":
                                        # Use target capacity as instance count approximation
                                        total_instances += fleet.get("TargetOnDemandCapacity", 0)

                            total_instances = max(total_instances, 1)  # At least 1 instance

                            # Calculate running hours (approximate based on age and state)
                            age_hours = (datetime.now(timezone.utc) - creation_time).total_seconds() / 3600
                            if state in ["RUNNING", "WAITING"]:
                                running_hours = min(age_hours, 730)  # Cap at 1 month
                            elif state == "TERMINATED":
                                end_time = cluster["Status"]["Timeline"].get("EndDateTime")
                                if end_time:
                                    running_hours = (end_time - creation_time).total_seconds() / 3600
                                else:
                                    running_hours = age_hours
                            else:
                                running_hours = age_hours

                            running_hours = int(running_hours)

                            # Fetch CloudWatch metrics
                            is_idle = False
                            apps_completed_30d = 0
                            hdfs_utilization_pct = 0.0

                            try:
                                async with self.session.client(
                                    "cloudwatch", region_name=region
                                ) as cloudwatch:
                                    # IsIdle metric (7 days)
                                    idle_response = await cloudwatch.get_metric_statistics(
                                        Namespace="AWS/ElasticMapReduce",
                                        MetricName="IsIdle",
                                        Dimensions=[
                                            {"Name": "JobFlowId", "Value": cluster_id}
                                        ],
                                        StartTime=datetime.now(timezone.utc) - timedelta(days=7),
                                        EndTime=datetime.now(timezone.utc),
                                        Period=86400,  # 1 day
                                        Statistics=["Maximum"],
                                    )
                                    if idle_response.get("Datapoints"):
                                        # If any datapoint shows idle (1), cluster is idle
                                        is_idle = any(
                                            dp["Maximum"] >= 1 for dp in idle_response["Datapoints"]
                                        )

                                    # AppsCompleted metric (30 days)
                                    apps_response = await cloudwatch.get_metric_statistics(
                                        Namespace="AWS/ElasticMapReduce",
                                        MetricName="AppsCompleted",
                                        Dimensions=[
                                            {"Name": "JobFlowId", "Value": cluster_id}
                                        ],
                                        StartTime=datetime.now(timezone.utc) - timedelta(days=30),
                                        EndTime=datetime.now(timezone.utc),
                                        Period=2592000,  # 30 days
                                        Statistics=["Sum"],
                                    )
                                    if apps_response.get("Datapoints"):
                                        apps_completed_30d = int(
                                            apps_response["Datapoints"][0].get("Sum", 0)
                                        )

                                    # HDFSUtilization metric (7 days average)
                                    hdfs_response = await cloudwatch.get_metric_statistics(
                                        Namespace="AWS/ElasticMapReduce",
                                        MetricName="HDFSUtilization",
                                        Dimensions=[
                                            {"Name": "JobFlowId", "Value": cluster_id}
                                        ],
                                        StartTime=datetime.now(timezone.utc) - timedelta(days=7),
                                        EndTime=datetime.now(timezone.utc),
                                        Period=604800,  # 7 days
                                        Statistics=["Average"],
                                    )
                                    if hdfs_response.get("Datapoints"):
                                        hdfs_utilization_pct = hdfs_response["Datapoints"][0].get(
                                            "Average", 0.0
                                        )

                            except Exception as e:
                                logger.warning(
                                    "emr.cloudwatch_error",
                                    cluster_id=cluster_id,
                                    region=region,
                                    error=str(e),
                                )

                            # Calculate monthly cost
                            monthly_cost = self._calculate_emr_cluster_monthly_cost(
                                instance_type, total_instances, running_hours, region
                            )

                            # Calculate optimization scenarios
                            optimization_scenarios = self._calculate_emr_cluster_optimization(
                                cluster_id=cluster_id,
                                cluster_name=cluster_name,
                                state=state,
                                instance_type=instance_type,
                                instance_count=total_instances,
                                running_hours=running_hours,
                                is_idle=is_idle,
                                apps_running=0,  # Would need to track active jobs separately
                                apps_completed_30d=apps_completed_30d,
                                hdfs_utilization_pct=hdfs_utilization_pct,
                                creation_time=creation_time,
                                region=region,
                            )

                            # Get tags
                            tags = {tag["Key"]: tag["Value"] for tag in cluster.get("Tags", [])}

                            # Apply detection rules filtering
                            resource_age_days = None
                            if isinstance(creation_time, datetime):
                                resource_age_days = (datetime.utcnow() - creation_time.replace(tzinfo=None)).days

                            if not self._should_include_resource("emr_cluster", resource_age_days):
                                logger.debug("inventory.emr_filtered", cluster_id=cluster_id, age=resource_age_days)
                                continue

                            resources.append(
                                AllCloudResourceData(
                                    provider="aws",
                                    account_id=self.access_key[:12],
                                    region=region,
                                    resource_type="emr_cluster",
                                    resource_id=cluster_id,
                                    resource_name=cluster_name,
                                    resource_arn=cluster.get("ClusterArn", ""),
                                    estimated_monthly_cost=monthly_cost,
                                    currency="USD",
                                    resource_metadata={
                                        "state": state,
                                        "instance_type": instance_type,
                                        "instance_count": total_instances,
                                        "running_hours_month": running_hours,
                                        "is_idle": is_idle,
                                        "apps_completed_30d": apps_completed_30d,
                                        "hdfs_utilization_pct": round(hdfs_utilization_pct, 2),
                                        "creation_time": creation_time.isoformat(),
                                        "tags": tags,
                                    },
                                    created_at_cloud=creation_time,
                                    optimization_scenarios=optimization_scenarios,
                                )
                            )

                            logger.info(
                                "emr.cluster_scanned",
                                cluster_id=cluster_id,
                                cluster_name=cluster_name,
                                region=region,
                                state=state,
                                monthly_cost=monthly_cost,
                                scenarios_count=len(optimization_scenarios),
                            )

                        except Exception as e:
                            logger.warning(
                                "emr.scan_error",
                                cluster_id=cluster_summary.get("Id", "unknown"),
                                region=region,
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "emr.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    def _calculate_sagemaker_notebook_monthly_cost(
        self, instance_type: str, running_hours: int, region: str
    ) -> float:
        """
        Calculate estimated monthly cost for AWS SageMaker Notebook Instance.

        SageMaker pricing is per-instance-hour based on instance type.

        Args:
            instance_type: SageMaker instance type (e.g., "ml.t3.medium")
            running_hours: Hours running per month (~730 for 24/7)
            region: AWS region

        Returns:
            Estimated monthly cost in USD
        """
        # SageMaker notebook instance pricing (approximate, varies by region)
        hourly_rate_map = {
            "ml.t3.medium": self.PRICING.get("sagemaker_ml_t3_medium", 0.0582),
            "ml.t3.large": 0.1164,
            "ml.t3.xlarge": 0.2328,
            "ml.m5.large": self.PRICING.get("sagemaker_ml_m5_large", 0.134),
            "ml.m5.xlarge": self.PRICING.get("sagemaker_ml_m5_xlarge", 0.269),
            "ml.m5.2xlarge": 0.538,
            "ml.m5.4xlarge": 1.075,
            "ml.p3.2xlarge": 3.825,  # GPU instance
        }

        hourly_rate = hourly_rate_map.get(instance_type, 0.134)  # Default to ml.m5.large
        monthly_cost = hourly_rate * running_hours

        return round(monthly_cost, 2)

    def _calculate_sagemaker_notebook_optimization(
        self,
        notebook_name: str,
        instance_type: str,
        status: str,
        running_hours: int,
        cpu_utilization_pct: float,
        memory_utilization_pct: float,
        last_modified_time: datetime,
        creation_time: datetime,
        region: str,
    ) -> list[OptimizationScenario]:
        """
        Calculate optimization scenarios for SageMaker Notebook Instance.

        Scenarios:
        1. Notebook Stopped But Not Deleted (CRITICAL) - InService but not accessed 30+ days
        2. Notebook Never Used (HIGH) - 0% CPU for 30 days
        3. Notebook Idle (MEDIUM) - <5% CPU for 7+ days
        4. Oversized Instance (MEDIUM) - <20% CPU/Memory utilization
        5. Notebook Running 24/7 Dev/Test (LOW) - Auto-stop script recommended

        Args:
            notebook_name: Notebook instance name
            instance_type: Instance type (ml.t3.medium, etc.)
            status: Notebook status (InService, Stopped, etc.)
            running_hours: Hours running this month
            cpu_utilization_pct: Average CPU utilization (7 days)
            memory_utilization_pct: Average memory utilization (7 days)
            last_modified_time: Last modification timestamp
            creation_time: Notebook creation timestamp
            region: AWS region

        Returns:
            List of optimization scenarios
        """
        scenarios = []
        age_days = (datetime.now(timezone.utc) - creation_time).days
        days_since_modified = (datetime.now(timezone.utc) - last_modified_time).days
        monthly_cost = self._calculate_sagemaker_notebook_monthly_cost(
            instance_type, running_hours, region
        )

        # Scenario 1: Notebook Stopped But Not Deleted (CRITICAL - 95)
        # Note: Stopped notebooks don't incur compute charges, but this scenario
        # identifies long-term stopped notebooks that should be deleted
        if status == "Stopped" and days_since_modified >= 30:
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Notebook Stopped Long-Term",
                    priority_level=95,
                    description=(
                        f"SageMaker notebook '{notebook_name}' has been stopped for {days_since_modified} days "
                        f"(last modified: {last_modified_time.strftime('%Y-%m-%d')}). Stopped notebooks don't "
                        f"incur charges, but should be deleted if no longer needed to avoid clutter."
                    ),
                    estimated_monthly_savings=0.0,  # No cost savings (already stopped)
                    recommended_action=(
                        "Delete stopped SageMaker notebook if no longer needed. If needed, can recreate "
                        "from saved work on EBS volume or Git repository."
                    ),
                    confidence_level="critical",
                )
            )

        # Scenario 2: Notebook Never Used (HIGH - 85)
        if status == "InService" and cpu_utilization_pct < 1.0 and age_days >= 30:
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Notebook Never Used",
                    priority_level=85,
                    description=(
                        f"SageMaker notebook '{notebook_name}' has been running for {age_days} days "
                        f"with minimal CPU usage ({cpu_utilization_pct:.1f}% avg). Notebook appears unused. "
                        f"Current monthly cost: ~${monthly_cost:.2f}/month."
                    ),
                    estimated_monthly_savings=monthly_cost,
                    recommended_action=(
                        "Stop or delete unused SageMaker notebook. If needed periodically, use auto-stop "
                        "lifecycle configuration to stop after inactivity."
                    ),
                    confidence_level="high",
                )
            )

        # Scenario 3: Notebook Idle (MEDIUM - 70)
        if status == "InService" and cpu_utilization_pct < 5.0 and age_days >= 7:
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Notebook Idle",
                    priority_level=70,
                    description=(
                        f"SageMaker notebook '{notebook_name}' has low CPU utilization ({cpu_utilization_pct:.1f}% avg) "
                        f"for {age_days} days. Notebook is running but appears idle. "
                        f"Current monthly cost: ~${monthly_cost:.2f}/month."
                    ),
                    estimated_monthly_savings=monthly_cost,
                    recommended_action=(
                        "Stop idle notebook instance when not in use. Configure auto-stop lifecycle script "
                        "to stop after 1 hour of inactivity (saves ~80% if used 8h/day)."
                    ),
                    confidence_level="medium",
                )
            )

        # Scenario 4: Oversized Instance (MEDIUM - 65)
        if (
            status == "InService"
            and cpu_utilization_pct > 0
            and cpu_utilization_pct < 20
            and memory_utilization_pct < 20
        ):
            # Estimate 50% savings by downsizing instance
            estimated_savings = monthly_cost * 0.50
            # Suggest smaller instance type
            downgrade_map = {
                "ml.m5.xlarge": "ml.m5.large",
                "ml.m5.2xlarge": "ml.m5.xlarge",
                "ml.m5.4xlarge": "ml.m5.2xlarge",
                "ml.t3.large": "ml.t3.medium",
                "ml.t3.xlarge": "ml.t3.large",
            }
            suggested_type = downgrade_map.get(instance_type, "ml.t3.medium")

            scenarios.append(
                OptimizationScenario(
                    scenario_name="Oversized Instance",
                    priority_level=65,
                    description=(
                        f"SageMaker notebook '{notebook_name}' has low resource utilization "
                        f"(CPU: {cpu_utilization_pct:.1f}%, Memory: {memory_utilization_pct:.1f}%) "
                        f"on instance type '{instance_type}'. Instance appears oversized. "
                        f"Current cost: ~${monthly_cost:.2f}/month."
                    ),
                    estimated_monthly_savings=estimated_savings,
                    recommended_action=(
                        f"Downgrade to smaller instance type (e.g., '{suggested_type}') for ~50% savings. "
                        f"Monitor performance after downgrade. Use ml.t3.medium for basic development work."
                    ),
                    confidence_level="medium",
                )
            )

        # Scenario 5: Notebook Running 24/7 Dev/Test (LOW - 50)
        if (
            status == "InService"
            and running_hours >= 700  # ~95% uptime
            and ("dev" in notebook_name.lower() or "test" in notebook_name.lower())
        ):
            # Estimate 50% savings with auto-stop (assuming 8h/day usage)
            estimated_savings = monthly_cost * 0.50
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Notebook Running 24/7 Dev/Test",
                    priority_level=50,
                    description=(
                        f"SageMaker notebook '{notebook_name}' appears to be a dev/test notebook running "
                        f"24/7 ({running_hours}h this month). Dev notebooks should auto-stop when idle. "
                        f"Current cost: ~${monthly_cost:.2f}/month."
                    ),
                    estimated_monthly_savings=estimated_savings,
                    recommended_action=(
                        "Configure auto-stop lifecycle script to stop after 1 hour of inactivity. "
                        "Can save ~50% by stopping during non-working hours (8h/day usage vs 24/7). "
                        "Use AWS-provided lifecycle scripts or custom CloudWatch-based automation."
                    ),
                    confidence_level="low",
                )
            )

        return scenarios

    async def scan_sagemaker_notebooks(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS SageMaker Notebook Instances for cost intelligence.

        SageMaker Notebook Instances are managed Jupyter notebook environments
        for ML development and experimentation.

        CloudWatch Metrics Used:
        - CPUUtilization (AWS/SageMaker) - CPU usage percentage (7 days)
        - MemoryUtilization (AWS/SageMaker) - Memory usage percentage (7 days)
        - DiskUtilization (AWS/SageMaker) - Disk usage percentage (7 days)

        API Calls:
        - list_notebook_instances() - List all notebook instances
        - describe_notebook_instance() - Get notebook details

        Cost Optimization Scenarios:
        1. Notebook Stopped Long-Term (CRITICAL) - Stopped 30+ days  Delete
        2. Notebook Never Used (HIGH) - <1% CPU 30 days  Stop/delete
        3. Notebook Idle (MEDIUM) - <5% CPU 7+ days  Auto-stop script
        4. Oversized Instance (MEDIUM) - <20% CPU/Memory  Downgrade ~50%
        5. 24/7 Dev/Test (LOW) - Always running  Auto-stop ~50% savings

        Args:
            region: AWS region to scan

        Returns:
            List of all SageMaker notebook instances with optimization recommendations
        """
        resources = []

        try:
            async with self.session.client("sagemaker", region_name=region) as sagemaker:
                # List all notebook instances
                paginator = sagemaker.get_paginator("list_notebook_instances")

                async for page in paginator.paginate():
                    for notebook_summary in page.get("NotebookInstances", []):
                        try:
                            notebook_name = notebook_summary.get("NotebookInstanceName")
                            status = notebook_summary.get("NotebookInstanceStatus", "Unknown")
                            instance_type = notebook_summary.get("InstanceType", "ml.t3.medium")
                            creation_time = notebook_summary.get("CreationTime")
                            last_modified_time = notebook_summary.get(
                                "LastModifiedTime", creation_time
                            )

                            # Get detailed notebook info
                            notebook_response = await sagemaker.describe_notebook_instance(
                                NotebookInstanceName=notebook_name
                            )
                            notebook = notebook_response

                            # Calculate running hours (approximate based on status and age)
                            age_hours = (
                                datetime.now(timezone.utc) - creation_time
                            ).total_seconds() / 3600

                            if status == "InService":
                                # Assume running continuously if InService
                                running_hours = min(age_hours, 730)  # Cap at 1 month
                            elif status == "Stopped":
                                # Not currently running
                                running_hours = 0
                            else:
                                # Unknown status, estimate conservatively
                                running_hours = age_hours * 0.5

                            running_hours = int(running_hours)

                            # Fetch CloudWatch metrics (only for InService notebooks)
                            cpu_utilization_pct = 0.0
                            memory_utilization_pct = 0.0

                            if status == "InService":
                                try:
                                    async with self.session.client(
                                        "cloudwatch", region_name=region
                                    ) as cloudwatch:
                                        # CPUUtilization metric (7 days)
                                        cpu_response = await cloudwatch.get_metric_statistics(
                                            Namespace="AWS/SageMaker",
                                            MetricName="CPUUtilization",
                                            Dimensions=[
                                                {"Name": "NotebookInstanceName", "Value": notebook_name}
                                            ],
                                            StartTime=datetime.now(timezone.utc) - timedelta(days=7),
                                            EndTime=datetime.now(timezone.utc),
                                            Period=604800,  # 7 days
                                            Statistics=["Average"],
                                        )
                                        if cpu_response.get("Datapoints"):
                                            cpu_utilization_pct = cpu_response["Datapoints"][0].get(
                                                "Average", 0.0
                                            )

                                        # MemoryUtilization metric (7 days)
                                        memory_response = await cloudwatch.get_metric_statistics(
                                            Namespace="AWS/SageMaker",
                                            MetricName="MemoryUtilization",
                                            Dimensions=[
                                                {"Name": "NotebookInstanceName", "Value": notebook_name}
                                            ],
                                            StartTime=datetime.now(timezone.utc) - timedelta(days=7),
                                            EndTime=datetime.now(timezone.utc),
                                            Period=604800,  # 7 days
                                            Statistics=["Average"],
                                        )
                                        if memory_response.get("Datapoints"):
                                            memory_utilization_pct = memory_response["Datapoints"][
                                                0
                                            ].get("Average", 0.0)

                                except Exception as e:
                                    logger.warning(
                                        "sagemaker.cloudwatch_error",
                                        notebook_name=notebook_name,
                                        region=region,
                                        error=str(e),
                                    )

                            # Calculate monthly cost
                            monthly_cost = self._calculate_sagemaker_notebook_monthly_cost(
                                instance_type, running_hours, region
                            )

                            # Calculate optimization scenarios
                            optimization_scenarios = self._calculate_sagemaker_notebook_optimization(
                                notebook_name=notebook_name,
                                instance_type=instance_type,
                                status=status,
                                running_hours=running_hours,
                                cpu_utilization_pct=cpu_utilization_pct,
                                memory_utilization_pct=memory_utilization_pct,
                                last_modified_time=last_modified_time,
                                creation_time=creation_time,
                                region=region,
                            )

                            # Get notebook ARN and other metadata
                            notebook_arn = notebook.get("NotebookInstanceArn", "")
                            subnet_id = notebook.get("SubnetId", "")
                            volume_size_gb = notebook.get("VolumeSizeInGB", 0)

                            # Apply detection rules filtering
                            resource_age_days = None
                            if isinstance(creation_time, datetime):
                                resource_age_days = (datetime.utcnow() - creation_time.replace(tzinfo=None)).days

                            if not self._should_include_resource("sagemaker_notebook", resource_age_days):
                                logger.debug("inventory.sagemaker_notebook_filtered", notebook_name=notebook_name, age=resource_age_days)
                                continue

                            resources.append(
                                AllCloudResourceData(
                                    provider="aws",
                                    account_id=self.access_key[:12],
                                    region=region,
                                    resource_type="sagemaker_notebook",
                                    resource_id=notebook_name,
                                    resource_name=notebook_name,
                                    resource_arn=notebook_arn,
                                    estimated_monthly_cost=monthly_cost,
                                    currency="USD",
                                    resource_metadata={
                                        "status": status,
                                        "instance_type": instance_type,
                                        "running_hours_month": running_hours,
                                        "cpu_utilization_pct": round(cpu_utilization_pct, 2),
                                        "memory_utilization_pct": round(memory_utilization_pct, 2),
                                        "volume_size_gb": volume_size_gb,
                                        "subnet_id": subnet_id,
                                        "creation_time": creation_time.isoformat(),
                                        "last_modified_time": last_modified_time.isoformat(),
                                    },
                                    created_at_cloud=creation_time,
                                    optimization_scenarios=optimization_scenarios,
                                )
                            )

                            logger.info(
                                "sagemaker.notebook_scanned",
                                notebook_name=notebook_name,
                                region=region,
                                status=status,
                                monthly_cost=monthly_cost,
                                scenarios_count=len(optimization_scenarios),
                            )

                        except Exception as e:
                            logger.warning(
                                "sagemaker.scan_error",
                                notebook_name=notebook_summary.get("NotebookInstanceName", "unknown"),
                                region=region,
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "sagemaker.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    def _calculate_transfer_family_monthly_cost(
        self, running_hours: int, data_transfer_gb: float, region: str
    ) -> float:
        """
        Calculate estimated monthly cost for AWS Transfer Family Server.

        Transfer Family pricing = Endpoint per-hour + Data transfer cost.
        IMPORTANT: Endpoint is charged even if idle (similar to Lightsail stopped instances).

        Args:
            running_hours: Hours endpoint is provisioned per month (~730 for 24/7)
            data_transfer_gb: Total GB transferred (in + out) per month
            region: AWS region

        Returns:
            Estimated monthly cost in USD
        """
        # Transfer Family endpoint pricing
        endpoint_per_hour = self.PRICING.get("transfer_family_endpoint_per_hour", 0.30)
        data_transfer_per_gb = self.PRICING.get("transfer_family_data_transfer_per_gb", 0.04)

        endpoint_cost = endpoint_per_hour * running_hours
        data_transfer_cost = data_transfer_per_gb * data_transfer_gb
        total_cost = endpoint_cost + data_transfer_cost

        return round(total_cost, 2)

    def _calculate_transfer_family_optimization(
        self,
        server_id: str,
        protocols: list[str],
        running_hours: int,
        files_transferred_30d: int,
        bytes_transferred_30d: float,
        endpoint_type: str,
        creation_time: datetime,
        region: str,
    ) -> list[OptimizationScenario]:
        """
        Calculate optimization scenarios for AWS Transfer Family Server.

        Scenarios:
        1. Endpoint Zero Transfers (CRITICAL) - 0 files transferred in 30 days
        2. Endpoint Idle Long-Term (HIGH) - 0 transfers for 90+ days
        3. Very Low Usage (MEDIUM) - <100 files/month (migrate to S3 presigned URLs)
        4. Dev/Test Endpoint Always On (MEDIUM) - Stop when not needed
        5. Low Data Transfer High Endpoint Cost (LOW) - Consider alternatives

        Args:
            server_id: Transfer Family server ID
            protocols: List of protocols (SFTP, FTP, FTPS)
            running_hours: Hours endpoint provisioned this month
            files_transferred_30d: Number of files transferred (30 days)
            bytes_transferred_30d: Total bytes transferred (30 days)
            endpoint_type: Endpoint type (PUBLIC, VPC, etc.)
            creation_time: Server creation timestamp
            region: AWS region

        Returns:
            List of optimization scenarios
        """
        scenarios = []
        age_days = (datetime.now(timezone.utc) - creation_time).days
        data_transfer_gb = bytes_transferred_30d / (1024**3)  # Convert bytes to GB

        # Calculate monthly cost (endpoint + data transfer)
        monthly_cost = self._calculate_transfer_family_monthly_cost(
            running_hours, data_transfer_gb, region
        )
        endpoint_cost = self.PRICING.get("transfer_family_endpoint_per_hour", 0.30) * running_hours

        # Scenario 1: Endpoint Zero Transfers (CRITICAL - 95)
        if files_transferred_30d == 0 and age_days >= 30:
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Endpoint Zero Transfers",
                    priority_level=95,
                    description=(
                        f"Transfer Family server '{server_id}' has transferred 0 files in the last 30 days "
                        f"(age: {age_days} days). Endpoint is charged ${endpoint_cost:.2f}/month even with no usage. "
                        f"Total monthly cost: ~${monthly_cost:.2f}/month."
                    ),
                    estimated_monthly_savings=endpoint_cost,  # Save endpoint cost
                    recommended_action=(
                        "Delete unused Transfer Family endpoint. If needed occasionally, recreate on-demand. "
                        "For infrequent file transfers, consider S3 presigned URLs or AWS CLI/SDK instead."
                    ),
                    confidence_level="critical",
                )
            )

        # Scenario 2: Endpoint Idle Long-Term (HIGH - 85)
        if files_transferred_30d == 0 and age_days >= 90:
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Endpoint Idle Long-Term",
                    priority_level=85,
                    description=(
                        f"Transfer Family server '{server_id}' has been idle for {age_days} days "
                        f"with 0 file transfers. Endpoint cost: ~${endpoint_cost:.2f}/month (charged even if idle). "
                        f"Total monthly cost: ~${monthly_cost:.2f}/month."
                    ),
                    estimated_monthly_savings=endpoint_cost,
                    recommended_action=(
                        "Delete idle Transfer Family endpoint. Validate server is not needed for scheduled "
                        "file transfers. Use AWS DataSync, S3 Transfer Acceleration, or presigned URLs as alternatives."
                    ),
                    confidence_level="high",
                )
            )

        # Scenario 3: Very Low Usage (MEDIUM - 70)
        if files_transferred_30d > 0 and files_transferred_30d < 100:
            # Low usage - endpoint cost likely exceeds value
            # Estimate full endpoint cost as savings (migrate to serverless alternative)
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Very Low Usage",
                    priority_level=70,
                    description=(
                        f"Transfer Family server '{server_id}' has very low usage ({files_transferred_30d} files "
                        f"in 30 days, {data_transfer_gb:.2f} GB transferred). Endpoint cost "
                        f"(${endpoint_cost:.2f}/month) likely exceeds value for low-volume transfers. "
                        f"Total monthly cost: ~${monthly_cost:.2f}/month."
                    ),
                    estimated_monthly_savings=endpoint_cost,
                    recommended_action=(
                        "Migrate to S3 presigned URLs for low-volume file transfers. For SFTP specifically, "
                        "consider AWS DataSync or third-party solutions. S3 presigned URLs cost only storage + "
                        "data transfer with no endpoint fees."
                    ),
                    confidence_level="medium",
                )
            )

        # Scenario 4: Dev/Test Endpoint Always On (MEDIUM - 65)
        if running_hours >= 700 and ("dev" in server_id.lower() or "test" in server_id.lower()):
            # Dev/test endpoint running 24/7
            # Estimate 50% savings by deleting when not in use
            estimated_savings = endpoint_cost * 0.50
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Dev/Test Endpoint Always On",
                    priority_level=65,
                    description=(
                        f"Transfer Family server '{server_id}' appears to be a dev/test endpoint running 24/7 "
                        f"({running_hours}h this month). Dev/test endpoints should be deleted when not actively used. "
                        f"Endpoint cost: ~${endpoint_cost:.2f}/month."
                    ),
                    estimated_monthly_savings=estimated_savings,
                    recommended_action=(
                        "Delete dev/test Transfer Family endpoint when not in use. Can recreate quickly via "
                        "CloudFormation/Terraform when needed. Estimate 50% savings by running only during testing periods."
                    ),
                    confidence_level="medium",
                )
            )

        # Scenario 5: Low Data Transfer High Endpoint Cost Ratio (LOW - 50)
        if data_transfer_gb > 0 and endpoint_cost / monthly_cost > 0.90:
            # Endpoint cost is >90% of total cost (very low data transfer)
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Low Data Transfer High Endpoint Cost",
                    priority_level=50,
                    description=(
                        f"Transfer Family server '{server_id}' has endpoint cost (${endpoint_cost:.2f}/month) "
                        f"representing {endpoint_cost / monthly_cost * 100:.1f}% of total cost "
                        f"(${monthly_cost:.2f}/month). Only {data_transfer_gb:.2f} GB transferred. "
                        f"Consider alternatives for low-volume transfers."
                    ),
                    estimated_monthly_savings=endpoint_cost * 0.8,  # 80% of endpoint cost
                    recommended_action=(
                        "Evaluate alternatives: (1) S3 presigned URLs for occasional transfers, "
                        "(2) AWS Lambda + API Gateway for file upload/download, (3) AWS DataSync for scheduled transfers. "
                        "These solutions can reduce costs significantly for low-volume use cases."
                    ),
                    confidence_level="low",
                )
            )

        return scenarios

    async def scan_transfer_family_servers(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS Transfer Family Servers for cost intelligence.

        AWS Transfer Family provides fully managed SFTP, FTP, and FTPS file transfer
        directly into and out of Amazon S3 or Amazon EFS.

        CloudWatch Metrics Used:
        - BytesIn (AWS/Transfer) - Bytes uploaded (30 days)
        - BytesOut (AWS/Transfer) - Bytes downloaded (30 days)
        - FilesIn (AWS/Transfer) - Files uploaded (30 days)
        - FilesOut (AWS/Transfer) - Files downloaded (30 days)

        API Calls:
        - list_servers() - List all Transfer Family servers
        - describe_server() - Get server details

        Cost Optimization Scenarios:
        1. Endpoint Zero Transfers (CRITICAL) - 0 files 30 days  Delete ~$216/month
        2. Endpoint Idle Long-Term (HIGH) - 0 transfers 90+ days  Delete
        3. Very Low Usage (MEDIUM) - <100 files/month  Migrate to S3 presigned URLs
        4. Dev/Test Always On (MEDIUM) - 24/7 dev/test  Delete when unused ~50% savings
        5. Low Data High Endpoint Cost (LOW) - >90% endpoint cost  Consider alternatives

        Args:
            region: AWS region to scan

        Returns:
            List of all Transfer Family servers with optimization recommendations
        """
        resources = []

        try:
            async with self.session.client("transfer", region_name=region) as transfer:
                # List all Transfer Family servers
                paginator = transfer.get_paginator("list_servers")

                async for page in paginator.paginate():
                    for server_summary in page.get("Servers", []):
                        try:
                            server_id = server_summary.get("ServerId")

                            # Get detailed server info
                            server_response = await transfer.describe_server(ServerId=server_id)
                            server = server_response["Server"]

                            protocols = server.get("Protocols", [])
                            endpoint_type = server.get("EndpointType", "PUBLIC")
                            state = server.get("State", "OFFLINE")

                            # For servers in certain states, skip cost calculation
                            if state in ["STOPPING", "STOPPED", "OFFLINE"]:
                                running_hours = 0
                            else:
                                # Assume provisioned for full month if online
                                running_hours = 730

                            # Try to get creation time from tags or use current time as fallback
                            tags = {tag["Key"]: tag["Value"] for tag in server.get("Tags", [])}
                            creation_time = datetime.now(timezone.utc) - timedelta(days=30)  # Default

                            # Fetch CloudWatch metrics
                            files_in = 0
                            files_out = 0
                            bytes_in = 0.0
                            bytes_out = 0.0

                            try:
                                async with self.session.client(
                                    "cloudwatch", region_name=region
                                ) as cloudwatch:
                                    # FilesIn metric (30 days)
                                    files_in_response = await cloudwatch.get_metric_statistics(
                                        Namespace="AWS/Transfer",
                                        MetricName="FilesIn",
                                        Dimensions=[{"Name": "ServerId", "Value": server_id}],
                                        StartTime=datetime.now(timezone.utc) - timedelta(days=30),
                                        EndTime=datetime.now(timezone.utc),
                                        Period=2592000,  # 30 days
                                        Statistics=["Sum"],
                                    )
                                    if files_in_response.get("Datapoints"):
                                        files_in = int(
                                            files_in_response["Datapoints"][0].get("Sum", 0)
                                        )

                                    # FilesOut metric (30 days)
                                    files_out_response = await cloudwatch.get_metric_statistics(
                                        Namespace="AWS/Transfer",
                                        MetricName="FilesOut",
                                        Dimensions=[{"Name": "ServerId", "Value": server_id}],
                                        StartTime=datetime.now(timezone.utc) - timedelta(days=30),
                                        EndTime=datetime.now(timezone.utc),
                                        Period=2592000,  # 30 days
                                        Statistics=["Sum"],
                                    )
                                    if files_out_response.get("Datapoints"):
                                        files_out = int(
                                            files_out_response["Datapoints"][0].get("Sum", 0)
                                        )

                                    # BytesIn metric (30 days)
                                    bytes_in_response = await cloudwatch.get_metric_statistics(
                                        Namespace="AWS/Transfer",
                                        MetricName="BytesIn",
                                        Dimensions=[{"Name": "ServerId", "Value": server_id}],
                                        StartTime=datetime.now(timezone.utc) - timedelta(days=30),
                                        EndTime=datetime.now(timezone.utc),
                                        Period=2592000,  # 30 days
                                        Statistics=["Sum"],
                                    )
                                    if bytes_in_response.get("Datapoints"):
                                        bytes_in = bytes_in_response["Datapoints"][0].get("Sum", 0.0)

                                    # BytesOut metric (30 days)
                                    bytes_out_response = await cloudwatch.get_metric_statistics(
                                        Namespace="AWS/Transfer",
                                        MetricName="BytesOut",
                                        Dimensions=[{"Name": "ServerId", "Value": server_id}],
                                        StartTime=datetime.now(timezone.utc) - timedelta(days=30),
                                        EndTime=datetime.now(timezone.utc),
                                        Period=2592000,  # 30 days
                                        Statistics=["Sum"],
                                    )
                                    if bytes_out_response.get("Datapoints"):
                                        bytes_out = bytes_out_response["Datapoints"][0].get(
                                            "Sum", 0.0
                                        )

                            except Exception as e:
                                logger.warning(
                                    "transfer_family.cloudwatch_error",
                                    server_id=server_id,
                                    region=region,
                                    error=str(e),
                                )

                            # Total files and bytes transferred
                            total_files = files_in + files_out
                            total_bytes = bytes_in + bytes_out

                            # Calculate monthly cost
                            monthly_cost = self._calculate_transfer_family_monthly_cost(
                                running_hours, total_bytes / (1024**3), region  # Convert to GB
                            )

                            # Calculate optimization scenarios
                            optimization_scenarios = self._calculate_transfer_family_optimization(
                                server_id=server_id,
                                protocols=protocols,
                                running_hours=running_hours,
                                files_transferred_30d=total_files,
                                bytes_transferred_30d=total_bytes,
                                endpoint_type=endpoint_type,
                                creation_time=creation_time,
                                region=region,
                            )

                            # Apply detection rules filtering
                            resource_age_days = None
                            if isinstance(creation_time, datetime):
                                resource_age_days = (datetime.utcnow() - creation_time.replace(tzinfo=None)).days

                            if not self._should_include_resource("transfer_family_server", resource_age_days):
                                logger.debug("inventory.transfer_family_filtered", server_id=server_id, age=resource_age_days)
                                continue

                            resources.append(
                                AllCloudResourceData(
                                    provider="aws",
                                    account_id=self.access_key[:12],
                                    region=region,
                                    resource_type="transfer_family_server",
                                    resource_id=server_id,
                                    resource_name=server_id,  # Transfer Family doesn't have names
                                    resource_arn=server.get("Arn", ""),
                                    estimated_monthly_cost=monthly_cost,
                                    currency="USD",
                                    resource_metadata={
                                        "state": state,
                                        "protocols": protocols,
                                        "endpoint_type": endpoint_type,
                                        "running_hours_month": running_hours,
                                        "files_transferred_30d": total_files,
                                        "bytes_transferred_30d": total_bytes,
                                        "bytes_transferred_gb": round(total_bytes / (1024**3), 2),
                                        "tags": tags,
                                    },
                                    created_at_cloud=creation_time,
                                    optimization_scenarios=optimization_scenarios,
                                )
                            )

                            logger.info(
                                "transfer_family.server_scanned",
                                server_id=server_id,
                                region=region,
                                state=state,
                                monthly_cost=monthly_cost,
                                scenarios_count=len(optimization_scenarios),
                            )

                        except Exception as e:
                            logger.warning(
                                "transfer_family.scan_error",
                                server_id=server_summary.get("ServerId", "unknown"),
                                region=region,
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "transfer_family.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    def _calculate_elastic_beanstalk_monthly_cost(
        self, instance_type: str, running_hours: int, has_load_balancer: bool, region: str
    ) -> float:
        """
        Calculate estimated monthly cost for AWS Elastic Beanstalk Environment.

        Elastic Beanstalk itself is free - you only pay for underlying resources:
        - EC2 instances
        - Elastic Load Balancer (if configured)
        - RDS instances (if configured)
        - Other AWS resources used

        Args:
            instance_type: EC2 instance type (e.g., "t3.micro")
            running_hours: Hours environment is running per month (~730 for 24/7)
            has_load_balancer: Whether environment has ELB configured
            region: AWS region

        Returns:
            Estimated monthly cost in USD
        """
        # EC2 instance cost (simplified - actual cost varies)
        ec2_hourly_rate = {
            "t3.micro": self.PRICING.get("elastic_beanstalk_ec2_t3_micro", 0.0104),
            "t3.small": 0.0208,
            "t3.medium": 0.0416,
            "m5.large": self.PRICING.get("elastic_beanstalk_ec2_m5_large", 0.096),
            "m5.xlarge": 0.192,
        }.get(instance_type, 0.0104)  # Default to t3.micro

        ec2_cost = ec2_hourly_rate * running_hours

        # Load balancer cost (if present)
        elb_cost = 0.0
        if has_load_balancer:
            elb_hourly_rate = self.PRICING.get("elastic_beanstalk_elb", 0.0225)
            elb_cost = elb_hourly_rate * running_hours

        total_cost = ec2_cost + elb_cost

        return round(total_cost, 2)

    def _calculate_elastic_beanstalk_optimization(
        self,
        environment_name: str,
        environment_id: str,
        status: str,
        health: str,
        instance_type: str,
        running_hours: int,
        requests_4xx_5xx: int,
        cpu_utilization_pct: float,
        creation_time: datetime,
        has_load_balancer: bool,
        region: str,
    ) -> list[OptimizationScenario]:
        """
        Calculate optimization scenarios for Elastic Beanstalk Environment.

        Scenarios:
        1. Environment Terminated/Terminating (CRITICAL) - Clean up terminated env
        2. Environment Unused (HIGH) - 0 requests for 30 days
        3. Environment Degraded Health (MEDIUM) - Degraded/Severe/Warning >7 days
        4. Dev/Test Always Running (MEDIUM) - Non-prod 24/7
        5. Oversized Instance Type (LOW) - Low CPU usage

        Args:
            environment_name: Environment name
            environment_id: Environment ID
            status: Environment status (Ready, Terminated, etc.)
            health: Environment health (Green, Yellow, Red, Grey)
            instance_type: EC2 instance type
            running_hours: Hours running this month
            requests_4xx_5xx: Number of 4xx/5xx requests (30 days)
            cpu_utilization_pct: Average CPU utilization (7 days)
            creation_time: Environment creation timestamp
            has_load_balancer: Whether ELB is configured
            region: AWS region

        Returns:
            List of optimization scenarios
        """
        scenarios = []
        age_days = (datetime.now(timezone.utc) - creation_time).days
        monthly_cost = self._calculate_elastic_beanstalk_monthly_cost(
            instance_type, running_hours, has_load_balancer, region
        )

        # Scenario 1: Environment Terminated/Terminating (CRITICAL - 95)
        if status in ["Terminated", "Terminating"]:
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Environment Terminated",
                    priority_level=95,
                    description=(
                        f"Elastic Beanstalk environment '{environment_name}' is in {status} state "
                        f"(age: {age_days} days). Terminated environments should be deleted from console. "
                        f"No longer incurring costs."
                    ),
                    estimated_monthly_savings=0.0,  # Already terminated
                    recommended_action=(
                        "Delete terminated Elastic Beanstalk environment from AWS console. "
                        "Environment is no longer running but still appears in listings."
                    ),
                    confidence_level="critical",
                )
            )

        # Scenario 2: Environment Never Used (HIGH - 85)
        if status == "Ready" and requests_4xx_5xx == 0 and age_days >= 30:
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Environment Never Used",
                    priority_level=85,
                    description=(
                        f"Elastic Beanstalk environment '{environment_name}' has received 0 requests "
                        f"in the last 30 days (age: {age_days} days). Environment appears unused. "
                        f"Current monthly cost: ~${monthly_cost:.2f}/month."
                    ),
                    estimated_monthly_savings=monthly_cost,
                    recommended_action=(
                        "Terminate unused Elastic Beanstalk environment. If needed in future, can recreate "
                        "from saved configuration or infrastructure-as-code templates (CloudFormation, Terraform)."
                    ),
                    confidence_level="high",
                )
            )

        # Scenario 3: Environment Degraded Health (MEDIUM - 70)
        if health in ["Yellow", "Red", "Grey"] and age_days >= 7 and status == "Ready":
            health_cost_impact = "Application may not be serving traffic properly"
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Environment Degraded Health",
                    priority_level=70,
                    description=(
                        f"Elastic Beanstalk environment '{environment_name}' has degraded health "
                        f"(status: {health}) for {age_days} days. {health_cost_impact}. "
                        f"Current monthly cost: ~${monthly_cost:.2f}/month."
                    ),
                    estimated_monthly_savings=monthly_cost,
                    recommended_action=(
                        f"Investigate and fix health issues (check EB console  Health  Causes). "
                        f"If unfixable or environment no longer needed, terminate to avoid ongoing costs. "
                        f"Health status '{health}' indicates infrastructure or application problems."
                    ),
                    confidence_level="medium",
                )
            )

        # Scenario 4: Dev/Test Environment Always Running (MEDIUM - 65)
        if (
            running_hours >= 700  # ~95% uptime
            and ("dev" in environment_name.lower() or "test" in environment_name.lower())
            and status == "Ready"
        ):
            # Estimate 50% savings by stopping during non-working hours
            estimated_savings = monthly_cost * 0.50
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Dev/Test Environment Always Running",
                    priority_level=65,
                    description=(
                        f"Elastic Beanstalk environment '{environment_name}' appears to be dev/test "
                        f"running 24/7 ({running_hours}h this month). Non-production environments "
                        f"should auto-stop during non-working hours. Current cost: ~${monthly_cost:.2f}/month."
                    ),
                    estimated_monthly_savings=estimated_savings,
                    recommended_action=(
                        "Implement auto-stop schedule for dev/test environments (e.g., stop at 7 PM, "
                        "start at 8 AM weekdays). Can save ~50% by running only during business hours. "
                        "Use CloudFormation scheduled actions or AWS Systems Manager maintenance windows."
                    ),
                    confidence_level="medium",
                )
            )

        # Scenario 5: Oversized Instance Type (LOW - 50)
        if (
            status == "Ready"
            and cpu_utilization_pct > 0
            and cpu_utilization_pct < 20
            and instance_type in ["m5.large", "m5.xlarge", "m5.2xlarge"]
        ):
            # Estimate 40% savings by downsizing
            estimated_savings = monthly_cost * 0.40
            downgrade_map = {
                "m5.xlarge": "m5.large",
                "m5.2xlarge": "m5.xlarge",
                "m5.large": "t3.medium",
            }
            suggested_type = downgrade_map.get(instance_type, "t3.medium")

            scenarios.append(
                OptimizationScenario(
                    scenario_name="Oversized Instance Type",
                    priority_level=50,
                    description=(
                        f"Elastic Beanstalk environment '{environment_name}' has low CPU utilization "
                        f"({cpu_utilization_pct:.1f}% avg) on instance type '{instance_type}'. "
                        f"Instance appears oversized. Current cost: ~${monthly_cost:.2f}/month."
                    ),
                    estimated_monthly_savings=estimated_savings,
                    recommended_action=(
                        f"Downgrade to smaller instance type (e.g., '{suggested_type}') for ~40% savings. "
                        f"Update EB configuration via console or update infrastructure code. "
                        f"Monitor performance after downgrade."
                    ),
                    confidence_level="low",
                )
            )

        return scenarios

    async def scan_elastic_beanstalk_environments(
        self, region: str
    ) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS Elastic Beanstalk Environments for cost intelligence.

        Elastic Beanstalk is a PaaS for deploying and scaling web applications
        (Java, .NET, PHP, Node.js, Python, Ruby, Go, Docker).

        CloudWatch Metrics Used:
        - EnvironmentHealth (AWS/ElasticBeanstalk) - Environment health status (7 days)
        - ApplicationRequests4xx/5xx (AWS/ElasticBeanstalk) - Request errors (30 days)
        - CPUUtilization (AWS/EC2) - Instance CPU usage (7 days)

        API Calls:
        - elasticbeanstalk.describe_environments() - List all environments
        - elasticbeanstalk.describe_environment_health() - Get detailed health

        Cost Optimization Scenarios:
        1. Environment Terminated (CRITICAL) - Terminated state  Delete from console
        2. Environment Never Used (HIGH) - 0 requests 30 days  Terminate
        3. Environment Degraded Health (MEDIUM) - Yellow/Red/Grey 7+ days  Fix or terminate
        4. Dev/Test Always Running (MEDIUM) - 24/7 non-prod  Auto-stop ~50% savings
        5. Oversized Instance Type (LOW) - <20% CPU  Downgrade ~40% savings

        Args:
            region: AWS region to scan

        Returns:
            List of all Elastic Beanstalk environments with optimization recommendations
        """
        resources = []

        try:
            async with self.session.client("elasticbeanstalk", region_name=region) as eb:
                # List all Elastic Beanstalk environments
                environments_response = await eb.describe_environments(
                    IncludeDeleted=False  # Don't include deleted environments
                )

                for environment in environments_response.get("Environments", []):
                    try:
                        environment_name = environment.get("EnvironmentName")
                        environment_id = environment.get("EnvironmentId")
                        status = environment.get("Status", "Unknown")
                        health = environment.get("Health", "Grey")
                        creation_time = environment.get("DateCreated")

                        # Get instance type from OptionSettings
                        instance_type = "t3.micro"  # Default
                        has_load_balancer = False

                        # Try to get detailed environment configuration
                        try:
                            config_response = await eb.describe_configuration_settings(
                                EnvironmentName=environment_name,
                                ApplicationName=environment.get("ApplicationName"),
                            )
                            if config_response.get("ConfigurationSettings"):
                                option_settings = config_response["ConfigurationSettings"][0].get(
                                    "OptionSettings", []
                                )
                                for option in option_settings:
                                    if option.get("OptionName") == "InstanceType":
                                        instance_type = option.get("Value", instance_type)
                                    if (
                                        option.get("Namespace")
                                        == "aws:elasticbeanstalk:environment"
                                        and option.get("OptionName") == "LoadBalancerType"
                                    ):
                                        has_load_balancer = True
                        except Exception as e:
                            logger.warning(
                                "elastic_beanstalk.config_error",
                                environment_name=environment_name,
                                error=str(e),
                            )

                        # Calculate running hours
                        age_hours = (
                            datetime.now(timezone.utc) - creation_time
                        ).total_seconds() / 3600
                        if status in ["Ready", "Updating"]:
                            running_hours = min(age_hours, 730)  # Cap at 1 month
                        elif status in ["Terminated", "Terminating"]:
                            running_hours = 0
                        else:
                            running_hours = age_hours * 0.5  # Estimate for other states

                        running_hours = int(running_hours)

                        # Fetch CloudWatch metrics
                        requests_4xx = 0
                        requests_5xx = 0
                        cpu_utilization_pct = 0.0

                        try:
                            async with self.session.client(
                                "cloudwatch", region_name=region
                            ) as cloudwatch:
                                # ApplicationRequests4xx metric (30 days)
                                requests_4xx_response = await cloudwatch.get_metric_statistics(
                                    Namespace="AWS/ElasticBeanstalk",
                                    MetricName="ApplicationRequests4xx",
                                    Dimensions=[
                                        {"Name": "EnvironmentName", "Value": environment_name}
                                    ],
                                    StartTime=datetime.now(timezone.utc) - timedelta(days=30),
                                    EndTime=datetime.now(timezone.utc),
                                    Period=2592000,  # 30 days
                                    Statistics=["Sum"],
                                )
                                if requests_4xx_response.get("Datapoints"):
                                    requests_4xx = int(
                                        requests_4xx_response["Datapoints"][0].get("Sum", 0)
                                    )

                                # ApplicationRequests5xx metric (30 days)
                                requests_5xx_response = await cloudwatch.get_metric_statistics(
                                    Namespace="AWS/ElasticBeanstalk",
                                    MetricName="ApplicationRequests5xx",
                                    Dimensions=[
                                        {"Name": "EnvironmentName", "Value": environment_name}
                                    ],
                                    StartTime=datetime.now(timezone.utc) - timedelta(days=30),
                                    EndTime=datetime.now(timezone.utc),
                                    Period=2592000,  # 30 days
                                    Statistics=["Sum"],
                                )
                                if requests_5xx_response.get("Datapoints"):
                                    requests_5xx = int(
                                        requests_5xx_response["Datapoints"][0].get("Sum", 0)
                                    )

                                # CPUUtilization metric from EC2 instances (7 days)
                                # Note: This requires knowing instance IDs, which is complex
                                # For simplicity, we'll estimate based on environment health
                                cpu_utilization_pct = 0.0

                        except Exception as e:
                            logger.warning(
                                "elastic_beanstalk.cloudwatch_error",
                                environment_name=environment_name,
                                region=region,
                                error=str(e),
                            )

                        # Calculate monthly cost
                        monthly_cost = self._calculate_elastic_beanstalk_monthly_cost(
                            instance_type, running_hours, has_load_balancer, region
                        )

                        # Calculate optimization scenarios
                        optimization_scenarios = self._calculate_elastic_beanstalk_optimization(
                            environment_name=environment_name,
                            environment_id=environment_id,
                            status=status,
                            health=health,
                            instance_type=instance_type,
                            running_hours=running_hours,
                            requests_4xx_5xx=requests_4xx + requests_5xx,
                            cpu_utilization_pct=cpu_utilization_pct,
                            creation_time=creation_time,
                            has_load_balancer=has_load_balancer,
                            region=region,
                        )

                        # Get environment ARN and other metadata
                        environment_arn = environment.get("EnvironmentArn", "")
                        application_name = environment.get("ApplicationName", "")
                        platform_arn = environment.get("PlatformArn", "")

                        # Apply detection rules filtering
                        resource_age_days = None
                        if isinstance(creation_time, datetime):
                            resource_age_days = (datetime.utcnow() - creation_time.replace(tzinfo=None)).days

                        if not self._should_include_resource("elastic_beanstalk_environment", resource_age_days):
                            logger.debug("inventory.elastic_beanstalk_filtered", environment_name=environment_name, age=resource_age_days)
                            continue

                        resources.append(
                            AllCloudResourceData(
                                provider="aws",
                                account_id=self.access_key[:12],
                                region=region,
                                resource_type="elastic_beanstalk_environment",
                                resource_id=environment_id,
                                resource_name=environment_name,
                                resource_arn=environment_arn,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata={
                                    "status": status,
                                    "health": health,
                                    "instance_type": instance_type,
                                    "running_hours_month": running_hours,
                                    "requests_4xx": requests_4xx,
                                    "requests_5xx": requests_5xx,
                                    "cpu_utilization_pct": round(cpu_utilization_pct, 2),
                                    "has_load_balancer": has_load_balancer,
                                    "application_name": application_name,
                                    "platform_arn": platform_arn,
                                    "creation_time": creation_time.isoformat(),
                                },
                                created_at_cloud=creation_time,
                                optimization_scenarios=optimization_scenarios,
                            )
                        )

                        logger.info(
                            "elastic_beanstalk.environment_scanned",
                            environment_name=environment_name,
                            region=region,
                            status=status,
                            monthly_cost=monthly_cost,
                            scenarios_count=len(optimization_scenarios),
                        )

                    except Exception as e:
                        logger.warning(
                            "elastic_beanstalk.scan_error",
                            environment_name=environment.get("EnvironmentName", "unknown"),
                            region=region,
                            error=str(e),
                        )
                        continue

        except Exception as e:
            logger.error(
                "elastic_beanstalk.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    def _calculate_direct_connect_monthly_cost(
        self, bandwidth: str, running_hours: int, data_transfer_out_gb: float, region: str
    ) -> float:
        """
        Calculate estimated monthly cost for AWS Direct Connect Connection.

        Direct Connect pricing = Port hour fee + Data transfer OUT (IN is free).

        Args:
            bandwidth: Connection bandwidth (1Gbps, 10Gbps, 100Gbps)
            running_hours: Hours connection is provisioned per month (~730 for 24/7)
            data_transfer_out_gb: Total GB transferred OUT per month (IN is free)
            region: AWS region

        Returns:
            Estimated monthly cost in USD
        """
        # Port hour pricing based on bandwidth
        if "10" in bandwidth or "10Gbps" in bandwidth:
            port_hourly_rate = self.PRICING.get("direct_connect_10gbps_port_hour", 2.25)
        elif "100" in bandwidth or "100Gbps" in bandwidth:
            port_hourly_rate = 25.0  # ~$18,000/month for 100Gbps
        else:  # 1Gbps or lower
            port_hourly_rate = self.PRICING.get("direct_connect_1gbps_port_hour", 0.30)

        # Data transfer OUT pricing (IN is free)
        data_transfer_rate = self.PRICING.get("direct_connect_data_transfer_out_per_gb", 0.02)

        port_cost = port_hourly_rate * running_hours
        data_transfer_cost = data_transfer_rate * data_transfer_out_gb
        total_cost = port_cost + data_transfer_cost

        return round(total_cost, 2)

    def _calculate_direct_connect_optimization(
        self,
        connection_id: str,
        connection_name: str,
        connection_state: str,
        bandwidth: str,
        running_hours: int,
        bytes_out_30d: float,
        bytes_in_30d: float,
        location: str,
        creation_time: datetime,
        region: str,
    ) -> list[OptimizationScenario]:
        """
        Calculate optimization scenarios for AWS Direct Connect Connection.

        Scenarios:
        1. Connection DOWN Long-Term (CRITICAL) - DOWN state >30 days
        2. Connection Zero Traffic (HIGH) - 0 bytes transferred 90 days
        3. Very Low Bandwidth Usage (MEDIUM) - <10% capacity utilization
        4. Data Transfer > Port Costs (MEDIUM) - Inefficient usage pattern
        5. Dev/Test Connection Always Provisioned (LOW) - Delete when not needed

        Args:
            connection_id: Direct Connect connection ID
            connection_name: Connection name
            connection_state: Connection state (available, down, etc.)
            bandwidth: Connection bandwidth (1Gbps, 10Gbps, 100Gbps)
            running_hours: Hours connection provisioned this month
            bytes_out_30d: Total bytes OUT (30 days)
            bytes_in_30d: Total bytes IN (30 days)
            location: Direct Connect location
            creation_time: Connection creation timestamp
            region: AWS region

        Returns:
            List of optimization scenarios
        """
        scenarios = []
        age_days = (datetime.now(timezone.utc) - creation_time).days
        data_transfer_out_gb = bytes_out_30d / (1024**3)  # Convert to GB
        data_transfer_in_gb = bytes_in_30d / (1024**3)

        # Calculate monthly cost (port + data transfer OUT)
        monthly_cost = self._calculate_direct_connect_monthly_cost(
            bandwidth, running_hours, data_transfer_out_gb, region
        )
        port_cost = self._calculate_direct_connect_monthly_cost(
            bandwidth, running_hours, 0, region
        )  # Port cost only

        # Scenario 1: Connection DOWN Long-Term (CRITICAL - 95)
        if connection_state == "down" and age_days >= 30:
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Connection DOWN Long-Term",
                    priority_level=95,
                    description=(
                        f"Direct Connect connection '{connection_name}' (ID: {connection_id}) has been "
                        f"DOWN for {age_days} days. Port is charged ${port_cost:.2f}/month even when DOWN. "
                        f"Total monthly cost: ~${monthly_cost:.2f}/month."
                    ),
                    estimated_monthly_savings=port_cost,
                    recommended_action=(
                        "Delete DOWN Direct Connect connection. If needed, create new connection after "
                        "fixing underlying network issues. Connection is charged even when DOWN."
                    ),
                    confidence_level="critical",
                )
            )

        # Scenario 2: Connection Zero Traffic (HIGH - 85)
        if (
            connection_state == "available"
            and bytes_out_30d == 0
            and bytes_in_30d == 0
            and age_days >= 90
        ):
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Connection Zero Traffic",
                    priority_level=85,
                    description=(
                        f"Direct Connect connection '{connection_name}' has transferred 0 bytes "
                        f"in the last 90 days (age: {age_days} days). Connection is available but unused. "
                        f"Port cost: ~${port_cost:.2f}/month (charged even with no traffic)."
                    ),
                    estimated_monthly_savings=port_cost,
                    recommended_action=(
                        "Delete unused Direct Connect connection. Validate connection is not needed for "
                        "disaster recovery or scheduled data transfers. Consider VPN Gateway as cheaper "
                        "alternative for low-volume connectivity."
                    ),
                    confidence_level="high",
                )
            )

        # Scenario 3: Very Low Bandwidth Usage (MEDIUM - 70)
        # Calculate bandwidth utilization (simplified - assumes constant traffic)
        bandwidth_gbps = 1  # Default
        if "10" in bandwidth:
            bandwidth_gbps = 10
        elif "100" in bandwidth:
            bandwidth_gbps = 100

        # Max theoretical transfer in 30 days at full capacity
        max_transfer_gb_30d = bandwidth_gbps * 1024 * 24 * 30 / 8  # Convert Gbps to GB/month
        total_transfer_gb = data_transfer_out_gb + data_transfer_in_gb
        utilization_pct = (total_transfer_gb / max_transfer_gb_30d * 100) if max_transfer_gb_30d > 0 else 0

        if utilization_pct > 0 and utilization_pct < 10 and bandwidth_gbps >= 10:
            # Low utilization - can downgrade
            # Estimate 50-80% savings by downgrading to 1Gbps
            if bandwidth_gbps == 10:
                estimated_savings = port_cost * 0.87  # 10Gbps  1Gbps saves ~87%
            else:  # 100Gbps
                estimated_savings = port_cost * 0.99  # 100Gbps  1Gbps saves ~99%

            scenarios.append(
                OptimizationScenario(
                    scenario_name="Very Low Bandwidth Usage",
                    priority_level=70,
                    description=(
                        f"Direct Connect connection '{connection_name}' has very low bandwidth usage "
                        f"({utilization_pct:.1f}% of {bandwidth} capacity over 30 days). "
                        f"Transferred {total_transfer_gb:.2f} GB total. Port cost: ~${port_cost:.2f}/month."
                    ),
                    estimated_monthly_savings=estimated_savings,
                    recommended_action=(
                        f"Downgrade from {bandwidth} to 1Gbps port for ~{estimated_savings/port_cost*100:.0f}% savings "
                        f"(~${estimated_savings:.2f}/month). Current usage ({utilization_pct:.1f}%) fits comfortably "
                        f"in lower bandwidth tier. Coordinate with network team before downgrade."
                    ),
                    confidence_level="medium",
                )
            )

        # Scenario 4: Data Transfer > Port Costs (MEDIUM - 65)
        data_transfer_cost = monthly_cost - port_cost
        if data_transfer_cost > port_cost and data_transfer_cost > 50:
            # Data transfer costs exceed port costs - inefficient usage
            # VPN Gateway might be cheaper for this pattern
            vpn_cost_estimate = data_transfer_out_gb * 0.09  # VPN data transfer ~$0.09/GB
            potential_savings = monthly_cost - vpn_cost_estimate

            scenarios.append(
                OptimizationScenario(
                    scenario_name="Data Transfer Exceeds Port Costs",
                    priority_level=65,
                    description=(
                        f"Direct Connect connection '{connection_name}' has data transfer costs "
                        f"(${data_transfer_cost:.2f}/month) exceeding port costs (${port_cost:.2f}/month). "
                        f"Transferred {data_transfer_out_gb:.2f} GB OUT. Total: ~${monthly_cost:.2f}/month."
                    ),
                    estimated_monthly_savings=potential_savings,
                    recommended_action=(
                        f"Consider AWS VPN Gateway as alternative for this usage pattern. VPN estimated cost: "
                        f"~${vpn_cost_estimate:.2f}/month (potential savings: ~${potential_savings:.2f}/month). "
                        f"Direct Connect is most cost-effective for consistent high-volume transfers."
                    ),
                    confidence_level="medium",
                )
            )

        # Scenario 5: Dev/Test Connection Always Provisioned (LOW - 50)
        if (
            running_hours >= 700
            and ("dev" in connection_name.lower() or "test" in connection_name.lower())
            and connection_state == "available"
        ):
            # Dev/test connection running 24/7
            # Estimate 50% savings by deleting when not needed
            estimated_savings = port_cost * 0.50
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Dev/Test Connection Always Provisioned",
                    priority_level=50,
                    description=(
                        f"Direct Connect connection '{connection_name}' appears to be dev/test "
                        f"provisioned 24/7 ({running_hours}h this month). Dev/test connections should be "
                        f"deleted when not actively used. Port cost: ~${port_cost:.2f}/month."
                    ),
                    estimated_monthly_savings=estimated_savings,
                    recommended_action=(
                        "Delete dev/test Direct Connect connection when not in use. Can recreate when needed, "
                        "though setup time (~1-2 weeks) should be considered. Estimate 50% savings by "
                        "provisioning only during active development/testing periods."
                    ),
                    confidence_level="low",
                )
            )

        return scenarios

    async def scan_direct_connect_connections(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS Direct Connect Connections for cost intelligence.

        AWS Direct Connect provides dedicated network connection from on-premise
        datacenter to AWS, bypassing the internet for consistent network performance.

        CloudWatch Metrics Used:
        - ConnectionState (AWS/DX) - Connection state (7 days)
        - ConnectionBpsEgress (AWS/DX) - Bits per second OUT (30 days)
        - ConnectionBpsIngress (AWS/DX) - Bits per second IN (30 days)
        - ConnectionPpsEgress (AWS/DX) - Packets per second OUT (30 days)
        - ConnectionPpsIngress (AWS/DX) - Packets per second IN (30 days)

        API Calls:
        - directconnect.describe_connections() - List all Direct Connect connections

        Cost Optimization Scenarios:
        1. Connection DOWN Long-Term (CRITICAL) - DOWN 30+ days  Delete ~$216-1,620/month
        2. Connection Zero Traffic (HIGH) - 0 bytes 90 days  Delete
        3. Very Low Bandwidth Usage (MEDIUM) - <10% capacity  Downgrade ~50-80%
        4. Data Transfer > Port Costs (MEDIUM) - Inefficient  Consider VPN
        5. Dev/Test Always Provisioned (LOW) - 24/7  Delete when unused ~50%

        Args:
            region: AWS region to scan

        Returns:
            List of all Direct Connect connections with optimization recommendations
        """
        resources = []

        try:
            async with self.session.client("directconnect", region_name=region) as dx:
                # List all Direct Connect connections
                connections_response = await dx.describe_connections()

                for connection in connections_response.get("connections", []):
                    try:
                        connection_id = connection.get("connectionId")
                        connection_name = connection.get("connectionName", connection_id)
                        connection_state = connection.get("connectionState", "unknown")
                        bandwidth = connection.get("bandwidth", "1Gbps")
                        location = connection.get("location", "")

                        # For connections in certain states, skip cost calculation
                        if connection_state in ["deleted", "deleting"]:
                            running_hours = 0
                        else:
                            # Assume provisioned for full month if active
                            running_hours = 730

                        # Try to get creation time (not directly available in API)
                        # We'll use current time - 30 days as default
                        creation_time = datetime.now(timezone.utc) - timedelta(days=30)

                        # Fetch CloudWatch metrics
                        bytes_out = 0.0
                        bytes_in = 0.0

                        try:
                            async with self.session.client(
                                "cloudwatch", region_name=region
                            ) as cloudwatch:
                                # ConnectionBpsEgress metric (30 days)
                                # Convert Bps to total bytes by integrating over time
                                egress_response = await cloudwatch.get_metric_statistics(
                                    Namespace="AWS/DX",
                                    MetricName="ConnectionBpsEgress",
                                    Dimensions=[
                                        {"Name": "ConnectionId", "Value": connection_id}
                                    ],
                                    StartTime=datetime.now(timezone.utc) - timedelta(days=30),
                                    EndTime=datetime.now(timezone.utc),
                                    Period=2592000,  # 30 days
                                    Statistics=["Average"],
                                )
                                if egress_response.get("Datapoints"):
                                    avg_bps_out = egress_response["Datapoints"][0].get("Average", 0.0)
                                    # Convert average Bps over 30 days to total bytes
                                    bytes_out = avg_bps_out * 30 * 24 * 3600 / 8

                                # ConnectionBpsIngress metric (30 days)
                                ingress_response = await cloudwatch.get_metric_statistics(
                                    Namespace="AWS/DX",
                                    MetricName="ConnectionBpsIngress",
                                    Dimensions=[
                                        {"Name": "ConnectionId", "Value": connection_id}
                                    ],
                                    StartTime=datetime.now(timezone.utc) - timedelta(days=30),
                                    EndTime=datetime.now(timezone.utc),
                                    Period=2592000,  # 30 days
                                    Statistics=["Average"],
                                )
                                if ingress_response.get("Datapoints"):
                                    avg_bps_in = ingress_response["Datapoints"][0].get("Average", 0.0)
                                    bytes_in = avg_bps_in * 30 * 24 * 3600 / 8

                        except Exception as e:
                            logger.warning(
                                "direct_connect.cloudwatch_error",
                                connection_id=connection_id,
                                region=region,
                                error=str(e),
                            )

                        # Calculate monthly cost
                        monthly_cost = self._calculate_direct_connect_monthly_cost(
                            bandwidth, running_hours, bytes_out / (1024**3), region
                        )

                        # Calculate optimization scenarios
                        optimization_scenarios = self._calculate_direct_connect_optimization(
                            connection_id=connection_id,
                            connection_name=connection_name,
                            connection_state=connection_state,
                            bandwidth=bandwidth,
                            running_hours=running_hours,
                            bytes_out_30d=bytes_out,
                            bytes_in_30d=bytes_in,
                            location=location,
                            creation_time=creation_time,
                            region=region,
                        )

                        # Apply detection rules filtering
                        resource_age_days = None
                        if isinstance(creation_time, datetime):
                            resource_age_days = (datetime.utcnow() - creation_time.replace(tzinfo=None)).days

                        if not self._should_include_resource("direct_connect_connection", resource_age_days):
                            logger.debug("inventory.direct_connect_filtered", connection_id=connection_id, age=resource_age_days)
                            continue

                        resources.append(
                            AllCloudResourceData(
                                provider="aws",
                                account_id=self.access_key[:12],
                                region=region,
                                resource_type="direct_connect_connection",
                                resource_id=connection_id,
                                resource_name=connection_name,
                                resource_arn=f"arn:aws:directconnect:{region}:{self.access_key[:12]}:dxcon/{connection_id}",
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata={
                                    "connection_state": connection_state,
                                    "bandwidth": bandwidth,
                                    "location": location,
                                    "running_hours_month": running_hours,
                                    "bytes_out_30d": bytes_out,
                                    "bytes_in_30d": bytes_in,
                                    "bytes_out_gb": round(bytes_out / (1024**3), 2),
                                    "bytes_in_gb": round(bytes_in / (1024**3), 2),
                                },
                                created_at_cloud=creation_time,
                                optimization_scenarios=optimization_scenarios,
                            )
                        )

                        logger.info(
                            "direct_connect.connection_scanned",
                            connection_id=connection_id,
                            region=region,
                            connection_state=connection_state,
                            monthly_cost=monthly_cost,
                            scenarios_count=len(optimization_scenarios),
                        )

                    except Exception as e:
                        logger.warning(
                            "direct_connect.scan_error",
                            connection_id=connection.get("connectionId", "unknown"),
                            region=region,
                            error=str(e),
                        )
                        continue

        except Exception as e:
            logger.error(
                "direct_connect.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    def _calculate_mq_broker_monthly_cost(
        self, instance_type: str, running_hours: int, storage_gb: float, region: str
    ) -> float:
        """
        Calculate estimated monthly cost for AWS MQ Broker.

        MQ pricing = Instance hour cost + Storage cost.

        Args:
            instance_type: Broker instance type (e.g., "mq.t3.micro")
            running_hours: Hours broker is running per month (~730 for 24/7)
            storage_gb: Storage allocated in GB
            region: AWS region

        Returns:
            Estimated monthly cost in USD
        """
        # Broker instance pricing
        instance_hourly_rate = {
            "mq.t3.micro": self.PRICING.get("mq_broker_t3_micro", 0.0375),
            "mq.t3.small": 0.075,
            "mq.t3.medium": 0.150,
            "mq.m5.large": self.PRICING.get("mq_broker_m5_large", 0.598),
            "mq.m5.xlarge": 1.195,
            "mq.m5.2xlarge": 2.390,
            "mq.m5.4xlarge": 4.780,
        }.get(instance_type, 0.0375)  # Default to t3.micro

        # Storage pricing
        storage_monthly_rate = self.PRICING.get("mq_storage_per_gb", 0.10)

        instance_cost = instance_hourly_rate * running_hours
        storage_cost = storage_monthly_rate * storage_gb
        total_cost = instance_cost + storage_cost

        return round(total_cost, 2)

    def _calculate_mq_broker_optimization(
        self,
        broker_id: str,
        broker_name: str,
        broker_state: str,
        instance_type: str,
        running_hours: int,
        total_connections: int,
        total_messages_30d: int,
        cpu_utilization_pct: float,
        heap_usage_pct: float,
        storage_gb: float,
        deployment_mode: str,
        creation_time: datetime,
        region: str,
    ) -> list[OptimizationScenario]:
        """
        Calculate optimization scenarios for AWS MQ Broker.

        Scenarios:
        1. Broker Running Zero Connections (CRITICAL) - 0 connections for 30 days
        2. Broker Never Used (HIGH) - 0 messages for 30 days
        3. Very Low Message Volume (MEDIUM) - <100 messages/day (migrate to SQS/SNS)
        4. Oversized Instance (MEDIUM) - <20% CPU/Heap usage
        5. Dev/Test Broker Always Running (LOW) - Non-prod 24/7

        Args:
            broker_id: MQ broker ID
            broker_name: Broker name
            broker_state: Broker state (RUNNING, CREATION_IN_PROGRESS, etc.)
            instance_type: Instance type (mq.t3.micro, mq.m5.large, etc.)
            running_hours: Hours broker running this month
            total_connections: Total active connections
            total_messages_30d: Total messages sent/received (30 days)
            cpu_utilization_pct: Average CPU utilization (7 days)
            heap_usage_pct: Average heap memory usage (7 days)
            storage_gb: Storage allocated (GB)
            deployment_mode: SINGLE_INSTANCE, ACTIVE_STANDBY_MULTI_AZ, CLUSTER_MULTI_AZ
            creation_time: Broker creation timestamp
            region: AWS region

        Returns:
            List of optimization scenarios
        """
        scenarios = []
        age_days = (datetime.now(timezone.utc) - creation_time).days
        monthly_cost = self._calculate_mq_broker_monthly_cost(
            instance_type, running_hours, storage_gb, region
        )

        # Scenario 1: Broker Running Zero Connections (CRITICAL - 95)
        if broker_state == "RUNNING" and total_connections == 0 and age_days >= 30:
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Broker Running Zero Connections",
                    priority_level=95,
                    description=(
                        f"MQ broker '{broker_name}' (ID: {broker_id}) has 0 active connections "
                        f"for {age_days} days. Broker is running but not serving any clients. "
                        f"Current monthly cost: ~${monthly_cost:.2f}/month."
                    ),
                    estimated_monthly_savings=monthly_cost,
                    recommended_action=(
                        "Delete unused MQ broker. If broker was used for testing or proof-of-concept, "
                        "it should be terminated. Can recreate when needed."
                    ),
                    confidence_level="critical",
                )
            )

        # Scenario 2: Broker Never Used (HIGH - 85)
        if broker_state == "RUNNING" and total_messages_30d == 0 and age_days >= 30:
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Broker Never Used",
                    priority_level=85,
                    description=(
                        f"MQ broker '{broker_name}' has processed 0 messages in the last 30 days "
                        f"(age: {age_days} days). Broker appears completely unused. "
                        f"Current monthly cost: ~${monthly_cost:.2f}/month."
                    ),
                    estimated_monthly_savings=monthly_cost,
                    recommended_action=(
                        "Terminate unused MQ broker. Validate broker is not needed for scheduled "
                        "message processing or disaster recovery scenarios."
                    ),
                    confidence_level="high",
                )
            )

        # Scenario 3: Very Low Message Volume (MEDIUM - 70)
        messages_per_day = total_messages_30d / 30 if total_messages_30d > 0 else 0
        if messages_per_day > 0 and messages_per_day < 100 and broker_state == "RUNNING":
            # Very low message volume - SQS/SNS would be ~90% cheaper
            sqs_cost_estimate = (total_messages_30d / 1000000) * 0.40  # $0.40 per million requests
            estimated_savings = monthly_cost * 0.90  # ~90% savings

            scenarios.append(
                OptimizationScenario(
                    scenario_name="Very Low Message Volume",
                    priority_level=70,
                    description=(
                        f"MQ broker '{broker_name}' has very low message volume "
                        f"({messages_per_day:.0f} messages/day, {total_messages_30d} total in 30 days). "
                        f"For low-volume messaging, SQS/SNS is ~90% cheaper. "
                        f"Current cost: ~${monthly_cost:.2f}/month."
                    ),
                    estimated_monthly_savings=estimated_savings,
                    recommended_action=(
                        f"Migrate to Amazon SQS or SNS for low-volume messaging. Estimated SQS cost: "
                        f"~${sqs_cost_estimate:.2f}/month (savings: ~${estimated_savings:.2f}/month). "
                        f"MQ is cost-effective for high-volume (>10K messages/day) or protocol-specific needs (JMS, AMQP)."
                    ),
                    confidence_level="medium",
                )
            )

        # Scenario 4: Oversized Instance (MEDIUM - 65)
        if (
            broker_state == "RUNNING"
            and cpu_utilization_pct > 0
            and cpu_utilization_pct < 20
            and heap_usage_pct < 20
            and instance_type in ["mq.m5.large", "mq.m5.xlarge", "mq.m5.2xlarge"]
        ):
            # Low CPU and heap usage - can downgrade
            estimated_savings = monthly_cost * 0.50
            downgrade_map = {
                "mq.m5.xlarge": "mq.m5.large",
                "mq.m5.2xlarge": "mq.m5.xlarge",
                "mq.m5.large": "mq.t3.medium",
            }
            suggested_type = downgrade_map.get(instance_type, "mq.t3.micro")

            scenarios.append(
                OptimizationScenario(
                    scenario_name="Oversized Instance",
                    priority_level=65,
                    description=(
                        f"MQ broker '{broker_name}' has low resource utilization "
                        f"(CPU: {cpu_utilization_pct:.1f}%, Heap: {heap_usage_pct:.1f}%) "
                        f"on instance type '{instance_type}'. Instance appears oversized. "
                        f"Current cost: ~${monthly_cost:.2f}/month."
                    ),
                    estimated_monthly_savings=estimated_savings,
                    recommended_action=(
                        f"Downgrade to smaller instance type (e.g., '{suggested_type}') for ~50% savings. "
                        f"Update broker configuration via AWS Console. Monitor performance after downgrade. "
                        f"Note: Downgrade requires broker recreation (brief downtime)."
                    ),
                    confidence_level="medium",
                )
            )

        # Scenario 5: Dev/Test Broker Always Running (LOW - 50)
        if (
            running_hours >= 700  # ~95% uptime
            and ("dev" in broker_name.lower() or "test" in broker_name.lower())
            and broker_state == "RUNNING"
        ):
            # Dev/test broker running 24/7
            estimated_savings = monthly_cost * 0.50
            scenarios.append(
                OptimizationScenario(
                    scenario_name="Dev/Test Broker Always Running",
                    priority_level=50,
                    description=(
                        f"MQ broker '{broker_name}' appears to be dev/test running 24/7 "
                        f"({running_hours}h this month). Non-production brokers should be deleted "
                        f"when not actively used. Current cost: ~${monthly_cost:.2f}/month."
                    ),
                    estimated_monthly_savings=estimated_savings,
                    recommended_action=(
                        "Delete dev/test MQ broker when not in use. Can recreate quickly via "
                        "CloudFormation/Terraform when needed. Estimate 50% savings by running "
                        "only during active development/testing periods."
                    ),
                    confidence_level="low",
                )
            )

        return scenarios

    async def scan_mq_brokers(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS MQ Brokers for cost intelligence.

        AWS MQ is a managed message broker service for Apache ActiveMQ and RabbitMQ
        that enables message queuing and pub/sub messaging patterns.

        CloudWatch Metrics Used:
        - CpuUtilization (AWS/AmazonMQ) - CPU usage percentage (7 days)
        - HeapUsage (AWS/AmazonMQ) - Heap memory usage percentage (7 days)
        - NetworkIn/Out (AWS/AmazonMQ) - Network traffic (30 days)
        - TotalMessageCount (AWS/AmazonMQ) - Total messages sent/received (30 days)
        - ConsumerCount (AWS/AmazonMQ) - Active consumers (current)
        - ProducerCount (AWS/AmazonMQ) - Active producers (current)

        API Calls:
        - mq.list_brokers() - List all MQ brokers
        - mq.describe_broker() - Get broker details

        Cost Optimization Scenarios:
        1. Broker Running Zero Connections (CRITICAL) - 0 connections 30 days  Delete
        2. Broker Never Used (HIGH) - 0 messages 30 days  Terminate
        3. Very Low Message Volume (MEDIUM) - <100 msg/day  Migrate to SQS ~90% cheaper
        4. Oversized Instance (MEDIUM) - <20% CPU/Heap  Downgrade ~50%
        5. Dev/Test Always Running (LOW) - 24/7 non-prod  Delete when unused ~50%

        Args:
            region: AWS region to scan

        Returns:
            List of all MQ brokers with optimization recommendations
        """
        resources = []

        try:
            async with self.session.client("mq", region_name=region) as mq:
                # List all MQ brokers
                brokers_response = await mq.list_brokers()

                for broker_summary in brokers_response.get("BrokerSummaries", []):
                    try:
                        broker_id = broker_summary.get("BrokerId")
                        broker_name = broker_summary.get("BrokerName", broker_id)
                        broker_state = broker_summary.get("BrokerState", "UNKNOWN")
                        deployment_mode = broker_summary.get("DeploymentMode", "SINGLE_INSTANCE")

                        # Get detailed broker information
                        broker_response = await mq.describe_broker(BrokerId=broker_id)
                        broker = broker_response

                        # Get instance type and storage
                        instance_type = broker.get("HostInstanceType", "mq.t3.micro")
                        storage_type = broker.get("StorageType", "ebs")

                        # Get storage size (default 20 GB if not specified)
                        if broker.get("Users"):
                            # Storage info not directly available, estimate based on instance type
                            storage_gb = 20.0  # Default EBS storage
                        else:
                            storage_gb = 20.0

                        creation_time = broker.get("Created", datetime.now(timezone.utc))

                        # Calculate running hours
                        age_hours = (
                            datetime.now(timezone.utc) - creation_time
                        ).total_seconds() / 3600
                        if broker_state == "RUNNING":
                            running_hours = min(age_hours, 730)  # Cap at 1 month
                        elif broker_state in ["DELETION_IN_PROGRESS"]:
                            running_hours = 0
                        else:
                            running_hours = age_hours * 0.5  # Estimate for other states

                        running_hours = int(running_hours)

                        # Fetch CloudWatch metrics
                        cpu_utilization_pct = 0.0
                        heap_usage_pct = 0.0
                        total_messages = 0
                        consumer_count = 0
                        producer_count = 0

                        try:
                            async with self.session.client(
                                "cloudwatch", region_name=region
                            ) as cloudwatch:
                                # CpuUtilization metric (7 days)
                                cpu_response = await cloudwatch.get_metric_statistics(
                                    Namespace="AWS/AmazonMQ",
                                    MetricName="CpuUtilization",
                                    Dimensions=[{"Name": "Broker", "Value": broker_name}],
                                    StartTime=datetime.now(timezone.utc) - timedelta(days=7),
                                    EndTime=datetime.now(timezone.utc),
                                    Period=604800,  # 7 days
                                    Statistics=["Average"],
                                )
                                if cpu_response.get("Datapoints"):
                                    cpu_utilization_pct = cpu_response["Datapoints"][0].get(
                                        "Average", 0.0
                                    )

                                # HeapUsage metric (7 days)
                                heap_response = await cloudwatch.get_metric_statistics(
                                    Namespace="AWS/AmazonMQ",
                                    MetricName="HeapUsage",
                                    Dimensions=[{"Name": "Broker", "Value": broker_name}],
                                    StartTime=datetime.now(timezone.utc) - timedelta(days=7),
                                    EndTime=datetime.now(timezone.utc),
                                    Period=604800,  # 7 days
                                    Statistics=["Average"],
                                )
                                if heap_response.get("Datapoints"):
                                    heap_usage_pct = heap_response["Datapoints"][0].get(
                                        "Average", 0.0
                                    )

                                # TotalMessageCount metric (30 days)
                                messages_response = await cloudwatch.get_metric_statistics(
                                    Namespace="AWS/AmazonMQ",
                                    MetricName="TotalMessageCount",
                                    Dimensions=[{"Name": "Broker", "Value": broker_name}],
                                    StartTime=datetime.now(timezone.utc) - timedelta(days=30),
                                    EndTime=datetime.now(timezone.utc),
                                    Period=2592000,  # 30 days
                                    Statistics=["Sum"],
                                )
                                if messages_response.get("Datapoints"):
                                    total_messages = int(
                                        messages_response["Datapoints"][0].get("Sum", 0)
                                    )

                                # ConsumerCount metric (current)
                                consumer_response = await cloudwatch.get_metric_statistics(
                                    Namespace="AWS/AmazonMQ",
                                    MetricName="ConsumerCount",
                                    Dimensions=[{"Name": "Broker", "Value": broker_name}],
                                    StartTime=datetime.now(timezone.utc) - timedelta(hours=1),
                                    EndTime=datetime.now(timezone.utc),
                                    Period=3600,  # 1 hour
                                    Statistics=["Average"],
                                )
                                if consumer_response.get("Datapoints"):
                                    consumer_count = int(
                                        consumer_response["Datapoints"][0].get("Average", 0)
                                    )

                        except Exception as e:
                            logger.warning(
                                "mq.cloudwatch_error",
                                broker_id=broker_id,
                                region=region,
                                error=str(e),
                            )

                        # Total connections = consumers + producers
                        total_connections = consumer_count + producer_count

                        # Calculate monthly cost
                        monthly_cost = self._calculate_mq_broker_monthly_cost(
                            instance_type, running_hours, storage_gb, region
                        )

                        # Calculate optimization scenarios
                        optimization_scenarios = self._calculate_mq_broker_optimization(
                            broker_id=broker_id,
                            broker_name=broker_name,
                            broker_state=broker_state,
                            instance_type=instance_type,
                            running_hours=running_hours,
                            total_connections=total_connections,
                            total_messages_30d=total_messages,
                            cpu_utilization_pct=cpu_utilization_pct,
                            heap_usage_pct=heap_usage_pct,
                            storage_gb=storage_gb,
                            deployment_mode=deployment_mode,
                            creation_time=creation_time,
                            region=region,
                        )

                        # Get broker ARN
                        broker_arn = broker.get("BrokerArn", "")
                        engine_type = broker.get("EngineType", "ACTIVEMQ")
                        engine_version = broker.get("EngineVersion", "")

                        # Apply detection rules filtering
                        resource_age_days = None
                        if isinstance(creation_time, datetime):
                            resource_age_days = (datetime.utcnow() - creation_time.replace(tzinfo=None)).days

                        if not self._should_include_resource("mq_broker", resource_age_days):
                            logger.debug("inventory.mq_broker_filtered", broker_id=broker_id, age=resource_age_days)
                            continue

                        resources.append(
                            AllCloudResourceData(
                                provider="aws",
                                account_id=self.access_key[:12],
                                region=region,
                                resource_type="mq_broker",
                                resource_id=broker_id,
                                resource_name=broker_name,
                                resource_arn=broker_arn,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata={
                                    "broker_state": broker_state,
                                    "instance_type": instance_type,
                                    "deployment_mode": deployment_mode,
                                    "engine_type": engine_type,
                                    "engine_version": engine_version,
                                    "storage_type": storage_type,
                                    "storage_gb": storage_gb,
                                    "running_hours_month": running_hours,
                                    "total_connections": total_connections,
                                    "total_messages_30d": total_messages,
                                    "cpu_utilization_pct": round(cpu_utilization_pct, 2),
                                    "heap_usage_pct": round(heap_usage_pct, 2),
                                    "creation_time": creation_time.isoformat(),
                                },
                                created_at_cloud=creation_time,
                                optimization_scenarios=optimization_scenarios,
                            )
                        )

                        logger.info(
                            "mq.broker_scanned",
                            broker_id=broker_id,
                            broker_name=broker_name,
                            region=region,
                            broker_state=broker_state,
                            monthly_cost=monthly_cost,
                            scenarios_count=len(optimization_scenarios),
                        )

                    except Exception as e:
                        logger.warning(
                            "mq.scan_error",
                            broker_id=broker_summary.get("BrokerId", "unknown"),
                            region=region,
                            error=str(e),
                        )
                        continue

        except Exception as e:
            logger.error(
                "mq.scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    def _calculate_kendra_monthly_cost(
        self, edition: str, running_hours: int, region: str
    ) -> float:
        """
        Calculate estimated monthly cost for AWS Kendra Index.

        Args:
            edition: Kendra edition ('DEVELOPER_EDITION' or 'ENTERPRISE_EDITION')
            running_hours: Number of hours the index was running (up to 730 hours/month)
            region: AWS region

        Returns:
            Estimated monthly cost in USD
        """
        # Get pricing for Kendra edition
        if edition == "ENTERPRISE_EDITION":
            hourly_rate = self.pricing.get("kendra_enterprise_edition_per_hour", 1.40)
        else:  # DEVELOPER_EDITION
            hourly_rate = self.pricing.get("kendra_developer_edition_per_hour", 0.97)

        # Calculate monthly cost (max 730 hours/month)
        monthly_hours = min(running_hours, 730)
        monthly_cost = hourly_rate * monthly_hours

        return monthly_cost

    def _calculate_kendra_optimization(
        self,
        index_id: str,
        index_name: str,
        index_status: str,
        edition: str,
        created_at: datetime,
        updated_at: datetime,
        document_count: int,
        region: str,
        tags: dict[str, str],
    ) -> list[OptimizationScenario]:
        """
        Calculate optimization scenarios for AWS Kendra Index.

        Amazon Kendra is an intelligent search service powered by machine learning.
        Costs can be significant ($700-$1,008/month), so unused or underutilized indexes
        should be identified and optimized.

        5 Optimization Scenarios:
        1. Index Running Zero Queries (CRITICAL) - No queries in 90 days
        2. Index Never Synced (HIGH) - No data sync in 60+ days
        3. Very Low Query Volume (MEDIUM) - <10 queries/day (OpenSearch ~70% cheaper)
        4. Oversized Enterprise Edition (MEDIUM) - Low usage on Enterprise tier
        5. Dev/Test Always Running (LOW) - Non-prod index running 24/7

        Args:
            index_id: Kendra Index ID
            index_name: Index name
            index_status: Index status (ACTIVE, CREATING, DELETING, FAILED, SYSTEM_UPDATING)
            edition: Edition (DEVELOPER_EDITION, ENTERPRISE_EDITION)
            created_at: Index creation timestamp
            updated_at: Last update timestamp
            document_count: Number of indexed documents
            region: AWS region
            tags: Resource tags

        Returns:
            List of optimization scenarios
        """
        scenarios: list[OptimizationScenario] = []
        monthly_cost = self._calculate_kendra_monthly_cost(edition, 730, region)

        # Helper: Get environment tag
        env = tags.get("Environment", tags.get("environment", "unknown")).lower()

        # Scenario 1: Index Running Zero Queries (CRITICAL)
        # Kendra doesn't expose query metrics in CloudWatch by default
        # We estimate based on last sync time - if no sync in 90 days, likely no queries
        days_since_update = (datetime.now() - updated_at).days
        if days_since_update >= 90:
            scenarios.append(
                OptimizationScenario(
                    priority="critical",
                    optimization_score=95,
                    estimated_monthly_savings=monthly_cost,
                    recommended_action=(
                        f"Delete Kendra index with zero queries in 90 days:\n"
                        f"1. Verify no active queries: Check CloudWatch Logs Insights\n"
                        f"2. aws kendra delete-index --index-id {index_id} --region {region}\n\n"
                        f" Monthly Savings: ${monthly_cost:.2f}\n\n"
                        f"Alternative: If search needed, migrate to OpenSearch (~70% cheaper for low volume)"
                    ),
                    confidence_level="critical",
                )
            )

        # Scenario 2: Index Never Synced (HIGH)
        # If document count is 0 or very low, index was never properly used
        if document_count == 0 and days_since_update >= 60:
            scenarios.append(
                OptimizationScenario(
                    priority="high",
                    optimization_score=85,
                    estimated_monthly_savings=monthly_cost,
                    recommended_action=(
                        f"Delete Kendra index that was never synced (0 documents for 60+ days):\n"
                        f"1. Index ID: {index_id}\n"
                        f"2. Document Count: {document_count}\n"
                        f"3. aws kendra delete-index --index-id {index_id} --region {region}\n\n"
                        f" Monthly Savings: ${monthly_cost:.2f}"
                    ),
                    confidence_level="high",
                )
            )

        # Scenario 3: Very Low Query Volume (MEDIUM)
        # If index has documents but low usage, OpenSearch might be better
        # We estimate low usage if: small document count (<1000) AND old update (30+ days)
        if (
            document_count > 0
            and document_count < 1000
            and days_since_update >= 30
            and "zero queries" not in [s.recommended_action for s in scenarios]
        ):
            opensearch_monthly_cost = 75  # ~$75/month for small OpenSearch domain (t3.small)
            potential_savings = monthly_cost - opensearch_monthly_cost

            scenarios.append(
                OptimizationScenario(
                    priority="medium",
                    optimization_score=70,
                    estimated_monthly_savings=potential_savings,
                    recommended_action=(
                        f"Migrate low-volume Kendra index to OpenSearch (~70% cheaper):\n"
                        f"1. Current Cost: ${monthly_cost:.2f}/month (Kendra)\n"
                        f"2. Estimated Cost: ${opensearch_monthly_cost}/month (OpenSearch)\n"
                        f"3. Document Count: {document_count} (low volume)\n"
                        f"4. Last Update: {days_since_update} days ago\n\n"
                        f" Monthly Savings: ${potential_savings:.2f}\n\n"
                        "Migration Steps:\n"
                        "1. Create OpenSearch domain (t3.small)\n"
                        "2. Export documents from Kendra\n"
                        "3. Import to OpenSearch with same schema\n"
                        "4. Update application to use OpenSearch API"
                    ),
                    confidence_level="medium",
                )
            )

        # Scenario 4: Oversized Enterprise Edition (MEDIUM)
        # If using Enterprise but very low document count, Developer edition sufficient
        if edition == "ENTERPRISE_EDITION" and document_count < 5000:
            developer_cost = self._calculate_kendra_monthly_cost("DEVELOPER_EDITION", 730, region)
            potential_savings = monthly_cost - developer_cost

            scenarios.append(
                OptimizationScenario(
                    priority="medium",
                    optimization_score=65,
                    estimated_monthly_savings=potential_savings,
                    recommended_action=(
                        f"Downgrade from Enterprise to Developer edition (30% savings):\n"
                        f"1. Current Cost: ${monthly_cost:.2f}/month (Enterprise)\n"
                        f"2. Developer Cost: ${developer_cost:.2f}/month\n"
                        f"3. Document Count: {document_count} (Developer supports up to 5,000)\n\n"
                        f" Monthly Savings: ${potential_savings:.2f}\n\n"
                        "Downgrade Steps:\n"
                        "1. Create new Developer edition index\n"
                        "2. Migrate documents from Enterprise index\n"
                        "3. Update application to use new index\n"
                        "4. Delete old Enterprise index"
                    ),
                    confidence_level="medium",
                )
            )

        # Scenario 5: Dev/Test Always Running (LOW)
        # Non-production indexes should be deleted when not in use
        if env in ["dev", "development", "test", "testing", "staging"]:
            estimated_savings = monthly_cost * 0.5  # Assume 50% savings by running only during work hours

            scenarios.append(
                OptimizationScenario(
                    priority="low",
                    optimization_score=50,
                    estimated_monthly_savings=estimated_savings,
                    recommended_action=(
                        f"Delete dev/test Kendra index when not in use:\n"
                        f"1. Environment: {env}\n"
                        f"2. aws kendra delete-index --index-id {index_id} --region {region}\n"
                        f"3. Recreate when needed (CloudFormation/Terraform)\n\n"
                        f" Monthly Savings: ${estimated_savings:.2f}\n\n"
                        "Note: Kendra doesn't support stop/start like RDS. Must delete and recreate."
                    ),
                    confidence_level="low",
                )
            )

        return scenarios

    async def scan_kendra_indexes(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS Kendra Indexes for cost intelligence.

        Amazon Kendra is an intelligent search service powered by machine learning
        that allows users to search across different content repositories with natural language.

        Pricing:
        - Enterprise Edition: ~$1.40/hour (~$1,008/month)
        - Developer Edition: ~$0.97/hour (~$700/month)
        - Additional query charges apply

        Kendra doesn't expose CloudWatch metrics for query volume by default.
        We estimate usage based on:
        - Last sync/update time
        - Document count
        - Index status

        Cost Optimization Scenarios:
        1. Index Running Zero Queries (CRITICAL) - No updates 90 days  Delete ~$700-1,008/month
        2. Index Never Synced (HIGH) - 0 documents 60+ days  Delete unused index
        3. Very Low Query Volume (MEDIUM) - <1000 docs + old  OpenSearch ~70% cheaper
        4. Oversized Enterprise (MEDIUM) - <5000 docs  Developer ~30% savings
        5. Dev/Test Always Running (LOW) - Non-prod 24/7  Delete ~50% savings

        Args:
            region: AWS region to scan

        Returns:
            List of all Kendra indexes with cost optimization scenarios
        """
        resources: list[AllCloudResourceData] = []

        try:
            async with self.session.client("kendra", region_name=region) as kendra:
                # List all Kendra indexes
                paginator = kendra.get_paginator("list_indices")
                async for page in paginator.paginate():
                    for index_summary in page.get("IndexConfigurationSummaryItems", []):
                        try:
                            index_id = index_summary.get("Id")
                            index_name = index_summary.get("Name", index_id)
                            index_status = index_summary.get("Status", "UNKNOWN")
                            edition = index_summary.get("Edition", "DEVELOPER_EDITION")
                            created_at = index_summary.get("CreatedAt", datetime.now())
                            updated_at = index_summary.get("UpdatedAt", created_at)

                            # Get detailed index information
                            index_details = await kendra.describe_index(Id=index_id)
                            index_config = index_details.get("IndexStatistics", {})
                            document_count = index_config.get("TextDocumentStatistics", {}).get(
                                "IndexedTextDocumentsCount", 0
                            )

                            # Get tags (optional - skip if error)
                            tags = {}
                            try:
                                # Note: We can't easily get account_id without additional API call (STS GetCallerIdentity)
                                # For now, we skip tags to avoid errors. Tags are optional for optimization scenarios.
                                pass
                            except Exception:
                                pass  # Tags are optional, continue without them

                            # Calculate monthly cost (assume always running = 730 hours/month)
                            monthly_cost = self._calculate_kendra_monthly_cost(edition, 730, region)

                            # Calculate optimization scenarios
                            optimization_scenarios = self._calculate_kendra_optimization(
                                index_id=index_id,
                                index_name=index_name,
                                index_status=index_status,
                                edition=edition,
                                created_at=created_at,
                                updated_at=updated_at,
                                document_count=document_count,
                                region=region,
                                tags=tags,
                            )

                            # Calculate resource age for detection rules filtering
                            resource_age_days = None
                            if isinstance(created_at, datetime):
                                resource_age_days = (datetime.now() - created_at).days

                            # Apply detection rules filtering
                            if not self._should_include_resource("kendra_index", resource_age_days):
                                logger.debug(
                                    "inventory.kendra_filtered",
                                    index_id=index_id,
                                    resource_age_days=resource_age_days,
                                )
                                continue

                            # Create resource data
                            resource = AllCloudResourceData(
                                resource_type="kendra_index",
                                resource_id=index_id,
                                resource_name=index_name,
                                region=region,
                                estimated_monthly_cost=monthly_cost,
                                currency="USD",
                                resource_metadata={
                                    "index_id": index_id,
                                    "index_name": index_name,
                                    "status": index_status,
                                    "edition": edition,
                                    "document_count": document_count,
                                    "created_at": created_at.isoformat() if isinstance(created_at, datetime) else str(created_at),
                                    "updated_at": updated_at.isoformat() if isinstance(updated_at, datetime) else str(updated_at),
                                    "days_since_update": (datetime.now() - updated_at).days if isinstance(updated_at, datetime) else 0,
                                    "tags": tags,
                                },
                                optimization_scenarios=optimization_scenarios,
                                is_optimizable=len(optimization_scenarios) > 0,
                            )

                            resources.append(resource)
                            logger.info(
                                "inventory.kendra_scanned",
                                index_id=index_id,
                                index_name=index_name,
                                status=index_status,
                                monthly_cost=monthly_cost,
                                scenarios=len(optimization_scenarios),
                            )

                        except Exception as e:
                            logger.warning(
                                "inventory.kendra_scan_error",
                                index_id=index_summary.get("Id", "unknown"),
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "inventory.kendra_scan_failed",
                region=region,
                error=str(e),
            )

        return resources

    def _calculate_cloudformation_monthly_cost(
        self, resource_count: int, region: str
    ) -> float:
        """
        Calculate estimated monthly cost for AWS CloudFormation Stack.

        CloudFormation itself is FREE - no charges for using the service.
        Cost comes from the resources created and managed by the stack.

        Args:
            resource_count: Number of resources in the stack
            region: AWS region

        Returns:
            Estimated monthly cost (always 0 for CloudFormation service itself)
        """
        # CloudFormation itself is FREE - only underlying resources have cost
        # We return 0 here, but list resource count in metadata
        return 0.0

    def _calculate_cloudformation_optimization(
        self,
        stack_name: str,
        stack_id: str,
        stack_status: str,
        creation_time: datetime,
        last_updated_time: datetime | None,
        resource_count: int,
        tags: dict[str, str],
        region: str,
    ) -> list[OptimizationScenario]:
        """
        Calculate optimization scenarios for AWS CloudFormation Stack.

        CloudFormation is Infrastructure-as-Code (IaC) service - FREE itself.
        However, failed/abandoned stacks clutter the console and may retain resources
        that continue to cost money.

        5 Optimization Scenarios:
        1. Stack in ROLLBACK_COMPLETE (CRITICAL) - Failed deployment cleanup
        2. Stack with Zero Resources (HIGH) - Empty/deleted stack remnant
        3. Stack Never Updated (MEDIUM) - Created >365 days, 0 updates (zombie)
        4. Stack in DELETE_FAILED (MEDIUM) - Partial deletion (manual cleanup)
        5. Dev/Test Stack Old (LOW) - Created >180 days, dev/test environment

        Args:
            stack_name: CloudFormation stack name
            stack_id: Stack ID (ARN)
            stack_status: Stack status (CREATE_COMPLETE, ROLLBACK_COMPLETE, etc.)
            creation_time: Stack creation timestamp
            last_updated_time: Last update timestamp (None if never updated)
            resource_count: Number of resources in the stack
            tags: Stack tags
            region: AWS region

        Returns:
            List of optimization scenarios
        """
        scenarios: list[OptimizationScenario] = []

        # Helper: Get environment tag
        env = tags.get("Environment", tags.get("environment", "unknown")).lower()

        # Helper: Calculate age
        days_since_creation = (datetime.now() - creation_time).days
        days_since_update = (
            (datetime.now() - last_updated_time).days if last_updated_time else days_since_creation
        )

        # Scenario 1: Stack in ROLLBACK_COMPLETE (CRITICAL)
        # Failed stack that should be cleaned up
        if stack_status == "ROLLBACK_COMPLETE":
            scenarios.append(
                OptimizationScenario(
                    priority="critical",
                    optimization_score=95,
                    estimated_monthly_savings=0.0,  # CloudFormation is free, but cleanup is important
                    recommended_action=(
                        f"Delete failed CloudFormation stack in ROLLBACK_COMPLETE state:\n"
                        f"1. Stack Name: {stack_name}\n"
                        f"2. Status: {stack_status}\n"
                        f"3. aws cloudformation delete-stack --stack-name {stack_name} --region {region}\n\n"
                        " ROLLBACK_COMPLETE indicates deployment failed and rolled back.\n"
                        "This stack is non-functional and should be deleted to clean up console.\n\n"
                        "Before deletion:\n"
                        "1. Review CloudFormation events to understand failure\n"
                        "2. Verify no resources are retained (check Retain policy)\n"
                        "3. Fix issues in template and redeploy"
                    ),
                    confidence_level="critical",
                )
            )

        # Scenario 2: Stack with Zero Resources (HIGH)
        # Empty stack (all resources deleted but stack remains)
        if resource_count == 0 and stack_status != "DELETE_IN_PROGRESS":
            scenarios.append(
                OptimizationScenario(
                    priority="high",
                    optimization_score=85,
                    estimated_monthly_savings=0.0,
                    recommended_action=(
                        f"Delete CloudFormation stack with zero resources:\n"
                        f"1. Stack Name: {stack_name}\n"
                        f"2. Resource Count: {resource_count}\n"
                        f"3. aws cloudformation delete-stack --stack-name {stack_name} --region {region}\n\n"
                        "This stack has no resources (likely already manually deleted).\n"
                        "Clean up to avoid console clutter."
                    ),
                    confidence_level="high",
                )
            )

        # Scenario 3: Stack Never Updated (MEDIUM)
        # Created >365 days ago, never updated - likely abandoned
        if (
            days_since_creation > 365
            and last_updated_time is None
            and resource_count > 0
            and stack_status == "CREATE_COMPLETE"
        ):
            scenarios.append(
                OptimizationScenario(
                    priority="medium",
                    optimization_score=70,
                    estimated_monthly_savings=0.0,
                    recommended_action=(
                        f"Review old CloudFormation stack never updated in {days_since_creation} days:\n"
                        f"1. Stack Name: {stack_name}\n"
                        f"2. Created: {days_since_creation} days ago\n"
                        f"3. Resource Count: {resource_count}\n\n"
                        "Actions:\n"
                        "1. Verify stack is still needed (check with team)\n"
                        "2. Review resources: aws cloudformation describe-stack-resources "
                        f"--stack-name {stack_name} --region {region}\n"
                        "3. If abandoned, delete: aws cloudformation delete-stack --stack-name "
                        f"{stack_name} --region {region}\n\n"
                        " Stack unchanged for 1+ year may indicate abandoned resources."
                    ),
                    confidence_level="medium",
                )
            )

        # Scenario 4: Stack in DELETE_FAILED (MEDIUM)
        # Partial deletion - manual cleanup needed
        if stack_status == "DELETE_FAILED":
            scenarios.append(
                OptimizationScenario(
                    priority="medium",
                    optimization_score=65,
                    estimated_monthly_savings=0.0,
                    recommended_action=(
                        f"Fix CloudFormation stack in DELETE_FAILED state:\n"
                        f"1. Stack Name: {stack_name}\n"
                        f"2. aws cloudformation describe-stack-events --stack-name {stack_name} "
                        f"--region {region}\n\n"
                        "Common causes of DELETE_FAILED:\n"
                        "- Resource has DeletionPolicy: Retain\n"
                        "- Resource manually deleted outside CloudFormation\n"
                        "- S3 bucket not empty\n"
                        "- Security group in use\n\n"
                        "Manual cleanup steps:\n"
                        "1. Identify failed resource in events\n"
                        "2. Manually delete blocking resource\n"
                        "3. Retry: aws cloudformation delete-stack --stack-name {stack_name} "
                        f"--region {region}"
                    ),
                    confidence_level="medium",
                )
            )

        # Scenario 5: Dev/Test Stack Old (LOW)
        # Dev/test stack created >180 days ago
        if env in ["dev", "development", "test", "testing", "staging"] and days_since_creation > 180:
            scenarios.append(
                OptimizationScenario(
                    priority="low",
                    optimization_score=50,
                    estimated_monthly_savings=0.0,
                    recommended_action=(
                        f"Review old dev/test CloudFormation stack ({days_since_creation} days):\n"
                        f"1. Stack Name: {stack_name}\n"
                        f"2. Environment: {env}\n"
                        f"3. Resource Count: {resource_count}\n\n"
                        "Actions:\n"
                        "1. Verify stack still needed for testing\n"
                        "2. List resources and check costs: aws cloudformation describe-stack-resources "
                        f"--stack-name {stack_name} --region {region}\n"
                        "3. If unused, delete: aws cloudformation delete-stack --stack-name "
                        f"{stack_name} --region {region}\n\n"
                        " Tip: Use temporary dev/test stacks and clean up regularly."
                    ),
                    confidence_level="low",
                )
            )

        return scenarios

    async def scan_cloudformation_stacks(self, region: str) -> list[AllCloudResourceData]:
        """
        Scan ALL AWS CloudFormation Stacks for cost intelligence.

        AWS CloudFormation is an Infrastructure as Code (IaC) service that allows
        you to model and provision AWS resources using templates.

        Pricing:
        - CloudFormation itself is FREE (no charges)
        - Only the resources created by stacks have costs
        - Focus on cleanup of failed/abandoned stacks

        CloudFormation doesn't have CloudWatch metrics.
        We analyze stack status and metadata:
        - Stack status (ROLLBACK_COMPLETE, DELETE_FAILED, etc.)
        - Resource count
        - Last update time
        - Tags

        Cost Optimization Scenarios:
        1. Stack in ROLLBACK_COMPLETE (CRITICAL) - Failed deployment  Cleanup console
        2. Stack with Zero Resources (HIGH) - Empty stack  Delete remnant
        3. Stack Never Updated (MEDIUM) - Created >365 days, 0 updates  Review zombie
        4. Stack in DELETE_FAILED (MEDIUM) - Partial deletion  Manual cleanup
        5. Dev/Test Stack Old (LOW) - Created >180 days, dev/test  Review if needed

        Args:
            region: AWS region to scan

        Returns:
            List of all CloudFormation stacks with optimization scenarios
        """
        resources: list[AllCloudResourceData] = []

        try:
            async with self.session.client("cloudformation", region_name=region) as cfn:
                # List all stacks (including deleted in last 90 days)
                # We filter out DELETE_COMPLETE stacks as they're already cleaned up
                paginator = cfn.get_paginator("list_stacks")
                async for page in paginator.paginate():
                    for stack_summary in page.get("StackSummaries", []):
                        try:
                            stack_status = stack_summary.get("StackStatus", "UNKNOWN")

                            # Skip fully deleted stacks
                            if stack_status == "DELETE_COMPLETE":
                                continue

                            stack_name = stack_summary.get("StackName")
                            stack_id = stack_summary.get("StackId")
                            creation_time = stack_summary.get("CreationTime", datetime.now())
                            last_updated_time = stack_summary.get("LastUpdatedTime", None)

                            # Get stack details including tags
                            stack_details = await cfn.describe_stacks(StackName=stack_name)
                            stack_info = stack_details["Stacks"][0]
                            tags = {tag["Key"]: tag["Value"] for tag in stack_info.get("Tags", [])}

                            # Get resource count
                            try:
                                resources_response = await cfn.list_stack_resources(StackName=stack_name)
                                resource_count = len(resources_response.get("StackResourceSummaries", []))
                            except Exception:
                                # If stack is in failed state, might not be able to list resources
                                resource_count = 0

                            # CloudFormation itself is FREE - cost is 0
                            monthly_cost = 0.0

                            # Calculate optimization scenarios
                            optimization_scenarios = self._calculate_cloudformation_optimization(
                                stack_name=stack_name,
                                stack_id=stack_id,
                                stack_status=stack_status,
                                creation_time=creation_time,
                                last_updated_time=last_updated_time,
                                resource_count=resource_count,
                                tags=tags,
                                region=region,
                            )

                            # Create resource data
                            resource = AllCloudResourceData(
                                resource_type="cloudformation_stack",
                                resource_id=stack_id,
                                resource_name=stack_name,
                                region=region,
                                estimated_monthly_cost=monthly_cost,  # CloudFormation is FREE
                                currency="USD",
                                resource_metadata={
                                    "stack_name": stack_name,
                                    "stack_id": stack_id,
                                    "stack_status": stack_status,
                                    "resource_count": resource_count,
                                    "creation_time": creation_time.isoformat() if isinstance(creation_time, datetime) else str(creation_time),
                                    "last_updated_time": last_updated_time.isoformat() if isinstance(last_updated_time, datetime) and last_updated_time else None,
                                    "days_since_creation": (datetime.now() - creation_time).days if isinstance(creation_time, datetime) else 0,
                                    "tags": tags,
                                },
                                optimization_scenarios=optimization_scenarios,
                                is_optimizable=len(optimization_scenarios) > 0,
                            )

                            resources.append(resource)
                            logger.info(
                                "inventory.cloudformation_scanned",
                                stack_name=stack_name,
                                stack_status=stack_status,
                                resource_count=resource_count,
                                scenarios=len(optimization_scenarios),
                            )

                        except Exception as e:
                            logger.warning(
                                "inventory.cloudformation_scan_error",
                                stack_name=stack_summary.get("StackName", "unknown"),
                                error=str(e),
                            )
                            continue

        except Exception as e:
            logger.error(
                "inventory.cloudformation_scan_failed",
                region=region,
                error=str(e),
            )

        return resources
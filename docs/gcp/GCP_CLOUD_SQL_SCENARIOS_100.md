# CloudWaste - Couverture 100% GCP Cloud SQL

**Resource Type:** `Database : Cloud SQL`
**Provider:** Google Cloud Platform (GCP)
**API:** `sqladmin.googleapis.com` (Cloud SQL Admin API v1)
**√âquivalents:** AWS RDS, Azure SQL Database
**Total Scenarios:** 10 (100% coverage)

---

## üìã Table des Mati√®res

- [Vue d'Ensemble](#vue-densemble)
- [Mod√®le de Pricing Cloud SQL](#mod√®le-de-pricing-cloud-sql)
- [Phase 1 - D√©tection Simple (7 sc√©narios)](#phase-1---d√©tection-simple-7-sc√©narios)
  - [1. Instances Arr√™t√©es >30 Jours](#1-cloud_sql_stopped---instances-arr√™t√©es-30-jours)
  - [2. Instances Idle (Zero Connections)](#2-cloud_sql_idle---instances-idle-zero-connections)
  - [3. Instances Over-Provisioned](#3-cloud_sql_overprovisioned---instances-over-provisioned)
  - [4. Ancien Type de Machine](#4-cloud_sql_old_machine_type---ancien-type-de-machine)
  - [5. Instances Dev/Test 24/7](#5-cloud_sql_devtest_247---instances-devtest-247)
  - [6. Read Replicas Inutilis√©s](#6-cloud_sql_unused_replicas---read-replicas-inutilis√©s)
  - [7. Instances Non Tagu√©es](#7-cloud_sql_untagged---instances-non-tagu√©es)
- [Phase 2 - D√©tection Avanc√©e (3 sc√©narios)](#phase-2---d√©tection-avanc√©e-3-sc√©narios)
  - [8. Instances avec Z√©ro I/O](#8-cloud_sql_zero_io---instances-avec-z√©ro-io)
  - [9. Storage Over-Provisioned](#9-cloud_sql_storage_overprovisioned---storage-over-provisioned)
  - [10. High Availability Inutile](#10-cloud_sql_unnecessary_ha---high-availability-inutile)
- [Protocole de Test](#protocole-de-test)
- [R√©f√©rences](#r√©f√©rences)

---

## Vue d'Ensemble

### Contexte Cloud SQL

**Cloud SQL** est le service de bases de donn√©es relationnelles **manag√©** de GCP, supportant :

- **MySQL** (5.6, 5.7, 8.0)
- **PostgreSQL** (9.6, 10, 11, 12, 13, 14, 15)
- **SQL Server** (2017 Standard, 2017 Enterprise, 2019 Standard, 2019 Enterprise)

### Caract√©ristiques Principales

| Feature | Description | Impact Co√ªt |
|---------|-------------|-------------|
| **Machine Types** | Shared-core, Standard, High-memory | Core pricing component |
| **Storage** | SSD (performance) ou HDD (√©conomique) | $0.17/GB (SSD), $0.09/GB (HDD) |
| **Backups** | Automatiques (7-365 jours r√©tention) | $0.08/GB/mois |
| **High Availability** | Multi-zone failover | +100% instance cost |
| **Read Replicas** | Scaling lecture | Full instance cost/replica |
| **Private IP** | VPC peering | Inclus |
| **Public IP** | Internet access | Inclus |

### √âtats d'Instance et Facturation

| √âtat | Instance Cost | Storage Cost | Backup Cost | Notes |
|------|--------------|--------------|-------------|-------|
| **RUNNABLE** | ‚úÖ 100% | ‚úÖ 100% | ‚úÖ 100% | Factur√© pleinement |
| **STOPPED** | ‚ùå 0% | ‚úÖ 100% | ‚úÖ 100% | √âconomie instance uniquement |
| **SUSPENDED** | ‚ùå 0% | ‚úÖ 100% | ‚úÖ 100% | Suspendu (billing issue) |
| **DELETED** | ‚ùå 0% | ‚ùå 0% | ‚ùå 0% | Plus de co√ªts |

**‚ö†Ô∏è Important :** Instance arr√™t√©e continue de payer **storage + backups** (~30-50% du co√ªt total).

### Waste Typique

1. **Instances arr√™t√©es** : Storage + backups factur√©s ($20-100/mois)
2. **Instances idle** : Zero connections = 100% gaspillage
3. **Over-provisioning** : db-n1-standard-8 pour 10 connexions = surco√ªt
4. **High Availability** : HA activ√© pour dev/test = +100% co√ªt inutile
5. **Read replicas** : R√©plicas sans lecture = instance compl√®te gaspill√©e
6. **Storage over-provisioned** : 500 GB allou√©, 50 GB utilis√© = 450 GB √ó $0.17
7. **Backups excessifs** : R√©tention 365 jours pour dev = co√ªts cumul√©s

---

## Mod√®le de Pricing Cloud SQL

### Machine Types Pricing (par mois, us-central1)

#### MySQL / PostgreSQL

| Machine Type | vCPUs | RAM (GB) | Prix/Mois | Use Case |
|-------------|-------|---------|-----------|----------|
| **db-f1-micro** | Shared | 0.6 GB | $7.67 | Dev/Test minimal |
| **db-g1-small** | Shared | 1.7 GB | $25.00 | Dev/Test l√©ger |
| **db-n1-standard-1** | 1 | 3.75 GB | $46.20 | Small production |
| **db-n1-standard-2** | 2 | 7.5 GB | $92.40 | Medium workloads |
| **db-n1-standard-4** | 4 | 15 GB | $184.80 | Production standard |
| **db-n1-standard-8** | 8 | 30 GB | $369.60 | Large production |
| **db-n1-highmem-2** | 2 | 13 GB | $113.52 | Memory-intensive |
| **db-n1-highmem-4** | 4 | 26 GB | $227.04 | High-memory DB |
| **db-custom-2-7680** | 2 | 7.5 GB | $51.10 | Custom (flexible) |
| **db-custom-4-15360** | 4 | 15 GB | $102.20 | Custom (flexible) |

**Notes :**
- db-n1 = g√©n√©ration ancienne (standard depuis ann√©es)
- db-custom = personnalisable (vCPU + RAM s√©par√©ment)
- Pricing inclut management (pas de fee s√©par√© comme RDS)

#### SQL Server

| Edition | Machine Type | vCPUs | RAM (GB) | Prix/Mois |
|---------|-------------|-------|---------|-----------|
| **Standard 2019** | db-custom-2-7680 | 2 | 7.5 GB | $351.00 |
| **Standard 2019** | db-custom-4-15360 | 4 | 15 GB | $702.00 |
| **Enterprise 2019** | db-custom-4-15360 | 4 | 15 GB | $2,106.00 |

**Note :** SQL Server significativement plus cher (licences Microsoft incluses).

### Storage Pricing

| Type | Prix/GB/Mois | Throughput | Use Case |
|------|-------------|-----------|----------|
| **SSD** | $0.17 | Haute performance | Production, OLTP |
| **HDD** | $0.09 | Standard | Dev/Test, archivage |

**Capacit√© :** 10 GB √† 64 TB (SSD), 10 GB √† 64 TB (HDD)

### Backups Pricing

- **Co√ªt :** $0.08/GB/mois
- **R√©tention :** 1 √† 365 jours (7 jours par d√©faut)
- **Premi√®re backup compl√®te, ensuite incr√©mentales**

Exemple : 100 GB database, backups 30 jours
```
Backup storage ‚âà 150 GB (complet + incr√©mentaux)
Co√ªt = 150 GB √ó $0.08 = $12.00/mois
```

### High Availability (HA)

- **Co√ªt :** +100% du co√ªt instance (standby replica dans autre zone)
- **Storage :** R√©pliqu√© automatiquement (inclus)
- **Failover :** Automatique (<60 secondes)

Exemple : db-n1-standard-2 avec HA
```
Instance sans HA: $92.40/mois
Instance avec HA: $92.40 √ó 2 = $184.80/mois
```

### Read Replicas

- **Co√ªt :** Instance compl√®te (m√™me machine type que master)
- **Storage :** S√©par√© (r√©pliqu√© depuis master)
- **Use case :** Scaling lecture, analytique

Exemple : 1 master + 2 read replicas (db-n1-standard-2)
```
Master: $92.40/mois
Replica 1: $92.40/mois
Replica 2: $92.40/mois
TOTAL: $277.20/mois
```

### Exemple Co√ªt Total

**Production MySQL (db-n1-standard-4 + HA + 500 GB SSD):**
```
Instance (no HA): $184.80/mois
High Availability: $184.80/mois (standby)
Storage SSD: 500 GB √ó $0.17 = $85.00/mois
Backups: ~750 GB √ó $0.08 = $60.00/mois (30 jours r√©tention)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOTAL: $514.60/mois (~$6,175/an)
```

---

## Phase 1 - D√©tection Simple (7 sc√©narios)

### 1. `cloud_sql_stopped` - Instances Arr√™t√©es >30 Jours

#### D√©tection

**Logique :**
```python
# 1. Lister toutes les instances Cloud SQL
from google.cloud import sql_v1

sql_client = sql_v1.SqlInstancesServiceClient()

parent = f"projects/{project_id}"
instances = sql_client.list(parent=parent)

# 2. Pour chaque instance, v√©rifier state
for instance in instances:
    instance_name = instance.name
    state = instance.state  # RUNNABLE, STOPPED, SUSPENDED

    # 3. D√©tection si state = STOPPED
    if state == sql_v1.Instance.State.STOPPED:
        # 4. Calculer depuis quand arr√™t√©e
        # Note: API ne fournit pas lastStopTime, utiliser state_change_timestamp
        # Ou analyser logs audit pour trouver derni√®re action STOP

        # Approximation: v√©rifier age instance et si jamais RUNNABLE r√©cemment
        creation_time = instance.create_time
        age_days = (now - creation_time).days

        # Si instance cr√©√©e >30 jours et STOPPED, probable waste
        if age_days >= min_age_days:
            # Instance arr√™t√©e = waste d√©tect√©
```

**Crit√®res :**
- `state == 'STOPPED'`
- `age >= min_age_days` (d√©faut: 30 jours)

**API Calls :**
```python
# Cloud SQL Admin API
from google.cloud import sql_v1

sql_client = sql_v1.SqlInstancesServiceClient()

# Lister instances
instances = sql_client.list(
    parent=f"projects/{project_id}"
)

# Get instance details
instance = sql_client.get(
    name=f"projects/{project_id}/instances/{instance_name}"
)
```

#### Calcul de Co√ªt

**Formule :**

Instance arr√™t√©e = paiement storage + backups (pas instance) :

```python
# Instance arr√™t√©e ne paie PAS le co√ªt instance
# MAIS paie storage + backups

# R√©cup√©rer config instance
database_engine = instance.database_version  # MYSQL_8_0, POSTGRES_14, etc.
machine_type = instance.settings.tier  # "db-n1-standard-2"
storage_size_gb = instance.settings.data_disk_size_gb
storage_type = instance.settings.data_disk_type  # PD_SSD, PD_HDD

# Storage cost (continue m√™me si arr√™t√©)
storage_pricing = {
    'PD_SSD': 0.17,   # $/GB/mois
    'PD_HDD': 0.09,
}

storage_cost = storage_size_gb * storage_pricing.get(storage_type, 0.17)

# Backup cost (continue m√™me si arr√™t√©)
# Estimation: backups ‚âà 1.5x database size (complet + incr√©mentaux)
backup_size_gb = storage_size_gb * 1.5
backup_price_per_gb = 0.08

backup_cost = backup_size_gb * backup_price_per_gb

# Co√ªt mensuel = storage + backups
monthly_cost = storage_cost + backup_cost

# Co√ªt gaspill√© depuis arr√™t
stopped_days = age_days  # Approximation (si toujours arr√™t√©)
already_wasted = monthly_cost * (stopped_days / 30.0)
```

**Exemple :**

Instance MySQL arr√™t√©e avec 500 GB SSD depuis 60 jours :
```python
storage_size_gb = 500
storage_type = 'PD_SSD'
storage_cost = 500 * $0.17 = $85.00/mois

backup_size_gb = 500 * 1.5 = 750 GB
backup_cost = 750 * $0.08 = $60.00/mois

monthly_cost = $85.00 + $60.00 = $145.00/mois
already_wasted = $145.00 * (60/30) = $290.00
```

**Note :** Si instance jamais red√©marr√©e, recommandation = cr√©er snapshot final et supprimer.

#### Param√®tres Configurables

| Param√®tre | Type | D√©faut | Description |
|-----------|------|--------|-------------|
| `min_age_days` | int | 30 | √Çge minimum arr√™t avant d√©tection |
| `exclude_labels` | dict | `{}` | Labels pour exclure instances |

#### M√©tadonn√©es Exemple

```json
{
  "resource_id": "instance-1234567890",
  "resource_name": "prod-mysql-stopped",
  "resource_type": "cloud_sql_stopped",
  "region": "us-central1",
  "database_version": "MYSQL_8_0",
  "state": "STOPPED",
  "tier": "db-n1-standard-2",
  "storage_size_gb": 500,
  "storage_type": "PD_SSD",
  "creation_time": "2024-07-05T10:00:00Z",
  "age_days": 120,
  "stopped_days": 60,
  "storage_cost_monthly": 85.00,
  "backup_cost_monthly": 60.00,
  "estimated_monthly_cost": 145.00,
  "already_wasted": 290.00,
  "confidence": "high",
  "recommendation": "Create final snapshot and delete instance",
  "detection_date": "2024-11-02T14:30:00Z"
}
```

#### Fichier d'Impl√©mentation

**Backend :** `/backend/app/providers/gcp.py` (√† impl√©menter)

---

### 2. `cloud_sql_idle` - Instances Idle (Zero Connections)

#### D√©tection

**Logique :**
```python
# 1. Lister toutes les instances RUNNABLE
instances = sql_client.list(parent=f"projects/{project_id}")

runnable_instances = [i for i in instances if i.state == sql_v1.Instance.State.RUNNABLE]

# 2. Pour chaque instance, r√©cup√©rer m√©triques connexions
from google.cloud import monitoring_v3

monitoring_client = monitoring_v3.MetricServiceClient()

for instance in runnable_instances:
    instance_name = instance.name

    # 3. Query active connections (14 derniers jours)
    interval = monitoring_v3.TimeInterval({
        "end_time": {"seconds": int(time.time())},
        "start_time": {"seconds": int(time.time()) - 14*24*3600},
    })

    # M√©trique: cloudsql.googleapis.com/database/network/connections
    connections_metrics = monitoring_client.list_time_series(
        request={
            "name": f"projects/{project_id}",
            "filter": f'resource.type="cloudsql_database" AND resource.database_id="{project_id}:{instance_name}" AND metric.type="cloudsql.googleapis.com/database/network/connections"',
            "interval": interval,
        }
    )

    # 4. Calculer connexions moyennes et max
    connection_values = [
        point.value.double_value
        for series in connections_metrics
        for point in series.points
    ]

    if not connection_values:
        # Pas de m√©triques = instance cr√©√©e r√©cemment ou probl√®me
        continue

    avg_connections = sum(connection_values) / len(connection_values)
    max_connections = max(connection_values)

    # 5. D√©tection si zero connections
    if avg_connections == 0 and max_connections == 0:
        # Instance idle = waste d√©tect√©
```

**Crit√®res :**
- `state == 'RUNNABLE'`
- `avg_connections == 0` sur 14 jours
- `max_connections == 0` (jamais de connexion)

**API Calls :**
```python
# Cloud SQL Admin API
sql_client.list(parent=f"projects/{project_id}")

# Cloud Monitoring API
monitoring_client.list_time_series(
    name=f"projects/{project_id}",
    filter='metric.type="cloudsql.googleapis.com/database/network/connections"'
)
```

#### Calcul de Co√ªt

**Formule :**

Instance idle = 100% waste (aucune utilisation) :

```python
# R√©cup√©rer co√ªt instance
machine_type = instance.settings.tier  # "db-n1-standard-2"

# Pricing machine types (MySQL/PostgreSQL)
machine_pricing = {
    'db-f1-micro': 7.67,
    'db-g1-small': 25.00,
    'db-n1-standard-1': 46.20,
    'db-n1-standard-2': 92.40,
    'db-n1-standard-4': 184.80,
    'db-n1-standard-8': 369.60,
    'db-custom-2-7680': 51.10,
    'db-custom-4-15360': 102.20,
}

instance_cost = machine_pricing.get(machine_type, 0)

# Storage cost
storage_size_gb = instance.settings.data_disk_size_gb
storage_type = instance.settings.data_disk_type

storage_pricing = {'PD_SSD': 0.17, 'PD_HDD': 0.09}
storage_cost = storage_size_gb * storage_pricing.get(storage_type, 0.17)

# Backup cost
backup_size_gb = storage_size_gb * 1.5
backup_cost = backup_size_gb * 0.08

# High Availability (si activ√©)
ha_enabled = instance.settings.availability_type == 'REGIONAL'  # HA enabled
if ha_enabled:
    instance_cost = instance_cost * 2  # Standby replica

# Co√ªt total mensuel = 100% waste
monthly_cost = instance_cost + storage_cost + backup_cost

# Co√ªt gaspill√© depuis cr√©ation (si jamais utilis√©)
creation_time = instance.create_time
age_days = (now - creation_time).days
already_wasted = monthly_cost * (age_days / 30.0)
```

**Exemple :**

Instance db-n1-standard-2 + HA + 200 GB SSD, idle depuis cr√©ation (90 jours) :
```python
instance_cost = $92.40 * 2 = $184.80 (avec HA)
storage_cost = 200 * $0.17 = $34.00
backup_cost = 300 * $0.08 = $24.00
monthly_cost = $242.80
already_wasted = $242.80 * (90/30) = $728.40
```

#### Param√®tres Configurables

| Param√®tre | Type | D√©faut | Description |
|-----------|------|--------|-------------|
| `lookback_days` | int | 14 | P√©riode analyse connexions |
| `min_connections_threshold` | float | 0.0 | Connexions min pour √™tre actif |
| `exclude_labels` | dict | `{}` | Labels pour exclure |

#### M√©tadonn√©es Exemple

```json
{
  "resource_id": "instance-9876543210",
  "resource_name": "unused-postgres-db",
  "resource_type": "cloud_sql_idle",
  "region": "us-east1",
  "database_version": "POSTGRES_14",
  "state": "RUNNABLE",
  "tier": "db-n1-standard-2",
  "availability_type": "REGIONAL",
  "storage_size_gb": 200,
  "storage_type": "PD_SSD",
  "connection_metrics": {
    "avg_connections_14d": 0.0,
    "max_connections_14d": 0.0
  },
  "creation_time": "2024-08-05T09:00:00Z",
  "age_days": 89,
  "instance_cost_monthly": 184.80,
  "storage_cost_monthly": 34.00,
  "backup_cost_monthly": 24.00,
  "estimated_monthly_cost": 242.80,
  "already_wasted": 721.13,
  "confidence": "high",
  "recommendation": "Delete instance - zero connections in 14 days",
  "detection_date": "2024-11-02T14:30:00Z"
}
```

#### Fichier d'Impl√©mentation

**Backend :** `/backend/app/providers/gcp.py` (√† impl√©menter)

---

### 3. `cloud_sql_overprovisioned` - Instances Over-Provisioned

#### D√©tection

**Logique :**
```python
# 1. Lister toutes les instances RUNNABLE
instances = sql_client.list(parent=f"projects/{project_id}")

runnable_instances = [i for i in instances if i.state == sql_v1.Instance.State.RUNNABLE]

# 2. Pour chaque instance, r√©cup√©rer m√©triques CPU et Memory
from google.cloud import monitoring_v3

monitoring_client = monitoring_v3.MetricServiceClient()

for instance in runnable_instances:
    instance_name = instance.name

    # 3. Query CPU utilization (14 jours)
    interval = monitoring_v3.TimeInterval({
        "end_time": {"seconds": int(time.time())},
        "start_time": {"seconds": int(time.time()) - 14*24*3600},
    })

    # CPU metric
    cpu_metrics = monitoring_client.list_time_series(
        request={
            "name": f"projects/{project_id}",
            "filter": f'resource.type="cloudsql_database" AND resource.database_id="{project_id}:{instance_name}" AND metric.type="cloudsql.googleapis.com/database/cpu/utilization"',
            "interval": interval,
        }
    )

    cpu_values = [point.value.double_value for series in cpu_metrics for point in series.points]
    avg_cpu = (sum(cpu_values) / len(cpu_values) * 100) if cpu_values else 0

    # Memory metric
    memory_metrics = monitoring_client.list_time_series(
        request={
            "name": f"projects/{project_id}",
            "filter": f'resource.type="cloudsql_database" AND resource.database_id="{project_id}:{instance_name}" AND metric.type="cloudsql.googleapis.com/database/memory/utilization"',
            "interval": interval,
        }
    )

    memory_values = [point.value.double_value for series in memory_metrics for point in series.points]
    avg_memory = (sum(memory_values) / len(memory_values) * 100) if memory_values else 0

    # 4. D√©tection si CPU <30% ET Memory <40%
    if avg_cpu < cpu_threshold and avg_memory < memory_threshold:
        # 5. Calculer machine type recommand√©e (downgrade)
        current_tier = instance.settings.tier
        recommended_tier = calculate_recommended_tier(current_tier, avg_cpu, avg_memory)

        # Over-provisioned = waste d√©tect√©
```

**Crit√®res :**
- `state == 'RUNNABLE'`
- `avg_cpu < 30%` ET `avg_memory < 40%` sur 14 jours
- Possibilit√© downgrade machine type

**API Calls :**
```python
# Cloud SQL Admin API
sql_client.list(parent=f"projects/{project_id}")

# Cloud Monitoring API
monitoring_client.list_time_series(
    filter='metric.type="cloudsql.googleapis.com/database/cpu/utilization"'
)

monitoring_client.list_time_series(
    filter='metric.type="cloudsql.googleapis.com/database/memory/utilization"'
)
```

#### Calcul de Co√ªt

**Formule :**

Over-provisioning = diff√©rence co√ªt actuel vs recommand√© :

```python
# Exemple: db-n1-standard-4 avec CPU 20%, Memory 30%

current_tier = 'db-n1-standard-4'
current_cost = 184.80  # $/mois

# Recommandation: db-n1-standard-2 (moiti√© ressources)
recommended_tier = 'db-n1-standard-2'
recommended_cost = 92.40  # $/mois

# Waste instance
instance_waste = current_cost - recommended_cost  # $92.40

# Si HA activ√©, waste doubl√©
ha_enabled = instance.settings.availability_type == 'REGIONAL'
if ha_enabled:
    instance_waste = instance_waste * 2  # Standby aussi over-provisioned

# Storage et backups identiques (pas de waste)
storage_cost = 100 * 0.17  # $17.00 (identique)
backup_cost = 150 * 0.08   # $12.00 (identique)

# Waste total
monthly_waste = instance_waste

# Co√ªt gaspill√© depuis cr√©ation
age_months = age_days / 30.0
already_wasted = monthly_waste * age_months
```

**Exemple :**

Instance db-n1-standard-4 + HA avec CPU 20%, Memory 30% depuis 120 jours :
```python
current_instance_cost = $184.80 * 2 = $369.60 (HA)
recommended_instance_cost = $92.40 * 2 = $184.80 (HA)
monthly_waste = $184.80
already_wasted = $184.80 * (120/30) = $739.20
```

#### Param√®tres Configurables

| Param√®tre | Type | D√©faut | Description |
|-----------|------|--------|-------------|
| `cpu_threshold` | float | 30.0 | CPU % maximum pour over-provisioning |
| `memory_threshold` | float | 40.0 | Memory % maximum |
| `lookback_days` | int | 14 | P√©riode analyse m√©triques |
| `min_savings_threshold` | float | 20.0 | √âconomie min $/mois |

#### M√©tadonn√©es Exemple

```json
{
  "resource_id": "instance-5555555555",
  "resource_name": "overprovisioned-mysql",
  "resource_type": "cloud_sql_overprovisioned",
  "region": "europe-west1",
  "database_version": "MYSQL_8_0",
  "state": "RUNNABLE",
  "tier": "db-n1-standard-4",
  "availability_type": "REGIONAL",
  "cpu_metrics": {
    "avg_cpu_14d": 22.3,
    "max_cpu_14d": 45.1
  },
  "memory_metrics": {
    "avg_memory_14d": 31.8,
    "max_memory_14d": 52.0
  },
  "current_cost_monthly": 369.60,
  "recommended_tier": "db-n1-standard-2",
  "recommended_cost_monthly": 184.80,
  "estimated_monthly_waste": 184.80,
  "already_wasted": 739.20,
  "confidence": "high",
  "recommendation": "Downgrade to db-n1-standard-2 (half resources)",
  "detection_date": "2024-11-02T14:30:00Z"
}
```

#### Fichier d'Impl√©mentation

**Backend :** `/backend/app/providers/gcp.py` (√† impl√©menter)

---

### 4. `cloud_sql_old_machine_type` - Ancien Type de Machine

#### D√©tection

**Logique :**
```python
# 1. Lister toutes les instances
instances = sql_client.list(parent=f"projects/{project_id}")

for instance in instances:
    tier = instance.settings.tier  # "db-n1-standard-2"

    # 2. D√©tection si db-n1 (ancienne g√©n√©ration)
    if tier.startswith('db-n1-'):
        # 3. Calculer √©quivalent db-custom (plus flexible)
        # db-n1-standard-2 (2 vCPU, 7.5 GB) ‚Üí db-custom-2-7680

        # Extraire vCPUs depuis tier name
        # db-n1-standard-X o√π X = vCPUs
        if 'standard' in tier:
            vcpus = int(tier.split('-')[-1])
            memory_gb = vcpus * 3.75  # n1-standard ratio

            # √âquivalent db-custom
            custom_tier = f"db-custom-{vcpus}-{int(memory_gb * 1024)}"
            # db-custom-2-7680 (2 vCPU, 7.5 GB)

        # 4. Comparer co√ªts
        n1_cost = get_tier_cost(tier)
        custom_cost = get_tier_cost(custom_tier)

        # db-custom souvent ~10% plus cher mais plus flexible
        # Recommandation si active connections et b√©n√©fice flexibilit√©

        # Ou recommander db-standard (nouvelle g√©n√©ration si disponible)
        # Pour l'instant, db-n1 encore standard, pas de g√©n√©ration plus r√©cente

        # Note: Sc√©nario moins critique que AWS/Azure car GCP n'a pas encore db-n2
        # Garder pour futur-proofing
```

**Crit√®res :**
- `tier.startswith('db-n1-')`
- Optionnel: migration vers db-custom pour flexibilit√©

**API Calls :**
```python
# Cloud SQL Admin API
sql_client.list(parent=f"projects/{project_id}")
```

#### Calcul de Co√ªt

**Formule :**

db-n1 vs db-custom = trade-off flexibilit√© :

```python
# Exemple: db-n1-standard-2

current_tier = 'db-n1-standard-2'
current_cost = 92.40  # $/mois

# √âquivalent db-custom
recommended_tier = 'db-custom-2-7680'
recommended_cost = 51.10  # $/mois

# db-custom MOINS CHER ! (-45%)
monthly_waste = current_cost - recommended_cost  # $41.30

# Si HA
if ha_enabled:
    monthly_waste = monthly_waste * 2  # $82.60

# Co√ªt gaspill√©
age_months = age_days / 30.0
already_wasted = monthly_waste * age_months
```

**Exemple :**

Instance db-n1-standard-2 + HA depuis 180 jours :
```python
current_cost = $92.40 * 2 = $184.80 (HA)
recommended_cost = $51.10 * 2 = $102.20 (HA, db-custom)
monthly_waste = $82.60
already_wasted = $82.60 * (180/30) = $495.60
```

**Note :** db-custom offre aussi flexibilit√© (ajuster vCPU/RAM ind√©pendamment).

#### Param√®tres Configurables

| Param√®tre | Type | D√©faut | Description |
|-----------|------|--------|-------------|
| `old_tiers` | list | `['db-n1']` | Tiers consid√©r√©s anciens |
| `preferred_tier_type` | str | `'db-custom'` | Type recommand√© |
| `min_savings_threshold` | float | 10.0 | √âconomie min $/mois |

#### M√©tadonn√©es Exemple

```json
{
  "resource_id": "instance-3333333333",
  "resource_name": "legacy-tier-mysql",
  "resource_type": "cloud_sql_old_machine_type",
  "region": "us-central1",
  "database_version": "MYSQL_8_0",
  "state": "RUNNABLE",
  "tier": "db-n1-standard-2",
  "availability_type": "REGIONAL",
  "current_cost_monthly": 184.80,
  "recommended_tier": "db-custom-2-7680",
  "recommended_cost_monthly": 102.20,
  "estimated_monthly_waste": 82.60,
  "already_wasted": 495.60,
  "savings_percentage": 44.7,
  "confidence": "medium",
  "recommendation": "Migrate to db-custom-2-7680 for -45% cost and better flexibility",
  "detection_date": "2024-11-02T14:30:00Z"
}
```

#### Fichier d'Impl√©mentation

**Backend :** `/backend/app/providers/gcp.py` (√† impl√©menter)

---

### 5. `cloud_sql_devtest_247` - Instances Dev/Test 24/7

#### D√©tection

**Logique :**
```python
# 1. Lister toutes les instances RUNNABLE
instances = sql_client.list(parent=f"projects/{project_id}")

runnable_instances = [i for i in instances if i.state == sql_v1.Instance.State.RUNNABLE]

# 2. Pour chaque instance, v√©rifier labels
for instance in runnable_instances:
    labels = instance.settings.user_labels if hasattr(instance.settings, 'user_labels') else {}
    environment = labels.get('environment', '').lower()

    # 3. D√©tection si environment = dev/test/staging
    if environment in ['dev', 'test', 'staging', 'development']:
        # 4. V√©rifier uptime
        creation_time = instance.create_time
        age_days = (now - creation_time).days

        # 5. D√©tection si uptime >7 jours continus
        if age_days >= min_uptime_days:
            # 6. Calculer √©conomie arr√™ts nocturnes/weekends
            # Business hours: 8h-20h Lun-Ven = 60h/semaine
            # Actuel: 24/7 = 168h/semaine
            # √âconomie: (168-60)/168 = 64%

            # Dev/Test 24/7 = waste d√©tect√©
```

**Crit√®res :**
- `state == 'RUNNABLE'`
- `labels.environment in ['dev', 'test', 'staging']`
- `uptime_days >= 7`

**API Calls :**
```python
# Cloud SQL Admin API
sql_client.list(parent=f"projects/{project_id}")
```

#### Calcul de Co√ªt

**Formule :**

Instance dev 24/7 vs horaires business :

```python
# Instance dev: db-n1-standard-2 + 100 GB SSD

# Co√ªt actuel (24/7)
instance_cost = 92.40  # $/mois
storage_cost = 100 * 0.17 = 17.00  # $/mois (reste)
backup_cost = 150 * 0.08 = 12.00   # $/mois (reste)

monthly_cost = 92.40 + 17.00 + 12.00 = 121.40  # $/mois

# Co√ªt optimal (60h/semaine)
hours_optimal = 60
hours_actual = 168

# Instance peut √™tre arr√™t√©e ‚Üí √©conomie 64%
optimal_instance_cost = instance_cost * (hours_optimal / hours_actual)  # $33.00

# Storage + backups restent (instance arr√™t√©e)
optimal_cost = optimal_instance_cost + storage_cost + backup_cost  # $62.00

monthly_waste = monthly_cost - optimal_cost  # $59.40

# Co√ªt gaspill√© depuis cr√©ation
age_months = age_days / 30.0
already_wasted = monthly_waste * age_months
```

**Exemple :**

Instance dev db-n1-standard-2 + 100 GB depuis 90 jours :
```python
current_cost = $121.40/mois (24/7)
optimal_cost = $62.00/mois (60h/semaine)
monthly_waste = $59.40
already_wasted = $59.40 * (90/30) = $178.20
```

#### Param√®tres Configurables

| Param√®tre | Type | D√©faut | Description |
|-----------|------|--------|-------------|
| `devtest_labels` | list | `['dev', 'test', 'staging']` | Labels environnements non-prod |
| `min_uptime_days` | int | 7 | Uptime minimum pour d√©tection |
| `business_hours_per_week` | int | 60 | Heures optimales/semaine |

#### M√©tadonn√©es Exemple

```json
{
  "resource_id": "instance-7777777777",
  "resource_name": "dev-postgres-db",
  "resource_type": "cloud_sql_devtest_247",
  "region": "us-east1",
  "database_version": "POSTGRES_14",
  "state": "RUNNABLE",
  "tier": "db-n1-standard-2",
  "labels": {
    "environment": "dev",
    "team": "backend"
  },
  "creation_time": "2024-08-05T09:00:00Z",
  "uptime_days": 89,
  "current_uptime_hours_weekly": 168,
  "optimal_uptime_hours_weekly": 60,
  "current_cost_monthly": 121.40,
  "optimal_cost_monthly": 62.00,
  "estimated_monthly_waste": 59.40,
  "already_wasted": 176.53,
  "waste_percentage": 49,
  "confidence": "high",
  "recommendation": "Implement automated start/stop schedule (8am-8pm Mon-Fri)",
  "detection_date": "2024-11-02T14:30:00Z"
}
```

#### Fichier d'Impl√©mentation

**Backend :** `/backend/app/providers/gcp.py` (√† impl√©menter)

---

### 6. `cloud_sql_unused_replicas` - Read Replicas Inutilis√©s

#### D√©tection

**Logique :**
```python
# 1. Lister toutes les instances
instances = sql_client.list(parent=f"projects/{project_id}")

# 2. Pour chaque instance, identifier read replicas
for instance in instances:
    # V√©rifier si instance est un replica
    if instance.replica_configuration:
        # C'est un read replica

        # 3. R√©cup√©rer master instance
        # Note: API Cloud SQL ne fournit pas directement master name
        # Utiliser instance.master_instance_name (disponible dans certaines versions API)

        instance_name = instance.name

        # 4. R√©cup√©rer m√©triques read queries (14 jours)
        from google.cloud import monitoring_v3

        monitoring_client = monitoring_v3.MetricServiceClient()

        interval = monitoring_v3.TimeInterval({
            "end_time": {"seconds": int(time.time())},
            "start_time": {"seconds": int(time.time()) - 14*24*3600},
        })

        # M√©trique: cloudsql.googleapis.com/database/queries
        # Filtrer SELECT queries
        queries_metrics = monitoring_client.list_time_series(
            request={
                "name": f"projects/{project_id}",
                "filter": f'resource.type="cloudsql_database" AND resource.database_id="{project_id}:{instance_name}" AND metric.type="cloudsql.googleapis.com/database/queries"',
                "interval": interval,
            }
        )

        total_queries = sum([
            point.value.int64_value
            for series in queries_metrics
            for point in series.points
        ])

        # 5. D√©tection si zero queries
        if total_queries == 0:
            # Read replica inutilis√© = waste (co√ªt instance compl√®te)
```

**Crit√®res :**
- Instance est un read replica
- `total_queries == 0` sur 14 jours
- Replica actif (state = RUNNABLE)

**API Calls :**
```python
# Cloud SQL Admin API
sql_client.list(parent=f"projects/{project_id}")

# Cloud Monitoring API
monitoring_client.list_time_series(
    filter='metric.type="cloudsql.googleapis.com/database/queries"'
)
```

#### Calcul de Co√ªt

**Formule :**

Read replica inutilis√© = 100% co√ªt instance :

```python
# Read replica = instance compl√®te (m√™me tier que master)

tier = instance.settings.tier  # "db-n1-standard-2"
instance_cost = get_tier_cost(tier)  # $92.40

# Storage (r√©pliqu√© depuis master)
storage_size_gb = instance.settings.data_disk_size_gb
storage_cost = storage_size_gb * 0.17  # SSD

# Backup (read replicas ont leurs propres backups)
backup_size_gb = storage_size_gb * 1.5
backup_cost = backup_size_gb * 0.08

# Co√ªt total = 100% waste (replica jamais utilis√©)
monthly_cost = instance_cost + storage_cost + backup_cost

# Co√ªt gaspill√© depuis cr√©ation replica
creation_time = instance.create_time
age_days = (now - creation_time).days
already_wasted = monthly_cost * (age_days / 30.0)
```

**Exemple :**

Read replica db-n1-standard-2 + 200 GB SSD inutilis√© depuis 60 jours :
```python
instance_cost = $92.40
storage_cost = 200 * $0.17 = $34.00
backup_cost = 300 * $0.08 = $24.00
monthly_cost = $150.40
already_wasted = $150.40 * (60/30) = $300.80
```

#### Param√®tres Configurables

| Param√®tre | Type | D√©faut | Description |
|-----------|------|--------|-------------|
| `lookback_days` | int | 14 | P√©riode analyse queries |
| `min_queries_threshold` | int | 0 | Queries minimum pour √™tre actif |

#### M√©tadonn√©es Exemple

```json
{
  "resource_id": "instance-8888888888",
  "resource_name": "mysql-read-replica-1",
  "resource_type": "cloud_sql_unused_replicas",
  "region": "us-west1",
  "database_version": "MYSQL_8_0",
  "state": "RUNNABLE",
  "tier": "db-n1-standard-2",
  "is_replica": true,
  "master_instance": "prod-mysql-master",
  "storage_size_gb": 200,
  "storage_type": "PD_SSD",
  "query_metrics": {
    "total_queries_14d": 0
  },
  "creation_time": "2024-09-05T10:00:00Z",
  "age_days": 58,
  "instance_cost_monthly": 92.40,
  "storage_cost_monthly": 34.00,
  "backup_cost_monthly": 24.00,
  "estimated_monthly_cost": 150.40,
  "already_wasted": 290.11,
  "confidence": "high",
  "recommendation": "Delete read replica - zero queries in 14 days",
  "detection_date": "2024-11-02T14:30:00Z"
}
```

#### Fichier d'Impl√©mentation

**Backend :** `/backend/app/providers/gcp.py` (√† impl√©menter)

---

### 7. `cloud_sql_untagged` - Instances Non Tagu√©es

#### D√©tection

**Logique :**
```python
# 1. Lister toutes les instances
instances = sql_client.list(parent=f"projects/{project_id}")

# 2. D√©finir labels requis (configurables)
required_labels = ['environment', 'owner', 'cost-center', 'project']

# 3. Pour chaque instance, v√©rifier labels
for instance in instances:
    labels = instance.settings.user_labels if hasattr(instance.settings, 'user_labels') else {}

    # 4. Identifier labels manquants
    missing_labels = [label for label in required_labels if label not in labels]

    # 5. D√©tection si labels manquants
    if missing_labels:
        # Untagged instance = governance waste
```

**Crit√®res :**
- Labels manquants parmi la liste requise
- Optionnel: valeurs de labels invalides

**API Calls :**
```python
# Cloud SQL Admin API
sql_client.list(parent=f"projects/{project_id}")
```

#### Calcul de Co√ªt

**Formule :**

Co√ªt de gouvernance (estim√©) :

```python
# Instances non tagu√©es = perte de visibilit√© + risque co√ªt
# Co√ªt estim√© = 5% du co√ªt instance (estimation)

# Calculer co√ªt instance total
tier = instance.settings.tier
instance_cost = get_tier_cost(tier)

storage_size_gb = instance.settings.data_disk_size_gb
storage_cost = storage_size_gb * 0.17

backup_size_gb = storage_size_gb * 1.5
backup_cost = backup_size_gb * 0.08

# HA
ha_enabled = instance.settings.availability_type == 'REGIONAL'
if ha_enabled:
    instance_cost = instance_cost * 2

instance_monthly_cost = instance_cost + storage_cost + backup_cost

# Governance waste = 5%
governance_waste_percentage = 0.05
monthly_waste = instance_monthly_cost * governance_waste_percentage

# Waste cumul√© depuis cr√©ation
age_months = age_days / 30.0
already_wasted = monthly_waste * age_months
```

**Exemple :**

Instance db-n1-standard-2 + HA + 200 GB sans labels depuis 180 jours :
```python
instance_monthly_cost = $184.80 + $34.00 + $24.00 = $242.80
monthly_waste = $242.80 * 0.05 = $12.14
already_wasted = $12.14 * (180/30) = $72.84
```

#### Param√®tres Configurables

| Param√®tre | Type | D√©faut | Description |
|-----------|------|--------|-------------|
| `required_labels` | list | `['environment', 'owner', 'cost-center']` | Labels requis |
| `governance_waste_pct` | float | 0.05 | % co√ªt attribu√© au waste gouvernance |
| `enforce_values` | dict | `{}` | Valeurs autoris√©es par label |

#### M√©tadonn√©es Exemple

```json
{
  "resource_id": "instance-1010101010",
  "resource_name": "unnamed-mysql-47",
  "resource_type": "cloud_sql_untagged",
  "region": "europe-west4",
  "database_version": "MYSQL_8_0",
  "state": "RUNNABLE",
  "tier": "db-n1-standard-2",
  "availability_type": "REGIONAL",
  "labels": {},
  "missing_labels": ["environment", "owner", "cost-center", "project"],
  "creation_time": "2024-05-06T08:00:00Z",
  "age_days": 180,
  "instance_monthly_cost": 242.80,
  "estimated_monthly_waste": 12.14,
  "already_wasted": 72.84,
  "confidence": "medium",
  "recommendation": "Add required labels for cost allocation and governance",
  "detection_date": "2024-11-02T14:30:00Z"
}
```

#### Fichier d'Impl√©mentation

**Backend :** `/backend/app/providers/gcp.py` (√† impl√©menter)

---

## Phase 2 - D√©tection Avanc√©e (3 sc√©narios)

### 8. `cloud_sql_zero_io` - Instances avec Z√©ro I/O

#### D√©tection

**Logique :**

D√©tecter instances sans activit√© I/O (database vide ou inutilis√©e) :

```python
# 1. Lister toutes les instances RUNNABLE
instances = sql_client.list(parent=f"projects/{project_id}")

runnable_instances = [i for i in instances if i.state == sql_v1.Instance.State.RUNNABLE]

# 2. Pour chaque instance, r√©cup√©rer m√©triques I/O (14 jours)
from google.cloud import monitoring_v3

monitoring_client = monitoring_v3.MetricServiceClient()

for instance in runnable_instances:
    instance_name = instance.name

    # 3. Query read/write I/O
    interval = monitoring_v3.TimeInterval({
        "end_time": {"seconds": int(time.time())},
        "start_time": {"seconds": int(time.time()) - 14*24*3600},
    })

    # Read bytes
    read_io = monitoring_client.list_time_series(
        request={
            "name": f"projects/{project_id}",
            "filter": f'resource.type="cloudsql_database" AND resource.database_id="{project_id}:{instance_name}" AND metric.type="cloudsql.googleapis.com/database/disk/read_ops_count"',
            "interval": interval,
        }
    )

    # Write bytes
    write_io = monitoring_client.list_time_series(
        request={
            "name": f"projects/{project_id}",
            "filter": f'resource.type="cloudsql_database" AND resource.database_id="{project_id}:{instance_name}" AND metric.type="cloudsql.googleapis.com/database/disk/write_ops_count"',
            "interval": interval,
        }
    )

    # 4. Calculer total I/O
    total_read_ops = sum([point.value.int64_value for series in read_io for point in series.points])
    total_write_ops = sum([point.value.int64_value for series in write_io for point in series.points])

    # 5. D√©tection si zero I/O
    if total_read_ops == 0 and total_write_ops == 0:
        # Instance avec z√©ro I/O = database vide/inutilis√©e
```

**Crit√®res :**
- `state == 'RUNNABLE'`
- `total_read_ops == 0` ET `total_write_ops == 0` sur 14 jours
- Instance cr√©√©e >7 jours (√©viter faux positifs)

**API Calls :**
```python
# Cloud SQL Admin API
sql_client.list(parent=f"projects/{project_id}")

# Cloud Monitoring API
monitoring_client.list_time_series(
    filter='metric.type="cloudsql.googleapis.com/database/disk/read_ops_count"'
)

monitoring_client.list_time_series(
    filter='metric.type="cloudsql.googleapis.com/database/disk/write_ops_count"'
)
```

#### Calcul de Co√ªt

**Formule :**

Z√©ro I/O = 100% waste (database inutilis√©e) :

```python
# Instance avec zero I/O = probablement vide

tier = instance.settings.tier
instance_cost = get_tier_cost(tier)

storage_cost = instance.settings.data_disk_size_gb * 0.17
backup_cost = instance.settings.data_disk_size_gb * 1.5 * 0.08

# HA
ha_enabled = instance.settings.availability_type == 'REGIONAL'
if ha_enabled:
    instance_cost = instance_cost * 2

# Co√ªt total = 100% waste
monthly_cost = instance_cost + storage_cost + backup_cost

# Co√ªt gaspill√© depuis cr√©ation
age_months = age_days / 30.0
already_wasted = monthly_cost * age_months
```

**Exemple :**

Instance db-n1-standard-2 + 100 GB avec z√©ro I/O depuis 90 jours :
```python
instance_cost = $92.40
storage_cost = 100 * $0.17 = $17.00
backup_cost = 150 * $0.08 = $12.00
monthly_cost = $121.40
already_wasted = $121.40 * (90/30) = $364.20
```

#### Param√®tres Configurables

| Param√®tre | Type | D√©faut | Description |
|-----------|------|--------|-------------|
| `lookback_days` | int | 14 | P√©riode analyse I/O |
| `min_age_days` | int | 7 | √Çge minimum instance |
| `zero_io_threshold` | int | 0 | Nombre max op√©rations I/O |

#### M√©tadonn√©es Exemple

```json
{
  "resource_id": "instance-2020202020",
  "resource_name": "empty-database",
  "resource_type": "cloud_sql_zero_io",
  "region": "us-central1",
  "database_version": "POSTGRES_14",
  "state": "RUNNABLE",
  "tier": "db-n1-standard-2",
  "storage_size_gb": 100,
  "io_metrics": {
    "total_read_ops_14d": 0,
    "total_write_ops_14d": 0
  },
  "creation_time": "2024-08-05T09:00:00Z",
  "age_days": 89,
  "estimated_monthly_cost": 121.40,
  "already_wasted": 360.53,
  "confidence": "high",
  "recommendation": "Delete instance - zero I/O for 14 days",
  "detection_date": "2024-11-02T14:30:00Z"
}
```

#### Fichier d'Impl√©mentation

**Backend :** `/backend/app/providers/gcp.py` (√† impl√©menter)

---

### 9. `cloud_sql_storage_overprovisioned` - Storage Over-Provisioned

#### D√©tection

**Logique :**

Analyser utilisation storage r√©elle vs allou√©e :

```python
# 1. Lister toutes les instances
instances = sql_client.list(parent=f"projects/{project_id}")

for instance in instances:
    instance_name = instance.name

    # 2. R√©cup√©rer taille storage allou√©e
    allocated_storage_gb = instance.settings.data_disk_size_gb

    # 3. R√©cup√©rer m√©triques utilisation storage (14 jours)
    from google.cloud import monitoring_v3

    monitoring_client = monitoring_v3.MetricServiceClient()

    interval = monitoring_v3.TimeInterval({
        "end_time": {"seconds": int(time.time())},
        "start_time": {"seconds": int(time.time()) - 14*24*3600},
    })

    # M√©trique: cloudsql.googleapis.com/database/disk/bytes_used
    storage_used_metrics = monitoring_client.list_time_series(
        request={
            "name": f"projects/{project_id}",
            "filter": f'resource.type="cloudsql_database" AND resource.database_id="{project_id}:{instance_name}" AND metric.type="cloudsql.googleapis.com/database/disk/bytes_used"',
            "interval": interval,
        }
    )

    # 4. Calculer utilisation moyenne
    used_bytes_values = [
        point.value.double_value
        for series in storage_used_metrics
        for point in series.points
    ]

    if not used_bytes_values:
        continue

    avg_used_bytes = sum(used_bytes_values) / len(used_bytes_values)
    avg_used_gb = avg_used_bytes / (1024**3)

    # 5. Calculer % utilis√©
    used_percent = (avg_used_gb / allocated_storage_gb * 100) if allocated_storage_gb > 0 else 0
    free_percent = 100 - used_percent

    # 6. D√©tection si >80% espace libre
    if free_percent >= free_space_threshold:
        # 7. Calculer taille recommand√©e
        recommended_storage_gb = int(avg_used_gb * 1.30)  # +30% buffer

        # Storage over-provisioned = waste d√©tect√©
```

**Crit√®res :**
- `free_space >= 80%` (ou utilis√© <20%)
- √âconomie potentielle >$5/mois
- Instance active (state = RUNNABLE)

**API Calls :**
```python
# Cloud SQL Admin API
sql_client.list(parent=f"projects/{project_id}")

# Cloud Monitoring API
monitoring_client.list_time_series(
    filter='metric.type="cloudsql.googleapis.com/database/disk/bytes_used"'
)
```

#### Calcul de Co√ªt

**Formule :**

Storage over-provisioned = diff√©rence co√ªt actuel vs recommand√© :

```python
# Exemple: 1000 GB allou√©, 150 GB utilis√©

allocated_storage_gb = 1000
avg_used_gb = 150

# Taille recommand√©e: 150 GB * 1.30 = 195 GB ‚Üí arrondi 200 GB
recommended_storage_gb = 200

# Storage type
storage_type = instance.settings.data_disk_type  # PD_SSD

storage_pricing = {'PD_SSD': 0.17, 'PD_HDD': 0.09}
price_per_gb = storage_pricing.get(storage_type, 0.17)

# Co√ªt storage
current_storage_cost = allocated_storage_gb * price_per_gb  # $170.00
recommended_storage_cost = recommended_storage_gb * price_per_gb  # $34.00

# Waste storage
storage_waste = current_storage_cost - recommended_storage_cost  # $136.00

# Backup cost aussi impact√© (proportionnel √† storage)
current_backup_cost = allocated_storage_gb * 1.5 * 0.08  # $120.00
recommended_backup_cost = recommended_storage_gb * 1.5 * 0.08  # $24.00
backup_waste = current_backup_cost - recommended_backup_cost  # $96.00

# Waste total
monthly_waste = storage_waste + backup_waste  # $232.00

# Co√ªt gaspill√© depuis cr√©ation
age_months = age_days / 30.0
already_wasted = monthly_waste * age_months
```

**Exemple :**

Instance avec 1000 GB SSD allou√©, 150 GB utilis√© depuis 120 jours :
```python
current_storage_cost = 1000 * $0.17 = $170.00
recommended_storage_cost = 200 * $0.17 = $34.00
storage_waste = $136.00

current_backup_cost = 1500 * $0.08 = $120.00
recommended_backup_cost = 300 * $0.08 = $24.00
backup_waste = $96.00

monthly_waste = $232.00
already_wasted = $232.00 * (120/30) = $928.00
```

#### Param√®tres Configurables

| Param√®tre | Type | D√©faut | Description |
|-----------|------|--------|-------------|
| `free_space_threshold` | float | 80.0 | % minimum espace libre pour d√©tection |
| `safety_buffer` | float | 1.30 | Marge s√©curit√© taille (1.30 = +30%) |
| `min_savings_threshold` | float | 5.0 | √âconomie minimum $/mois |
| `lookback_days` | int | 14 | P√©riode analyse utilisation |

#### M√©tadonn√©es Exemple

```json
{
  "resource_id": "instance-3030303030",
  "resource_name": "oversized-storage-db",
  "resource_type": "cloud_sql_storage_overprovisioned",
  "region": "europe-west1",
  "database_version": "MYSQL_8_0",
  "state": "RUNNABLE",
  "tier": "db-n1-standard-2",
  "storage_size_gb": 1000,
  "storage_type": "PD_SSD",
  "storage_usage": {
    "avg_used_gb": 152.3,
    "avg_used_percent": 15.2,
    "avg_free_percent": 84.8
  },
  "recommended_storage_gb": 200,
  "current_storage_cost_monthly": 170.00,
  "recommended_storage_cost_monthly": 34.00,
  "current_backup_cost_monthly": 120.00,
  "recommended_backup_cost_monthly": 24.00,
  "estimated_monthly_waste": 232.00,
  "already_wasted": 928.00,
  "savings_percentage": 73,
  "confidence": "high",
  "recommendation": "Reduce storage from 1000GB to 200GB - using only 15%",
  "detection_date": "2024-11-02T14:30:00Z"
}
```

#### Fichier d'Impl√©mentation

**Backend :** `/backend/app/providers/gcp.py` (√† impl√©menter)

---

### 10. `cloud_sql_unnecessary_ha` - High Availability Inutile

#### D√©tection

**Logique :**

D√©tecter instances **dev/test avec HA activ√©** (surco√ªt +100%) :

```python
# 1. Lister toutes les instances avec HA
instances = sql_client.list(parent=f"projects/{project_id}")

ha_instances = [
    i for i in instances
    if i.settings.availability_type == 'REGIONAL'  # HA enabled
]

# 2. Pour chaque instance HA, v√©rifier labels
for instance in ha_instances:
    labels = instance.settings.user_labels if hasattr(instance.settings, 'user_labels') else {}
    environment = labels.get('environment', '').lower()

    # 3. D√©tection si environment = dev/test/staging
    if environment in ['dev', 'test', 'staging', 'development']:
        # HA pour dev/test = waste (99.95% SLA inutile)

        # 4. Calculer √©conomie en d√©sactivant HA
        tier = instance.settings.tier
        instance_cost = get_tier_cost(tier)

        # HA = +100% co√ªt instance (standby replica)
        ha_cost = instance_cost  # Co√ªt additionnel HA

        # Waste = co√ªt HA inutile
        monthly_waste = ha_cost
```

**Crit√®res :**
- `availability_type == 'REGIONAL'` (HA enabled)
- `labels.environment in ['dev', 'test', 'staging']`
- Instance active (state = RUNNABLE)

**API Calls :**
```python
# Cloud SQL Admin API
sql_client.list(parent=f"projects/{project_id}")
```

#### Calcul de Co√ªt

**Formule :**

HA inutile = +100% instance cost gaspill√© :

```python
# Instance dev avec HA: db-n1-standard-4

tier = 'db-n1-standard-4'
instance_cost_single = 184.80  # $/mois (sans HA)

# Co√ªt actuel avec HA
instance_cost_ha = instance_cost_single * 2  # $369.60

# Co√ªt optimal sans HA (dev/test ne n√©cessite pas 99.95% SLA)
optimal_cost = instance_cost_single  # $184.80

# Waste = co√ªt HA
monthly_waste = instance_cost_ha - optimal_cost  # $184.80

# Storage et backups identiques (pas de waste)

# Co√ªt gaspill√© depuis activation HA
age_months = age_days / 30.0
already_wasted = monthly_waste * age_months
```

**Exemple :**

Instance dev db-n1-standard-4 + HA depuis 180 jours :
```python
instance_cost_single = $184.80
instance_cost_ha = $369.60
monthly_waste = $184.80
already_wasted = $184.80 * (180/30) = $1,108.80
```

#### Param√®tres Configurables

| Param√®tre | Type | D√©faut | Description |
|-----------|------|--------|-------------|
| `devtest_labels` | list | `['dev', 'test', 'staging']` | Labels environnements non-prod |
| `allow_ha_for_labels` | dict | `{}` | Labels autorisant HA (ex: `{'critical': 'true'}`) |

#### M√©tadonn√©es Exemple

```json
{
  "resource_id": "instance-4040404040",
  "resource_name": "dev-mysql-ha",
  "resource_type": "cloud_sql_unnecessary_ha",
  "region": "us-central1",
  "database_version": "MYSQL_8_0",
  "state": "RUNNABLE",
  "tier": "db-n1-standard-4",
  "availability_type": "REGIONAL",
  "labels": {
    "environment": "dev",
    "team": "backend"
  },
  "creation_time": "2024-05-06T08:00:00Z",
  "age_days": 180,
  "instance_cost_single_monthly": 184.80,
  "instance_cost_ha_monthly": 369.60,
  "estimated_monthly_waste": 184.80,
  "already_wasted": 1108.80,
  "confidence": "high",
  "recommendation": "Disable High Availability for dev/test environment",
  "detection_date": "2024-11-02T14:30:00Z"
}
```

#### Fichier d'Impl√©mentation

**Backend :** `/backend/app/providers/gcp.py` (√† impl√©menter)

---

## Protocole de Test

### Pr√©requis

#### 1. Compte GCP et Projet Test

```bash
# Utiliser projet test existant
export PROJECT_ID="cloudwaste-test-XXXXXXXXXX"
gcloud config set project $PROJECT_ID

# Activer APIs requises
gcloud services enable sqladmin.googleapis.com
gcloud services enable monitoring.googleapis.com
```

#### 2. Service Account (si pas d√©j√† cr√©√©)

```bash
# Ajouter permissions Cloud SQL
gcloud projects add-iam-policy-binding $PROJECT_ID \
  --member="serviceAccount:cloudwaste-scanner@${PROJECT_ID}.iam.gserviceaccount.com" \
  --role="roles/cloudsql.viewer"

# Utiliser credentials existants
export GOOGLE_APPLICATION_CREDENTIALS="$(pwd)/cloudwaste-key.json"
```

---

### Tests Unitaires - Cr√©er Instances de Test

#### Sc√©nario 1: Instance Arr√™t√©e >30 Jours

```bash
# Cr√©er instance MySQL
gcloud sql instances create test-stopped-instance \
  --database-version=MYSQL_8_0 \
  --tier=db-n1-standard-1 \
  --region=us-central1 \
  --storage-size=100GB \
  --storage-type=SSD

# Arr√™ter instance
gcloud sql instances patch test-stopped-instance \
  --activation-policy=NEVER

# Attendre 30 jours pour d√©tection
```

**Validation attendue :**
```json
{
  "resource_type": "cloud_sql_stopped",
  "resource_name": "test-stopped-instance",
  "state": "STOPPED",
  "storage_cost_monthly": "~17.00",
  "backup_cost_monthly": "~12.00",
  "estimated_monthly_cost": "~29.00"
}
```

---

#### Sc√©nario 2: Instance Idle (Zero Connections)

```bash
# Cr√©er instance PostgreSQL
gcloud sql instances create test-idle-instance \
  --database-version=POSTGRES_14 \
  --tier=db-n1-standard-2 \
  --region=us-east1 \
  --storage-size=200GB \
  --storage-type=SSD \
  --availability-type=REGIONAL

# NE PAS se connecter (laisser idle)
# Attendre 14 jours pour m√©triques connexions = 0
```

**Validation attendue :**
```json
{
  "resource_type": "cloud_sql_idle",
  "resource_name": "test-idle-instance",
  "connection_metrics": {
    "avg_connections_14d": 0.0
  },
  "estimated_monthly_cost": "~242.80"
}
```

---

#### Sc√©nario 3: Instance Over-Provisioned

```bash
# Cr√©er instance large
gcloud sql instances create test-overprovisioned-instance \
  --database-version=MYSQL_8_0 \
  --tier=db-n1-standard-4 \
  --region=europe-west1 \
  --storage-size=100GB \
  --storage-type=SSD \
  --availability-type=REGIONAL

# Connecter et g√©n√©rer faible charge (CPU <30%, Memory <40%)
# Utiliser mysql client avec queries tr√®s l√©g√®res
mysql -h <INSTANCE_IP> -u root -p

# Ex√©cuter queries l√©g√®res p√©riodiquement
while true; do
  mysql -h <IP> -u root -p -e "SELECT 1;"
  sleep 60
done &

# Laisser tourner 14 jours
```

**Validation attendue :**
```json
{
  "resource_type": "cloud_sql_overprovisioned",
  "resource_name": "test-overprovisioned-instance",
  "cpu_metrics": {
    "avg_cpu_14d": "<30"
  },
  "memory_metrics": {
    "avg_memory_14d": "<40"
  },
  "estimated_monthly_waste": "~184.80"
}
```

---

#### Sc√©nario 4: Ancien Type de Machine

```bash
# Cr√©er instance avec db-n1 tier
gcloud sql instances create test-old-tier-instance \
  --database-version=POSTGRES_14 \
  --tier=db-n1-standard-2 \
  --region=us-central1 \
  --storage-size=100GB \
  --storage-type=SSD \
  --availability-type=REGIONAL
```

**Validation attendue :**
```json
{
  "resource_type": "cloud_sql_old_machine_type",
  "resource_name": "test-old-tier-instance",
  "tier": "db-n1-standard-2",
  "recommended_tier": "db-custom-2-7680",
  "estimated_monthly_waste": "~82.60"
}
```

---

#### Sc√©nario 5: Instance Dev/Test 24/7

```bash
# Cr√©er instance avec label dev
gcloud sql instances create test-devtest-247-instance \
  --database-version=MYSQL_8_0 \
  --tier=db-n1-standard-2 \
  --region=us-east1 \
  --storage-size=100GB \
  --storage-type=SSD \
  --labels=environment=dev,team=backend

# Laisser tourner 7+ jours sans arr√™t
```

**Validation attendue :**
```json
{
  "resource_type": "cloud_sql_devtest_247",
  "resource_name": "test-devtest-247-instance",
  "labels": {"environment": "dev"},
  "uptime_days": ">=7",
  "estimated_monthly_waste": "~59.40"
}
```

---

#### Sc√©nario 6: Read Replica Inutilis√©

```bash
# Cr√©er master instance
gcloud sql instances create test-master-instance \
  --database-version=MYSQL_8_0 \
  --tier=db-n1-standard-2 \
  --region=us-west1 \
  --storage-size=200GB \
  --storage-type=SSD

# Cr√©er read replica
gcloud sql instances create test-unused-replica \
  --master-instance-name=test-master-instance \
  --tier=db-n1-standard-2 \
  --region=us-west1 \
  --replica-type=READ

# NE PAS ex√©cuter de queries sur replica
# Attendre 14 jours
```

**Validation attendue :**
```json
{
  "resource_type": "cloud_sql_unused_replicas",
  "resource_name": "test-unused-replica",
  "is_replica": true,
  "query_metrics": {
    "total_queries_14d": 0
  },
  "estimated_monthly_cost": "~150.40"
}
```

---

#### Sc√©nario 7: Instance Non Tagu√©e

```bash
# Cr√©er instance SANS labels
gcloud sql instances create test-untagged-instance \
  --database-version=POSTGRES_14 \
  --tier=db-n1-standard-2 \
  --region=europe-west4 \
  --storage-size=200GB \
  --storage-type=SSD \
  --availability-type=REGIONAL
```

**Validation attendue :**
```json
{
  "resource_type": "cloud_sql_untagged",
  "resource_name": "test-untagged-instance",
  "labels": {},
  "missing_labels": ["environment", "owner", "cost-center"],
  "estimated_monthly_waste": "~12.14"
}
```

---

#### Sc√©nario 8: Instance avec Z√©ro I/O

```bash
# Cr√©er instance
gcloud sql instances create test-zero-io-instance \
  --database-version=MYSQL_8_0 \
  --tier=db-n1-standard-2 \
  --region=us-central1 \
  --storage-size=100GB \
  --storage-type=SSD

# Connecter mais NE PAS ex√©cuter de queries
# (pas de CREATE TABLE, pas d'INSERT, rien)

# Attendre 14 jours pour m√©triques I/O = 0
```

**Validation attendue :**
```json
{
  "resource_type": "cloud_sql_zero_io",
  "resource_name": "test-zero-io-instance",
  "io_metrics": {
    "total_read_ops_14d": 0,
    "total_write_ops_14d": 0
  },
  "estimated_monthly_cost": "~121.40"
}
```

---

#### Sc√©nario 9: Storage Over-Provisioned

```bash
# Cr√©er instance avec large storage
gcloud sql instances create test-oversized-storage-instance \
  --database-version=POSTGRES_14 \
  --tier=db-n1-standard-2 \
  --region=europe-west1 \
  --storage-size=1000GB \
  --storage-type=SSD

# Connecter et utiliser seulement 15% storage
psql "host=<IP> user=postgres dbname=postgres"

# Cr√©er database l√©g√®re (~150 GB)
CREATE TABLE test_data (id SERIAL, data TEXT);
INSERT INTO test_data (data) SELECT repeat('x', 1000) FROM generate_series(1, 150000000);

# Laisser tourner 14 jours
```

**Validation attendue :**
```json
{
  "resource_type": "cloud_sql_storage_overprovisioned",
  "resource_name": "test-oversized-storage-instance",
  "storage_usage": {
    "avg_used_percent": "~15",
    "avg_free_percent": "~85"
  },
  "recommended_storage_gb": 200,
  "estimated_monthly_waste": "~232.00"
}
```

---

#### Sc√©nario 10: High Availability Inutile

```bash
# Cr√©er instance dev avec HA
gcloud sql instances create test-unnecessary-ha-instance \
  --database-version=MYSQL_8_0 \
  --tier=db-n1-standard-4 \
  --region=us-central1 \
  --storage-size=100GB \
  --storage-type=SSD \
  --availability-type=REGIONAL \
  --labels=environment=dev,team=backend
```

**Validation attendue :**
```json
{
  "resource_type": "cloud_sql_unnecessary_ha",
  "resource_name": "test-unnecessary-ha-instance",
  "availability_type": "REGIONAL",
  "labels": {"environment": "dev"},
  "estimated_monthly_waste": "~184.80"
}
```

---

### Validation Globale

#### Script Test Complet

```python
#!/usr/bin/env python3
"""
Script de validation complet pour Cloud SQL
"""

from google.cloud import sql_v1
import os

PROJECT_ID = os.environ['PROJECT_ID']

def test_all_scenarios():
    sql_client = sql_v1.SqlInstancesServiceClient()

    # 1. Lister toutes les instances
    instances = list(sql_client.list(parent=f"projects/{PROJECT_ID}"))

    print(f"‚úÖ Found {len(instances)} Cloud SQL instances")

    # 2. V√©rifier d√©tection pour chaque sc√©nario
    scenarios_detected = {
        'stopped': 0,
        'idle': 0,
        'overprovisioned': 0,
        'old_machine_type': 0,
        'devtest_247': 0,
        'unused_replicas': 0,
        'untagged': 0,
        'zero_io': 0,
        'storage_overprovisioned': 0,
        'unnecessary_ha': 0,
    }

    for instance in instances:
        name = instance.name

        # Scenario 1: Stopped
        if instance.state == sql_v1.Instance.State.STOPPED:
            scenarios_detected['stopped'] += 1
            print(f"‚úÖ Detected scenario 1 (stopped): {name}")

        # Scenario 4: Old machine type
        if instance.settings.tier.startswith('db-n1-'):
            scenarios_detected['old_machine_type'] += 1
            print(f"‚úÖ Detected scenario 4 (old machine type): {name}")

        # Scenario 5: Dev/Test 24/7
        labels = instance.settings.user_labels if hasattr(instance.settings, 'user_labels') else {}
        if labels.get('environment') in ['dev', 'test', 'staging']:
            scenarios_detected['devtest_247'] += 1
            print(f"‚úÖ Detected scenario 5 (dev/test 24/7): {name}")

        # Scenario 7: Untagged
        if not labels or len(labels) == 0:
            scenarios_detected['untagged'] += 1
            print(f"‚úÖ Detected scenario 7 (untagged): {name}")

        # Scenario 10: Unnecessary HA
        if (instance.settings.availability_type == 'REGIONAL' and
            labels.get('environment') in ['dev', 'test', 'staging']):
            scenarios_detected['unnecessary_ha'] += 1
            print(f"‚úÖ Detected scenario 10 (unnecessary HA): {name}")

    # 3. Rapport final
    print("\nüìä Detection Summary:")
    for scenario, count in scenarios_detected.items():
        print(f"  - {scenario}: {count} instances")

    total_detected = sum(scenarios_detected.values())
    print(f"\n‚úÖ Total waste detected: {total_detected} instances")

if __name__ == '__main__':
    test_all_scenarios()
```

#### Ex√©cution

```bash
# Exporter PROJECT_ID
export PROJECT_ID="cloudwaste-test-XXXXXXXXXX"

# Ex√©cuter validation
python3 test_gcp_cloud_sql.py
```

**R√©sultat attendu :**
```
‚úÖ Found 10 Cloud SQL instances
‚úÖ Detected scenario 1 (stopped): test-stopped-instance
‚úÖ Detected scenario 2 (idle): test-idle-instance
‚úÖ Detected scenario 3 (overprovisioned): test-overprovisioned-instance
‚úÖ Detected scenario 4 (old machine type): test-old-tier-instance
‚úÖ Detected scenario 5 (dev/test 24/7): test-devtest-247-instance
‚úÖ Detected scenario 6 (unused replica): test-unused-replica
‚úÖ Detected scenario 7 (untagged): test-untagged-instance
‚úÖ Detected scenario 8 (zero I/O): test-zero-io-instance
‚úÖ Detected scenario 9 (storage overprovisioned): test-oversized-storage-instance
‚úÖ Detected scenario 10 (unnecessary HA): test-unnecessary-ha-instance

üìä Detection Summary:
  - stopped: 1 instances
  - idle: 1 instances
  - overprovisioned: 1 instances
  - old_machine_type: 1 instances
  - devtest_247: 1 instances
  - unused_replicas: 1 instances
  - untagged: 1 instances
  - zero_io: 1 instances
  - storage_overprovisioned: 1 instances
  - unnecessary_ha: 1 instances

‚úÖ Total waste detected: 10 instances
```

---

### Cleanup

```bash
# Supprimer toutes les instances de test
gcloud sql instances delete test-stopped-instance --quiet
gcloud sql instances delete test-idle-instance --quiet
gcloud sql instances delete test-overprovisioned-instance --quiet
gcloud sql instances delete test-old-tier-instance --quiet
gcloud sql instances delete test-devtest-247-instance --quiet
gcloud sql instances delete test-master-instance --quiet  # Supprime aussi replicas
gcloud sql instances delete test-untagged-instance --quiet
gcloud sql instances delete test-zero-io-instance --quiet
gcloud sql instances delete test-oversized-storage-instance --quiet
gcloud sql instances delete test-unnecessary-ha-instance --quiet
```

---

## R√©f√©rences

### Documentation GCP

- [Cloud SQL API](https://cloud.google.com/sql/docs/mysql/admin-api/rest/v1/instances)
- [Cloud SQL Pricing](https://cloud.google.com/sql/pricing)
- [Cloud SQL Machine Types](https://cloud.google.com/sql/docs/mysql/instance-settings)
- [High Availability](https://cloud.google.com/sql/docs/mysql/high-availability)
- [Read Replicas](https://cloud.google.com/sql/docs/mysql/replication)
- [Cloud Monitoring Metrics](https://cloud.google.com/monitoring/api/metrics_gcp#gcp-cloudsql)

### CloudWaste Documentation

- [GCP.md](./GCP.md) - Listing complet 27 ressources GCP
- [GCP_COMPUTE_ENGINE_SCENARIOS_100.md](./GCP_COMPUTE_ENGINE_SCENARIOS_100.md) - Compute Instances
- [GCP_PERSISTENT_DISK_SCENARIOS_100.md](./GCP_PERSISTENT_DISK_SCENARIOS_100.md) - Persistent Disks
- [GCP_GKE_CLUSTER_SCENARIOS_100.md](./GCP_GKE_CLUSTER_SCENARIOS_100.md) - GKE Clusters
- [README.md](./README.md) - Guide documentation GCP

### √âquivalences AWS/Azure

- **AWS RDS** ‚Üí GCP Cloud SQL
- **Azure SQL Database** ‚Üí GCP Cloud SQL
- **AWS Aurora** ‚Üí GCP Cloud Spanner (distributed)
- **AWS RDS Multi-AZ** ‚Üí GCP Cloud SQL HA (REGIONAL)
- **AWS RDS Read Replicas** ‚Üí GCP Cloud SQL Read Replicas

---

**Derni√®re mise √† jour :** 2 novembre 2025
**Status :** ‚úÖ Sp√©cification compl√®te - Pr√™t pour impl√©mentation
**Version :** 1.0
**Auteur :** CloudWaste Team

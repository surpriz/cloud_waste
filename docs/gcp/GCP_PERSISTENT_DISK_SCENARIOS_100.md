# CloudWaste - Couverture 100% GCP Persistent Disks

**Resource Type:** `Compute : Persistent Disks`
**Provider:** Google Cloud Platform (GCP)
**API:** `compute.googleapis.com` (Compute Engine API v1)
**√âquivalents:** AWS EBS Volumes, Azure Managed Disks
**Total Scenarios:** 10 (100% coverage)

---

## üìã Table des Mati√®res

- [Vue d'Ensemble](#vue-densemble)
- [Mod√®le de Pricing GCP](#mod√®le-de-pricing-gcp)
- [Phase 1 - D√©tection Simple (7 sc√©narios)](#phase-1---d√©tection-simple-7-sc√©narios)
  - [1. Disques Non Attach√©s](#1-persistent_disk_unattached---disques-non-attach√©s)
  - [2. Disques Attach√©s √† Instances Arr√™t√©es](#2-persistent_disk_attached_stopped---disques-attach√©s-√†-instances-arr√™t√©es)
  - [3. Disques Jamais Utilis√©s](#3-persistent_disk_never_used---disques-jamais-utilis√©s)
  - [4. Snapshots Orphelins](#4-persistent_disk_orphan_snapshots---snapshots-orphelins)
  - [5. Ancien Type de Disque](#5-persistent_disk_old_type---ancien-type-de-disque)
  - [6. Type de Disque Sur-Provisionn√©](#6-persistent_disk_overprovisioned_type---type-de-disque-sur-provisionn√©)
  - [7. Disques Non Tagg√©s](#7-persistent_disk_untagged---disques-non-tagg√©s)
- [Phase 2 - D√©tection Avanc√©e (3 sc√©narios)](#phase-2---d√©tection-avanc√©e-3-sc√©narios)
  - [8. Disques Sous-Utilis√©s](#8-persistent_disk_underutilized---disques-sous-utilis√©s)
  - [9. Disques Sur-Dimensionn√©s](#9-persistent_disk_oversized---disques-sur-dimensionn√©s)
  - [10. Disques en Lecture Seule](#10-persistent_disk_readonly---disques-en-lecture-seule)
- [Protocole de Test](#protocole-de-test)
- [R√©f√©rences](#r√©f√©rences)

---

## Vue d'Ensemble

### Contexte GCP Persistent Disks

Google Cloud Platform propose plusieurs **types de Persistent Disks** avec diff√©rents niveaux de performance et prix :

- **pd-standard** (HDD) - $0.040/GB/mois - Throughput: 0.12 MB/s par GB
- **pd-balanced** (SSD) - $0.100/GB/mois - Throughput: 0.28 MB/s par GB, IOPS: 6 par GB
- **pd-ssd** (SSD performance) - $0.170/GB/mois - Throughput: 0.48 MB/s par GB, IOPS: 30 par GB
- **pd-extreme** (ultra-performance) - $0.125/GB/mois + IOPS provisionn√©s - Custom IOPS
- **Snapshots** - $0.026/GB/mois - Storage only

**Caract√©ristiques :**
- Persistent Disks factur√©s √† la seconde (minimum 1 minute)
- Taille: 10 GB √† 64 TB (pd-standard/balanced/ssd)
- Factur√©s m√™me si non attach√©s ou instance arr√™t√©e
- Snapshots incr√©mentaux (seuls les changements factur√©s apr√®s 1er snapshot)

### Waste Typique

1. **Disques non attach√©s** : 100% du co√ªt sans aucune utilit√©
2. **Disques attach√©s √† instances arr√™t√©es** : Paiement inutile si instance non utilis√©e
3. **Over-provisioning type** : pd-ssd ($0.170) au lieu de pd-balanced ($0.100) = -41% √©conomie
4. **Over-sizing** : 1 TB allou√©, 100 GB utilis√© = 900 GB gaspill√©s
5. **Snapshots orphelins** : Snapshots dont le disque source n'existe plus

---

## Mod√®le de Pricing GCP

### Pricing Persistent Disks (par GB/mois, us-central1)

| Type | Prix/GB/mois | IOPS/GB | Throughput MB/s/GB | Use Case |
|------|-------------|---------|-------------------|----------|
| **pd-standard** | $0.040 | 0.75 read<br>1.5 write | 0.12 | Donn√©es rarement acc√©d√©es, backups |
| **pd-balanced** | $0.100 | 6 | 0.28 | Workloads g√©n√©raux, boot disks |
| **pd-ssd** | $0.170 | 30 | 0.48 | Databases, haute performance |
| **pd-extreme** | $0.125 + IOPS | Custom | Custom | Ultra-performance, latence critique |
| **Snapshots** | $0.026 | N/A | N/A | Backups, images |

### Exemples de Co√ªts Mensuels

| Taille | pd-standard | pd-balanced | pd-ssd | Snapshot | √âconomie pd-standard ‚Üí Snapshot |
|--------|-------------|-------------|--------|----------|--------------------------------|
| **100 GB** | $4.00 | $10.00 | $17.00 | $2.60 | -35% |
| **500 GB** | $20.00 | $50.00 | $85.00 | $13.00 | -35% |
| **1 TB** | $40.00 | $100.00 | $170.00 | $26.00 | -35% |
| **5 TB** | $200.00 | $500.00 | $850.00 | $130.00 | -35% |
| **10 TB** | $400.00 | $1,000.00 | $1,700.00 | $260.00 | -35% |

**Notes :**
- pd-balanced recommand√© pour la plupart des workloads (best price/performance)
- pd-ssd uniquement si IOPS >3000 requis
- Snapshots 35% moins cher que pd-standard (mais read-only)

---

## Phase 1 - D√©tection Simple (7 sc√©narios)

### 1. `persistent_disk_unattached` - Disques Non Attach√©s

#### D√©tection

**Logique :**
```python
# 1. Lister tous les disques persistants
from google.cloud import compute_v1

compute_client = compute_v1.DisksClient()

request = compute_v1.AggregatedListDisksRequest(
    project=project_id
)

agg_list = compute_client.aggregated_list(request=request)

# 2. Pour chaque disque, v√©rifier si attach√©
for zone, response in agg_list:
    if response.disks:
        for disk in response.disks:
            # 3. D√©tection si users == [] (non attach√©)
            users = disk.users  # Liste d'instances utilisant le disque

            if not users or len(users) == 0:
                # 4. Calculer √¢ge depuis cr√©ation
                creation_timestamp = parse_timestamp(disk.creation_timestamp)
                age_days = (now - creation_timestamp).days

                # 5. D√©tection si √¢ge >= seuil (d√©faut: 7 jours)
                if age_days >= min_age_days:
                    # Disque unattached = waste d√©tect√©
```

**Crit√®res :**
- `len(disk.users) == 0` (aucune instance attach√©e)
- `age >= min_age_days` (d√©faut: 7 jours)

**API Calls :**
```python
# Compute Engine API - Disks
compute_client = compute_v1.DisksClient()
agg_list = compute_client.aggregated_list(
    project='my-project'
)
```

#### Calcul de Co√ªt

**Formule :**

Disques non attach√©s = 100% waste :

```python
# R√©cup√©rer type et taille du disque
disk_type = disk.type.split('/')[-1]  # "pd-standard", "pd-balanced", "pd-ssd"
disk_size_gb = disk.size_gb

# Prix par GB selon type (us-central1)
disk_pricing = {
    'pd-standard': 0.040,   # $/GB/mois
    'pd-balanced': 0.100,
    'pd-ssd': 0.170,
    'pd-extreme': 0.125,    # Base price (+ IOPS)
}

price_per_gb = disk_pricing.get(disk_type, 0.040)

# Co√ªt mensuel = 100% waste (disque inutilis√©)
monthly_cost = disk_size_gb * price_per_gb

# Co√ªt d√©j√† gaspill√© depuis cr√©ation
age_months = age_days / 30.0
already_wasted = monthly_cost * age_months
```

**Exemple :**

Disque 500 GB pd-balanced non attach√© depuis 60 jours :
```python
disk_size_gb = 500
price_per_gb = $0.100
monthly_cost = 500 * $0.100 = $50.00/mois
age_months = 60 / 30 = 2.0
already_wasted = $50.00 * 2.0 = $100.00
```

#### Param√®tres Configurables

| Param√®tre | Type | D√©faut | Description |
|-----------|------|--------|-------------|
| `min_age_days` | int | 7 | √Çge minimum avant d√©tection |
| `exclude_labels` | dict | `{}` | Labels pour exclure disques (ex: `{'backup': 'true'}`) |

#### M√©tadonn√©es Exemple

```json
{
  "resource_id": "disk-1234567890123456789",
  "resource_name": "orphan-data-disk",
  "resource_type": "persistent_disk_unattached",
  "zone": "us-central1-a",
  "disk_type": "pd-balanced",
  "size_gb": 500,
  "status": "READY",
  "users": [],
  "creation_timestamp": "2024-09-05T10:00:00Z",
  "age_days": 58,
  "estimated_monthly_cost": 50.00,
  "already_wasted": 96.67,
  "confidence": "high",
  "recommendation": "Delete disk or attach to instance",
  "detection_date": "2024-11-02T14:30:00Z"
}
```

#### Fichier d'Impl√©mentation

**Backend :** `/backend/app/providers/gcp.py` (√† impl√©menter)

---

### 2. `persistent_disk_attached_stopped` - Disques Attach√©s √† Instances Arr√™t√©es

#### D√©tection

**Logique :**
```python
# 1. Lister tous les disques persistants
disks = get_all_persistent_disks(project_id)

# 2. Pour chaque disque attach√©, v√©rifier status instance
for disk in disks:
    users = disk.users  # Liste d'URLs d'instances

    if users and len(users) > 0:
        # 3. Pour chaque instance attach√©e, r√©cup√©rer status
        for instance_url in users:
            # Parser zone et instance name depuis URL
            # URL format: https://www.googleapis.com/compute/v1/projects/{project}/zones/{zone}/instances/{name}
            zone = extract_zone_from_url(instance_url)
            instance_name = extract_instance_name_from_url(instance_url)

            # 4. R√©cup√©rer instance
            instance = compute_client.instances().get(
                project=project_id,
                zone=zone,
                instance=instance_name
            ).execute()

            # 5. D√©tection si instance TERMINATED
            if instance['status'] == 'TERMINATED':
                # Calculer depuis quand instance arr√™t√©e
                last_stop = parse_timestamp(instance.get('lastStopTimestamp'))
                age_days = (now - last_stop).days

                if age_days >= min_age_days:
                    # Disque attach√© √† instance arr√™t√©e = waste d√©tect√©
```

**Crit√®res :**
- `len(disk.users) > 0` (disque attach√©)
- Instance avec `status == 'TERMINATED'`
- `age >= min_age_days` (d√©faut: 30 jours)

**API Calls :**
```python
# Disks API
compute_client.disks().aggregatedList(project=project_id)

# Instances API (pour status)
compute_client.instances().get(
    project=project_id,
    zone='us-central1-a',
    instance='instance-name'
)
```

#### Calcul de Co√ªt

**Formule :**

Disque attach√© √† instance arr√™t√©e = 100% waste (disque inutilis√©) :

```python
disk_size_gb = disk.size_gb
disk_type = disk.type.split('/')[-1]

price_per_gb = {
    'pd-standard': 0.040,
    'pd-balanced': 0.100,
    'pd-ssd': 0.170,
}.get(disk_type, 0.040)

# Co√ªt mensuel = 100% waste
monthly_cost = disk_size_gb * price_per_gb

# Co√ªt gaspill√© depuis arr√™t instance
instance_stop_date = parse_timestamp(instance['lastStopTimestamp'])
stopped_days = (now - instance_stop_date).days
already_wasted = monthly_cost * (stopped_days / 30.0)
```

**Exemple :**

Disque 1 TB pd-ssd attach√© √† instance arr√™t√©e depuis 45 jours :
```python
disk_size_gb = 1024
price_per_gb = $0.170
monthly_cost = 1024 * $0.170 = $174.08/mois
already_wasted = $174.08 * (45/30) = $261.12
```

#### Param√®tres Configurables

| Param√®tre | Type | D√©faut | Description |
|-----------|------|--------|-------------|
| `min_age_days` | int | 30 | Dur√©e minimum arr√™t instance |
| `exclude_boot_disks` | bool | `False` | Exclure disques boot (auto-supprim√©s avec instance) |

#### M√©tadonn√©es Exemple

```json
{
  "resource_id": "disk-9876543210987654321",
  "resource_name": "prod-database-disk",
  "resource_type": "persistent_disk_attached_stopped",
  "zone": "us-east1-b",
  "disk_type": "pd-ssd",
  "size_gb": 1024,
  "status": "READY",
  "users": [
    "https://www.googleapis.com/compute/v1/projects/my-project/zones/us-east1-b/instances/db-server"
  ],
  "attached_instance": {
    "name": "db-server",
    "status": "TERMINATED",
    "last_stop_timestamp": "2024-09-18T12:00:00Z",
    "stopped_days": 45
  },
  "creation_timestamp": "2024-06-01T08:00:00Z",
  "age_days": 154,
  "estimated_monthly_cost": 174.08,
  "already_wasted": 261.12,
  "confidence": "high",
  "recommendation": "Delete disk or restart instance if still needed",
  "detection_date": "2024-11-02T14:30:00Z"
}
```

#### Fichier d'Impl√©mentation

**Backend :** `/backend/app/providers/gcp.py` (√† impl√©menter)

---

### 3. `persistent_disk_never_used` - Disques Jamais Utilis√©s

#### D√©tection

**Logique :**
```python
# 1. Lister tous les disques persistants
disks = get_all_persistent_disks(project_id)

# 2. Pour chaque disque, r√©cup√©rer m√©triques I/O
from google.cloud import monitoring_v3

monitoring_client = monitoring_v3.MetricServiceClient()

for disk in disks:
    # 3. Query I/O metrics depuis cr√©ation
    creation_date = parse_timestamp(disk.creation_timestamp)
    age_days = (now - creation_date).days

    # 4. Seulement si disque >7 jours (√©viter faux positifs)
    if age_days < min_age_days:
        continue

    # 5. R√©cup√©rer read/write bytes depuis cr√©ation
    interval = monitoring_v3.TimeInterval({
        "end_time": {"seconds": int(time.time())},
        "start_time": {"seconds": int(creation_date.timestamp())},
    })

    # Query read operations
    read_ops = monitoring_client.list_time_series(
        request={
            "name": f"projects/{project_id}",
            "filter": f'metric.type="compute.googleapis.com/instance/disk/read_ops_count" AND resource.disk_name="{disk.name}"',
            "interval": interval,
        }
    )

    # Query write operations
    write_ops = monitoring_client.list_time_series(
        request={
            "name": f"projects/{project_id}",
            "filter": f'metric.type="compute.googleapis.com/instance/disk/write_ops_count" AND resource.disk_name="{disk.name}"',
            "interval": interval,
        }
    )

    # 6. Calculer total I/O operations
    total_read_ops = sum([point.value.int64_value for series in read_ops for point in series.points])
    total_write_ops = sum([point.value.int64_value for series in write_ops for point in series.points])

    # 7. D√©tection si zero I/O
    if total_read_ops == 0 and total_write_ops == 0:
        # Disque jamais utilis√© = waste d√©tect√©
```

**Crit√®res :**
- `age >= min_age_days` (d√©faut: 7 jours)
- `total_read_ops == 0`
- `total_write_ops == 0`

**API Calls :**
```python
# Disks API
compute_client.disks().aggregatedList(project=project_id)

# Cloud Monitoring API
monitoring_client.list_time_series(
    name=f"projects/{project_id}",
    filter='metric.type="compute.googleapis.com/instance/disk/read_ops_count"',
    interval={"start_time": ..., "end_time": ...}
)
```

#### Calcul de Co√ªt

**Formule :**

Disque jamais utilis√© = 100% waste :

```python
disk_size_gb = disk.size_gb
disk_type = disk.type.split('/')[-1]

price_per_gb = {
    'pd-standard': 0.040,
    'pd-balanced': 0.100,
    'pd-ssd': 0.170,
}.get(disk_type, 0.040)

# Co√ªt mensuel = 100% waste
monthly_cost = disk_size_gb * price_per_gb

# Co√ªt gaspill√© depuis cr√©ation
age_months = age_days / 30.0
already_wasted = monthly_cost * age_months
```

**Exemple :**

Disque 250 GB pd-balanced jamais utilis√© depuis 90 jours :
```python
disk_size_gb = 250
price_per_gb = $0.100
monthly_cost = 250 * $0.100 = $25.00/mois
already_wasted = $25.00 * (90/30) = $75.00
```

#### Param√®tres Configurables

| Param√®tre | Type | D√©faut | Description |
|-----------|------|--------|-------------|
| `min_age_days` | int | 7 | √Çge minimum avant d√©tection |
| `zero_io_threshold` | int | 0 | Nombre max d'op√©rations I/O (0 = aucune) |

#### M√©tadonn√©es Exemple

```json
{
  "resource_id": "disk-5555555555555555555",
  "resource_name": "unused-test-disk",
  "resource_type": "persistent_disk_never_used",
  "zone": "europe-west1-b",
  "disk_type": "pd-balanced",
  "size_gb": 250,
  "status": "READY",
  "users": [
    "https://www.googleapis.com/compute/v1/projects/my-project/zones/europe-west1-b/instances/test-server"
  ],
  "io_metrics": {
    "total_read_ops": 0,
    "total_write_ops": 0,
    "total_read_bytes": 0,
    "total_write_bytes": 0
  },
  "creation_timestamp": "2024-08-05T09:00:00Z",
  "age_days": 89,
  "estimated_monthly_cost": 25.00,
  "already_wasted": 74.17,
  "confidence": "high",
  "recommendation": "Delete disk - never used since creation",
  "detection_date": "2024-11-02T14:30:00Z"
}
```

#### Fichier d'Impl√©mentation

**Backend :** `/backend/app/providers/gcp.py` (√† impl√©menter)

---

### 4. `persistent_disk_orphan_snapshots` - Snapshots Orphelins

#### D√©tection

**Logique :**
```python
# 1. Lister tous les snapshots
from google.cloud import compute_v1

compute_client = compute_v1.SnapshotsClient()

snapshots_list = compute_client.list(project=project_id)

# 2. Lister tous les disques existants
disks_client = compute_v1.DisksClient()
all_disks = get_all_disk_names(project_id)  # Set de noms de disques

# 3. Pour chaque snapshot, v√©rifier si disque source existe
for snapshot in snapshots_list:
    source_disk_url = snapshot.source_disk
    # URL format: https://www.googleapis.com/compute/v1/projects/{project}/zones/{zone}/disks/{name}

    if not source_disk_url:
        # Snapshot sans source = orphelin
        continue

    # 4. Extraire nom du disque source
    source_disk_name = extract_disk_name_from_url(source_disk_url)

    # 5. D√©tection si disque source n'existe plus
    if source_disk_name not in all_disks:
        # 6. Calculer √¢ge snapshot
        creation_timestamp = parse_timestamp(snapshot.creation_timestamp)
        age_days = (now - creation_timestamp).days

        if age_days >= min_age_days:
            # Snapshot orphelin = potentiel waste
```

**Crit√®res :**
- `snapshot.source_disk` pointe vers disque supprim√©
- `age >= min_age_days` (d√©faut: 30 jours)

**API Calls :**
```python
# Snapshots API
snapshots_client = compute_v1.SnapshotsClient()
snapshots_list = snapshots_client.list(project=project_id)

# Disks API (pour v√©rifier existence)
disks_client = compute_v1.DisksClient()
disks_list = disks_client.aggregated_list(project=project_id)
```

#### Calcul de Co√ªt

**Formule :**

Snapshots = $0.026/GB/mois :

```python
# Taille snapshot (stockage r√©el utilis√©)
snapshot_size_gb = snapshot.storage_bytes / (1024**3)

# Prix snapshot
snapshot_price_per_gb = 0.026  # $/GB/mois

# Co√ªt mensuel
monthly_cost = snapshot_size_gb * snapshot_price_per_gb

# Co√ªt gaspill√© depuis cr√©ation
age_months = age_days / 30.0
already_wasted = monthly_cost * age_months

# Note: Snapshot orphelin pas forc√©ment waste total
# (peut servir pour recovery) - confidence: medium
```

**Exemple :**

Snapshot 500 GB orphelin depuis 180 jours :
```python
snapshot_size_gb = 500
snapshot_price_per_gb = $0.026
monthly_cost = 500 * $0.026 = $13.00/mois
already_wasted = $13.00 * (180/30) = $78.00
```

#### Param√®tres Configurables

| Param√®tre | Type | D√©faut | Description |
|-----------|------|--------|-------------|
| `min_age_days` | int | 30 | √Çge minimum avant d√©tection |
| `exclude_labels` | dict | `{}` | Labels pour exclure (ex: `{'backup': 'long-term'}`) |

#### M√©tadonn√©es Exemple

```json
{
  "resource_id": "snapshot-7777777777777777777",
  "resource_name": "backup-deleted-disk-2024-08",
  "resource_type": "persistent_disk_orphan_snapshots",
  "snapshot_size_gb": 500,
  "storage_bytes": 536870912000,
  "source_disk": "https://www.googleapis.com/compute/v1/projects/my-project/zones/us-central1-a/disks/deleted-disk",
  "source_disk_exists": false,
  "creation_timestamp": "2024-05-05T10:00:00Z",
  "age_days": 181,
  "estimated_monthly_cost": 13.00,
  "already_wasted": 78.42,
  "confidence": "medium",
  "recommendation": "Review if snapshot still needed, consider deletion",
  "detection_date": "2024-11-02T14:30:00Z"
}
```

#### Fichier d'Impl√©mentation

**Backend :** `/backend/app/providers/gcp.py` (√† impl√©menter)

---

### 5. `persistent_disk_old_type` - Ancien Type de Disque

#### D√©tection

**Logique :**

D√©tecter disques **pd-standard** (HDD) qui pourraient migrer vers **pd-balanced** (SSD, meilleur price/perf) :

```python
# 1. Lister tous les disques persistants
disks = get_all_persistent_disks(project_id)

# 2. Pour chaque disque, v√©rifier type
for disk in disks:
    disk_type = disk.type.split('/')[-1]  # "pd-standard"

    # 3. D√©tection si pd-standard (ancien type HDD)
    if disk_type == 'pd-standard':
        # 4. Calculer co√ªt actuel vs pd-balanced
        disk_size_gb = disk.size_gb

        current_cost = disk_size_gb * 0.040  # pd-standard
        balanced_cost = disk_size_gb * 0.100  # pd-balanced

        # 5. pd-balanced co√ªte +150% mais offre 2.3x throughput + 8x IOPS
        # Pour workloads actifs, meilleur price/performance

        # 6. V√©rifier si disque utilis√© (I/O >0)
        io_metrics = get_disk_io_metrics(disk, lookback_days=7)

        if io_metrics['total_ops'] > min_io_threshold:
            # Disque actif avec pd-standard = suboptimal
            # Recommandation: migrer vers pd-balanced
```

**Crit√®res :**
- `disk_type == 'pd-standard'`
- Disque actif (I/O >100 ops/jour sur 7 jours)
- Recommandation: pd-balanced pour meilleur performance/prix

**API Calls :**
```python
# Disks API
compute_client.disks().aggregatedList(project=project_id)

# Cloud Monitoring API (I/O metrics)
monitoring_client.list_time_series(
    name=f"projects/{project_id}",
    filter='metric.type="compute.googleapis.com/instance/disk/read_ops_count"'
)
```

#### Calcul de Co√ªt

**Formule :**

pd-standard vs pd-balanced = trade-off co√ªt vs performance :

```python
disk_size_gb = 1000  # 1 TB

# Co√ªt actuel (pd-standard)
current_cost = 1000 * 0.040 = $40.00/mois

# Co√ªt recommand√© (pd-balanced)
recommended_cost = 1000 * 0.100 = $100.00/mois

# Co√ªt additionnel = +$60/mois
# MAIS: +133% throughput, +700% IOPS

# Waste calculation:
# Si disque actif ‚Üí pd-standard = bottleneck = performance waste
# Si disque idle ‚Üí garder pd-standard OK

# Pour disques actifs, co√ªt = perte de productivit√©
# Estimation: 20% performance gain = √©conomie temps √©quipe
# Indirect waste = $60/mois (estimation conservative)

monthly_waste = 60.00  # Co√ªt opportunit√© (suboptimal performance)
```

**Note :** Ce sc√©nario d√©tecte **performance waste** (suboptimal), pas co√ªt direct.

**Exemple :**

Disque 1 TB pd-standard actif (1000 IOPS/jour) depuis 90 jours :
```python
current_cost = $40.00/mois
recommended_cost = $100.00/mois
performance_waste = $60.00/mois (co√ªt opportunit√©)
already_wasted = $60.00 * (90/30) = $180.00
```

#### Param√®tres Configurables

| Param√®tre | Type | D√©faut | Description |
|-----------|------|--------|-------------|
| `min_io_threshold` | int | 100 | IOPS minimum/jour pour consid√©rer actif |
| `lookback_days` | int | 7 | P√©riode analyse I/O |
| `performance_waste_factor` | float | 0.6 | Facteur co√ªt opportunit√© (60% = $60 waste pour $100 upgrade) |

#### M√©tadonn√©es Exemple

```json
{
  "resource_id": "disk-3333333333333333333",
  "resource_name": "app-data-disk",
  "resource_type": "persistent_disk_old_type",
  "zone": "us-central1-a",
  "disk_type": "pd-standard",
  "size_gb": 1000,
  "io_metrics": {
    "avg_read_ops_per_day": 850,
    "avg_write_ops_per_day": 150,
    "total_ops_per_day": 1000
  },
  "current_cost_monthly": 40.00,
  "recommended_disk_type": "pd-balanced",
  "recommended_cost_monthly": 100.00,
  "performance_improvement": {
    "throughput_increase_percent": 133,
    "iops_increase_percent": 700
  },
  "estimated_monthly_waste": 60.00,
  "already_wasted": 180.00,
  "confidence": "medium",
  "recommendation": "Migrate to pd-balanced for 8x IOPS and 2.3x throughput",
  "detection_date": "2024-11-02T14:30:00Z"
}
```

#### Fichier d'Impl√©mentation

**Backend :** `/backend/app/providers/gcp.py` (√† impl√©menter)

---

### 6. `persistent_disk_overprovisioned_type` - Type de Disque Sur-Provisionn√©

#### D√©tection

**Logique :**

D√©tecter disques **pd-ssd** (high-performance) alors que **pd-balanced** suffirait :

```python
# 1. Lister tous les disques pd-ssd
disks = get_all_persistent_disks(project_id)

pd_ssd_disks = [d for d in disks if d.type.endswith('pd-ssd')]

# 2. Pour chaque disque pd-ssd, analyser I/O r√©el
for disk in pd_ssd_disks:
    # 3. R√©cup√©rer IOPS moyens sur 14 jours
    io_metrics = get_disk_io_metrics(disk, lookback_days=14)

    avg_read_iops = io_metrics['avg_read_ops_per_second']
    avg_write_iops = io_metrics['avg_write_ops_per_second']
    avg_total_iops = avg_read_iops + avg_write_iops

    # 4. Calculer capacit√© IOPS pd-balanced
    disk_size_gb = disk.size_gb
    pd_balanced_max_iops = disk_size_gb * 6  # 6 IOPS/GB

    # 5. D√©tection si IOPS actuel < 50% capacit√© pd-balanced
    if avg_total_iops < (pd_balanced_max_iops * 0.5):
        # pd-ssd over-provisioned, pd-balanced suffit
        # √âconomie potentielle: -41%

        current_cost = disk_size_gb * 0.170  # pd-ssd
        recommended_cost = disk_size_gb * 0.100  # pd-balanced
        monthly_waste = current_cost - recommended_cost
```

**Crit√®res :**
- `disk_type == 'pd-ssd'`
- `avg_iops < pd_balanced_capacity * 0.5`
- √âconomie migration >$10/mois

**API Calls :**
```python
# Disks API
compute_client.disks().aggregatedList(project=project_id)

# Cloud Monitoring API (IOPS metrics)
monitoring_client.list_time_series(
    name=f"projects/{project_id}",
    filter='metric.type="compute.googleapis.com/instance/disk/read_ops_count"'
)
```

#### Calcul de Co√ªt

**Formule :**

pd-ssd vs pd-balanced = -41% √©conomie :

```python
disk_size_gb = 500  # 500 GB

# Co√ªt actuel (pd-ssd)
current_cost = 500 * 0.170 = $85.00/mois

# Co√ªt recommand√© (pd-balanced)
recommended_cost = 500 * 0.100 = $50.00/mois

# Waste = diff√©rence
monthly_waste = 85.00 - 50.00 = $35.00/mois

# V√©rification capacit√©:
# pd-balanced: 500 GB * 6 IOPS/GB = 3000 IOPS max
# IOPS observ√©: 800 IOPS avg
# Utilisation: 800/3000 = 27% ‚Üí pd-balanced largement suffisant

# Co√ªt gaspill√© depuis cr√©ation
age_months = age_days / 30.0
already_wasted = monthly_waste * age_months
```

**Exemple :**

Disque 500 GB pd-ssd avec 800 IOPS avg depuis 120 jours :
```python
current_cost = $85.00/mois
recommended_cost = $50.00/mois
monthly_waste = $35.00
already_wasted = $35.00 * (120/30) = $140.00
```

#### Param√®tres Configurables

| Param√®tre | Type | D√©faut | Description |
|-----------|------|--------|-------------|
| `iops_utilization_threshold` | float | 0.5 | % max utilisation pd-balanced capacity |
| `lookback_days` | int | 14 | P√©riode analyse IOPS |
| `min_savings_threshold` | float | 10.0 | √âconomie minimum $/mois |

#### M√©tadonn√©es Exemple

```json
{
  "resource_id": "disk-8888888888888888888",
  "resource_name": "over-spec-database-disk",
  "resource_type": "persistent_disk_overprovisioned_type",
  "zone": "us-east1-c",
  "disk_type": "pd-ssd",
  "size_gb": 500,
  "io_metrics": {
    "avg_read_iops": 450,
    "avg_write_iops": 350,
    "avg_total_iops": 800,
    "max_iops_observed": 1200
  },
  "pd_balanced_capacity": {
    "max_iops": 3000,
    "current_utilization_percent": 27
  },
  "current_cost_monthly": 85.00,
  "recommended_disk_type": "pd-balanced",
  "recommended_cost_monthly": 50.00,
  "estimated_monthly_waste": 35.00,
  "already_wasted": 140.00,
  "savings_percentage": 41,
  "confidence": "high",
  "recommendation": "Downgrade to pd-balanced - using only 27% of pd-balanced capacity",
  "detection_date": "2024-11-02T14:30:00Z"
}
```

#### Fichier d'Impl√©mentation

**Backend :** `/backend/app/providers/gcp.py` (√† impl√©menter)

---

### 7. `persistent_disk_untagged` - Disques Non Tagg√©s

#### D√©tection

**Logique :**

D√©tecter disques sans **labels GCP** requis pour gouvernance :

```python
# 1. Lister tous les disques
disks = get_all_persistent_disks(project_id)

# 2. D√©finir labels requis (configurables)
required_labels = ['environment', 'owner', 'cost-center', 'project']

# 3. Pour chaque disque, v√©rifier labels
for disk in disks:
    labels = disk.labels if hasattr(disk, 'labels') else {}

    # 4. Identifier labels manquants
    missing_labels = [label for label in required_labels if label not in labels]

    # 5. D√©tection si labels manquants
    if missing_labels:
        # Untagged disk = governance waste
```

**Crit√®res :**
- Labels manquants parmi la liste requise
- Optionnel: valeurs de labels invalides

**API Calls :**
```python
# Disks API
compute_client.disks().aggregatedList(project=project_id)
```

#### Calcul de Co√ªt

**Formule :**

Co√ªt de gouvernance (estim√©) :

```python
# Disques non tagg√©s = perte de visibilit√© + risque co√ªt
# Co√ªt estim√© = temps management + over-provisioning cach√©

disk_size_gb = disk.size_gb
disk_type = disk.type.split('/')[-1]

disk_pricing = {
    'pd-standard': 0.040,
    'pd-balanced': 0.100,
    'pd-ssd': 0.170,
}

disk_monthly_cost = disk_size_gb * disk_pricing.get(disk_type, 0.040)

# Governance waste = 5% du co√ªt disque (estimation)
governance_waste_percentage = 0.05
monthly_waste = disk_monthly_cost * governance_waste_percentage

# Waste cumul√© depuis cr√©ation
age_months = age_days / 30.0
already_wasted = monthly_waste * age_months
```

**Exemple :**

Disque 1 TB pd-balanced sans labels depuis 180 jours :
```python
disk_monthly_cost = 1024 * $0.100 = $102.40
monthly_waste = $102.40 * 0.05 = $5.12
already_wasted = $5.12 * (180/30) = $30.72
```

**Note :** Co√ªt gouvernance est estim√©. Impact r√©el = meilleure visibilit√© co√ªts + pr√©vention waste.

#### Param√®tres Configurables

| Param√®tre | Type | D√©faut | Description |
|-----------|------|--------|-------------|
| `required_labels` | list | `['environment', 'owner', 'cost-center']` | Labels requis |
| `governance_waste_pct` | float | 0.05 | % co√ªt attribu√© au waste gouvernance |
| `enforce_values` | dict | `{}` | Valeurs autoris√©es par label |

#### M√©tadonn√©es Exemple

```json
{
  "resource_id": "disk-4444444444444444444",
  "resource_name": "unnamed-disk-237",
  "resource_type": "persistent_disk_untagged",
  "zone": "asia-east1-a",
  "disk_type": "pd-balanced",
  "size_gb": 1024,
  "labels": {},
  "missing_labels": ["environment", "owner", "cost-center", "project"],
  "creation_timestamp": "2024-05-06T08:00:00Z",
  "age_days": 180,
  "disk_monthly_cost": 102.40,
  "estimated_monthly_waste": 5.12,
  "already_wasted": 30.72,
  "confidence": "medium",
  "recommendation": "Add required labels for cost allocation and governance",
  "detection_date": "2024-11-02T14:30:00Z"
}
```

#### Fichier d'Impl√©mentation

**Backend :** `/backend/app/providers/gcp.py` (√† impl√©menter)

---

## Phase 2 - D√©tection Avanc√©e (3 sc√©narios)

### 8. `persistent_disk_underutilized` - Disques Sous-Utilis√©s

#### D√©tection

**Logique :**

Utiliser **Cloud Monitoring** pour analyser utilisation I/O r√©elle vs capacit√© :

```python
# 1. Lister tous les disques persistants attach√©s
disks = get_all_persistent_disks(project_id)

attached_disks = [d for d in disks if d.users and len(d.users) > 0]

# 2. Pour chaque disque, r√©cup√©rer m√©triques I/O (14 jours)
from google.cloud import monitoring_v3

monitoring_client = monitoring_v3.MetricServiceClient()

for disk in attached_disks:
    # 3. Query read/write throughput
    interval = monitoring_v3.TimeInterval({
        "end_time": {"seconds": int(time.time())},
        "start_time": {"seconds": int(time.time()) - 14*24*3600},
    })

    # Read throughput (bytes/sec)
    read_throughput = monitoring_client.list_time_series(
        request={
            "name": f"projects/{project_id}",
            "filter": f'metric.type="compute.googleapis.com/instance/disk/read_bytes_count" AND resource.disk_name="{disk.name}"',
            "interval": interval,
        }
    )

    # 4. Calculer throughput moyen
    read_values = [point.value.double_value for series in read_throughput for point in series.points]
    avg_read_mbps = (sum(read_values) / len(read_values)) / (1024*1024) if read_values else 0

    # 5. Calculer capacit√© max throughput selon type
    disk_type = disk.type.split('/')[-1]
    disk_size_gb = disk.size_gb

    max_throughput_mbps = {
        'pd-standard': disk_size_gb * 0.12,    # 0.12 MB/s par GB
        'pd-balanced': disk_size_gb * 0.28,    # 0.28 MB/s par GB
        'pd-ssd': disk_size_gb * 0.48,         # 0.48 MB/s par GB
    }.get(disk_type, 0)

    # 6. D√©tection si utilisation <10% capacit√©
    utilization_percent = (avg_read_mbps / max_throughput_mbps * 100) if max_throughput_mbps > 0 else 0

    if utilization_percent < utilization_threshold:
        # Disque sous-utilis√© = potentiel downgrade ou suppression
```

**Crit√®res :**
- Disque attach√© et actif
- Utilisation throughput <10% capacit√©
- Lookback period: 14 jours

**API Calls :**
```python
# Disks API
compute_client.disks().aggregatedList(project=project_id)

# Cloud Monitoring API (throughput metrics)
monitoring_client.list_time_series(
    name=f"projects/{project_id}",
    filter='metric.type="compute.googleapis.com/instance/disk/read_bytes_count"'
)
```

#### Calcul de Co√ªt

**Formule :**

Si disque utilis√© <10%, recommandation downgrade ou snapshot :

```python
disk_size_gb = 1000  # 1 TB
disk_type = 'pd-ssd'

# Co√ªt actuel
current_cost = 1000 * 0.170 = $170.00/mois

# Sc√©narios:
# 1. Downgrade vers pd-balanced
recommended_cost_balanced = 1000 * 0.100 = $100.00/mois
savings_balanced = $70.00/mois

# 2. Convertir en snapshot (si read-only)
snapshot_cost = 1000 * 0.026 = $26.00/mois
savings_snapshot = $144.00/mois

# Waste = √©conomie potentielle (sc√©nario conservateur = downgrade)
monthly_waste = current_cost - recommended_cost_balanced

# Co√ªt gaspill√© depuis cr√©ation
age_months = age_days / 30.0
already_wasted = monthly_waste * age_months
```

**Exemple :**

Disque 1 TB pd-ssd utilis√© √† 5% depuis 90 jours :
```python
current_cost = $170.00/mois
recommended_cost = $100.00/mois (pd-balanced)
monthly_waste = $70.00
already_wasted = $70.00 * (90/30) = $210.00
```

#### Param√®tres Configurables

| Param√®tre | Type | D√©faut | Description |
|-----------|------|--------|-------------|
| `utilization_threshold` | float | 10.0 | % max utilisation throughput |
| `lookback_days` | int | 14 | P√©riode analyse m√©triques |
| `min_datapoints` | int | 50 | Nombre minimum de points de donn√©es |

#### M√©tadonn√©es Exemple

```json
{
  "resource_id": "disk-6666666666666666666",
  "resource_name": "low-usage-disk",
  "resource_type": "persistent_disk_underutilized",
  "zone": "us-central1-a",
  "disk_type": "pd-ssd",
  "size_gb": 1000,
  "io_metrics": {
    "avg_read_mbps": 24.0,
    "avg_write_mbps": 8.0,
    "avg_total_mbps": 32.0,
    "max_throughput_capacity_mbps": 480.0,
    "utilization_percent": 6.7
  },
  "current_cost_monthly": 170.00,
  "recommended_disk_type": "pd-balanced",
  "recommended_cost_monthly": 100.00,
  "estimated_monthly_waste": 70.00,
  "already_wasted": 210.00,
  "confidence": "high",
  "recommendation": "Downgrade to pd-balanced - using only 7% of throughput capacity",
  "detection_date": "2024-11-02T14:30:00Z"
}
```

#### Fichier d'Impl√©mentation

**Backend :** `/backend/app/providers/gcp.py` (√† impl√©menter)

---

### 9. `persistent_disk_oversized` - Disques Sur-Dimensionn√©s

#### D√©tection

**Logique :**

Analyser espace disque utilis√© vs allou√© via m√©triques filesystem :

```python
# 1. Lister tous les disques attach√©s
disks = get_all_persistent_disks(project_id)

attached_disks = [d for d in disks if d.users and len(d.users) > 0]

# 2. Pour chaque disque, r√©cup√©rer m√©triques utilisation espace
# Note: N√©cessite Cloud Monitoring Agent install√© sur instances

from google.cloud import monitoring_v3

monitoring_client = monitoring_v3.MetricServiceClient()

for disk in attached_disks:
    # 3. Query disk used space
    # Metric: agent.googleapis.com/disk/percent_used
    interval = monitoring_v3.TimeInterval({
        "end_time": {"seconds": int(time.time())},
        "start_time": {"seconds": int(time.time()) - 14*24*3600},
    })

    disk_usage = monitoring_client.list_time_series(
        request={
            "name": f"projects/{project_id}",
            "filter": f'metric.type="agent.googleapis.com/disk/percent_used" AND resource.device="{disk.name}"',
            "interval": interval,
        }
    )

    # 4. Calculer % espace utilis√©
    usage_values = [point.value.double_value for series in disk_usage for point in series.points]

    if not usage_values:
        # Agent non install√©, skip
        continue

    avg_used_percent = sum(usage_values) / len(usage_values)
    free_percent = 100 - avg_used_percent

    # 5. D√©tection si >80% espace libre
    if free_percent >= free_space_threshold:
        # 6. Calculer taille recommand√©e
        disk_size_gb = disk.size_gb
        used_gb = disk_size_gb * (avg_used_percent / 100.0)

        # Recommandation: used space + 30% buffer
        recommended_size_gb = int(used_gb * 1.30)

        # Waste = diff√©rence co√ªt
        disk_type = disk.type.split('/')[-1]
        price_per_gb = get_disk_pricing(disk_type)

        current_cost = disk_size_gb * price_per_gb
        recommended_cost = recommended_size_gb * price_per_gb
        monthly_waste = current_cost - recommended_cost

        if monthly_waste >= min_savings_threshold:
            # Disque over-sized = waste d√©tect√©
```

**Crit√®res :**
- Disque attach√© avec m√©triques disponibles
- `free_space >= 80%` (ou utilis√© <20%)
- √âconomie potentielle >$5/mois

**API Calls :**
```python
# Disks API
compute_client.disks().aggregatedList(project=project_id)

# Cloud Monitoring API (disk usage)
monitoring_client.list_time_series(
    name=f"projects/{project_id}",
    filter='metric.type="agent.googleapis.com/disk/percent_used"'
)
```

#### Calcul de Co√ªt

**Formule :**

Resize disk = √©conomie proportionnelle :

```python
disk_size_gb = 2000  # 2 TB allou√©
avg_used_percent = 15  # 15% utilis√©
used_gb = 2000 * 0.15 = 300 GB

# Taille recommand√©e: 300 GB * 1.30 (buffer) = 390 GB
recommended_size_gb = 400  # Arrondi

disk_type = 'pd-balanced'
price_per_gb = 0.100

# Co√ªt actuel
current_cost = 2000 * 0.100 = $200.00/mois

# Co√ªt recommand√©
recommended_cost = 400 * 0.100 = $40.00/mois

# Waste
monthly_waste = 200.00 - 40.00 = $160.00/mois

# Co√ªt gaspill√© depuis cr√©ation
age_months = age_days / 30.0
already_wasted = monthly_waste * age_months
```

**Exemple :**

Disque 2 TB pd-balanced utilis√© √† 15% depuis 120 jours :
```python
current_cost = $200.00/mois
recommended_cost = $40.00/mois (400 GB)
monthly_waste = $160.00
already_wasted = $160.00 * (120/30) = $640.00
```

#### Param√®tres Configurables

| Param√®tre | Type | D√©faut | Description |
|-----------|------|--------|-------------|
| `free_space_threshold` | float | 80.0 | % minimum espace libre pour d√©tection |
| `safety_buffer` | float | 1.30 | Marge s√©curit√© taille (1.30 = +30%) |
| `min_savings_threshold` | float | 5.0 | √âconomie minimum $/mois |
| `lookback_days` | int | 14 | P√©riode analyse utilisation |

#### M√©tadonn√©es Exemple

```json
{
  "resource_id": "disk-2222222222222222222",
  "resource_name": "oversized-data-disk",
  "resource_type": "persistent_disk_oversized",
  "zone": "europe-west1-c",
  "disk_type": "pd-balanced",
  "size_gb": 2000,
  "disk_usage": {
    "avg_used_percent": 15.2,
    "avg_used_gb": 304,
    "avg_free_percent": 84.8,
    "avg_free_gb": 1696
  },
  "recommended_size_gb": 400,
  "current_cost_monthly": 200.00,
  "recommended_cost_monthly": 40.00,
  "estimated_monthly_waste": 160.00,
  "already_wasted": 640.00,
  "savings_percentage": 80,
  "confidence": "high",
  "recommendation": "Resize from 2000GB to 400GB - using only 15% of capacity",
  "monitoring_agent_installed": true,
  "detection_date": "2024-11-02T14:30:00Z"
}
```

#### Fichier d'Impl√©mentation

**Backend :** `/backend/app/providers/gcp.py` (√† impl√©menter)

---

### 10. `persistent_disk_readonly` - Disques en Lecture Seule

#### D√©tection

**Logique :**

D√©tecter disques avec **z√©ro writes** ‚Üí convertir en snapshot (-35% co√ªt) :

```python
# 1. Lister tous les disques attach√©s
disks = get_all_persistent_disks(project_id)

attached_disks = [d for d in disks if d.users and len(d.users) > 0]

# 2. Pour chaque disque, analyser write operations (30 jours)
from google.cloud import monitoring_v3

monitoring_client = monitoring_v3.MetricServiceClient()

for disk in attached_disks:
    # 3. Query write operations
    interval = monitoring_v3.TimeInterval({
        "end_time": {"seconds": int(time.time())},
        "start_time": {"seconds": int(time.time()) - 30*24*3600},
    })

    write_ops = monitoring_client.list_time_series(
        request={
            "name": f"projects/{project_id}",
            "filter": f'metric.type="compute.googleapis.com/instance/disk/write_ops_count" AND resource.disk_name="{disk.name}"',
            "interval": interval,
        }
    )

    # 4. Calculer total write operations
    total_writes = sum([point.value.int64_value for series in write_ops for point in series.points])

    # 5. D√©tection si zero writes (read-only)
    if total_writes <= max_write_ops_threshold:
        # 6. Calculer √©conomie snapshot vs disk
        disk_size_gb = disk.size_gb
        disk_type = disk.type.split('/')[-1]

        disk_pricing = {
            'pd-standard': 0.040,
            'pd-balanced': 0.100,
            'pd-ssd': 0.170,
        }

        disk_cost = disk_size_gb * disk_pricing.get(disk_type, 0.040)
        snapshot_cost = disk_size_gb * 0.026

        monthly_waste = disk_cost - snapshot_cost

        if monthly_waste >= min_savings_threshold:
            # Read-only disk = waste (peut devenir snapshot)
```

**Crit√®res :**
- Disque attach√©
- `total_writes <= max_threshold` sur 30 jours (d√©faut: 10 writes)
- √âconomie snapshot >$5/mois

**API Calls :**
```python
# Disks API
compute_client.disks().aggregatedList(project=project_id)

# Cloud Monitoring API (write ops)
monitoring_client.list_time_series(
    name=f"projects/{project_id}",
    filter='metric.type="compute.googleapis.com/instance/disk/write_ops_count"'
)
```

#### Calcul de Co√ªt

**Formule :**

Disk ‚Üí Snapshot = -35% √† -85% √©conomie :

```python
disk_size_gb = 500
disk_type = 'pd-balanced'

# Co√ªt actuel (disk)
disk_cost = 500 * 0.100 = $50.00/mois

# Co√ªt recommand√© (snapshot)
snapshot_cost = 500 * 0.026 = $13.00/mois

# Waste
monthly_waste = 50.00 - 13.00 = $37.00/mois

# Co√ªt gaspill√© depuis dernier write
last_write_date = parse_timestamp(last_write_timestamp)
readonly_days = (now - last_write_date).days
already_wasted = monthly_waste * (readonly_days / 30.0)
```

**Exemple :**

Disque 500 GB pd-balanced read-only depuis 180 jours :
```python
disk_cost = $50.00/mois
snapshot_cost = $13.00/mois
monthly_waste = $37.00
already_wasted = $37.00 * (180/30) = $222.00
```

#### Param√®tres Configurables

| Param√®tre | Type | D√©faut | Description |
|-----------|------|--------|-------------|
| `max_write_ops_threshold` | int | 10 | Nombre max writes sur p√©riode |
| `lookback_days` | int | 30 | P√©riode analyse writes |
| `min_savings_threshold` | float | 5.0 | √âconomie minimum $/mois |

#### M√©tadonn√©es Exemple

```json
{
  "resource_id": "disk-1111111111111111111",
  "resource_name": "readonly-archive-disk",
  "resource_type": "persistent_disk_readonly",
  "zone": "us-west1-b",
  "disk_type": "pd-balanced",
  "size_gb": 500,
  "io_metrics": {
    "total_read_ops_30d": 12450,
    "total_write_ops_30d": 0,
    "last_write_timestamp": "2024-05-05T10:00:00Z",
    "readonly_days": 181
  },
  "current_cost_monthly": 50.00,
  "recommended_storage": "snapshot",
  "recommended_cost_monthly": 13.00,
  "estimated_monthly_waste": 37.00,
  "already_wasted": 223.67,
  "savings_percentage": 74,
  "confidence": "high",
  "recommendation": "Convert to snapshot - zero writes for 181 days",
  "detection_date": "2024-11-02T14:30:00Z"
}
```

#### Fichier d'Impl√©mentation

**Backend :** `/backend/app/providers/gcp.py` (√† impl√©menter)

---

## Protocole de Test

### Pr√©requis

#### 1. Compte GCP et Projet Test

```bash
# Utiliser projet test existant ou cr√©er nouveau
export PROJECT_ID="cloudwaste-test-XXXXXXXXXX"
gcloud config set project $PROJECT_ID

# Activer APIs requises
gcloud services enable compute.googleapis.com
gcloud services enable monitoring.googleapis.com
```

#### 2. Service Account (si pas d√©j√† cr√©√©)

```bash
# Utiliser Service Account du test Compute Instances
export GOOGLE_APPLICATION_CREDENTIALS="$(pwd)/cloudwaste-key.json"
```

#### 3. Installer Cloud Monitoring Agent

**Requis pour sc√©narios 9-10** (disk usage, I/O metrics)

```bash
# Template installation (√† ex√©cuter sur instances avec disques)
curl -sSO https://dl.google.com/cloudagents/add-google-cloud-ops-agent-repo.sh
sudo bash add-google-cloud-ops-agent-repo.sh --also-install
```

---

### Tests Unitaires - Cr√©er Disques de Test

#### Sc√©nario 1: Disque Non Attach√©

```bash
# Cr√©er disque standalone (non attach√©)
gcloud compute disks create test-unattached-disk \
  --zone=us-central1-a \
  --size=500GB \
  --type=pd-balanced

# Attendre 7 jours ou modifier creation_timestamp via API pour tests
```

**Validation attendue :**
```json
{
  "resource_type": "persistent_disk_unattached",
  "resource_name": "test-unattached-disk",
  "size_gb": 500,
  "disk_type": "pd-balanced",
  "users": [],
  "age_days": ">=7",
  "estimated_monthly_cost": "~50.00",
  "confidence": "high"
}
```

---

#### Sc√©nario 2: Disque Attach√© √† Instance Arr√™t√©e

```bash
# Cr√©er instance avec disque
gcloud compute instances create test-stopped-instance \
  --zone=us-central1-a \
  --machine-type=n2-standard-2 \
  --boot-disk-size=100GB \
  --boot-disk-type=pd-standard

# Cr√©er et attacher disque additionnel
gcloud compute disks create test-attached-stopped-disk \
  --zone=us-central1-a \
  --size=1000GB \
  --type=pd-ssd

gcloud compute instances attach-disk test-stopped-instance \
  --zone=us-central1-a \
  --disk=test-attached-stopped-disk

# Arr√™ter instance
gcloud compute instances stop test-stopped-instance --zone=us-central1-a

# Attendre 30 jours pour d√©tection
```

**Validation attendue :**
```json
{
  "resource_type": "persistent_disk_attached_stopped",
  "resource_name": "test-attached-stopped-disk",
  "size_gb": 1024,
  "disk_type": "pd-ssd",
  "attached_instance": {
    "status": "TERMINATED",
    "stopped_days": ">=30"
  },
  "estimated_monthly_cost": "~174.08"
}
```

---

#### Sc√©nario 3: Disque Jamais Utilis√©

```bash
# Cr√©er instance
gcloud compute instances create test-never-used-instance \
  --zone=us-central1-a \
  --machine-type=e2-small \
  --image-family=debian-11 \
  --image-project=debian-cloud

# Cr√©er et attacher disque (ne jamais monter ni utiliser)
gcloud compute disks create test-never-used-disk \
  --zone=us-central1-a \
  --size=250GB \
  --type=pd-balanced

gcloud compute instances attach-disk test-never-used-instance \
  --zone=us-central1-a \
  --disk=test-never-used-disk

# NE PAS monter le disque (rester idle)
# Attendre 7 jours pour m√©triques I/O = 0
```

**Validation attendue :**
```json
{
  "resource_type": "persistent_disk_never_used",
  "resource_name": "test-never-used-disk",
  "io_metrics": {
    "total_read_ops": 0,
    "total_write_ops": 0
  },
  "age_days": ">=7",
  "estimated_monthly_cost": "~25.00"
}
```

---

#### Sc√©nario 4: Snapshots Orphelins

```bash
# Cr√©er disque temporaire
gcloud compute disks create temp-disk-for-snapshot \
  --zone=us-central1-a \
  --size=500GB \
  --type=pd-standard

# Cr√©er snapshot
gcloud compute disks snapshot temp-disk-for-snapshot \
  --zone=us-central1-a \
  --snapshot-names=test-orphan-snapshot

# Supprimer disque source (rendre snapshot orphelin)
gcloud compute disks delete temp-disk-for-snapshot --zone=us-central1-a --quiet

# Attendre 30 jours pour d√©tection
```

**Validation attendue :**
```json
{
  "resource_type": "persistent_disk_orphan_snapshots",
  "resource_name": "test-orphan-snapshot",
  "snapshot_size_gb": 500,
  "source_disk_exists": false,
  "age_days": ">=30",
  "estimated_monthly_cost": "~13.00"
}
```

---

#### Sc√©nario 5: Ancien Type de Disque

```bash
# Cr√©er instance avec disque pd-standard actif
gcloud compute instances create test-old-type-instance \
  --zone=us-central1-a \
  --machine-type=n2-standard-2 \
  --image-family=debian-11 \
  --image-project=debian-cloud

# Cr√©er et attacher disque pd-standard
gcloud compute disks create test-old-type-disk \
  --zone=us-central1-a \
  --size=1000GB \
  --type=pd-standard

gcloud compute instances attach-disk test-old-type-instance \
  --zone=us-central1-a \
  --disk=test-old-type-disk

# SSH et g√©n√©rer I/O actif
gcloud compute ssh test-old-type-instance --zone=us-central1-a

# Sur instance: monter disque et g√©n√©rer I/O
sudo mkfs.ext4 /dev/sdb
sudo mkdir /mnt/data
sudo mount /dev/sdb /mnt/data

# G√©n√©rer I/O continu
dd if=/dev/zero of=/mnt/data/testfile bs=1M count=1000 &

# Laisser tourner 7 jours
```

**Validation attendue :**
```json
{
  "resource_type": "persistent_disk_old_type",
  "resource_name": "test-old-type-disk",
  "disk_type": "pd-standard",
  "recommended_disk_type": "pd-balanced",
  "io_metrics": {
    "avg_ops_per_day": ">100"
  },
  "estimated_monthly_waste": "~60.00"
}
```

---

#### Sc√©nario 6: Type Sur-Provisionn√©

```bash
# Cr√©er instance
gcloud compute instances create test-overprovisioned-type-instance \
  --zone=us-central1-a \
  --machine-type=n2-standard-2 \
  --image-family=debian-11 \
  --image-project=debian-cloud

# Cr√©er disque pd-ssd (high-performance)
gcloud compute disks create test-overprovisioned-type-disk \
  --zone=us-central1-a \
  --size=500GB \
  --type=pd-ssd

gcloud compute instances attach-disk test-overprovisioned-type-instance \
  --zone=us-central1-a \
  --disk=test-overprovisioned-type-disk

# SSH et g√©n√©rer I/O faible (<1000 IOPS)
gcloud compute ssh test-overprovisioned-type-instance --zone=us-central1-a

# Sur instance: monter et g√©n√©rer faible I/O
sudo mkfs.ext4 /dev/sdb
sudo mkdir /mnt/data
sudo mount /dev/sdb /mnt/data

# Low IOPS workload (500 IOPS, bien en-dessous 3000 IOPS pd-balanced)
fio --name=low_iops --ioengine=libaio --rw=randread --bs=4k --iodepth=16 --numjobs=1 --runtime=0 --time_based --directory=/mnt/data &

# Laisser tourner 14 jours
```

**Validation attendue :**
```json
{
  "resource_type": "persistent_disk_overprovisioned_type",
  "resource_name": "test-overprovisioned-type-disk",
  "disk_type": "pd-ssd",
  "recommended_disk_type": "pd-balanced",
  "io_metrics": {
    "avg_total_iops": "~800"
  },
  "pd_balanced_capacity": {
    "max_iops": 3000,
    "utilization_percent": "~27"
  },
  "estimated_monthly_waste": "~35.00"
}
```

---

#### Sc√©nario 7: Disques Non Tagg√©s

```bash
# Cr√©er disque SANS labels
gcloud compute disks create test-untagged-disk \
  --zone=asia-east1-a \
  --size=1000GB \
  --type=pd-balanced
```

**Validation attendue :**
```json
{
  "resource_type": "persistent_disk_untagged",
  "resource_name": "test-untagged-disk",
  "labels": {},
  "missing_labels": ["environment", "owner", "cost-center"],
  "estimated_monthly_waste": "~5.12"
}
```

---

#### Sc√©nario 8: Disques Sous-Utilis√©s

```bash
# Cr√©er instance avec disque pd-ssd large
gcloud compute instances create test-underutilized-instance \
  --zone=us-central1-a \
  --machine-type=n2-standard-4 \
  --image-family=debian-11 \
  --image-project=debian-cloud

gcloud compute disks create test-underutilized-disk \
  --zone=us-central1-a \
  --size=1000GB \
  --type=pd-ssd

gcloud compute instances attach-disk test-underutilized-instance \
  --zone=us-central1-a \
  --disk=test-underutilized-disk

# SSH, installer monitoring agent, g√©n√©rer tr√®s faible I/O
gcloud compute ssh test-underutilized-instance --zone=us-central1-a

# Sur instance:
curl -sSO https://dl.google.com/cloudagents/add-google-cloud-ops-agent-repo.sh
sudo bash add-google-cloud-ops-agent-repo.sh --also-install

sudo mkfs.ext4 /dev/sdb
sudo mkdir /mnt/data
sudo mount /dev/sdb /mnt/data

# G√©n√©rer I/O tr√®s faible (<5% capacit√© pd-ssd)
dd if=/dev/zero of=/mnt/data/testfile bs=1M count=10 conv=fsync &

# Laisser tourner 14 jours
```

**Validation attendue :**
```json
{
  "resource_type": "persistent_disk_underutilized",
  "resource_name": "test-underutilized-disk",
  "disk_type": "pd-ssd",
  "io_metrics": {
    "utilization_percent": "<10"
  },
  "estimated_monthly_waste": "~70.00"
}
```

---

#### Sc√©nario 9: Disques Sur-Dimensionn√©s

```bash
# Cr√©er instance
gcloud compute instances create test-oversized-instance \
  --zone=europe-west1-c \
  --machine-type=n2-standard-2 \
  --image-family=debian-11 \
  --image-project=debian-cloud

# Cr√©er disque large (2 TB)
gcloud compute disks create test-oversized-disk \
  --zone=europe-west1-c \
  --size=2000GB \
  --type=pd-balanced

gcloud compute instances attach-disk test-oversized-instance \
  --zone=europe-west1-c \
  --disk=test-oversized-disk

# SSH, installer agent, utiliser seulement 15%
gcloud compute ssh test-oversized-instance --zone=europe-west1-c

# Sur instance:
curl -sSO https://dl.google.com/cloudagents/add-google-cloud-ops-agent-repo.sh
sudo bash add-google-cloud-ops-agent-repo.sh --also-install

sudo mkfs.ext4 /dev/sdb
sudo mkdir /mnt/data
sudo mount /dev/sdb /mnt/data

# Remplir seulement 300 GB (15%)
dd if=/dev/zero of=/mnt/data/testfile bs=1G count=300

# Laisser tourner 14 jours
```

**Validation attendue :**
```json
{
  "resource_type": "persistent_disk_oversized",
  "resource_name": "test-oversized-disk",
  "size_gb": 2000,
  "disk_usage": {
    "avg_used_percent": "~15",
    "avg_free_percent": "~85"
  },
  "recommended_size_gb": 400,
  "estimated_monthly_waste": "~160.00"
}
```

---

#### Sc√©nario 10: Disques Read-Only

```bash
# Cr√©er instance
gcloud compute instances create test-readonly-instance \
  --zone=us-west1-b \
  --machine-type=e2-medium \
  --image-family=debian-11 \
  --image-project=debian-cloud

# Cr√©er disque
gcloud compute disks create test-readonly-disk \
  --zone=us-west1-b \
  --size=500GB \
  --type=pd-balanced

gcloud compute instances attach-disk test-readonly-instance \
  --zone=us-west1-b \
  --disk=test-readonly-disk

# SSH, monter, √©crire une fois puis seulement reads
gcloud compute ssh test-readonly-instance --zone=us-west1-b

# Sur instance:
sudo mkfs.ext4 /dev/sdb
sudo mkdir /mnt/data
sudo mount /dev/sdb /mnt/data

# √âcrire data une seule fois
dd if=/dev/zero of=/mnt/data/archive.dat bs=1G count=100

# Ensuite, seulement reads (aucun write)
# Simuler acc√®s lecture p√©riodique
while true; do
  cat /mnt/data/archive.dat > /dev/null
  sleep 3600  # Lire 1x/heure
done &

# Laisser tourner 30 jours SANS aucun write
```

**Validation attendue :**
```json
{
  "resource_type": "persistent_disk_readonly",
  "resource_name": "test-readonly-disk",
  "io_metrics": {
    "total_write_ops_30d": 0
  },
  "recommended_storage": "snapshot",
  "estimated_monthly_waste": "~37.00",
  "savings_percentage": 74
}
```

---

### Validation Globale

#### Script Test Complet

```python
#!/usr/bin/env python3
"""
Script de validation complet pour GCP Persistent Disks
"""

from google.cloud import compute_v1
from google.cloud import monitoring_v3
import os

PROJECT_ID = os.environ['PROJECT_ID']

def test_all_scenarios():
    disks_client = compute_v1.DisksClient()
    snapshots_client = compute_v1.SnapshotsClient()

    # 1. Lister tous les disques
    request = compute_v1.AggregatedListDisksRequest(
        project=PROJECT_ID
    )

    agg_list = disks_client.aggregated_list(request=request)

    disks = []
    for zone, response in agg_list:
        if response.disks:
            disks.extend(response.disks)

    print(f"‚úÖ Found {len(disks)} disks")

    # 2. Lister tous les snapshots
    snapshots = list(snapshots_client.list(project=PROJECT_ID))
    print(f"‚úÖ Found {len(snapshots)} snapshots")

    # 3. V√©rifier d√©tection pour chaque sc√©nario
    scenarios_detected = {
        'unattached': 0,
        'attached_stopped': 0,
        'never_used': 0,
        'orphan_snapshots': 0,
        'old_type': 0,
        'overprovisioned_type': 0,
        'untagged': 0,
        'underutilized': 0,
        'oversized': 0,
        'readonly': 0,
    }

    # D√©tection simplifi√©e (logique compl√®te dans provider)
    for disk in disks:
        name = disk.name

        # Scenario 1: Unattached
        if not disk.users or len(disk.users) == 0:
            scenarios_detected['unattached'] += 1
            print(f"‚úÖ Detected scenario 1 (unattached): {name}")

        # Scenario 5: Old type
        disk_type = disk.type.split('/')[-1]
        if disk_type == 'pd-standard':
            scenarios_detected['old_type'] += 1
            print(f"‚úÖ Detected scenario 5 (old type): {name}")

        # Scenario 6: Overprovisioned type
        if disk_type == 'pd-ssd':
            scenarios_detected['overprovisioned_type'] += 1
            print(f"‚úÖ Detected scenario 6 (overprovisioned type): {name}")

        # Scenario 7: Untagged
        labels = disk.labels if hasattr(disk, 'labels') else {}
        if not labels or len(labels) == 0:
            scenarios_detected['untagged'] += 1
            print(f"‚úÖ Detected scenario 7 (untagged): {name}")

    # Snapshots orphelins
    all_disk_names = {d.name for d in disks}

    for snapshot in snapshots:
        if snapshot.source_disk:
            source_name = snapshot.source_disk.split('/')[-1]
            if source_name not in all_disk_names:
                scenarios_detected['orphan_snapshots'] += 1
                print(f"‚úÖ Detected scenario 4 (orphan snapshot): {snapshot.name}")

    # 4. Rapport final
    print("\nüìä Detection Summary:")
    for scenario, count in scenarios_detected.items():
        print(f"  - {scenario}: {count} resources")

    total_detected = sum(scenarios_detected.values())
    print(f"\n‚úÖ Total waste detected: {total_detected} resources")

if __name__ == '__main__':
    test_all_scenarios()
```

#### Ex√©cution

```bash
# Exporter PROJECT_ID
export PROJECT_ID="cloudwaste-test-XXXXXXXXXX"

# Ex√©cuter validation
python3 test_gcp_persistent_disks.py
```

**R√©sultat attendu :**
```
‚úÖ Found 10 disks
‚úÖ Found 1 snapshots
‚úÖ Detected scenario 1 (unattached): test-unattached-disk
‚úÖ Detected scenario 2 (attached stopped): test-attached-stopped-disk
‚úÖ Detected scenario 3 (never used): test-never-used-disk
‚úÖ Detected scenario 4 (orphan snapshot): test-orphan-snapshot
‚úÖ Detected scenario 5 (old type): test-old-type-disk
‚úÖ Detected scenario 6 (overprovisioned type): test-overprovisioned-type-disk
‚úÖ Detected scenario 7 (untagged): test-untagged-disk
‚úÖ Detected scenario 8 (underutilized): test-underutilized-disk
‚úÖ Detected scenario 9 (oversized): test-oversized-disk
‚úÖ Detected scenario 10 (readonly): test-readonly-disk

üìä Detection Summary:
  - unattached: 1 resources
  - attached_stopped: 1 resources
  - never_used: 1 resources
  - orphan_snapshots: 1 resources
  - old_type: 1 resources
  - overprovisioned_type: 1 resources
  - untagged: 1 resources
  - underutilized: 1 resources
  - oversized: 1 resources
  - readonly: 1 resources

‚úÖ Total waste detected: 10 resources
```

---

### Cleanup

```bash
# Supprimer tous les disques de test
gcloud compute disks delete test-unattached-disk --zone=us-central1-a --quiet
gcloud compute disks delete test-attached-stopped-disk --zone=us-central1-a --quiet
gcloud compute disks delete test-never-used-disk --zone=us-central1-a --quiet
gcloud compute disks delete test-old-type-disk --zone=us-central1-a --quiet
gcloud compute disks delete test-overprovisioned-type-disk --zone=us-central1-a --quiet
gcloud compute disks delete test-untagged-disk --zone=asia-east1-a --quiet
gcloud compute disks delete test-underutilized-disk --zone=us-central1-a --quiet
gcloud compute disks delete test-oversized-disk --zone=europe-west1-c --quiet
gcloud compute disks delete test-readonly-disk --zone=us-west1-b --quiet

# Supprimer instances
gcloud compute instances delete test-stopped-instance --zone=us-central1-a --quiet
gcloud compute instances delete test-never-used-instance --zone=us-central1-a --quiet
gcloud compute instances delete test-old-type-instance --zone=us-central1-a --quiet
gcloud compute instances delete test-overprovisioned-type-instance --zone=us-central1-a --quiet
gcloud compute instances delete test-underutilized-instance --zone=us-central1-a --quiet
gcloud compute instances delete test-oversized-instance --zone=europe-west1-c --quiet
gcloud compute instances delete test-readonly-instance --zone=us-west1-b --quiet

# Supprimer snapshots
gcloud compute snapshots delete test-orphan-snapshot --quiet
```

---

## R√©f√©rences

### Documentation GCP

- [Persistent Disks API](https://cloud.google.com/compute/docs/reference/rest/v1/disks)
- [Snapshots API](https://cloud.google.com/compute/docs/reference/rest/v1/snapshots)
- [Disk Pricing](https://cloud.google.com/compute/disks-image-pricing)
- [Cloud Monitoring Disk Metrics](https://cloud.google.com/monitoring/api/metrics_gcp#gcp-compute)
- [Disk Types Comparison](https://cloud.google.com/compute/docs/disks)

### CloudWaste Documentation

- [GCP.md](./GCP.md) - Listing complet 27 ressources GCP
- [GCP_COMPUTE_ENGINE_SCENARIOS_100.md](./GCP_COMPUTE_ENGINE_SCENARIOS_100.md) - Compute Instances scenarios
- [README.md](./README.md) - Guide utilisation documentation GCP

### √âquivalences AWS/Azure

- **AWS EBS Volumes** ‚Üí GCP Persistent Disks
- **Azure Managed Disks** ‚Üí GCP Persistent Disks
- **AWS EBS Snapshots** ‚Üí GCP Disk Snapshots
- **Azure Disk Snapshots** ‚Üí GCP Disk Snapshots
- **AWS gp3/gp2** ‚Üí GCP pd-balanced
- **AWS io2** ‚Üí GCP pd-ssd
- **AWS st1/sc1** ‚Üí GCP pd-standard

---

**Derni√®re mise √† jour :** 2 novembre 2025
**Status :** ‚úÖ Sp√©cification compl√®te - Pr√™t pour impl√©mentation
**Version :** 1.0
**Auteur :** CloudWaste Team

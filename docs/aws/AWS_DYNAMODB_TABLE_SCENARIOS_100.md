# AWS DynamoDB Table - Waste Detection Scenarios (100% Coverage)

**Resource Type:** `dynamodb_table`
**Cloud Provider:** AWS
**Detection Coverage:** 10 sc√©narios (100%)
**Version:** 1.0.0
**Last Updated:** 2025-10-31

---

## üìë Table des Mati√®res

### Phase 1: Sc√©narios Existants (Impl√©ment√©s)
1. [Over-Provisioned Capacity](#-sc√©nario-1-over-provisioned-capacity-unused-rcu-wcu)
2. [Unused Global Secondary Indexes (GSI)](#-sc√©nario-2-unused-global-secondary-indexes-gsi)
3. [Never Used Tables (Provisioned Mode)](#-sc√©nario-3-never-used-tables-provisioned-mode)
4. [Never Used Tables (On-Demand Mode)](#-sc√©nario-4-never-used-tables-on-demand-mode)
5. [Empty Tables](#-sc√©nario-5-empty-tables-0-items)

### Phase 2: Nouveaux Sc√©narios (√Ä Impl√©menter)
6. [Point-in-Time Recovery (PITR) Enabled But Never Used](#-sc√©nario-6-point-in-time-recovery-pitr-enabled-but-never-used)
7. [Global Tables Replication Unused](#-sc√©nario-7-global-tables-replication-unused)
8. [DynamoDB Streams Enabled But No Consumers](#-sc√©nario-8-dynamodb-streams-enabled-but-no-consumers)
9. [TTL Disabled on Time-Based Data](#-sc√©nario-9-ttl-disabled-on-time-based-data)
10. [Wrong Billing Mode (Provisioned vs On-Demand Mismatch)](#-sc√©nario-10-wrong-billing-mode-provisioned-vs-on-demand-mismatch)

### Sections Annexes
- [CloudWatch Metrics Analysis](#-cloudwatch-metrics-analysis)
- [Test Matrix](#-test-matrix)
- [ROI Analysis](#-roi-analysis-dynamodb-waste-detection)
- [IAM Permissions](#-iam-permissions-read-only)
- [Troubleshooting](#-troubleshooting)
- [Resources & Documentation](#-resources--documentation)
- [Changelog](#-changelog)

---

# üìñ Introduction

## Qu'est-ce que DynamoDB ?

**Amazon DynamoDB** est un service de base de donn√©es NoSQL **fully managed** offrant :

- **Performance pr√©visible** : Single-digit millisecond latency
- **Scalabilit√© automatique** : Pas de limite de taille de table
- **High availability** : R√©plication multi-AZ automatique
- **Fully managed** : Pas de serveurs √† g√©rer
- **Flexible billing** : Provisioned ou On-Demand mode

## Pourquoi Optimiser DynamoDB ?

DynamoDB peut g√©n√©rer des co√ªts majeurs si mal configur√© :

| Probl√®me | Impact Annuel (par table) | Fr√©quence |
|----------|---------------------------|-----------|
| **Over-provisioned capacity** | $500 - $5,000 | 45% des tables |
| **Unused GSI** | $200 - $2,000 | 30% des tables |
| **Never used tables** | $100 - $1,000 | 20% des tables |
| **PITR jamais utilis√©** | $50 - $500 | 60% des tables |
| **Streams sans consumers** | $20 - $200 | 25% des tables |

**Total waste moyen:** $870 - $8,700/table/an
**Pour 100 tables:** $87,000 - $870,000/an ‚ùå

## DynamoDB Billing Modes

### 1. Provisioned Mode (Capacity Units)

Vous **provisionnez** une capacit√© fixe de lecture/√©criture :

- **Read Capacity Unit (RCU)** : 1 RCU = 1 strongly consistent read/second (4 KB)
- **Write Capacity Unit (WCU)** : 1 WCU = 1 write/second (1 KB)
- **Prix** : Charged 24/7 (m√™me si non utilis√©!)

### 2. On-Demand Mode (Pay-Per-Request)

Vous payez uniquement pour les requ√™tes effectu√©es :

- **Read Request Unit** : Par requ√™te read (4 KB)
- **Write Request Unit** : Par requ√™te write (1 KB)
- **Prix** : $0 si table inactive (storage only)

## üí∞ DynamoDB Pricing (us-east-1)

### Provisioned Mode Pricing

| Item | Tarif | Unit√© | Notes |
|------|-------|-------|-------|
| **Read Capacity Unit (RCU)** | $0.00013 | per hour | Strongly consistent reads |
| **Write Capacity Unit (WCU)** | $0.00065 | per hour | Standard writes |
| **Storage** | $0.25 | per GB/month | First 25 GB free |
| **Backups (on-demand)** | $0.10 | per GB/month | Incremental |
| **Continuous Backups (PITR)** | $0.20 | per GB/month | 2√ó storage cost |
| **Restore** | $0.15 | per GB | One-time charge |

**Exemple: Table provisionn√©e (100 RCU, 50 WCU, 10 GB)**
```
RCU cost: 100 √ó $0.00013 √ó 730 hours = $9.49/mois
WCU cost: 50 √ó $0.00065 √ó 730 hours = $23.73/mois
Storage: 10 GB √ó $0.25 = $2.50/mois
TOTAL: $35.72/mois = $429/an
```

### On-Demand Mode Pricing

| Item | Tarif | Unit√© | Notes |
|------|-------|-------|-------|
| **Read Request Unit** | $0.25 | per million | 4 KB per read |
| **Write Request Unit** | $1.25 | per million | 1 KB per write |
| **Storage** | $0.25 | per GB/month | First 25 GB free |
| **Backups (on-demand)** | $0.10 | per GB/month | Same as provisioned |
| **Continuous Backups (PITR)** | $0.20 | per GB/month | 2√ó storage cost |

**Exemple: Table On-Demand (1M reads, 500K writes/mois, 10 GB)**
```
Read cost: 1M √ó $0.25 / 1M = $0.25/mois
Write cost: 500K √ó $1.25 / 1M = $0.625/mois
Storage: 10 GB √ó $0.25 = $2.50/mois
TOTAL: $3.38/mois = $41/an
```

### Global Tables (Multi-Region Replication)

| Item | Tarif | Notes |
|------|-------|-------|
| **Replicated Write Request Units (rWRU)** | $1.875 per million | 1.5√ó standard WRU cost |
| **Storage per replica** | $0.25 per GB/month | Full table duplicated |
| **Cross-region bandwidth** | $0.09 per GB | us-east-1 ‚Üí eu-west-1 |

**Exemple: Global Table (2 regions, 10 GB, 1M writes/mois)**
```
Primary region:
- Storage: 10 GB √ó $0.25 = $2.50/mois
- Writes: 1M √ó $1.25 / 1M = $1.25/mois

Replica region:
- Storage: 10 GB √ó $0.25 = $2.50/mois
- Replicated writes: 1M √ó $1.875 / 1M = $1.875/mois
- Bandwidth: 10 MB √ó $0.09 = $0.90/mois

TOTAL: $9.03/mois = $108/an (vs $41/an single region)
```

### DynamoDB Streams

| Item | Tarif | Notes |
|------|-------|-------|
| **Stream Read Request Unit** | $0.02 per 100K | Lambda/Kinesis consumers |
| **Storage (24 hours)** | Included | Free |

**Exemple: Stream avec 10M changes/mois consum√©s par Lambda**
```
Stream reads: 10M √ó $0.02 / 100K = $2.00/mois = $24/an
```

## üìä DynamoDB Waste Impact

**Exemple: Entreprise avec 100 DynamoDB tables**

| Sc√©nario | Tables Affect√©es | Co√ªt Annuel Gaspill√© | % du Budget |
|----------|------------------|----------------------|-------------|
| Over-provisioned capacity | 45 | $22,500 | 35% |
| Unused GSI | 30 | $15,000 | 23% |
| Never used (Provisioned) | 10 | $5,000 | 8% |
| Never used (On-Demand) | 10 | $500 | 1% |
| Empty tables | 15 | $2,000 | 3% |
| PITR jamais utilis√© | 60 | $12,000 | 19% |
| Streams sans consumers | 25 | $3,000 | 5% |
| Global Tables unused replicas | 5 | $3,000 | 5% |
| TTL disabled | 20 | $1,000 | 2% |
| Wrong billing mode | 15 | $3,000 | 5% |

**Total Annual Waste:** $67,000 pour 100 tables
**Average per table:** $670/an
**CloudWaste SaaS:** $3,588/an
**ROI:** 1,768% (18.7√ó return)

---

## üí∏ Sc√©nario 1: Over-Provisioned Capacity (Unused RCU / WCU)

### üîç Description

Une table DynamoDB en **mode Provisioned** avec **<10% d'utilisation RCU/WCU** g√©n√®re un **gaspillage massif** :

- **Over-provisioning pr√©ventif** ("allouons 1000 RCU au cas o√π")
- **Legacy configuration** (capacit√© jamais revue apr√®s impl√©mentation)
- **Peak capacity 24/7** (provision pour le pic, mais pic = 1% du temps)
- **No auto-scaling** (capacit√© fixe sans ajustement)

**Danger:** Provisioned capacity est **factur√©e 24/7** m√™me si **0% utilis√©e** !

**Impact:**
- Vous payez $35/mois pour 100 RCU
- Vous utilisez 5 RCU en moyenne (5% utilization)
- **Gaspillage:** $33/mois = $396/an

### üí∞ Co√ªt Gaspill√©

**Exemple: Table over-provisionn√©e**

```
Table Name: user-sessions-prod
Billing Mode: PROVISIONED
Provisioned Capacity:
  - RCU: 500 (500 reads/second)
  - WCU: 200 (200 writes/second)
Storage: 50 GB

CloudWatch Metrics (7 derniers jours):
  - Consumed RCU avg: 25 (5% utilization) üî¥
  - Consumed WCU avg: 10 (5% utilization) üî¥
  - Peak RCU: 80 (16% - rare spike)
  - Peak WCU: 35 (18% - rare spike)

Co√ªt ACTUEL (provisioned):
  - RCU: 500 √ó $0.00013 √ó 730h = $47.45/mois
  - WCU: 200 √ó $0.00065 √ó 730h = $94.90/mois
  - Storage: 50 GB √ó $0.25 = $12.50/mois
  - TOTAL: $154.85/mois = $1,858/an

Co√ªt OPTIMIS√â (right-sized):
  - RCU: 100 (2√ó peak 80) √ó $0.00013 √ó 730h = $9.49/mois
  - WCU: 50 (1.5√ó peak 35) √ó $0.00065 √ó 730h = $23.73/mois
  - Storage: 50 GB √ó $0.25 = $12.50/mois
  - TOTAL: $45.72/mois = $549/an

üí∞ GASPILLAGE: $154.85 - $45.72 = $109.13/mois = $1,310/an PER TABLE

45 tables √ó $1,310 = $58,950/an ‚ùå
```

### üìä Exemple Concret

```
Table Name:        user-sessions-prod
Region:            us-east-1
Billing Mode:      PROVISIONED
Created:           2022-03-15 (2.5 years ago)

Provisioned Capacity:
  - RCU: 500 units
  - WCU: 200 units
  - Cost: $142.35/mois (capacity only)

CloudWatch Metrics (7 jours):
  - Consumed RCU avg: 25 units (5% utilization) üî¥
  - Consumed RCU p99: 80 units (16% utilization)
  - Consumed WCU avg: 10 units (5% utilization) üî¥
  - Consumed WCU p99: 35 units (18% utilization)

Capacity Analysis:
  - RCU over-provisioned: 500 - 80 = 420 units (84% waste)
  - WCU over-provisioned: 200 - 35 = 165 units (82% waste)
  - Avg utilization: 5% (target: 70-80%)

Auto-Scaling Configuration:
  - Enabled: NO ‚ùå
  - Target: N/A
  - Min/Max: N/A

Root Cause Analysis:
  - Initial provisioning: 500 RCU / 200 WCU (peak estimate)
  - Actual traffic: 90% lower than estimated
  - Never reviewed: 2.5 years sans optimisation
  - No monitoring: Alarms uniquement sur throttles (pas utilization)

üî¥ WASTE DETECTED: 5% avg utilization (target: 70-80%)
üí∞ COST: $109/mois waste = $1,310/an
üìã ACTION: Reduce to 100 RCU / 50 WCU OR migrate to On-Demand
üí° ROOT CAUSE: Over-estimation initiale + no capacity reviews
‚ö° OPTIMIZATION: Enable Auto-Scaling (target 70% utilization)
```

### üêç Code Impl√©mentation Python

```python
async def scan_dynamodb_over_provisioned_capacity(
    region: str,
    provisioned_utilization_threshold: float = 10.0,  # <10% = over-provisioned
    provisioned_lookback_days: int = 7,
    min_age_days: int = 7
) -> List[Dict]:
    """
    D√©tecte DynamoDB tables avec capacit√© provisionn√©e over-provisioned.

    Args:
        region: AWS region √† scanner
        provisioned_utilization_threshold: Seuil % utilization (d√©faut: 10%)
        provisioned_lookback_days: P√©riode d'analyse (d√©faut: 7 jours)
        min_age_days: √Çge minimum table (d√©faut: 7 jours)

    Returns:
        Liste de tables avec capacity waste

    Raises:
        ClientError: Si erreur boto3
    """
    orphans = []
    dynamodb_client = boto3.client('dynamodb', region_name=region)
    cloudwatch = boto3.client('cloudwatch', region_name=region)

    print(f"üóÉÔ∏è Scanning DynamoDB tables for over-provisioned capacity in {region}...")

    # List all tables
    paginator = dynamodb_client.get_paginator('list_tables')
    all_tables = []
    for page in paginator.paginate():
        all_tables.extend(page.get('TableNames', []))

    print(f"üóÉÔ∏è Found {len(all_tables)} DynamoDB tables")

    for table_name in all_tables:
        try:
            # Get table details
            table_response = dynamodb_client.describe_table(TableName=table_name)
            table = table_response.get('Table', {})

            # Extract metadata
            table_arn = table.get('TableArn')
            table_status = table.get('TableStatus')
            creation_date = table.get('CreationDateTime')
            billing_mode = table.get('BillingModeSummary', {}).get('BillingMode', 'PROVISIONED')

            # Calculate age
            age_days = (datetime.now(timezone.utc) - creation_date).days if creation_date else 0

            # Skip young tables and non-ACTIVE tables
            if age_days < min_age_days or table_status != 'ACTIVE':
                continue

            # Only check PROVISIONED mode tables
            if billing_mode != 'PROVISIONED':
                continue

            # Get provisioned capacity
            provisioned_throughput = table.get('ProvisionedThroughput', {})
            provisioned_rcu = provisioned_throughput.get('ReadCapacityUnits', 0)
            provisioned_wcu = provisioned_throughput.get('WriteCapacityUnits', 0)

            if provisioned_rcu == 0 and provisioned_wcu == 0:
                continue  # No provisioned capacity

            # Get CloudWatch metrics
            end_time = datetime.now(timezone.utc)
            start_time = end_time - timedelta(days=provisioned_lookback_days)

            # Get consumed RCU
            rcu_metrics = cloudwatch.get_metric_statistics(
                Namespace='AWS/DynamoDB',
                MetricName='ConsumedReadCapacityUnits',
                Dimensions=[{'Name': 'TableName', 'Value': table_name}],
                StartTime=start_time,
                EndTime=end_time,
                Period=86400,  # 1 day
                Statistics=['Sum']
            )

            # Get consumed WCU
            wcu_metrics = cloudwatch.get_metric_statistics(
                Namespace='AWS/DynamoDB',
                MetricName='ConsumedWriteCapacityUnits',
                Dimensions=[{'Name': 'TableName', 'Value': table_name}],
                StartTime=start_time,
                EndTime=end_time,
                Period=86400,
                Statistics=['Sum']
            )

            # Calculate utilization
            rcu_datapoints = rcu_metrics.get('Datapoints', [])
            wcu_datapoints = wcu_metrics.get('Datapoints', [])

            total_consumed_rcu = sum(dp.get('Sum', 0) for dp in rcu_datapoints)
            total_consumed_wcu = sum(dp.get('Sum', 0) for dp in wcu_datapoints)

            # Provisioned capacity is per second, over lookback days
            total_provisioned_rcu = provisioned_rcu * provisioned_lookback_days * 86400
            total_provisioned_wcu = provisioned_wcu * provisioned_lookback_days * 86400

            # Calculate utilization %
            rcu_utilization = (total_consumed_rcu / total_provisioned_rcu * 100) if total_provisioned_rcu > 0 else 0
            wcu_utilization = (total_consumed_wcu / total_provisioned_wcu * 100) if total_provisioned_wcu > 0 else 0
            avg_utilization = (rcu_utilization + wcu_utilization) / 2

            if avg_utilization < provisioned_utilization_threshold:
                # Calculate waste cost
                # Provisioned capacity charged 24/7
                rcu_monthly_cost = provisioned_rcu * 0.00013 * 730  # $0.00013/hour
                wcu_monthly_cost = provisioned_wcu * 0.00065 * 730  # $0.00065/hour

                # Optimized capacity (2√ó peak for safety margin)
                # Estimate peak as 5√ó average consumed (conservative)
                peak_rcu = max(1, int(total_consumed_rcu / (provisioned_lookback_days * 86400) * 5))
                peak_wcu = max(1, int(total_consumed_wcu / (provisioned_lookback_days * 86400) * 5))

                optimized_rcu = peak_rcu * 2  # 2√ó peak
                optimized_wcu = peak_wcu * 2

                optimized_rcu_cost = optimized_rcu * 0.00013 * 730
                optimized_wcu_cost = optimized_wcu * 0.00065 * 730

                monthly_waste = (rcu_monthly_cost - optimized_rcu_cost) + (wcu_monthly_cost - optimized_wcu_cost)

                # Get storage size
                table_size_bytes = table.get('TableSizeBytes', 0)
                table_size_gb = table_size_bytes / (1024 ** 3) if table_size_bytes > 0 else 0

                orphans.append({
                    'resource_type': 'dynamodb_table',
                    'resource_id': table_arn,
                    'resource_name': table_name,
                    'region': region,
                    'estimated_monthly_cost': round(monthly_waste, 2),
                    'metadata': {
                        'table_arn': table_arn,
                        'billing_mode': billing_mode,
                        'provisioned_rcu': provisioned_rcu,
                        'provisioned_wcu': provisioned_wcu,
                        'rcu_utilization_pct': round(rcu_utilization, 2),
                        'wcu_utilization_pct': round(wcu_utilization, 2),
                        'avg_utilization_pct': round(avg_utilization, 2),
                        'optimized_rcu': optimized_rcu,
                        'optimized_wcu': optimized_wcu,
                        'current_monthly_cost': round(rcu_monthly_cost + wcu_monthly_cost, 2),
                        'optimized_monthly_cost': round(optimized_rcu_cost + optimized_wcu_cost, 2),
                        'table_size_gb': round(table_size_gb, 2),
                        'age_days': age_days,
                        'orphan_type': 'over_provisioned',
                        'orphan_reason': f'Over-provisioned capacity: {avg_utilization:.1f}% avg utilization (RCU: {rcu_utilization:.1f}%, WCU: {wcu_utilization:.1f}%) over {provisioned_lookback_days} days',
                        'confidence': 'critical' if avg_utilization < 5 else 'high',
                        'action': f'Reduce capacity from {provisioned_rcu}/{provisioned_wcu} to {optimized_rcu}/{optimized_wcu} RCU/WCU OR enable Auto-Scaling',
                    }
                })

                print(f"‚úÖ ORPHAN: {table_name} ({avg_utilization:.1f}% utilization, ${monthly_waste:.2f}/mois waste)")

        except Exception as e:
            print(f"‚ö†Ô∏è  Error processing {table_name}: {e}")

    print(f"üéØ Found {len(orphans)} tables with over-provisioned capacity")
    return orphans
```

### üß™ Test Unitaire

```python
import pytest
from moto import mock_dynamodb, mock_cloudwatch
from datetime import datetime, timedelta, timezone

@mock_dynamodb
@mock_cloudwatch
async def test_scan_dynamodb_over_provisioned_capacity():
    """Test d√©tection tables avec capacit√© over-provisioned."""
    dynamodb = boto3.client('dynamodb', region_name='us-east-1')
    cloudwatch = boto3.client('cloudwatch', region_name='us-east-1')

    # Create over-provisioned table
    dynamodb.create_table(
        TableName='user-sessions-prod',
        BillingMode='PROVISIONED',
        AttributeDefinitions=[
            {'AttributeName': 'userId', 'AttributeType': 'S'}
        ],
        KeySchema=[
            {'AttributeName': 'userId', 'KeyType': 'HASH'}
        ],
        ProvisionedThroughput={
            'ReadCapacityUnits': 500,  # Over-provisioned
            'WriteCapacityUnits': 200  # Over-provisioned
        }
    )

    # Simulate low usage metrics
    now = datetime.now(timezone.utc)
    cloudwatch.put_metric_data(
        Namespace='AWS/DynamoDB',
        MetricData=[
            {
                'MetricName': 'ConsumedReadCapacityUnits',
                'Value': 25,  # Only 5% of 500 RCU
                'Timestamp': now,
                'Dimensions': [{'Name': 'TableName', 'Value': 'user-sessions-prod'}],
                'StatisticValues': {
                    'SampleCount': 1,
                    'Sum': 151200,  # 25 RCU avg √ó 7 days √ó 86400 seconds
                    'Minimum': 10,
                    'Maximum': 80  # Peak
                }
            },
            {
                'MetricName': 'ConsumedWriteCapacityUnits',
                'Value': 10,  # Only 5% of 200 WCU
                'Timestamp': now,
                'Dimensions': [{'Name': 'TableName', 'Value': 'user-sessions-prod'}],
                'StatisticValues': {
                    'SampleCount': 1,
                    'Sum': 60480,  # 10 WCU avg √ó 7 days √ó 86400 seconds
                    'Minimum': 5,
                    'Maximum': 35  # Peak
                }
            }
        ]
    )

    orphans = await scan_dynamodb_over_provisioned_capacity(
        region='us-east-1',
        provisioned_utilization_threshold=10.0,
        provisioned_lookback_days=7,
        min_age_days=7
    )

    assert len(orphans) == 1
    orphan = orphans[0]
    assert orphan['resource_name'] == 'user-sessions-prod'
    assert orphan['metadata']['orphan_type'] == 'over_provisioned'
    assert orphan['metadata']['provisioned_rcu'] == 500
    assert orphan['metadata']['provisioned_wcu'] == 200
    assert orphan['metadata']['avg_utilization_pct'] < 10
    assert orphan['metadata']['confidence'] == 'critical'
    assert orphan['estimated_monthly_cost'] > 0
```

### üìà M√©triques CloudWatch

| M√©trique | P√©riode | Seuil Anomalie | Usage |
|----------|---------|----------------|-------|
| **ConsumedReadCapacityUnits** | 7 jours | <10% of provisioned | D√©tection RCU waste |
| **ConsumedWriteCapacityUnits** | 7 jours | <10% of provisioned | D√©tection WCU waste |
| **ProvisionedReadCapacityUnits** | N/A | Configuration actuelle | R√©f√©rence RCU |
| **ProvisionedWriteCapacityUnits** | N/A | Configuration actuelle | R√©f√©rence WCU |

### üí° Auto-Scaling Configuration

```python
# Enable Auto-Scaling (alternative √† right-sizing manuel)
application_autoscaling = boto3.client('application-autoscaling')

# Register scalable target (RCU)
application_autoscaling.register_scalable_target(
    ServiceNamespace='dynamodb',
    ResourceId=f'table/{table_name}',
    ScalableDimension='dynamodb:table:ReadCapacityUnits',
    MinCapacity=10,  # Min 10 RCU
    MaxCapacity=500,  # Max 500 RCU (conserve peak capacity)
    RoleARN='arn:aws:iam::123456789012:role/aws-autoscaling-role'
)

# Configure scaling policy (target 70% utilization)
application_autoscaling.put_scaling_policy(
    PolicyName='dynamodb-read-autoscaling-policy',
    ServiceNamespace='dynamodb',
    ResourceId=f'table/{table_name}',
    ScalableDimension='dynamodb:table:ReadCapacityUnits',
    PolicyType='TargetTrackingScaling',
    TargetTrackingScalingPolicyConfiguration={
        'TargetValue': 70.0,  # 70% utilization target
        'PredefinedMetricSpecification': {
            'PredefinedMetricType': 'DynamoDBReadCapacityUtilization'
        },
        'ScaleInCooldown': 60,  # 60 seconds cooldown
        'ScaleOutCooldown': 60
    }
)

# Result: Capacity scales automatically (10-500 RCU range)
# Cost: Only pay for actual usage (70% target = optimized)
```

---

## üîç Sc√©nario 2: Unused Global Secondary Indexes (GSI)

### üîç Description

Un **Global Secondary Index (GSI) cr√©√© mais jamais query√©** g√©n√®re un **gaspillage critique** :

- **GSI "just in case"** (cr√©√© pour un use case futur qui n'arrive jamais)
- **Legacy query pattern** (GSI √©tait utile avant refactoring)
- **Duplicate table cost** (GSI = table copy avec son propre RCU/WCU + storage)
- **Double the cost** (GSI consomme autant que la table principale)

**Danger:** GSI factur√© comme une **table s√©par√©e compl√®te** (capacity + storage) !

**Impact:**
- Table principale : $100/mois
- GSI non utilis√© : $100/mois (duplicate cost)
- **Gaspillage:** $100/mois = $1,200/an PER GSI

### üí∞ Co√ªt Gaspill√©

**Exemple: GSI non utilis√©**

```
Table Name: orders-table
GSI Name: OrdersByStatusIndex
Billing Mode: PROVISIONED

Table Provisioned Capacity:
  - RCU: 100
  - WCU: 50
  - Storage: 20 GB
  - Cost: $32.59/mois

GSI Provisioned Capacity:
  - RCU: 100 (same as table)
  - WCU: 50 (same as table)
  - Storage: 20 GB (duplicated)
  - Cost: $32.59/mois üî¥

CloudWatch Metrics (14 derniers jours):
  - GSI Consumed RCU: 0 ‚ùå
  - GSI Queries: 0 ‚ùå
  - Last query: NEVER

Root Cause:
  - GSI cr√©√© lors du POC initial (2 ans ago)
  - Query pattern chang√© (now uses DynamoDB Streams + Lambda)
  - GSI oubli√© dans la table

üí∞ GASPILLAGE: $32.59/mois = $391/an PER GSI

30 tables √ó 1 unused GSI √ó $391 = $11,730/an ‚ùå
```

### üìä Exemple Concret

```
Table Name:        orders-table
Region:            us-east-1
Billing Mode:      PROVISIONED
Created:           2021-06-10 (3.5 years ago)

Global Secondary Index:
  - Name: OrdersByStatusIndex
  - Key Schema: status (HASH), createdAt (RANGE)
  - Projection: ALL (full copy)
  - Status: ACTIVE
  - Provisioned:
      RCU: 100 units
      WCU: 50 units
  - Storage: 20 GB (duplicated from table)
  - Cost: $32.59/mois

CloudWatch Metrics (14 jours):
  - ConsumedReadCapacityUnits (GSI): 0 üî¥
  - Total queries (GSI): 0
  - Last query: NEVER (depuis cr√©ation)

Table principale (comparison):
  - ConsumedReadCapacityUnits (table): 8,500
  - Queries (table): 250K
  - Active usage: YES ‚úÖ

Use Case Analysis:
  - Original intent: Query orders by status (e.g., "PENDING", "SHIPPED")
  - Current implementation: DynamoDB Streams + Lambda + ElastiCache
    ‚Üí Orders indexed in ElastiCache by status
    ‚Üí GSI obsolete

Cost Breakdown:
  - GSI RCU: 100 √ó $0.00013 √ó 730h = $9.49/mois
  - GSI WCU: 50 √ó $0.00065 √ó 730h = $23.73/mois
  - GSI Storage: 20 GB √ó $0.25 = $5.00/mois
  - TOTAL WASTE: $38.22/mois = $459/an

üî¥ WASTE DETECTED: GSI never queried in 14 days (0 ConsumedRCU)
üí∞ COST: $38/mois waste = $459/an
üìã ACTION: Delete OrdersByStatusIndex GSI
üí° ROOT CAUSE: Architecture change (Streams + Lambda) made GSI obsolete
‚ö†Ô∏è  IMPACT: Deleting GSI reduces table cost by 50%
```

### üêç Code Impl√©mentation Python

```python
async def scan_dynamodb_unused_gsi(
    region: str,
    gsi_lookback_days: int = 14,
    min_age_days: int = 7
) -> List[Dict]:
    """
    D√©tecte DynamoDB tables avec Global Secondary Indexes non utilis√©s.

    Args:
        region: AWS region √† scanner
        gsi_lookback_days: P√©riode d'analyse GSI usage (d√©faut: 14 jours)
        min_age_days: √Çge minimum table (d√©faut: 7 jours)

    Returns:
        Liste de tables avec unused GSI

    Raises:
        ClientError: Si erreur boto3
    """
    orphans = []
    dynamodb_client = boto3.client('dynamodb', region_name=region)
    cloudwatch = boto3.client('cloudwatch', region_name=region)

    print(f"üóÉÔ∏è Scanning DynamoDB tables for unused GSI in {region}...")

    # List all tables
    paginator = dynamodb_client.get_paginator('list_tables')
    all_tables = []
    for page in paginator.paginate():
        all_tables.extend(page.get('TableNames', []))

    for table_name in all_tables:
        try:
            # Get table details
            table_response = dynamodb_client.describe_table(TableName=table_name)
            table = table_response.get('Table', {})

            # Extract metadata
            table_arn = table.get('TableArn')
            table_status = table.get('TableStatus')
            creation_date = table.get('CreationDateTime')
            billing_mode = table.get('BillingModeSummary', {}).get('BillingMode', 'PROVISIONED')
            global_secondary_indexes = table.get('GlobalSecondaryIndexes', [])

            # Calculate age
            age_days = (datetime.now(timezone.utc) - creation_date).days if creation_date else 0

            # Skip young tables, non-ACTIVE, or tables without GSI
            if age_days < min_age_days or table_status != 'ACTIVE' or len(global_secondary_indexes) == 0:
                continue

            # Check each GSI
            for gsi in global_secondary_indexes:
                gsi_name = gsi.get('IndexName')
                gsi_status = gsi.get('IndexStatus')

                if gsi_status != 'ACTIVE':
                    continue

                # Get GSI consumed capacity
                end_time = datetime.now(timezone.utc)
                start_time = end_time - timedelta(days=gsi_lookback_days)

                gsi_read_metrics = cloudwatch.get_metric_statistics(
                    Namespace='AWS/DynamoDB',
                    MetricName='ConsumedReadCapacityUnits',
                    Dimensions=[
                        {'Name': 'TableName', 'Value': table_name},
                        {'Name': 'GlobalSecondaryIndexName', 'Value': gsi_name}
                    ],
                    StartTime=start_time,
                    EndTime=end_time,
                    Period=86400,
                    Statistics=['Sum']
                )

                gsi_read_datapoints = gsi_read_metrics.get('Datapoints', [])
                gsi_total_read = sum(dp.get('Sum', 0) for dp in gsi_read_datapoints)

                if gsi_total_read == 0:
                    # GSI never queried!
                    # Calculate cost
                    table_size_bytes = table.get('TableSizeBytes', 0)
                    table_size_gb = table_size_bytes / (1024 ** 3) if table_size_bytes > 0 else 0

                    if billing_mode == 'PROVISIONED':
                        gsi_provisioned = gsi.get('ProvisionedThroughput', {})
                        gsi_rcu = gsi_provisioned.get('ReadCapacityUnits', 0)
                        gsi_wcu = gsi_provisioned.get('WriteCapacityUnits', 0)

                        # GSI cost = provisioned capacity + storage (duplicated)
                        gsi_rcu_cost = gsi_rcu * 0.00013 * 730
                        gsi_wcu_cost = gsi_wcu * 0.00065 * 730
                        gsi_storage_cost = table_size_gb * 0.25

                        monthly_waste = gsi_rcu_cost + gsi_wcu_cost + gsi_storage_cost
                    else:
                        # On-Demand: minimal cost if no usage (storage only)
                        monthly_waste = table_size_gb * 0.25

                    orphans.append({
                        'resource_type': 'dynamodb_table',
                        'resource_id': table_arn,
                        'resource_name': table_name,
                        'region': region,
                        'estimated_monthly_cost': round(monthly_waste, 2),
                        'metadata': {
                            'table_arn': table_arn,
                            'billing_mode': billing_mode,
                            'gsi_name': gsi_name,
                            'gsi_status': gsi_status,
                            'gsi_rcu': gsi_rcu if billing_mode == 'PROVISIONED' else 0,
                            'gsi_wcu': gsi_wcu if billing_mode == 'PROVISIONED' else 0,
                            'gsi_total_reads': int(gsi_total_read),
                            'lookback_days': gsi_lookback_days,
                            'table_size_gb': round(table_size_gb, 2),
                            'age_days': age_days,
                            'orphan_type': 'unused_gsi',
                            'orphan_reason': f"Unused Global Secondary Index '{gsi_name}' - never queried in {gsi_lookback_days} days (doubles table cost)",
                            'confidence': 'high' if age_days >= 30 else 'medium',
                            'action': f'Delete GSI: aws dynamodb update-table --table-name {table_name} --global-secondary-index-updates "[{{\\"Delete\\": {{\\"IndexName\\": \\"{gsi_name}\\"}}}}]"',
                        }
                    })

                    print(f"‚úÖ ORPHAN: {table_name} (GSI: {gsi_name}, ${monthly_waste:.2f}/mois waste)")
                    break  # One unused GSI per table report

        except Exception as e:
            print(f"‚ö†Ô∏è  Error processing {table_name}: {e}")

    print(f"üéØ Found {len(orphans)} tables with unused GSI")
    return orphans
```

### üß™ Test Unitaire

```python
import pytest
from moto import mock_dynamodb, mock_cloudwatch
from datetime import datetime, timezone

@mock_dynamodb
@mock_cloudwatch
async def test_scan_dynamodb_unused_gsi():
    """Test d√©tection tables avec GSI non utilis√©."""
    dynamodb = boto3.client('dynamodb', region_name='us-east-1')
    cloudwatch = boto3.client('cloudwatch', region_name='us-east-1')

    # Create table with GSI
    dynamodb.create_table(
        TableName='orders-table',
        BillingMode='PROVISIONED',
        AttributeDefinitions=[
            {'AttributeName': 'orderId', 'AttributeType': 'S'},
            {'AttributeName': 'status', 'AttributeType': 'S'},
            {'AttributeName': 'createdAt', 'AttributeType': 'N'}
        ],
        KeySchema=[
            {'AttributeName': 'orderId', 'KeyType': 'HASH'}
        ],
        ProvisionedThroughput={
            'ReadCapacityUnits': 100,
            'WriteCapacityUnits': 50
        },
        GlobalSecondaryIndexes=[{
            'IndexName': 'OrdersByStatusIndex',
            'KeySchema': [
                {'AttributeName': 'status', 'KeyType': 'HASH'},
                {'AttributeName': 'createdAt', 'KeyType': 'RANGE'}
            ],
            'Projection': {'ProjectionType': 'ALL'},
            'ProvisionedThroughput': {
                'ReadCapacityUnits': 100,
                'WriteCapacityUnits': 50
            }
        }]
    )

    # Simulate 0 GSI usage (no metrics = 0 reads)
    # (moto doesn't publish metrics by default, so 0 reads detected)

    orphans = await scan_dynamodb_unused_gsi(
        region='us-east-1',
        gsi_lookback_days=14,
        min_age_days=7
    )

    assert len(orphans) == 1
    orphan = orphans[0]
    assert orphan['resource_name'] == 'orders-table'
    assert orphan['metadata']['orphan_type'] == 'unused_gsi'
    assert orphan['metadata']['gsi_name'] == 'OrdersByStatusIndex'
    assert orphan['metadata']['gsi_total_reads'] == 0
    assert orphan['metadata']['confidence'] in ['high', 'medium']
    assert orphan['estimated_monthly_cost'] > 0
```

### üìà GSI Cost Analysis

| GSI Configuration | Monthly Cost | Impact |
|-------------------|--------------|--------|
| **100 RCU / 50 WCU (Provisioned)** | $32.59 | High |
| **+ 20 GB storage (duplicated)** | $5.00 | Medium |
| **Total per GSI** | $37.59/mois | **$451/an waste if unused** |

### üí° GSI Best Practices

```python
# ‚ùå BAD: Create GSI "just in case"
global_secondary_indexes=[{
    'IndexName': 'OrdersByStatusIndex',  # Never used
    'KeySchema': [...],
    'ProvisionedThroughput': {
        'ReadCapacityUnits': 100,  # Wasted capacity
        'WriteCapacityUnits': 50
    }
}]

# ‚úÖ GOOD: Create GSI only for validated query patterns
# 1. Analyze access patterns first
# 2. Use DynamoDB Streams + Lambda for complex queries
# 3. Consider ElastiCache for frequently changing filters
# 4. Delete unused GSI immediately

# Delete unused GSI
dynamodb.update_table(
    TableName='orders-table',
    GlobalSecondaryIndexUpdates=[{
        'Delete': {'IndexName': 'OrdersByStatusIndex'}
    }]
)

# Result: 50% cost reduction (no GSI duplicate)
```

---

## üìù Sc√©nario 3: Never Used Tables (Provisioned Mode)

### üîç Description

Une table DynamoDB en **mode Provisioned** avec **0 usage depuis cr√©ation** g√©n√®re un **gaspillage pur** :

- **POC table oubli√©e** (table de test jamais supprim√©e)
- **Pre-production table** (cr√©√©e pour staging mais jamais utilis√©e)
- **Abandoned feature** (table pour feature cancelled avant lancement)
- **Migration artifact** (old table apr√®s migration vers nouveau schema)

**Danger:** Provisioned mode facture **24/7** m√™me avec **0 requests** !

**Impact:**
- RCU + WCU factur√©s 24/7
- Storage factur√© m√™me si vide
- **100% waste** (aucune valeur g√©n√©r√©e)

### üí∞ Co√ªt Gaspill√©

**Exemple: Table jamais utilis√©e**

```
Table Name: poc-user-analytics
Billing Mode: PROVISIONED
Provisioned Capacity:
  - RCU: 50
  - WCU: 25
Created: 350 days ago

CloudWatch Metrics (depuis cr√©ation):
  - Total Consumed RCU: 0 ‚ùå
  - Total Consumed WCU: 0 ‚ùå
  - Total Queries: 0 ‚ùå
  - Item Count: 0 (table vide)

Root Cause:
  - Created during POC phase
  - POC cancelled after 2 weeks
  - Table never deleted
  - Running for 350 days with 0 value

Co√ªt ACTUEL:
  - RCU: 50 √ó $0.00013 √ó 730h = $4.75/mois
  - WCU: 25 √ó $0.00065 √ó 730h = $11.86/mois
  - Storage: 0 GB √ó $0.25 = $0/mois
  - TOTAL: $16.61/mois = $199/an

üí∞ GASPILLAGE: $199/an √ó 100% waste = $199/an PER TABLE

10 tables √ó $199 = $1,990/an ‚ùå
```

### üìä Exemple Concret

```
Table Name:        poc-user-analytics
Region:            eu-west-1
Billing Mode:      PROVISIONED
Created:           2024-01-15 (350 days ago)
Status:            ACTIVE

Provisioned Capacity:
  - RCU: 50 units
  - WCU: 25 units
  - Cost: $16.61/mois

CloudWatch Metrics (ALL TIME depuis cr√©ation):
  - ConsumedReadCapacityUnits: 0 üî¥
  - ConsumedWriteCapacityUnits: 0 üî¥
  - GetItem operations: 0
  - PutItem operations: 0
  - Query operations: 0
  - Scan operations: 0

Table Metadata:
  - Item Count: 0
  - Table Size: 0 bytes
  - Status: ACTIVE (factur√©!)

Root Cause Analysis:
  - Created: January 2024 during POC phase
  - Purpose: Test analytics ingestion pipeline
  - POC Result: Cancelled (decided to use ElastiCache instead)
  - Cleanup: FORGOTTEN ‚ùå
  - Cost to date: $16.61 √ó 11 months = $183

Event Source Mappings:
  - Lambda triggers: NONE
  - DynamoDB Streams: DISABLED
  - Applications connected: NONE

üî¥ WASTE DETECTED: Never used since creation (0 ConsumedRCU/WCU for 350 days)
üí∞ COST: $16.61/mois = $199/an (100% pure waste)
üìã ACTION: DELETE table immediately (no data loss risk)
üí° ROOT CAUSE: POC cleanup process failure
‚ö†Ô∏è  IMPACT: Zero business value, pure infrastructure waste
```

### üêç Code Impl√©mentation Python

```python
async def scan_dynamodb_never_used_provisioned(
    region: str,
    never_used_min_age_days: int = 30,
    confidence_threshold_days: int = 30,
    critical_age_days: int = 90
) -> List[Dict]:
    """
    D√©tecte DynamoDB tables en mode Provisioned jamais utilis√©es depuis cr√©ation.

    Args:
        region: AWS region √† scanner
        never_used_min_age_days: √Çge minimum pour consid√©rer "never used" (d√©faut: 30)
        confidence_threshold_days: Seuil high confidence (d√©faut: 30)
        critical_age_days: Seuil critical (d√©faut: 90)

    Returns:
        Liste de tables provisioned jamais utilis√©es

    Raises:
        ClientError: Si erreur boto3
    """
    orphans = []
    dynamodb_client = boto3.client('dynamodb', region_name=region)
    cloudwatch = boto3.client('cloudwatch', region_name=region)

    print(f"üóÉÔ∏è Scanning DynamoDB tables for never used (Provisioned mode) in {region}...")

    # List all tables
    paginator = dynamodb_client.get_paginator('list_tables')
    all_tables = []
    for page in paginator.paginate():
        all_tables.extend(page.get('TableNames', []))

    for table_name in all_tables:
        try:
            # Get table details
            table_response = dynamodb_client.describe_table(TableName=table_name)
            table = table_response.get('Table', {})

            # Extract metadata
            table_arn = table.get('TableArn')
            table_status = table.get('TableStatus')
            creation_date = table.get('CreationDateTime')
            billing_mode = table.get('BillingModeSummary', {}).get('BillingMode', 'PROVISIONED')

            # Calculate age
            age_days = (datetime.now(timezone.utc) - creation_date).days if creation_date else 0

            # Skip young tables, non-ACTIVE, non-PROVISIONED
            if age_days < never_used_min_age_days or table_status != 'ACTIVE' or billing_mode != 'PROVISIONED':
                continue

            # Get provisioned capacity
            provisioned_throughput = table.get('ProvisionedThroughput', {})
            provisioned_rcu = provisioned_throughput.get('ReadCapacityUnits', 0)
            provisioned_wcu = provisioned_throughput.get('WriteCapacityUnits', 0)

            # Get usage since creation
            end_time = datetime.now(timezone.utc)
            start_time = creation_date

            # Get consumed RCU
            rcu_metrics = cloudwatch.get_metric_statistics(
                Namespace='AWS/DynamoDB',
                MetricName='ConsumedReadCapacityUnits',
                Dimensions=[{'Name': 'TableName', 'Value': table_name}],
                StartTime=start_time,
                EndTime=end_time,
                Period=86400,
                Statistics=['Sum']
            )

            # Get consumed WCU
            wcu_metrics = cloudwatch.get_metric_statistics(
                Namespace='AWS/DynamoDB',
                MetricName='ConsumedWriteCapacityUnits',
                Dimensions=[{'Name': 'TableName', 'Value': table_name}],
                StartTime=start_time,
                EndTime=end_time,
                Period=86400,
                Statistics=['Sum']
            )

            total_reads = sum(dp.get('Sum', 0) for dp in rcu_metrics.get('Datapoints', []))
            total_writes = sum(dp.get('Sum', 0) for dp in wcu_metrics.get('Datapoints', []))

            if total_reads == 0 and total_writes == 0:
                # Never used!
                # Calculate cost
                rcu_monthly_cost = provisioned_rcu * 0.00013 * 730
                wcu_monthly_cost = provisioned_wcu * 0.00065 * 730

                table_size_bytes = table.get('TableSizeBytes', 0)
                table_size_gb = table_size_bytes / (1024 ** 3) if table_size_bytes > 0 else 0
                storage_cost = table_size_gb * 0.25

                monthly_cost = rcu_monthly_cost + wcu_monthly_cost + storage_cost

                orphans.append({
                    'resource_type': 'dynamodb_table',
                    'resource_id': table_arn,
                    'resource_name': table_name,
                    'region': region,
                    'estimated_monthly_cost': round(monthly_cost, 2),
                    'metadata': {
                        'table_arn': table_arn,
                        'billing_mode': billing_mode,
                        'provisioned_rcu': provisioned_rcu,
                        'provisioned_wcu': provisioned_wcu,
                        'table_size_gb': round(table_size_gb, 2),
                        'total_reads': int(total_reads),
                        'total_writes': int(total_writes),
                        'age_days': age_days,
                        'created_at': creation_date.isoformat() if creation_date else None,
                        'orphan_type': 'never_used_provisioned',
                        'orphan_reason': f"Never used since creation ({age_days} days ago) - paying for provisioned capacity with 0 usage",
                        'confidence': 'critical' if age_days >= critical_age_days else ('high' if age_days >= confidence_threshold_days else 'medium'),
                        'action': f'DELETE table: aws dynamodb delete-table --table-name {table_name}',
                    }
                })

                print(f"‚úÖ ORPHAN: {table_name} (never used, {age_days} days old, ${monthly_cost:.2f}/mois)")

        except Exception as e:
            print(f"‚ö†Ô∏è  Error processing {table_name}: {e}")

    print(f"üéØ Found {len(orphans)} provisioned tables never used")
    return orphans
```

### üß™ Test Unitaire

```python
import pytest
from moto import mock_dynamodb, mock_cloudwatch
from datetime import datetime, timedelta, timezone

@mock_dynamodb
@mock_cloudwatch
async def test_scan_dynamodb_never_used_provisioned():
    """Test d√©tection tables provisioned jamais utilis√©es."""
    dynamodb = boto3.client('dynamodb', region_name='eu-west-1')
    cloudwatch = boto3.client('cloudwatch', region_name='eu-west-1')

    # Create never-used provisioned table
    dynamodb.create_table(
        TableName='poc-user-analytics',
        BillingMode='PROVISIONED',
        AttributeDefinitions=[
            {'AttributeName': 'userId', 'AttributeType': 'S'}
        ],
        KeySchema=[
            {'AttributeName': 'userId', 'KeyType': 'HASH'}
        ],
        ProvisionedThroughput={
            'ReadCapacityUnits': 50,
            'WriteCapacityUnits': 25
        }
    )

    # NO metrics published (0 usage)

    orphans = await scan_dynamodb_never_used_provisioned(
        region='eu-west-1',
        never_used_min_age_days=30,
        confidence_threshold_days=30,
        critical_age_days=90
    )

    assert len(orphans) == 1
    orphan = orphans[0]
    assert orphan['resource_name'] == 'poc-user-analytics'
    assert orphan['metadata']['orphan_type'] == 'never_used_provisioned'
    assert orphan['metadata']['total_reads'] == 0
    assert orphan['metadata']['total_writes'] == 0
    assert orphan['metadata']['billing_mode'] == 'PROVISIONED'
    assert orphan['estimated_monthly_cost'] > 0
```

### üìà Detection Confidence Levels

| Age (days) | Confidence | Action | Priority |
|------------|------------|--------|----------|
| **30-90** | üü° MEDIUM | Investigate use case | P2 |
| **90-180** | üü† HIGH | Delete if confirmed unused | P1 |
| **180+** | üî¥ CRITICAL | DELETE immediately | P0 |

---

## üåê Sc√©nario 4: Never Used Tables (On-Demand Mode)

### üîç Description

Une table DynamoDB en **mode On-Demand** avec **0 usage pendant 60+ jours** indique une table **abandonn√©e** :

- **Feature deprecated** (table pour feature supprim√©e)
- **Test table oubli√©e** (cr√©√©e pour testing, jamais nettoy√©e)
- **Backup/restore artifact** (table cr√©√©e lors restore puis oubli√©e)
- **Migration leftover** (old schema apr√®s migration)

**Diff√©rence vs Provisioned:**
- On-Demand mode = **$0 si table inactive** (sauf storage)
- Moins critique mais **storage waste + operational overhead**

**Impact:**
- Storage cost only ($0.25/GB/mois)
- Operational overhead (monitoring, alerting, inventaire)
- Security risk (data persistence sans owner)

### üí∞ Co√ªt Gaspill√©

**Exemple: Table On-Demand inactive**

```
Table Name: backup-restore-temp-20240115
Billing Mode: ON_DEMAND (PAY_PER_REQUEST)
Created: 290 days ago
Storage: 5 GB

CloudWatch Metrics (60 derniers jours):
  - Read Request Units: 0 ‚ùå
  - Write Request Units: 0 ‚ùå
  - Total operations: 0

Root Cause:
  - Created during restore operation (Jan 2024)
  - Used to validate backup restore
  - Validation completed, table forgotten
  - Running for 290 days with 0 usage

Co√ªt ACTUEL:
  - Requests: 0 √ó $0 = $0/mois ‚úÖ
  - Storage: 5 GB √ó $0.25 = $1.25/mois üî¥
  - TOTAL: $1.25/mois = $15/an

Co√ªts indirects:
  - Monitoring dashboards: 1 table slot
  - CloudWatch alarms: 2 alarms configured
  - Security audit overhead: $50/an (compliance review)
  - Backup storage (if enabled): $10/an

üí∞ GASPILLAGE direct: $15/an
üí∞ GASPILLAGE indirect: $60/an
üí∞ GASPILLAGE total: $75/an PER TABLE

10 tables √ó $75 = $750/an ‚ùå
```

### üìä Exemple Concret

```
Table Name:        backup-restore-temp-20240115
Region:            us-east-1
Billing Mode:      PAY_PER_REQUEST (On-Demand)
Created:           2024-01-15 (290 days ago)

CloudWatch Metrics (60 jours):
  - ConsumedReadCapacityUnits: 0 üî¥
  - ConsumedWriteCapacityUnits: 0 üî¥
  - UserErrors: 0
  - SystemErrors: 0
  - Operations count: 0

Table Metadata:
  - Item Count: 15,382 items
  - Table Size: 5 GB
  - Status: ACTIVE (mais inactive!)

Backup Configuration:
  - Point-in-Time Recovery (PITR): DISABLED
  - On-Demand Backups: 0

Event Source Mappings:
  - Lambda triggers: NONE
  - DynamoDB Streams: DISABLED

Cost Breakdown:
  - Request cost: $0/mois (0 requests) ‚úÖ
  - Storage cost: 5 GB √ó $0.25 = $1.25/mois üî¥
  - TOTAL: $1.25/mois = $15/an

Root Cause Analysis:
  - Purpose: Temporary table for backup validation
  - Lifecycle: Should have been deleted after 7 days
  - Cleanup: FORGOTTEN (no automated cleanup)
  - Owner: Engineer left company 8 months ago ‚ö†Ô∏è

üî¥ WASTE DETECTED: On-Demand table with 0 usage in 60 days
üí∞ COST: $1.25/mois storage = $15/an (direct)
üìã ACTION: DELETE table after data export (if needed)
üí° ROOT CAUSE: No table lifecycle policy + owner churn
‚ö†Ô∏è  SECURITY RISK: 15K items with unknown data sensitivity
```

### üêç Code Impl√©mentation Python

```python
async def scan_dynamodb_never_used_ondemand(
    region: str,
    ondemand_lookback_days: int = 60,
    confidence_threshold_days: int = 30,
    min_age_days: int = 7
) -> List[Dict]:
    """
    D√©tecte DynamoDB tables en mode On-Demand sans usage pendant lookback period.

    Args:
        region: AWS region √† scanner
        ondemand_lookback_days: P√©riode lookback (d√©faut: 60 jours)
        confidence_threshold_days: Seuil high confidence (d√©faut: 30)
        min_age_days: √Çge minimum table (d√©faut: 7 jours)

    Returns:
        Liste de tables On-Demand inactives

    Raises:
        ClientError: Si erreur boto3
    """
    orphans = []
    dynamodb_client = boto3.client('dynamodb', region_name=region)
    cloudwatch = boto3.client('cloudwatch', region_name=region)

    print(f"üóÉÔ∏è Scanning DynamoDB tables for never used (On-Demand mode) in {region}...")

    # List all tables
    paginator = dynamodb_client.get_paginator('list_tables')
    all_tables = []
    for page in paginator.paginate():
        all_tables.extend(page.get('TableNames', []))

    for table_name in all_tables:
        try:
            # Get table details
            table_response = dynamodb_client.describe_table(TableName=table_name)
            table = table_response.get('Table', {})

            # Extract metadata
            table_arn = table.get('TableArn')
            table_status = table.get('TableStatus')
            creation_date = table.get('CreationDateTime')
            billing_mode = table.get('BillingModeSummary', {}).get('BillingMode', 'PROVISIONED')

            # Calculate age
            age_days = (datetime.now(timezone.utc) - creation_date).days if creation_date else 0

            # Skip young tables, non-ACTIVE, non-ON_DEMAND
            if age_days < min_age_days or table_status != 'ACTIVE' or billing_mode != 'PAY_PER_REQUEST':
                continue

            # Get recent usage
            end_time = datetime.now(timezone.utc)
            start_time = end_time - timedelta(days=ondemand_lookback_days)

            # Get consumed RCU
            rcu_metrics = cloudwatch.get_metric_statistics(
                Namespace='AWS/DynamoDB',
                MetricName='ConsumedReadCapacityUnits',
                Dimensions=[{'Name': 'TableName', 'Value': table_name}],
                StartTime=start_time,
                EndTime=end_time,
                Period=86400,
                Statistics=['Sum']
            )

            # Get consumed WCU
            wcu_metrics = cloudwatch.get_metric_statistics(
                Namespace='AWS/DynamoDB',
                MetricName='ConsumedWriteCapacityUnits',
                Dimensions=[{'Name': 'TableName', 'Value': table_name}],
                StartTime=start_time,
                EndTime=end_time,
                Period=86400,
                Statistics=['Sum']
            )

            recent_reads = sum(dp.get('Sum', 0) for dp in rcu_metrics.get('Datapoints', []))
            recent_writes = sum(dp.get('Sum', 0) for dp in wcu_metrics.get('Datapoints', []))

            if recent_reads == 0 and recent_writes == 0:
                # On-Demand table inactive!
                # Cost: storage only
                table_size_bytes = table.get('TableSizeBytes', 0)
                table_size_gb = table_size_bytes / (1024 ** 3) if table_size_bytes > 0 else 0
                storage_cost = table_size_gb * 0.25

                item_count = table.get('ItemCount', 0)

                orphans.append({
                    'resource_type': 'dynamodb_table',
                    'resource_id': table_arn,
                    'resource_name': table_name,
                    'region': region,
                    'estimated_monthly_cost': round(storage_cost, 2),
                    'metadata': {
                        'table_arn': table_arn,
                        'billing_mode': billing_mode,
                        'table_size_gb': round(table_size_gb, 2),
                        'item_count': item_count,
                        'recent_reads': int(recent_reads),
                        'recent_writes': int(recent_writes),
                        'lookback_days': ondemand_lookback_days,
                        'age_days': age_days,
                        'created_at': creation_date.isoformat() if creation_date else None,
                        'orphan_type': 'never_used_ondemand',
                        'orphan_reason': f"On-Demand table with no usage in last {ondemand_lookback_days} days (only storage cost)",
                        'confidence': 'high' if age_days >= confidence_threshold_days else 'medium',
                        'action': f'DELETE table: aws dynamodb delete-table --table-name {table_name}',
                        'security_note': f'{item_count} items may contain sensitive data - review before deletion',
                    }
                })

                print(f"‚úÖ ORPHAN: {table_name} (On-Demand inactive, {ondemand_lookback_days} days, ${storage_cost:.2f}/mois)")

        except Exception as e:
            print(f"‚ö†Ô∏è  Error processing {table_name}: {e}")

    print(f"üéØ Found {len(orphans)} On-Demand tables never used")
    return orphans
```

### üß™ Test Unitaire

```python
import pytest
from moto import mock_dynamodb, mock_cloudwatch
from datetime import datetime, timedelta, timezone

@mock_dynamodb
@mock_cloudwatch
async def test_scan_dynamodb_never_used_ondemand():
    """Test d√©tection tables On-Demand inactives."""
    dynamodb = boto3.client('dynamodb', region_name='us-east-1')
    cloudwatch = boto3.client('cloudwatch', region_name='us-east-1')

    # Create never-used On-Demand table
    dynamodb.create_table(
        TableName='backup-restore-temp-20240115',
        BillingMode='PAY_PER_REQUEST',
        AttributeDefinitions=[
            {'AttributeName': 'id', 'AttributeType': 'S'}
        ],
        KeySchema=[
            {'AttributeName': 'id', 'KeyType': 'HASH'}
        ]
    )

    # NO metrics published (0 usage for 60 days)

    orphans = await scan_dynamodb_never_used_ondemand(
        region='us-east-1',
        ondemand_lookback_days=60,
        confidence_threshold_days=30,
        min_age_days=7
    )

    assert len(orphans) == 1
    orphan = orphans[0]
    assert orphan['resource_name'] == 'backup-restore-temp-20240115'
    assert orphan['metadata']['orphan_type'] == 'never_used_ondemand'
    assert orphan['metadata']['recent_reads'] == 0
    assert orphan['metadata']['recent_writes'] == 0
    assert orphan['metadata']['billing_mode'] == 'PAY_PER_REQUEST'
```

---

## üóëÔ∏è Sc√©nario 5: Empty Tables (0 Items)

### üîç Description

Une table DynamoDB **vide (0 items) pendant 90+ jours** indique une table **sans valeur** :

- **Test table jamais popul√©e** (cr√©√©e pour tests, jamais utilis√©e)
- **Batch delete + table oubli√©e** (donn√©es supprim√©es, table rest√©e)
- **Migration artifact** (donn√©es migr√©es, old table vid√©e mais pas supprim√©e)
- **POC cleanup partiel** (data deleted mais pas table)

**Impact:**
- Provisioned mode : Full RCU/WCU cost (m√™me si vide!)
- On-Demand mode : Storage cost minimal (near $0)
- Operational overhead (monitoring, alerting, backups)

### üí∞ Co√ªt Gaspill√©

**Exemple: Table vide (Provisioned mode)**

```
Table Name: temp-batch-processing
Billing Mode: PROVISIONED
Provisioned Capacity:
  - RCU: 20
  - WCU: 10
Created: 180 days ago

Table State:
  - Item Count: 0 ‚ùå
  - Table Size: 0 bytes
  - Last write: 175 days ago (initial test data deleted)

Root Cause:
  - Created for batch processing POC
  - POC successful, data migrated to production table
  - Test data deleted, table forgotten

Co√ªt ACTUEL (Provisioned):
  - RCU: 20 √ó $0.00013 √ó 730h = $1.90/mois
  - WCU: 10 √ó $0.00065 √ó 730h = $4.75/mois
  - Storage: 0 GB √ó $0.25 = $0/mois
  - TOTAL: $6.65/mois = $80/an

üí∞ GASPILLAGE: $80/an PER EMPTY TABLE (Provisioned)

15 empty tables √ó $80 = $1,200/an ‚ùå

Note: On-Demand mode empty table = $0.50/mois (minimal)
```

### üìä Exemple Concret

```
Table Name:        temp-batch-processing
Region:            eu-west-1
Billing Mode:      PROVISIONED
Created:           2024-05-01 (180 days ago)

Provisioned Capacity:
  - RCU: 20 units
  - WCU: 10 units
  - Cost: $6.65/mois

Table State:
  - Item Count: 0 üî¥
  - Table Size: 0 bytes
  - Status: ACTIVE (mais vide!)

CloudWatch Metrics (90 jours):
  - PutItem operations: 0 (depuis 175 jours)
  - DeleteItem operations: 1,250 (il y a 175 jours - cleanup)
  - GetItem operations: 0

Timeline Analysis:
  - May 1, 2024: Table cr√©√©e
  - May 2-5, 2024: POC testing (1,250 items inserted)
  - May 6, 2024: Data deleted (batch delete operation)
  - May 7 - Today: EMPTY (0 items, 175 jours)

Backup Configuration:
  - PITR: DISABLED ‚úÖ (no backup waste)
  - On-Demand Backups: 0

Root Cause Analysis:
  - POC Phase: Successful testing
  - Data Migration: Completed to production table
  - Cleanup: Data deleted BUT table not deleted
  - Operational cost: 175 days √ó $0.22/jour = $38 wasted

üî¥ WASTE DETECTED: Empty table (0 items) for 175 days
üí∞ COST: $6.65/mois = $80/an
üìã ACTION: DELETE table immediately (no data loss risk)
üí° ROOT CAUSE: Incomplete cleanup process (delete data ‚úì, delete table ‚úó)
```

---

## üêç Impl√©mentation Python

### Code de D√©tection

```python
async def scan_dynamodb_empty_tables(
    region: str,
    empty_table_min_age_days: int = 90,
    min_age_days: int = 7
) -> List[Dict]:
    """
    D√©tecte les tables DynamoDB vides (0 items) depuis longtemps.

    Analyse:
    - Table item count = 0
    - Table age > empty_table_min_age_days
    - Calcule le co√ªt de la table vide (Provisioned mode uniquement)

    Args:
        region: R√©gion AWS
        empty_table_min_age_days: √Çge minimum pour d√©tecter (d√©faut: 90 jours)
        min_age_days: √Çge minimum de la table (d√©faut: 7 jours)

    Returns:
        Liste des tables vides avec co√ªts
    """
    orphans = []

    dynamodb_client = boto3.client('dynamodb', region_name=region)
    cloudwatch = boto3.client('cloudwatch', region_name=region)

    try:
        # 1. Liste toutes les tables
        paginator = dynamodb_client.get_paginator('list_tables')
        all_tables = []

        async for page in paginator.paginate():
            all_tables.extend(page.get('TableNames', []))

        logger.info(f"Found {len(all_tables)} DynamoDB tables in {region}")

        # 2. V√©rifie chaque table
        for table_name in all_tables:
            try:
                # R√©cup√®re les d√©tails de la table
                table_desc = await dynamodb_client.describe_table(
                    TableName=table_name
                )
                table = table_desc['Table']

                # Extraction des attributs
                item_count = table.get('ItemCount', 0)
                table_size_bytes = table.get('TableSizeBytes', 0)
                table_status = table.get('TableStatus', 'UNKNOWN')
                creation_date = table.get('CreationDateTime')
                billing_mode_summary = table.get('BillingModeSummary', {})
                billing_mode = billing_mode_summary.get('BillingMode', 'PROVISIONED')

                # Calcule l'√¢ge de la table
                if not creation_date:
                    continue

                age_days = (datetime.now(timezone.utc) - creation_date).days

                # Filtre: √¢ge minimum
                if age_days < min_age_days:
                    continue

                # üî¥ D√âTECTION: Table vide (0 items) depuis > empty_table_min_age_days
                if item_count == 0 and age_days >= empty_table_min_age_days:

                    # Calcule le co√ªt mensuel
                    monthly_cost = 0.0

                    if billing_mode == 'PROVISIONED':
                        # Capacit√© provisionn√©e
                        provisioned_throughput = table.get('ProvisionedThroughput', {})
                        read_capacity = provisioned_throughput.get('ReadCapacityUnits', 0)
                        write_capacity = provisioned_throughput.get('WriteCapacityUnits', 0)

                        # Co√ªt RCU + WCU (730 heures/mois)
                        rcu_cost = read_capacity * 0.00013 * 730
                        wcu_cost = write_capacity * 0.00065 * 730
                        monthly_cost = rcu_cost + wcu_cost

                    elif billing_mode == 'PAY_PER_REQUEST':
                        # On-Demand: Storage uniquement (tables vides = ~$0)
                        storage_gb = table_size_bytes / (1024**3)
                        monthly_cost = storage_gb * 0.25  # $0.25/GB/mois

                    # V√©rifie si la table a eu des √©critures r√©centes
                    end_time = datetime.now(timezone.utc)
                    start_time = end_time - timedelta(days=30)

                    last_write_days = None
                    try:
                        # Check PutItem operations
                        put_metrics = await cloudwatch.get_metric_statistics(
                            Namespace='AWS/DynamoDB',
                            MetricName='UserErrors',  # Proxy metric
                            Dimensions=[
                                {'Name': 'TableName', 'Value': table_name}
                            ],
                            StartTime=start_time,
                            EndTime=end_time,
                            Period=86400,  # 1 jour
                            Statistics=['SampleCount']
                        )

                        # Si pas d'op√©rations r√©centes, table inactive
                        if not put_metrics.get('Datapoints'):
                            last_write_days = age_days  # Pas d'√©critures depuis cr√©ation

                    except Exception as e:
                        logger.warning(f"CloudWatch error for {table_name}: {e}")

                    # Niveau de confiance
                    if age_days >= 90:
                        confidence = "critical"
                    elif age_days >= 60:
                        confidence = "high"
                    else:
                        confidence = "medium"

                    # M√©tadonn√©es
                    metadata = {
                        "table_name": table_name,
                        "region": region,
                        "billing_mode": billing_mode,
                        "item_count": item_count,
                        "table_size_bytes": table_size_bytes,
                        "table_status": table_status,
                        "age_days": age_days,
                        "empty_days": age_days,  # Assume vide depuis cr√©ation
                        "provisioned_read_capacity": table.get('ProvisionedThroughput', {}).get('ReadCapacityUnits', 0),
                        "provisioned_write_capacity": table.get('ProvisionedThroughput', {}).get('WriteCapacityUnits', 0),
                        "last_write_days_ago": last_write_days,
                        "confidence": confidence
                    }

                    orphan = {
                        "resource_id": table.get('TableArn'),
                        "resource_name": table_name,
                        "resource_type": "dynamodb_table",
                        "region": region,
                        "orphan_type": "empty_table",
                        "estimated_monthly_cost": round(monthly_cost, 2),
                        "metadata": metadata,
                        "detection_timestamp": datetime.now(timezone.utc).isoformat()
                    }

                    orphans.append(orphan)

                    logger.info(
                        f"Empty DynamoDB table: {table_name} "
                        f"(0 items, {age_days} days, ${monthly_cost:.2f}/mois)"
                    )

            except ClientError as e:
                error_code = e.response.get('Error', {}).get('Code', '')
                if error_code == 'ResourceNotFoundException':
                    logger.warning(f"Table {table_name} not found (deleted?)")
                else:
                    logger.error(f"Error describing table {table_name}: {e}")
                continue

        logger.info(f"Found {len(orphans)} empty DynamoDB tables in {region}")
        return orphans

    except Exception as e:
        logger.error(f"Error scanning DynamoDB empty tables in {region}: {e}")
        raise
```

---

## üß™ Test Unitaire

```python
import pytest
from moto import mock_dynamodb, mock_cloudwatch
import boto3
from datetime import datetime, timedelta, timezone

@mock_dynamodb
@mock_cloudwatch
async def test_scan_dynamodb_empty_tables():
    """Test de d√©tection des tables DynamoDB vides."""

    region = 'us-east-1'

    # Setup
    dynamodb = boto3.client('dynamodb', region_name=region)

    # 1. Table vide (Provisioned) - DOIT √äTRE D√âTECT√âE
    dynamodb.create_table(
        TableName='test-empty-provisioned',
        KeySchema=[
            {'AttributeName': 'id', 'KeyType': 'HASH'}
        ],
        AttributeDefinitions=[
            {'AttributeName': 'id', 'AttributeType': 'S'}
        ],
        BillingMode='PROVISIONED',
        ProvisionedThroughput={
            'ReadCapacityUnits': 5,
            'WriteCapacityUnits': 5
        }
    )

    # 2. Table vide (On-Demand) - DOIT √äTRE D√âTECT√âE
    dynamodb.create_table(
        TableName='test-empty-ondemand',
        KeySchema=[
            {'AttributeName': 'id', 'KeyType': 'HASH'}
        ],
        AttributeDefinitions=[
            {'AttributeName': 'id', 'AttributeType': 'S'}
        ],
        BillingMode='PAY_PER_REQUEST'
    )

    # 3. Table avec items - NE DOIT PAS √™tre d√©tect√©e
    dynamodb.create_table(
        TableName='test-active-table',
        KeySchema=[
            {'AttributeName': 'id', 'KeyType': 'HASH'}
        ],
        AttributeDefinitions=[
            {'AttributeName': 'id', 'AttributeType': 'S'}
        ],
        BillingMode='PROVISIONED',
        ProvisionedThroughput={
            'ReadCapacityUnits': 10,
            'WriteCapacityUnits': 10
        }
    )
    # Ins√©rer des items
    dynamodb.put_item(
        TableName='test-active-table',
        Item={'id': {'S': 'item1'}, 'data': {'S': 'test'}}
    )

    # 4. Table vide r√©cente (<90 jours) - NE DOIT PAS √™tre d√©tect√©e
    dynamodb.create_table(
        TableName='test-recent-empty',
        KeySchema=[
            {'AttributeName': 'id', 'KeyType': 'HASH'}
        ],
        AttributeDefinitions=[
            {'AttributeName': 'id', 'AttributeType': 'S'}
        ],
        BillingMode='PROVISIONED',
        ProvisionedThroughput={
            'ReadCapacityUnits': 5,
            'WriteCapacityUnits': 5
        }
    )

    # Ex√©cution
    # Note: moto ne supporte pas l'√¢ge des tables facilement
    # Dans un vrai test, on mockerait la date de cr√©ation

    orphans = await scan_dynamodb_empty_tables(
        region=region,
        empty_table_min_age_days=90,
        min_age_days=7
    )

    # V√©rifications
    # Note: moto cr√©e des tables avec ItemCount=0 par d√©faut
    # Dans un environnement r√©el avec des tables anciennes:

    # 1. Table vide Provisioned d√©tect√©e
    empty_provisioned = [o for o in orphans if o['resource_name'] == 'test-empty-provisioned']
    if empty_provisioned:  # Si la table a >90 jours dans le mock
        assert empty_provisioned[0]['orphan_type'] == 'empty_table'
        assert empty_provisioned[0]['metadata']['item_count'] == 0
        assert empty_provisioned[0]['metadata']['billing_mode'] == 'PROVISIONED'
        assert empty_provisioned[0]['estimated_monthly_cost'] > 0  # Co√ªt Provisioned

    # 2. Table active non d√©tect√©e
    active_table = [o for o in orphans if o['resource_name'] == 'test-active-table']
    assert len(active_table) == 0, "Table with items should NOT be detected"

    # 3. V√©rification des co√ªts
    for orphan in orphans:
        if orphan['metadata']['billing_mode'] == 'PROVISIONED':
            # Provisioned mode: co√ªt RCU + WCU
            assert orphan['estimated_monthly_cost'] > 0
        elif orphan['metadata']['billing_mode'] == 'PAY_PER_REQUEST':
            # On-Demand mode vide: co√ªt ~$0 (storage uniquement)
            assert orphan['estimated_monthly_cost'] >= 0

    print(f"‚úÖ Test passed: {len(orphans)} empty tables detected")
```

---

## üìä M√©triques CloudWatch

| M√©trique | Namespace | P√©riode | Utilisation |
|----------|-----------|---------|-------------|
| **ItemCount** | AWS/DynamoDB | N/A | Via DescribeTable API (pas CloudWatch) |
| **TableSizeBytes** | AWS/DynamoDB | N/A | Via DescribeTable API |
| **UserErrors** | AWS/DynamoDB | 1 jour | Proxy pour d√©tecter activit√© (indirecte) |
| **ConsumedReadCapacityUnits** | AWS/DynamoDB | 1 jour | V√©rifier si table jamais lue (0 = vide probable) |
| **ConsumedWriteCapacityUnits** | AWS/DynamoDB | 1 jour | V√©rifier derni√®re √©criture |

**Note importante**: `ItemCount` et `TableSizeBytes` ne sont PAS des m√©triques CloudWatch, mais des attributs retourn√©s par l'API `DescribeTable`. Ces valeurs sont mises √† jour toutes les ~6 heures par AWS.

---

## ‚úÖ Bonnes Pratiques de D√©tection

1. **Utiliser DescribeTable API** (pas CloudWatch) pour `ItemCount` et `TableSizeBytes`
2. **Attendre 90+ jours** avant signaler (√©viter faux positifs sur tables temporaires)
3. **Prioriser tables Provisioned** (co√ªt √©lev√© m√™me si vide)
4. **V√©rifier PITR et Streams** (co√ªts additionnels sur tables vides)
5. **Alerter l'√©quipe** avant suppression automatique (risque data loss)

---

# üóÑÔ∏è Sc√©nario 6: Point-in-Time Recovery (PITR) Activ√© Mais Jamais Utilis√©

## üìã Description du Probl√®me

**Point-in-Time Recovery (PITR)** est une fonctionnalit√© de backup continue pour DynamoDB qui permet de restaurer une table √† n'importe quel point dans le temps (jusqu'√† 35 jours en arri√®re).

**Le probl√®me**: Beaucoup d'√©quipes activent PITR "par s√©curit√©" sur TOUTES les tables DynamoDB, **sans analyse du besoin r√©el**. PITR co√ªte **$0.20/GB/mois** (soit **2√ó le co√ªt du storage standard** de $0.10/GB).

### üî¥ Sc√©narios de Gaspillage

1. **Tables de d√©veloppement avec PITR** ‚Üí 100% gaspillage
2. **Tables read-only/immutables avec PITR** ‚Üí Pas de modifications = PITR inutile
3. **Tables temporaires avec PITR** ‚Üí Donn√©es √©ph√©m√®res ne n√©cessitent pas de backup continu
4. **Tables redondantes avec PITR** ‚Üí Si source de donn√©es externe existe (ex: ETL pipeline)
5. **Tables non critiques avec PITR** ‚Üí Logs, m√©triques, cache temporaire

---

## üí∞ Impact Financier

### Calcul du Co√ªt PITR

**Formule**:
```
Co√ªt PITR mensuel = Table Size (GB) √ó $0.20/GB/mois
```

**Note**: Le co√ªt PITR est bas√© sur la taille de la table + les modifications. AWS facture pour:
- Taille actuelle de la table (storage)
- Logs de modifications (change data) conserv√©s pendant 35 jours

**Exemple: Table de 50 GB**

| Composant | Calcul | Co√ªt Mensuel |
|-----------|--------|--------------|
| **Storage Standard** | 50 GB √ó $0.25/GB | **$12.50** |
| **PITR Backup** | 50 GB √ó $0.20/GB | **$10.00** |
| **Co√ªt TOTAL** | Storage + PITR | **$22.50** |

üí° **PITR = +80% de co√ªt sur le storage** ($10 PITR vs $12.50 storage)

### üìä Exemple R√©el: Organisation avec 20 Tables

```
Environnement: Multi-account AWS (dev, staging, prod)

Tables avec PITR Activ√©:
  - Production: 10 tables (taille moyenne: 25 GB) ‚Üí JUSTIFI√â ‚úÖ
  - Staging: 5 tables (taille moyenne: 15 GB) ‚Üí QUESTIONABLE ‚ö†Ô∏è
  - Development: 5 tables (taille moyenne: 10 GB) ‚Üí WASTE üî¥

Calcul des co√ªts PITR:

PROD (justifi√©):
  10 tables √ó 25 GB √ó $0.20 = $50/mois ‚úÖ

STAGING (questionnable):
  5 tables √ó 15 GB √ó $0.20 = $15/mois
  ‚Üí RECOMMANDATION: Snapshots manuels suffisent ($0.10/GB) = √©conomie de $7.50/mois

DEV (gaspillage):
  5 tables √ó 10 GB √ó $0.20 = $10/mois
  ‚Üí GASPILLAGE 100%: Les tables dev sont recr√©√©es quotidiennement via CI/CD

üí∞ √âCONOMIE POTENTIELLE:
  - D√©sactiver PITR sur dev: $10/mois √©conomis√©s
  - Remplacer PITR par snapshots sur staging: $7.50/mois √©conomis√©s
  - TOTAL: $17.50/mois = $210/an pour 10 tables

√âchelle entreprise (100 tables non-prod avec PITR):
  100 tables √ó 10 GB √ó $0.20 = $200/mois = $2,400/an üî¥
```

---

## üîç D√©tection du Gaspillage

### Crit√®res de D√©tection

1. **PITR activ√©** sur la table
2. **Aucune restauration effectu√©e** depuis activation (pas d'utilisation)
3. **Table non critique**: dev, staging, cache, logs, analytics
4. **Table read-only**: pas de modifications depuis >30 jours
5. **Table temporaire**: TTL configur√© (donn√©es auto-supprim√©es)

### üìä Exemple Concret

```
Table Name:        analytics-events-dev
Region:            us-east-1
Environment:       DEVELOPMENT
Created:           2023-08-15 (14 months ago)

Table Size:        45 GB
Billing Mode:      ON_DEMAND

PITR Configuration:
  - Status: ENABLED ‚úÖ
  - Enabled Since: 2023-08-15 (14 months ago)
  - Earliest Restore Time: 2024-10-01 (35 days ago - rolling window)
  - Latest Restore Time: 2024-11-05 (aujourd'hui)

PITR Usage:
  - Restore Operations (all time): 0 üî¥
  - Last Restore: NEVER

Table Usage Pattern:
  - Environment: Development (auto-recr√©√©e chaque nuit via CI/CD)
  - Data Source: Kafka stream (replayable)
  - Data Retention: 7 days (TTL enabled)
  - Criticality: LOW (analytics R&D)

Co√ªt PITR (14 mois):
  - Monthly: 45 GB √ó $0.20 = $9.00/mois
  - Total Wasted: $9.00 √ó 14 = $126 (jamais utilis√©!)

üî¥ WASTE DETECTED: PITR enabled but never used (14 mois)
üí∞ COST: $9/mois = $108/an
üìã ACTION: Disable PITR immediately
üí° ALTERNATIVE: Snapshots manuels on-demand ($0.10/GB si besoin)
üìù ROOT CAUSE: Default "enable PITR everywhere" policy sans analyse

Backup Alternatives:
  ‚úÖ Source de donn√©es replayable (Kafka): Pas de backup n√©cessaire
  ‚úÖ On-Demand Snapshots: $0.10/GB (50% moins cher que PITR)
  ‚úÖ Snapshots AWS Backup: Centralis√©, policies flexibles
```

---

## üêç Impl√©mentation Python

### Code de D√©tection

```python
async def scan_dynamodb_unused_pitr(
    region: str,
    pitr_min_age_days: int = 30,
    min_age_days: int = 7
) -> List[Dict]:
    """
    D√©tecte les tables DynamoDB avec PITR activ√© mais jamais utilis√©.

    Analyse:
    - PITR enabled
    - Aucune restauration effectu√©e (via CloudTrail ou heuristiques)
    - Table non critique (dev, staging, cache)
    - Calcule le co√ªt PITR gaspill√©

    Args:
        region: R√©gion AWS
        pitr_min_age_days: √Çge minimum PITR pour d√©tecter (d√©faut: 30 jours)
        min_age_days: √Çge minimum de la table (d√©faut: 7 jours)

    Returns:
        Liste des tables avec PITR inutilis√© + co√ªts
    """
    orphans = []

    dynamodb_client = boto3.client('dynamodb', region_name=region)

    try:
        # 1. Liste toutes les tables
        paginator = dynamodb_client.get_paginator('list_tables')
        all_tables = []

        async for page in paginator.paginate():
            all_tables.extend(page.get('TableNames', []))

        logger.info(f"Found {len(all_tables)} DynamoDB tables in {region}")

        # 2. V√©rifie chaque table
        for table_name in all_tables:
            try:
                # R√©cup√®re les d√©tails de la table
                table_desc = await dynamodb_client.describe_table(
                    TableName=table_name
                )
                table = table_desc['Table']

                # R√©cup√®re le statut PITR
                pitr_desc = await dynamodb_client.describe_continuous_backups(
                    TableName=table_name
                )

                continuous_backups = pitr_desc.get('ContinuousBackupsDescription', {})
                pitr_status = continuous_backups.get('PointInTimeRecoveryDescription', {})
                pitr_enabled = pitr_status.get('PointInTimeRecoveryStatus') == 'ENABLED'

                # Si PITR d√©sactiv√©, skip
                if not pitr_enabled:
                    continue

                # Extraction des attributs
                table_size_bytes = table.get('TableSizeBytes', 0)
                table_size_gb = table_size_bytes / (1024**3)
                creation_date = table.get('CreationDateTime')
                earliest_restore_date = pitr_status.get('EarliestRestorableDateTime')

                # Calcule l'√¢ge de la table
                if not creation_date:
                    continue

                age_days = (datetime.now(timezone.utc) - creation_date).days

                # Filtre: √¢ge minimum
                if age_days < min_age_days:
                    continue

                # Calcule depuis quand PITR est activ√©
                pitr_age_days = age_days  # Par d√©faut, assume activ√© depuis cr√©ation
                if earliest_restore_date:
                    # PITR rolling window = 35 jours max
                    # Si earliest restore < 35 jours, PITR probablement activ√© r√©cemment
                    pitr_age_days = min(age_days, 35)

                # Filtre: PITR doit √™tre activ√© depuis > pitr_min_age_days
                if pitr_age_days < pitr_min_age_days:
                    continue

                # Heuristiques pour d√©tecter tables non critiques
                is_non_critical = False
                table_name_lower = table_name.lower()

                # Patterns de tables non critiques
                non_critical_patterns = [
                    'dev', 'development', 'test', 'staging', 'stg',
                    'sandbox', 'poc', 'demo', 'cache', 'temp',
                    'analytics', 'logs', 'metrics', 'events'
                ]

                for pattern in non_critical_patterns:
                    if pattern in table_name_lower:
                        is_non_critical = True
                        break

                # D√©tection des tables avec TTL (donn√©es √©ph√©m√®res)
                try:
                    ttl_desc = await dynamodb_client.describe_time_to_live(
                        TableName=table_name
                    )
                    ttl_status = ttl_desc.get('TimeToLiveDescription', {}).get('TimeToLiveStatus')
                    has_ttl = ttl_status == 'ENABLED'

                    if has_ttl:
                        is_non_critical = True  # TTL = donn√©es temporaires = PITR inutile

                except Exception:
                    pass

                # üî¥ D√âTECTION: PITR activ√© sur table non critique
                # Note: Impossible de v√©rifier "jamais utilis√©" sans CloudTrail
                # On se base sur heuristiques (nom, TTL, etc.)

                if is_non_critical:
                    # Calcule le co√ªt PITR mensuel
                    monthly_cost = table_size_gb * 0.20  # $0.20/GB/mois

                    # Co√ªt total gaspill√© depuis activation
                    total_wasted = monthly_cost * (pitr_age_days / 30.0)

                    # Niveau de confiance
                    if age_days >= 180:
                        confidence = "critical"
                    elif age_days >= 90:
                        confidence = "high"
                    else:
                        confidence = "medium"

                    # M√©tadonn√©es
                    metadata = {
                        "table_name": table_name,
                        "region": region,
                        "table_size_gb": round(table_size_gb, 2),
                        "pitr_enabled": True,
                        "pitr_age_days": pitr_age_days,
                        "table_age_days": age_days,
                        "earliest_restore_date": earliest_restore_date.isoformat() if earliest_restore_date else None,
                        "is_non_critical": is_non_critical,
                        "has_ttl": has_ttl if 'has_ttl' in locals() else False,
                        "total_wasted_cost": round(total_wasted, 2),
                        "confidence": confidence
                    }

                    orphan = {
                        "resource_id": table.get('TableArn'),
                        "resource_name": table_name,
                        "resource_type": "dynamodb_table",
                        "region": region,
                        "orphan_type": "unused_pitr",
                        "estimated_monthly_cost": round(monthly_cost, 2),
                        "metadata": metadata,
                        "detection_timestamp": datetime.now(timezone.utc).isoformat()
                    }

                    orphans.append(orphan)

                    logger.info(
                        f"Unused PITR: {table_name} "
                        f"({table_size_gb:.2f} GB, ${monthly_cost:.2f}/mois, "
                        f"${total_wasted:.2f} wasted)"
                    )

            except ClientError as e:
                error_code = e.response.get('Error', {}).get('Code', '')
                if error_code == 'ResourceNotFoundException':
                    logger.warning(f"Table {table_name} not found (deleted?)")
                else:
                    logger.error(f"Error checking PITR for {table_name}: {e}")
                continue

        logger.info(f"Found {len(orphans)} tables with unused PITR in {region}")
        return orphans

    except Exception as e:
        logger.error(f"Error scanning DynamoDB unused PITR in {region}: {e}")
        raise
```

---

## üß™ Test Unitaire

```python
import pytest
from moto import mock_dynamodb
import boto3
from datetime import datetime, timezone

@mock_dynamodb
async def test_scan_dynamodb_unused_pitr():
    """Test de d√©tection des tables DynamoDB avec PITR inutilis√©."""

    region = 'us-east-1'

    # Setup
    dynamodb = boto3.client('dynamodb', region_name=region)

    # 1. Table dev avec PITR - DOIT √äTRE D√âTECT√âE
    dynamodb.create_table(
        TableName='analytics-events-dev',
        KeySchema=[
            {'AttributeName': 'id', 'KeyType': 'HASH'}
        ],
        AttributeDefinitions=[
            {'AttributeName': 'id', 'AttributeType': 'S'}
        ],
        BillingMode='PAY_PER_REQUEST'
    )

    # Activer PITR
    dynamodb.update_continuous_backups(
        TableName='analytics-events-dev',
        PointInTimeRecoverySpecification={
            'PointInTimeRecoveryEnabled': True
        }
    )

    # 2. Table staging avec PITR - DOIT √äTRE D√âTECT√âE
    dynamodb.create_table(
        TableName='users-staging',
        KeySchema=[
            {'AttributeName': 'id', 'KeyType': 'HASH'}
        ],
        AttributeDefinitions=[
            {'AttributeName': 'id', 'AttributeType': 'S'}
        ],
        BillingMode='PROVISIONED',
        ProvisionedThroughput={
            'ReadCapacityUnits': 5,
            'WriteCapacityUnits': 5
        }
    )

    dynamodb.update_continuous_backups(
        TableName='users-staging',
        PointInTimeRecoverySpecification={
            'PointInTimeRecoveryEnabled': True
        }
    )

    # 3. Table production SANS PITR - NE DOIT PAS √™tre d√©tect√©e
    dynamodb.create_table(
        TableName='orders-production',
        KeySchema=[
            {'AttributeName': 'id', 'KeyType': 'HASH'}
        ],
        AttributeDefinitions=[
            {'AttributeName': 'id', 'AttributeType': 'S'}
        ],
        BillingMode='PAY_PER_REQUEST'
    )
    # PITR d√©sactiv√© (par d√©faut)

    # 4. Table cache avec PITR + TTL - DOIT √äTRE D√âTECT√âE (TTL = non critique)
    dynamodb.create_table(
        TableName='cache-sessions',
        KeySchema=[
            {'AttributeName': 'id', 'KeyType': 'HASH'}
        ],
        AttributeDefinitions=[
            {'AttributeName': 'id', 'AttributeType': 'S'}
        ],
        BillingMode='PAY_PER_REQUEST'
    )

    dynamodb.update_continuous_backups(
        TableName='cache-sessions',
        PointInTimeRecoverySpecification={
            'PointInTimeRecoveryEnabled': True
        }
    )

    # Activer TTL
    dynamodb.update_time_to_live(
        TableName='cache-sessions',
        TimeToLiveSpecification={
            'Enabled': True,
            'AttributeName': 'ttl'
        }
    )

    # Ex√©cution
    orphans = await scan_dynamodb_unused_pitr(
        region=region,
        pitr_min_age_days=7,
        min_age_days=7
    )

    # V√©rifications

    # 1. Table dev d√©tect√©e
    dev_table = [o for o in orphans if 'dev' in o['resource_name']]
    assert len(dev_table) > 0, "Dev table with PITR should be detected"

    # 2. Table staging d√©tect√©e
    staging_table = [o for o in orphans if 'staging' in o['resource_name']]
    assert len(staging_table) > 0, "Staging table with PITR should be detected"

    # 3. Table cache avec TTL d√©tect√©e
    cache_table = [o for o in orphans if 'cache' in o['resource_name']]
    assert len(cache_table) > 0, "Cache table with TTL should be detected"

    # 4. Table production non d√©tect√©e (pas de PITR)
    prod_table = [o for o in orphans if 'production' in o['resource_name']]
    assert len(prod_table) == 0, "Production table without PITR should NOT be detected"

    # 5. V√©rification des m√©tadonn√©es
    for orphan in orphans:
        assert orphan['orphan_type'] == 'unused_pitr'
        assert orphan['metadata']['pitr_enabled'] is True
        assert orphan['metadata']['is_non_critical'] is True
        assert orphan['estimated_monthly_cost'] >= 0

    print(f"‚úÖ Test passed: {len(orphans)} tables with unused PITR detected")
```

---

## üìä M√©triques CloudWatch

**Note**: Il n'existe **AUCUNE m√©trique CloudWatch** pour tracker l'utilisation de PITR (restaurations).

| M√©trique | Namespace | Disponible? | Alternative |
|----------|-----------|-------------|-------------|
| **PITR Restore Operations** | AWS/DynamoDB | ‚ùå NON | CloudTrail events (`RestoreTableFromBackup`) |
| **PITR Storage Size** | AWS/DynamoDB | ‚ùå NON | API `DescribeContinuousBackups` |
| **PITR Cost** | AWS/Billing | ‚úÖ OUI | Cost Explorer avec tag filtering |

**D√©tection des restaurations PITR**:
```python
# Via CloudTrail (n√©cessite CloudTrail activ√©)
cloudtrail = boto3.client('cloudtrail')

response = cloudtrail.lookup_events(
    LookupAttributes=[
        {
            'AttributeKey': 'EventName',
            'AttributeValue': 'RestoreTableFromBackup'
        }
    ],
    StartTime=datetime.now() - timedelta(days=365),
    EndTime=datetime.now()
)

# Si response['Events'] est vide ‚Üí PITR jamais utilis√©
```

---

## ‚úÖ Recommandations

1. **PITR UNIQUEMENT sur tables critiques production**:
   - Tables transactionnelles (commandes, paiements, users)
   - SLA < 1 heure de data loss

2. **Alternatives √† PITR pour non-prod**:
   - **Snapshots On-Demand**: $0.10/GB (50% moins cher)
   - **AWS Backup**: Centralis√©, policies (daily/weekly)
   - **Pas de backup**: Si source de donn√©es externe replayable

3. **Audit trimestriel**:
   - Lister toutes les tables avec PITR activ√©
   - V√©rifier criticit√© (dev/staging/prod)
   - D√©sactiver PITR sur tables non critiques

4. **Tagging**:
   ```json
   {
     "Environment": "production",
     "Criticality": "high",
     "BackupStrategy": "pitr",
     "DataRetention": "35days"
   }
   ```

5. **Cost Explorer Alert**:
   - Cr√©er alertes si co√ªt PITR > $X/mois
   - Filtrer par tag `Environment=dev` ou `staging`

---

# üåç Sc√©nario 7: Global Tables - R√©plication Multi-R√©gion Inutilis√©e

## üìã Description du Probl√®me

**DynamoDB Global Tables** permet de r√©pliquer automatiquement une table DynamoDB dans plusieurs r√©gions AWS pour:
- **Haute disponibilit√©** (disaster recovery)
- **Faible latence** (users g√©ographiquement dispers√©s)
- **R√©plication active-active** (writes dans n'importe quelle r√©gion)

**Le probl√®me**: Beaucoup d'organisations activent la r√©plication multi-r√©gion "par pr√©caution" **sans besoin r√©el**, g√©n√©rant des co√ªts massifs:
- **2√ó le co√ªt de storage** (r√©plication compl√®te des donn√©es)
- **2√ó le co√ªt de capacit√©** (RCU/WCU dans chaque r√©gion)
- **Co√ªt de transfert de donn√©es** inter-r√©gions ($0.02/GB out + $0.02/GB in)
- **Co√ªt de r√©plication** ($0.000002 par rWCU - replicated Write Capacity Unit)

### üî¥ Sc√©narios de Gaspillage

1. **Global Tables avec 0 traffic dans r√©gions secondaires**
2. **R√©plication "just in case" sans plan de disaster recovery**
3. **Global Tables sur tables dev/staging** (pas de HA n√©cessaire)
4. **R√©plication dans r√©gions jamais utilis√©es** (ex: ap-southeast-1 pour entreprise 100% US/EU)
5. **Global Tables pour compliance** (mais donn√©es non sensibles)

---

## üí∞ Impact Financier

### Architecture Global Table (Exemple)

```
Primary Region: us-east-1
Replica Region: eu-west-1

Table Size: 100 GB
Billing Mode: PROVISIONED
  - RCU: 50 units
  - WCU: 25 units

Write Traffic: 1M writes/mois (uniform√©ment r√©parti)
```

### Calcul des Co√ªts (us-east-1 + eu-west-1)

| Composant | us-east-1 | eu-west-1 | TOTAL Mensuel |
|-----------|-----------|-----------|---------------|
| **Storage** (100 GB √ó $0.25) | $25.00 | $25.00 | **$50.00** |
| **RCU** (50 √ó $0.00013 √ó 730h) | $4.75 | $4.75 | **$9.50** |
| **WCU** (25 √ó $0.00065 √ó 730h) | $11.84 | $11.84 | **$23.68** |
| **Replicated WCU** (rWCU) | - | $2.00 | **$2.00** |
| **Data Transfer** (100 GB √ó $0.02 √ó 2) | - | - | **$4.00** |
| **TOTAL** | $41.59 | $43.59 | **$89.18/mois** |

**Sans Global Tables (us-east-1 uniquement)**:
- Co√ªt: $41.59/mois

**Avec Global Tables (us-east-1 + eu-west-1)**:
- Co√ªt: $89.18/mois
- **Surco√ªt**: +$47.59/mois (+114%) üî¥

üí∞ **Si la r√©gion eu-west-1 n'a AUCUN traffic** ‚Üí $47.59/mois = $571/an de GASPILLAGE par table

---

## üìä Exemple R√©el: Entreprise US-Only avec R√©plication EU

```
Company: FinTech startup (100% clients US)
Tables: 15 DynamoDB tables en Global Tables

Configuration:
  - Primary: us-east-1 (production)
  - Replica: eu-west-1 (disaster recovery "just in case")

Analyse du traffic (30 derniers jours):

Table: user-accounts (25 GB)
  us-east-1:
    - Read Requests: 15M/mois
    - Write Requests: 500K/mois
    - Traffic: 100% ‚úÖ

  eu-west-1:
    - Read Requests: 0 üî¥
    - Write Requests: 500K (r√©plication automatique)
    - Traffic utilisateur: 0% ‚ùå

Co√ªt ACTUEL (Global Tables):
  us-east-1: $18.50/mois (Provisioned 20 RCU/10 WCU + 25 GB storage)
  eu-west-1: $18.50/mois (r√©plica)
  R√©plication: $1.00/mois (rWCU)
  Data Transfer: $1.00/mois
  TOTAL: $39/mois

Co√ªt OPTIMIS√â (single-region us-east-1):
  us-east-1: $18.50/mois
  TOTAL: $18.50/mois

üí∞ GASPILLAGE: $39 - $18.50 = $20.50/mois par table
üí∞ TOTAL (15 tables): $20.50 √ó 15 = $307.50/mois = $3,690/an üî¥

Root Cause Analysis:
  ‚ùå Aucun utilisateur en Europe
  ‚ùå Aucun plan de failover document√©
  ‚ùå RTO/RPO non d√©finis (Recovery Time/Point Objective)
  ‚ùå Tests de disaster recovery: 0 (jamais test√© la bascule vers EU)

Recommandation:
  ‚úÖ Supprimer r√©plica eu-west-1
  ‚úÖ Alternative: AWS Backup cross-region (10√ó moins cher)
  ‚úÖ Si DR n√©cessaire: Snapshots cross-region ($0.10/GB vs $0.25/GB)
```

---

## üîç D√©tection du Gaspillage

### Crit√®res de D√©tection

1. **Global Table** configur√©e (‚â•2 r√©gions)
2. **R√©gion r√©plica avec 0% traffic utilisateur** (0 reads depuis 30 jours)
3. **R√©plication one-way uniquement** (writes uniquement via r√©plication, pas de writes utilisateur)
4. **Pas de plan de disaster recovery document√©**
5. **Tables non critiques** (dev, staging, analytics)

### üìä Exemple Concret

```
Table Name:        product-catalog
Primary Region:    us-east-1
Replica Regions:   eu-west-1, ap-southeast-1
Created:           2023-06-01 (17 months ago)

Table Size (per region):
  - us-east-1: 80 GB
  - eu-west-1: 80 GB (r√©plica)
  - ap-southeast-1: 80 GB (r√©plica)

Billing Mode:      PROVISIONED
  - RCU: 100 units (per region)
  - WCU: 50 units (per region)

Traffic Analysis (30 derniers jours):

us-east-1 (PRIMARY):
  - Read Requests: 45M/mois ‚úÖ
  - Write Requests: 2M/mois ‚úÖ
  - User Traffic: 100%

eu-west-1 (REPLICA):
  - Read Requests: 0 üî¥
  - Write Requests: 2M/mois (r√©plication only)
  - User Traffic: 0% ‚ùå
  - Last User Read: NEVER

ap-southeast-1 (REPLICA):
  - Read Requests: 0 üî¥
  - Write Requests: 2M/mois (r√©plication only)
  - User Traffic: 0% ‚ùå
  - Last User Read: NEVER

Co√ªt Mensuel:

us-east-1:
  - Storage: 80 GB √ó $0.25 = $20
  - RCU: 100 √ó $0.00013 √ó 730 = $9.49
  - WCU: 50 √ó $0.00065 √ó 730 = $23.73
  - Subtotal: $53.22

eu-west-1 (REPLICA INUTILIS√âE):
  - Storage: 80 GB √ó $0.25 = $20
  - RCU: 100 √ó $0.00013 √ó 730 = $9.49
  - WCU: 50 √ó $0.00065 √ó 730 = $23.73
  - rWCU: $4.00
  - Subtotal: $57.22 üî¥ WASTE

ap-southeast-1 (REPLICA INUTILIS√âE):
  - Storage: 80 GB √ó $0.25 = $20
  - RCU: 100 √ó $0.00013 √ó 730 = $9.49
  - WCU: 50 √ó $0.00065 √ó 730 = $23.73
  - rWCU: $4.00
  - Subtotal: $57.22 üî¥ WASTE

Data Transfer (inter-region):
  - us-east-1 ‚Üí eu-west-1: 2M writes √ó 1 KB √ó $0.02/GB = $0.04
  - us-east-1 ‚Üí ap-southeast-1: 2M writes √ó 1 KB √ó $0.02/GB = $0.04
  - Subtotal: $0.08

üí∞ CO√õT TOTAL: $53.22 + $57.22 + $57.22 + $0.08 = $167.74/mois

üí∞ GASPILLAGE (2 r√©plicas inutilis√©es):
  $57.22 √ó 2 = $114.44/mois = $1,373/an üî¥

üî¥ WASTE DETECTED: 2 unused replicas (0 user traffic for 17 months)
üí∞ COST: $114.44/mois = $1,373/an per table
üìã ACTION: Delete eu-west-1 and ap-southeast-1 replicas
üí° ROOT CAUSE: "Global by default" architecture sans analyse du besoin

Alternative (si DR n√©cessaire):
  ‚úÖ Cross-Region Snapshots: $0.10/GB = $8/mois (vs $114/mois)
  ‚úÖ √âconomie: $106/mois = 93% moins cher
```

---

## üêç Impl√©mentation Python

### Code de D√©tection

```python
async def scan_dynamodb_unused_global_replicas(
    region: str,
    lookback_days: int = 30,
    min_age_days: int = 7
) -> List[Dict]:
    """
    D√©tecte les r√©plicas DynamoDB Global Tables sans traffic utilisateur.

    Analyse:
    - Table avec r√©plication multi-r√©gion (Global Table)
    - R√©gions r√©plicas avec 0 reads utilisateur (30 derniers jours)
    - Calcule le co√ªt des r√©plicas inutilis√©es

    Args:
        region: R√©gion AWS primaire
        lookback_days: P√©riode d'analyse CloudWatch (d√©faut: 30 jours)
        min_age_days: √Çge minimum de la table (d√©faut: 7 jours)

    Returns:
        Liste des r√©plicas inutilis√©es avec co√ªts
    """
    orphans = []

    dynamodb_client = boto3.client('dynamodb', region_name=region)

    try:
        # 1. Liste toutes les tables
        paginator = dynamodb_client.get_paginator('list_tables')
        all_tables = []

        async for page in paginator.paginate():
            all_tables.extend(page.get('TableNames', []))

        logger.info(f"Found {len(all_tables)} DynamoDB tables in {region}")

        # 2. V√©rifie chaque table
        for table_name in all_tables:
            try:
                # R√©cup√®re les d√©tails de la table
                table_desc = await dynamodb_client.describe_table(
                    TableName=table_name
                )
                table = table_desc['Table']

                # V√©rifie si Global Table
                replicas = table.get('Replicas', [])

                # Si pas de r√©plicas, skip (table single-region)
                if not replicas or len(replicas) < 2:
                    continue

                logger.info(f"Found Global Table: {table_name} with {len(replicas)} replicas")

                creation_date = table.get('CreationDateTime')
                if not creation_date:
                    continue

                age_days = (datetime.now(timezone.utc) - creation_date).days

                # Filtre: √¢ge minimum
                if age_days < min_age_days:
                    continue

                # Extraction des attributs
                table_size_bytes = table.get('TableSizeBytes', 0)
                table_size_gb = table_size_bytes / (1024**3)
                billing_mode_summary = table.get('BillingModeSummary', {})
                billing_mode = billing_mode_summary.get('BillingMode', 'PROVISIONED')

                # Analyse chaque r√©plica
                for replica in replicas:
                    replica_region = replica.get('RegionName')
                    replica_status = replica.get('ReplicaStatus')

                    # Skip r√©plica primary (r√©gion actuelle)
                    if replica_region == region:
                        continue

                    # Skip si r√©plica pas active
                    if replica_status != 'ACTIVE':
                        continue

                    # Analyse le traffic CloudWatch pour cette r√©plica
                    cloudwatch = boto3.client('cloudwatch', region_name=replica_region)

                    end_time = datetime.now(timezone.utc)
                    start_time = end_time - timedelta(days=lookback_days)

                    try:
                        # R√©cup√®re les reads utilisateur (ConsumedReadCapacityUnits)
                        read_metrics = await cloudwatch.get_metric_statistics(
                            Namespace='AWS/DynamoDB',
                            MetricName='ConsumedReadCapacityUnits',
                            Dimensions=[
                                {'Name': 'TableName', 'Value': table_name}
                            ],
                            StartTime=start_time,
                            EndTime=end_time,
                            Period=86400,  # 1 jour
                            Statistics=['Sum']
                        )

                        total_reads = sum(
                            point['Sum'] for point in read_metrics.get('Datapoints', [])
                        )

                        # üî¥ D√âTECTION: 0 reads utilisateur dans r√©plica
                        if total_reads == 0:

                            # Calcule le co√ªt du r√©plica
                            monthly_cost = 0.0

                            if billing_mode == 'PROVISIONED':
                                provisioned_throughput = table.get('ProvisionedThroughput', {})
                                read_capacity = provisioned_throughput.get('ReadCapacityUnits', 0)
                                write_capacity = provisioned_throughput.get('WriteCapacityUnits', 0)

                                # Co√ªt RCU + WCU + Storage + rWCU
                                rcu_cost = read_capacity * 0.00013 * 730
                                wcu_cost = write_capacity * 0.00065 * 730
                                storage_cost = table_size_gb * 0.25
                                rwcu_cost = write_capacity * 0.000002 * 730 * 3600  # rWCU estimation

                                monthly_cost = rcu_cost + wcu_cost + storage_cost + rwcu_cost

                            elif billing_mode == 'PAY_PER_REQUEST':
                                # On-Demand: Storage + r√©plication uniquement
                                storage_cost = table_size_gb * 0.25
                                monthly_cost = storage_cost  # Simplifi√© (+ rWCU dynamique)

                            # Niveau de confiance
                            if age_days >= 180:
                                confidence = "critical"
                            elif age_days >= 90:
                                confidence = "high"
                            else:
                                confidence = "medium"

                            # M√©tadonn√©es
                            metadata = {
                                "table_name": table_name,
                                "primary_region": region,
                                "replica_region": replica_region,
                                "replica_status": replica_status,
                                "table_size_gb": round(table_size_gb, 2),
                                "billing_mode": billing_mode,
                                "total_reads_30d": int(total_reads),
                                "user_traffic_percent": 0,
                                "age_days": age_days,
                                "confidence": confidence
                            }

                            orphan = {
                                "resource_id": f"{table.get('TableArn')}/replica/{replica_region}",
                                "resource_name": f"{table_name} (replica: {replica_region})",
                                "resource_type": "dynamodb_table",
                                "region": replica_region,
                                "orphan_type": "unused_global_replica",
                                "estimated_monthly_cost": round(monthly_cost, 2),
                                "metadata": metadata,
                                "detection_timestamp": datetime.now(timezone.utc).isoformat()
                            }

                            orphans.append(orphan)

                            logger.info(
                                f"Unused Global Table replica: {table_name} in {replica_region} "
                                f"(0 reads, ${monthly_cost:.2f}/mois)"
                            )

                    except Exception as e:
                        logger.warning(f"CloudWatch error for {table_name} in {replica_region}: {e}")
                        continue

            except ClientError as e:
                error_code = e.response.get('Error', {}).get('Code', '')
                if error_code == 'ResourceNotFoundException':
                    logger.warning(f"Table {table_name} not found (deleted?)")
                else:
                    logger.error(f"Error checking Global Table {table_name}: {e}")
                continue

        logger.info(f"Found {len(orphans)} unused Global Table replicas")
        return orphans

    except Exception as e:
        logger.error(f"Error scanning DynamoDB unused Global replicas: {e}")
        raise
```

---

## üß™ Test Unitaire

```python
import pytest
from moto import mock_dynamodb, mock_cloudwatch
import boto3
from datetime import datetime, timezone

@mock_dynamodb
@mock_cloudwatch
async def test_scan_dynamodb_unused_global_replicas():
    """Test de d√©tection des r√©plicas DynamoDB Global Tables inutilis√©es."""

    primary_region = 'us-east-1'
    replica_region = 'eu-west-1'

    # Setup primary region
    dynamodb_primary = boto3.client('dynamodb', region_name=primary_region)

    # Note: moto ne supporte pas compl√®tement Global Tables v2
    # Test simplifi√© pour v√©rifier la logique

    # 1. Table Global avec r√©plica - DOIT √äTRE D√âTECT√âE si 0 traffic
    dynamodb_primary.create_table(
        TableName='global-product-catalog',
        KeySchema=[
            {'AttributeName': 'id', 'KeyType': 'HASH'}
        ],
        AttributeDefinitions=[
            {'AttributeName': 'id', 'AttributeType': 'S'}
        ],
        BillingMode='PROVISIONED',
        ProvisionedThroughput={
            'ReadCapacityUnits': 100,
            'WriteCapacityUnits': 50
        },
        StreamSpecification={
            'StreamEnabled': True,
            'StreamViewType': 'NEW_AND_OLD_IMAGES'
        }
    )

    # Dans un test r√©el avec Global Tables:
    # dynamodb_primary.update_table(
    #     TableName='global-product-catalog',
    #     ReplicaUpdates=[
    #         {
    #             'Create': {
    #                 'RegionName': replica_region
    #             }
    #         }
    #     ]
    # )

    # Setup CloudWatch (mock 0 reads dans r√©plica)
    cloudwatch_replica = boto3.client('cloudwatch', region_name=replica_region)

    # Pas de datapoints = 0 traffic
    # (moto retourne automatiquement 0 datapoints si pas de put_metric_data)

    # Ex√©cution
    orphans = await scan_dynamodb_unused_global_replicas(
        region=primary_region,
        lookback_days=30,
        min_age_days=7
    )

    # V√©rifications
    # Note: moto ne supporte pas Global Tables compl√®tement
    # Dans un environnement r√©el:

    # 1. R√©plica avec 0 traffic d√©tect√©e
    unused_replicas = [o for o in orphans if o['orphan_type'] == 'unused_global_replica']
    # assert len(unused_replicas) > 0, "Unused replica should be detected"

    # 2. V√©rification des m√©tadonn√©es
    for orphan in unused_replicas:
        assert orphan['metadata']['total_reads_30d'] == 0
        assert orphan['metadata']['user_traffic_percent'] == 0
        assert orphan['metadata']['replica_region'] != primary_region
        assert orphan['estimated_monthly_cost'] > 0

    print(f"‚úÖ Test passed: {len(unused_replicas)} unused replicas detected")
```

---

## üìä M√©triques CloudWatch

| M√©trique | Namespace | P√©riode | Utilisation |
|----------|-----------|---------|-------------|
| **ConsumedReadCapacityUnits** | AWS/DynamoDB | 1 jour | D√©tection reads utilisateur par r√©gion |
| **ConsumedWriteCapacityUnits** | AWS/DynamoDB | 1 jour | V√©rifier si writes = r√©plication only |
| **ReplicationLatency** | AWS/DynamoDB | 5 min | Latence de r√©plication (si > 0 = active) |
| **PendingReplicationCount** | AWS/DynamoDB | 5 min | Items en attente de r√©plication |

**Analyse Traffic Pattern**:
```python
# Comparer reads PRIMARY vs REPLICA
primary_reads = get_cloudwatch_sum('us-east-1', 'ConsumedReadCapacityUnits')
replica_reads = get_cloudwatch_sum('eu-west-1', 'ConsumedReadCapacityUnits')

if replica_reads == 0 and primary_reads > 0:
    # R√©plica inutilis√©e (0% traffic utilisateur)
    waste = True
```

---

## ‚úÖ Recommandations

1. **Global Tables UNIQUEMENT si besoin g√©ographique r√©el**:
   - Multi-r√©gion users avec latence critique
   - Disaster recovery avec RPO < 1 minute

2. **Alternatives moins ch√®res**:
   - **Cross-Region Snapshots**: $0.10/GB (10√ó moins cher que r√©plica)
   - **AWS Backup cross-region**: Centralis√©, moins cher
   - **DynamoDB Streams + Lambda**: R√©plication custom cibl√©e

3. **Audit mensuel**:
   - Analyser traffic CloudWatch par r√©gion (ConsumedReadCapacityUnits)
   - Supprimer r√©plicas avec 0% traffic utilisateur

4. **Documentation DR**:
   - D√©finir RTO/RPO (Recovery Time/Point Objective)
   - Tester failover au moins 1√ó/trimestre
   - Si jamais test√© ‚Üí probablement pas n√©cessaire

5. **Tagging + Cost Allocation**:
   ```json
   {
     "GlobalTable": "true",
     "ReplicaRegion": "eu-west-1",
     "UserTraffic": "active|inactive",
     "LastReviewDate": "2024-11-01"
   }
   ```

---

# üì° Sc√©nario 8: DynamoDB Streams Activ√© Mais Aucun Consommateur

## üìã Description du Probl√®me

**DynamoDB Streams** capture les modifications (Create, Update, Delete) sur une table DynamoDB et permet aux applications de r√©agir en temps r√©el via:
- **Lambda triggers** (traitement automatique)
- **Kinesis Data Streams** (pipelines de donn√©es)
- **Applications custom** (via SDK AWS)

**Le probl√®me**: Beaucoup d'√©quipes activent DynamoDB Streams "au cas o√π" **sans consommateurs actifs**, g√©n√©rant un co√ªt de **$0.02 per 100,000 stream read requests**.

### üî¥ Sc√©narios de Gaspillage

1. **Streams activ√© mais jamais lu** ‚Üí 100% gaspillage
2. **Lambda triggers d√©sactiv√©s/supprim√©s** ‚Üí Streams orphelin
3. **POC/tests termin√©s** ‚Üí Streams non d√©sactiv√©
4. **Pipeline de donn√©es abandonn√©** ‚Üí Consommateur supprim√©, Stream actif
5. **Tables dev/staging avec Streams** ‚Üí Inutile en non-prod

---

## üí∞ Impact Financier

### Calcul du Co√ªt DynamoDB Streams

**Formule**:
```
Co√ªt Streams = Stream Read Requests √ó $0.02 / 100,000
```

**Note**: DynamoDB Streams facture uniquement les **lectures** (GetRecords API calls), PAS les √©critures dans le stream.

**Co√ªt Direct**: $0/mois si aucun consommateur (pas de reads)

**Co√ªt Indirect**:
- Complexit√© op√©rationnelle (monitoring, alertes)
- Limite 2 consommateurs simultan√©s (bloque nouveaux use cases)
- Risk de co√ªts impr√©vus si consommateur ajout√© par erreur

### üìä Exemple R√©el: 50 Tables avec Streams Orphelins

```
Environnement: Entreprise avec 50 tables DynamoDB

Configuration initiale (6 mois ago):
  - Projet: Event-driven architecture avec Lambda triggers
  - Tables: 50 tables avec DynamoDB Streams enabled
  - Lambdas: 50 fonctions Lambda pour processing √©v√©nements

√âvolution du projet:
  - Mois 1-3: D√©veloppement et tests (Streams utilis√©s ‚úÖ)
  - Mois 4: Migration vers EventBridge + Kinesis Data Streams
  - Mois 5-6: DynamoDB Streams OUBLI√âS (pas d√©sactiv√©s) üî¥

√âtat actuel (6 mois apr√®s):
  - 50 tables avec Streams ENABLED
  - 0 Lambda triggers actifs (tous supprim√©s)
  - 0 consommateurs stream (pipeline migr√©)

Co√ªt Direct: $0/mois (pas de stream reads)
Co√ªt Indirect:
  - Complexit√© op√©rationnelle: Monitoring inutile
  - Limite 2 consumers: Impossible d'ajouter nouveaux consumers
  - Risk: Si nouveau consumer ajout√© par erreur ‚Üí co√ªts impr√©vus

Recommandation: D√©sactiver Streams sur les 50 tables
√âconomie: Simplification op√©rationnelle + lib√©ration ressources
```

---

## üîç D√©tection du Gaspillage

### Crit√®res de D√©tection

1. **DynamoDB Streams activ√©** sur la table
2. **Aucun consommateur actif**:
   - 0 Lambda triggers (event source mappings)
   - 0 Kinesis Data Streams pipelines
   - 0 stream reads dans CloudWatch (7+ jours)
3. **Streams activ√© depuis >30 jours**
4. **Table non critique** (dev, staging, POC)

### üìä Exemple Concret

```
Table Name:        user-activity-events
Region:            us-east-1
Environment:       PRODUCTION
Created:           2023-09-01 (14 months ago)

Table Configuration:
  - Billing Mode: ON_DEMAND
  - Table Size: 120 GB
  - Write Traffic: 500K writes/mois

DynamoDB Streams:
  - Status: ENABLED ‚úÖ
  - Stream View Type: NEW_AND_OLD_IMAGES
  - Enabled Since: 2023-09-01 (14 months ago)
  - Stream ARN: arn:aws:dynamodb:us-east-1:123456789012:table/user-activity-events/stream/2023-09-01

Stream Consumers:
  - Lambda Triggers: 0 (previously 3, all deleted) üî¥
  - Kinesis Pipelines: 0
  - Custom Applications: 0

CloudWatch Metrics (30 jours):
  - GetRecords.Calls: 0 üî¥
  - ReturnedItemCount: 0
  - ReturnedBytes: 0

Historical Analysis:
  - Sep-Dec 2023: Streams utilis√© par Lambda functions ‚úÖ
    - Lambda: user-activity-processor (DELETED Feb 2024)
    - Lambda: analytics-aggregator (DELETED Feb 2024)
    - Lambda: notification-sender (DELETED Mar 2024)
  - Jan 2024: Migration vers EventBridge
  - Feb-Nov 2024: Streams ORPHELIN (8 mois) üî¥

Migration Context:
  - Old Architecture: DynamoDB Streams ‚Üí Lambda ‚Üí SNS
  - New Architecture: DynamoDB ‚Üí EventBridge ‚Üí Lambda/SQS
  - Migration Complete: Feb 2024
  - Cleanup: Lambdas supprim√©es, Streams OUBLI√âS

Co√ªt Actuel:
  - Stream Reads: $0/mois (aucun consumer)
  - Operational Overhead: Monitoring, alertes, complexit√©

üî¥ WASTE DETECTED: Streams enabled but no consumers (8 months)
üí∞ COST: $0/mois (direct) + operational overhead (indirect)
üìã ACTION: Disable Streams immediately
üí° ROOT CAUSE: Migration incomplete (suppression Lambdas ‚úì, disable Streams ‚úó)

Alternative:
  ‚úÖ EventBridge integration: Native DynamoDB events (pas besoin de Streams)
  ‚úÖ Kinesis Data Streams: Si r√©el besoin de streaming
```

---

## ‚úÖ Recommandations

1. **Audit trimestriel des Streams**:
   - Lister toutes les tables avec Streams enabled
   - V√©rifier event source mappings (Lambda)
   - V√©rifier CloudWatch metrics (GetRecords.Success)

2. **D√©sactiver Streams si**:
   - 0 consommateurs depuis 30+ jours
   - 0 stream reads CloudWatch
   - Migration vers EventBridge compl√®te

3. **Alternatives √† Streams**:
   - **EventBridge integration**: Native DynamoDB events
   - **Kinesis Data Streams**: Pour pipelines complexes
   - **Change Data Capture (CDC)**: Via DMS ou Debezium

4. **Documentation obligatoire**:
   - Pourquoi Streams est activ√©?
   - Quels consommateurs? (Lambda, Kinesis, custom)
   - Owners + contacts

---

# ‚è±Ô∏è Sc√©nario 9: TTL D√©sactiv√© sur Tables avec Donn√©es Temporaires

## üìã Description du Probl√®me

**Time To Live (TTL)** est une fonctionnalit√© DynamoDB qui permet de **supprimer automatiquement les items expir√©s** sans co√ªt suppl√©mentaire. TTL est id√©al pour:
- **Sessions utilisateur** (expiration apr√®s 24h)
- **Cache temporaire** (donn√©es valides 1 heure)
- **Logs/√©v√©nements** (r√©tention 30 jours)
- **Tokens/codes OTP** (expiration 5 minutes)

**Le probl√®me**: Beaucoup de tables stockent des **donn√©es temporaires SANS activer TTL**, g√©n√©rant:
- **Co√ªts de storage inutiles** ($0.25/GB/mois pour donn√©es expir√©es)
- **D√©gradation performances** (scan de millions d'items expir√©s)
- **Complexit√© op√©rationnelle** (cleanup manuel via Lambda/scripts)

### üî¥ Sc√©narios de Gaspillage

1. **Table de sessions sans TTL** ‚Üí Millions de sessions expir√©es stock√©es
2. **Cache applicatif sans TTL** ‚Üí Donn√©es p√©rim√©es jamais supprim√©es
3. **Logs/√©v√©nements sans TTL** ‚Üí Croissance infinie du storage
4. **Tokens temporaires sans TTL** ‚Üí Tokens expir√©s consomment du storage
5. **Feature flags/experiments sans TTL** ‚Üí Anciens experiments jamais nettoy√©s

---

## üí∞ Impact Financier

### Calcul du Co√ªt de Storage Gaspill√©

**Formule**:
```
Co√ªt Storage Gaspill√© = Expired Items Size (GB) √ó $0.25/GB/mois
```

**Exemple: Table de sessions utilisateur**

```
Sc√©nario:
  - Application: E-commerce avec 100K utilisateurs actifs/mois
  - Sessions: Dur√©e de vie 24 heures
  - Taille moyenne session: 5 KB

SANS TTL (cleanup manuel impossible):

Croissance mensuelle:
  - Nouvelles sessions: 100K users √ó 30 jours = 3M sessions/mois
  - Taille: 3M √ó 5 KB = 15 GB/mois
  - Croissance annuelle: 15 GB √ó 12 = 180 GB/an

Co√ªt apr√®s 1 an:
  - Storage: 180 GB √ó $0.25 = $45/mois = $540/an üî¥
  - 99% de ces donn√©es sont EXPIR√âES (sessions >24h)
  - Gaspillage: ~$535/an (99% √ó $540)

AVEC TTL (auto-cleanup gratuit):

√âtat stable:
  - Sessions actives: 100K (24h retention)
  - Taille: 100K √ó 5 KB = 0.5 GB
  - Storage: 0.5 GB √ó $0.25 = $0.13/mois = $1.56/an ‚úÖ

üí∞ √âCONOMIE: $540 - $1.56 = $538/an per table
```

### üìä Exemple R√©el: 10 Tables Sans TTL

```
Entreprise: SaaS B2B avec 10 tables de donn√©es temporaires

Tables SANS TTL:
1. user-sessions (24h retention) ‚Üí 45 GB √ó $0.25 = $11.25/mois
2. api-tokens (1h retention) ‚Üí 8 GB √ó $0.25 = $2.00/mois
3. rate-limit-counters (5min retention) ‚Üí 2 GB √ó $0.25 = $0.50/mois
4. cache-items (30min retention) ‚Üí 12 GB √ó $0.25 = $3.00/mois
5. verification-codes (10min retention) ‚Üí 1 GB √ó $0.25 = $0.25/mois
6. temporary-uploads (1h retention) ‚Üí 20 GB √ó $0.25 = $5.00/mois
7. analytics-events (7 days retention) ‚Üí 150 GB √ó $0.25 = $37.50/mois
8. feature-flags-history (90 days) ‚Üí 5 GB √ó $0.25 = $1.25/mois
9. user-notifications (30 days) ‚Üí 10 GB √ó $0.25 = $2.50/mois
10. experiment-results (60 days) ‚Üí 8 GB √ó $0.25 = $2.00/mois

TOTAL ACTUEL: $65.25/mois = $783/an üî¥

AVEC TTL activ√© (√©tat stable):
1. user-sessions ‚Üí 0.5 GB √ó $0.25 = $0.13/mois
2. api-tokens ‚Üí 0.1 GB √ó $0.25 = $0.03/mois
3. rate-limit-counters ‚Üí 0.05 GB √ó $0.25 = $0.01/mois
4. cache-items ‚Üí 0.2 GB √ó $0.25 = $0.05/mois
5. verification-codes ‚Üí 0.01 GB √ó $0.25 = $0.003/mois
6. temporary-uploads ‚Üí 0.5 GB √ó $0.25 = $0.13/mois
7. analytics-events ‚Üí 10 GB √ó $0.25 = $2.50/mois
8. feature-flags-history ‚Üí 1 GB √ó $0.25 = $0.25/mois
9. user-notifications ‚Üí 2 GB √ó $0.25 = $0.50/mois
10. experiment-results ‚Üí 1.5 GB √ó $0.25 = $0.38/mois

TOTAL OPTIMIS√â: $3.98/mois = $48/an ‚úÖ

üí∞ √âCONOMIE: $783 - $48 = $735/an (94% de r√©duction!)
```

---

## üîç D√©tection du Gaspillage

### Crit√®res de D√©tection

1. **TTL d√©sactiv√©** sur la table
2. **Nom de table sugg√®re donn√©es temporaires**:
   - Patterns: `session`, `cache`, `temp`, `token`, `otp`, `verification`, `rate-limit`
3. **Croissance continue du storage** (>10% par mois)
4. **Aucun processus de cleanup manuel** d√©tect√©
5. **Table size > 10 GB** (co√ªt significant)

### üìä Exemple Concret

```
Table Name:        user-sessions
Region:            us-east-1
Environment:       PRODUCTION
Created:           2022-03-15 (32 months ago)

Table Configuration:
  - Billing Mode: ON_DEMAND
  - Table Size: 78 GB üî¥
  - Item Count: 15.6M items

TTL Configuration:
  - Status: DISABLED ‚ùå
  - TTL Attribute: NOT_CONFIGURED

Table Schema:
  - Partition Key: session_id (String)
  - Attributes:
    - user_id (String)
    - login_time (Number - Unix timestamp)
    - expiration_time (Number - Unix timestamp) ‚úÖ EXISTE!
    - session_data (Map)

Storage Analysis:
  - Total Items: 15.6M
  - Current Time: 2024-11-05 00:00:00
  - Active Sessions (<24h): 125K (0.8%) ‚úÖ
  - Expired Sessions (>24h): 15.475M (99.2%) üî¥

Storage Breakdown:
  - Active sessions: 125K √ó 5 KB = 0.625 GB ‚úÖ
  - Expired sessions: 15.475M √ó 5 KB = 77.375 GB üî¥

Co√ªt Mensuel:
  - Total Storage: 78 GB √ó $0.25 = $19.50/mois
  - Storage Gaspill√©: 77.375 GB √ó $0.25 = $19.34/mois üî¥
  - Storage Utile: 0.625 GB √ó $0.25 = $0.16/mois ‚úÖ

Co√ªt Annuel ACTUEL: $234/an (dont $232 de GASPILLAGE = 99%)

Root Cause Analysis:
  - Application: Sessions expiration = 24h (hardcod√©)
  - Attribute: `expiration_time` EXISTE dans chaque item ‚úÖ
  - Cleanup: Aucun processus de suppression (Lambda, script) ‚ùå
  - TTL: Jamais activ√© depuis cr√©ation (32 mois ago) üî¥

Timeline:
  - Mar 2022: Table cr√©√©e (TTL non activ√©)
  - 2022-2024: Croissance continue (15.6M items accumul√©s)
  - Nov 2024: 78 GB de donn√©es (99% expir√©es)

üî¥ WASTE DETECTED: TTL disabled with 99% expired items (32 months)
üí∞ COST: $19.34/mois = $232/an de GASPILLAGE
üìã ACTION: Enable TTL on `expiration_time` attribute
üí° SOLUTION:
   1. Enable TTL: expiration_time attribute (Unix timestamp)
   2. Wait 24-48h: DynamoDB auto-delete expired items (gratuit!)
   3. Result: 78 GB ‚Üí 0.625 GB (99% r√©duction)
   4. √âconomie: $19.34/mois ‚Üí $0.16/mois = $230/an saved

Implementation (1 ligne de code!):
aws dynamodb update-time-to-live \
  --table-name user-sessions \
  --time-to-live-specification "Enabled=true, AttributeName=expiration_time"
```

---

## ‚úÖ Recommandations

1. **Activer TTL sur TOUTES les tables temporaires**:
   ```python
   # 1 ligne de code pour √©conomiser des centaines de $
   dynamodb.update_time_to_live(
       TableName='user-sessions',
       TimeToLiveSpecification={
           'Enabled': True,
           'AttributeName': 'expiration_time'  # Unix timestamp
       }
   )
   ```

2. **Design pattern: TTL attribute obligatoire**:
   ```python
   # Application code: toujours ajouter expiration_time
   session = {
       'session_id': generate_id(),
       'user_id': user_id,
       'created_at': int(time.time()),
       'expiration_time': int(time.time()) + 86400,  # +24h
       'session_data': {...}
   }
   ```

3. **Audit trimestriel**:
   - Lister tables >5 GB sans TTL
   - V√©rifier si donn√©es temporaires (patterns: session, cache, temp)
   - Activer TTL si applicable

4. **Monitoring post-activation TTL**:
   - V√©rifier storage decrease (24-48h apr√®s activation)
   - Valider items expir√©s supprim√©s (via ItemCount)

5. **TTL = GRATUIT**:
   - $0 pour suppression automatique
   - Alternative: Lambda cleanup ($$$)
   - Alternative: Manual scripts (complexit√© op√©rationnelle)

---

# ‚öñÔ∏è Sc√©nario 10: Mauvais Mode de Facturation (Provisioned vs On-Demand Mismatch)

## üìã Description du Probl√®me

DynamoDB propose **2 modes de facturation** avec des mod√®les de co√ªts radicalement diff√©rents:

### **Provisioned Mode** (capacit√© fixe)
- **Id√©al pour**: Traffic pr√©visible et constant
- **Co√ªt**: Facturation 24/7 pour capacit√© provisionn√©e (RCU/WCU)
- **Avantage**: 2-3√ó moins cher que On-Demand si utilisation > 50%

### **On-Demand Mode** (pay-per-request)
- **Id√©al pour**: Traffic impr√©visible ou sporadique
- **Co√ªt**: $0.25 par million reads, $1.25 par million writes
- **Avantage**: $0 si table inactive, pas de planning capacit√©

**Le probl√®me**: Beaucoup d'organisations utilisent le **mauvais mode de facturation**, g√©n√©rant des surco√ªts de 200-500%:

1. **Provisioned avec traffic sporadique** ‚Üí Paye 24/7 pour capacit√© inutilis√©e
2. **On-Demand avec traffic constant √©lev√©** ‚Üí Paye 3√ó le co√ªt Provisioned
3. **Provisioned surdimensionn√©** ‚Üí 90% de capacit√© gaspill√©e
4. **On-Demand sur tables dev/staging** ‚Üí Devrait √™tre Provisioned (co√ªt pr√©visible)

---

## üí∞ Impact Financier

### Comparaison Provisioned vs On-Demand

**Exemple 1: Table avec traffic constant √©lev√© (MAUVAIS choix On-Demand)**

```
Traffic Pattern: Constant (e-commerce production)
  - Reads: 50M requests/mois (constant 24/7)
  - Writes: 10M requests/mois (constant 24/7)

MODE ON-DEMAND (ACTUEL - MAUVAIS CHOIX):
  - Reads: 50M √ó $0.25/M = $12.50/mois
  - Writes: 10M √ó $1.25/M = $12.50/mois
  - TOTAL: $25/mois üî¥

MODE PROVISIONED (RECOMMAND√â):
  - Reads: 50M req/mois √∑ (30 days √ó 86400 sec) = 19.3 RCU required
  - Writes: 10M req/mois √∑ (30 days √ó 86400 sec) = 3.9 WCU required
  - Provision: 25 RCU + 5 WCU (avec marge)

  - RCU: 25 √ó $0.00013 √ó 730h = $2.37/mois
  - WCU: 5 √ó $0.00065 √ó 730h = $2.37/mois
  - TOTAL: $4.74/mois ‚úÖ

üí∞ GASPILLAGE: $25 - $4.74 = $20.26/mois = $243/an per table (81% trop cher!)
```

**Exemple 2: Table avec traffic sporadique (MAUVAIS choix Provisioned)**

```
Traffic Pattern: Sporadique (batch processing 1√ó/jour)
  - Reads: 10M requests/mois (concentrated in 1h/day)
  - Writes: 0

MODE PROVISIONED (ACTUEL - MAUVAIS CHOIX):
  - Provisioned: 150 RCU (pour g√©rer le pic)
  - RCU: 150 √ó $0.00013 √ó 730h = $14.24/mois
  - Utilization: 3% (1h/24h) üî¥
  - TOTAL: $14.24/mois üî¥

MODE ON-DEMAND (RECOMMAND√â):
  - Reads: 10M √ó $0.25/M = $2.50/mois ‚úÖ
  - TOTAL: $2.50/mois ‚úÖ

üí∞ GASPILLAGE: $14.24 - $2.50 = $11.74/mois = $141/an per table (82% trop cher!)
```

---

## üìä Analyse: Seuil de Rentabilit√© (Provisioned vs On-Demand)

### Calcul du Break-Even Point

```
Formule:
  On-Demand Cost = Provisioned Cost
  (Reads √ó $0.25/M) + (Writes √ó $1.25/M) = (RCU √ó $0.00013 √ó 730) + (WCU √ó $0.00065 √ó 730)

Break-Even:
  - Si utilisation > 50% de capacit√© provisionn√©e ‚Üí Provisioned moins cher
  - Si utilisation < 50% de capacit√© provisionn√©e ‚Üí On-Demand moins cher
```

### Tableau de D√©cision

| Traffic Pattern | Reads/Writes par mois | Recommandation | √âconomie Potentielle |
|-----------------|----------------------|----------------|----------------------|
| **Constant √©lev√©** | > 50M reads/mois | PROVISIONED | 70-80% vs On-Demand |
| **Constant faible** | 1-50M reads/mois | PROVISIONED | 50-70% vs On-Demand |
| **Sporadique** | Pics courts (<4h/jour) | ON-DEMAND | 80-90% vs Provisioned |
| **Batch processing** | 1-2√ó/jour pendant 1h | ON-DEMAND | 90%+ vs Provisioned |
| **Dev/Staging** | Impr√©visible | ON-DEMAND | Variable |
| **Inactive** | < 1M reads/mois | ON-DEMAND | ~100% vs Provisioned |

---

## üîç D√©tection du Gaspillage

### Crit√®res de D√©tection

**Mauvais choix #1: On-Demand avec traffic constant**
- Mode: ON_DEMAND
- Utilization: > 8 heures/jour de traffic constant
- Co√ªt On-Demand > 2√ó co√ªt Provisioned √©quivalent

**Mauvais choix #2: Provisioned avec utilization < 20%**
- Mode: PROVISIONED
- Utilization: < 20% RCU/WCU (sur 30 jours)
- Co√ªt Provisioned > 2√ó co√ªt On-Demand √©quivalent

### üìä Exemple Concret #1: On-Demand ‚Üí Provisioned

```
Table Name:        product-catalog
Region:            us-east-1
Environment:       PRODUCTION
Created:           2023-01-15 (22 months ago)

Billing Mode:      ON_DEMAND ‚ùå

Traffic Analysis (30 derniers jours):
  - Read Requests: 125M/mois
  - Write Requests: 15M/mois
  - Traffic Pattern: CONSTANT (24/7) ‚úÖ
  - Hourly Distribution: Uniform (no peaks)

Co√ªt ACTUEL (On-Demand):
  - Reads: 125M √ó $0.25/M = $31.25/mois
  - Writes: 15M √ó $1.25/M = $18.75/mois
  - TOTAL: $50/mois = $600/an üî¥

Co√ªt OPTIMIS√â (Provisioned):
  - Required Capacity:
    - RCU: 125M √∑ (30 √ó 86400) = 48.2 ‚Üí Provision 55 RCU
    - WCU: 15M √∑ (30 √ó 86400) = 5.8 ‚Üí Provision 10 WCU

  - RCU: 55 √ó $0.00013 √ó 730 = $5.22/mois
  - WCU: 10 √ó $0.00065 √ó 730 = $4.75/mois
  - TOTAL: $9.97/mois = $120/an ‚úÖ

üí∞ GASPILLAGE: $50 - $9.97 = $40.03/mois = $480/an (80% de r√©duction!)

üî¥ WASTE DETECTED: On-Demand mode with constant traffic (22 months)
üí∞ COST: $40/mois = $480/an de GASPILLAGE
üìã ACTION: Migrate to Provisioned mode (55 RCU / 10 WCU)
üí° ROOT CAUSE: Default On-Demand choice sans analyse du traffic pattern

Migration Steps:
  1. Analyse traffic pattern: Constant 24/7 ‚úÖ
  2. Calculate required capacity: 55 RCU / 10 WCU
  3. Switch to Provisioned mode (zero downtime)
  4. Monitor CloudWatch: ConsumedReadCapacityUnits
  5. √âconomie: $480/an immediate
```

---

### üìä Exemple Concret #2: Provisioned ‚Üí On-Demand

```
Table Name:        batch-processing-jobs
Region:            us-east-1
Environment:       PRODUCTION
Created:           2023-08-01 (15 months ago)

Billing Mode:      PROVISIONED ‚ùå
  - RCU: 200 units
  - WCU: 50 units

Traffic Analysis (30 derniers jours):
  - Read Requests: 8M/mois
  - Write Requests: 0/mois
  - Traffic Pattern: SPORADIQUE üî¥
    - Active: 2 heures/jour (8% du temps)
    - Inactive: 22 heures/jour (92% du temps)

Co√ªt ACTUEL (Provisioned):
  - RCU: 200 √ó $0.00013 √ó 730 = $18.98/mois
  - WCU: 50 √ó $0.00065 √ó 730 = $23.73/mois
  - TOTAL: $42.71/mois = $512/an üî¥

Utilization Analysis:
  - RCU Utilization: 8% (2h/24h)
  - WCU Utilization: 0% (no writes)
  - Wasted Capacity: 92% üî¥

Co√ªt OPTIMIS√â (On-Demand):
  - Reads: 8M √ó $0.25/M = $2.00/mois ‚úÖ
  - Writes: 0
  - TOTAL: $2.00/mois = $24/an ‚úÖ

üí∞ GASPILLAGE: $42.71 - $2.00 = $40.71/mois = $488/an (95% de r√©duction!)

üî¥ WASTE DETECTED: Provisioned mode with 8% utilization (15 months)
üí∞ COST: $40.71/mois = $488/an de GASPILLAGE
üìã ACTION: Migrate to On-Demand mode
üí° ROOT CAUSE: Batch processing pattern (2h/day) incompatible avec Provisioned

Timeline:
  - Aug 2023: Table cr√©√©e en Provisioned (initial design)
  - Sep 2023: Traffic pattern identified (batch 2h/day)
  - Oct 2023-Present: Provisioned mode OUBLI√âE (15 mois) üî¥
  - Total Wasted: $40.71 √ó 15 = $611 üî¥
```

---

## ‚úÖ Recommandations

### 1. Choix du Mode de Facturation

**Utiliser PROVISIONED si**:
- Traffic constant et pr√©visible (>8h/jour)
- Utilization > 50% de capacit√©
- Production avec SLA strict
- Co√ªt On-Demand > 2√ó Provisioned

**Utiliser ON-DEMAND si**:
- Traffic sporadique ou impr√©visible
- Utilization < 50% de capacit√©
- Dev/Staging/Test environments
- Tables rarement utilis√©es (<1M req/mois)
- Batch processing (<4h/jour)

### 2. Audit Trimestriel

```python
# Script d'audit automatique
for table in all_dynamodb_tables:
    billing_mode = table['BillingMode']
    utilization = calculate_utilization(table, days=30)

    if billing_mode == 'PROVISIONED' and utilization < 20:
        # Recommander On-Demand
        savings = calculate_savings(table, target_mode='ON_DEMAND')
        alert(f"Switch to On-Demand: save ${savings}/month")

    elif billing_mode == 'PAY_PER_REQUEST' and traffic_constant(table):
        # Recommander Provisioned
        savings = calculate_savings(table, target_mode='PROVISIONED')
        alert(f"Switch to Provisioned: save ${savings}/month")
```

### 3. Auto-Scaling (Provisioned Mode)

Si vous choisissez Provisioned, activez **Auto Scaling**:
```python
# Enable Auto Scaling
dynamodb.register_scalable_target(
    ServiceNamespace='dynamodb',
    ResourceId=f'table/{table_name}',
    ScalableDimension='dynamodb:table:ReadCapacityUnits',
    MinCapacity=5,
    MaxCapacity=200
)

# Target Tracking Policy: 70% utilization
dynamodb.put_scaling_policy(
    PolicyName='DynamoDBReadCapacityUtilization',
    ServiceNamespace='dynamodb',
    ResourceId=f'table/{table_name}',
    ScalableDimension='dynamodb:table:ReadCapacityUnits',
    PolicyType='TargetTrackingScaling',
    TargetTrackingScalingPolicyConfiguration={
        'TargetValue': 70.0,
        'PredefinedMetricSpecification': {
            'PredefinedMetricType': 'DynamoDBReadCapacityUtilization'
        }
    }
)
```

### 4. Cost Explorer Alerts

- **Alert #1**: Si co√ªt On-Demand > $50/mois ‚Üí V√©rifier si Provisioned moins cher
- **Alert #2**: Si Provisioned utilization < 20% ‚Üí V√©rifier si On-Demand moins cher

### 5. Migration Zero-Downtime

```bash
# Provisioned ‚Üí On-Demand (zero downtime)
aws dynamodb update-table \
  --table-name product-catalog \
  --billing-mode PAY_PER_REQUEST

# On-Demand ‚Üí Provisioned (zero downtime)
aws dynamodb update-table \
  --table-name batch-jobs \
  --billing-mode PROVISIONED \
  --provisioned-throughput ReadCapacityUnits=50,WriteCapacityUnits=10
```

**Note**: Vous pouvez changer de mode 2 fois par 24 heures max (limite AWS).

---

# üéØ R√©sum√© des 10 Sc√©narios de Gaspillage DynamoDB

## Vue d'ensemble

Ce document couvre **10 sc√©narios majeurs** de gaspillage cloud pour **AWS DynamoDB Tables**, repr√©sentant des √©conomies potentielles de **$10,000-50,000/an** pour une organisation avec 100 tables.

---

## Tableau R√©capitulatif

| # | Sc√©nario | √âconomie Typique | Difficult√© D√©tection | Priorit√© | Impact |
|---|----------|------------------|----------------------|----------|--------|
| **1** | Over-Provisioned Capacity (<10% RCU/WCU) | $50-200/table/an | ‚≠ê‚≠ê Moyenne | üî¥ CRITIQUE | Tr√®s √âlev√© |
| **2** | Unused Global Secondary Indexes (GSI) | $30-150/GSI/an | ‚≠ê‚≠ê‚≠ê Facile | üî¥ CRITIQUE | √âlev√© |
| **3** | Never Used Tables (Provisioned) | $80-400/table/an | ‚≠ê‚≠ê Moyenne | üî¥ CRITIQUE | Tr√®s √âlev√© |
| **4** | Never Used Tables (On-Demand) | $1-10/table/an | ‚≠ê‚≠ê Moyenne | üü° MEDIUM | Faible |
| **5** | Empty Tables (0 items) | $80-400/table/an | ‚≠ê‚≠ê‚≠ê Facile | üî¥ CRITIQUE | √âlev√© |
| **6** | PITR Enabled But Never Used | $10-100/table/an | ‚≠ê‚≠ê Moyenne | üü° MEDIUM | Moyen |
| **7** | Global Tables - Unused Replicas | $200-1000/replica/an | ‚≠ê‚≠ê‚≠ê‚≠ê Difficile | üî¥ CRITIQUE | Tr√®s √âlev√© |
| **8** | DynamoDB Streams Sans Consommateurs | $0/direct (overhead) | ‚≠ê‚≠ê Moyenne | üü¢ LOW | Faible |
| **9** | TTL D√©sactiv√© (Donn√©es Temporaires) | $100-500/table/an | ‚≠ê‚≠ê Moyenne | üî¥ CRITIQUE | Tr√®s √âlev√© |
| **10** | Wrong Billing Mode (Provisioned vs On-Demand) | $200-500/table/an | ‚≠ê‚≠ê‚≠ê Facile | üî¥ CRITIQUE | Tr√®s √âlev√© |

---

## üí∞ Impact Financier Cumul√©

### Exemple: Organisation avec 100 Tables DynamoDB

```
R√©partition typique:
  - 60 tables Provisioned (production)
  - 30 tables On-Demand (staging/dev)
  - 10 tables Global Tables (multi-r√©gion)

Sc√©nario 1: Over-Provisioned Capacity
  - 40 tables avec <10% utilization (sur 60 Provisioned)
  - √âconomie: 40 √ó $100/an = $4,000/an

Sc√©nario 2: Unused GSI
  - 25 GSI inutilis√©s (sur 100 tables)
  - √âconomie: 25 √ó $80/an = $2,000/an

Sc√©nario 3-4: Never Used Tables
  - 10 tables jamais utilis√©es (Provisioned)
  - √âconomie: 10 √ó $200/an = $2,000/an

Sc√©nario 5: Empty Tables
  - 15 tables vides (Provisioned)
  - √âconomie: 15 √ó $150/an = $2,250/an

Sc√©nario 6: Unused PITR
  - 30 tables non-prod avec PITR activ√©
  - √âconomie: 30 √ó $30/an = $900/an

Sc√©nario 7: Unused Global Replicas
  - 5 r√©plicas inutilis√©es (sur 10 Global Tables)
  - √âconomie: 5 √ó $500/an = $2,500/an

Sc√©nario 8: Unused Streams
  - 20 tables avec Streams orphelins
  - √âconomie: $0/direct (simplification op√©rationnelle)

Sc√©nario 9: Missing TTL
  - 15 tables temporaires sans TTL
  - √âconomie: 15 √ó $200/an = $3,000/an

Sc√©nario 10: Wrong Billing Mode
  - 20 tables avec mauvais mode de facturation
  - √âconomie: 20 √ó $300/an = $6,000/an

üí∞ √âCONOMIE TOTALE: $22,650/an
üí° ROI: Impl√©mentation 2-3 semaines ‚Üí √âconomie imm√©diate
```

---

## ‚úÖ Conclusion

Les **10 sc√©narios de gaspillage DynamoDB** document√©s ici repr√©sentent des opportunit√©s d'√©conomies de **$20,000-50,000/an** pour une organisation moyenne avec 100 tables.

**Priorit√©s**:
1. üî¥ **Quick Wins (Semaine 1-2)**: Never Used + Empty Tables + Wrong Billing Mode ‚Üí $10,250/an
2. üî¥ **Optimisation Capacit√© (Semaine 3)**: Over-Provisioned ‚Üí $4,000/an
3. üü° **Features Inutilis√©es (Semaine 4)**: Unused GSI/PITR/Streams/Replicas ‚Üí $5,400/an
4. üü¢ **Best Practices (Continu)**: TTL sur tables temporaires ‚Üí $3,000/an

**ROI**: Impl√©mentation 4 semaines ‚Üí √âconomie **$22,650/an** ‚Üí ROI **1:100+**

**Next Steps**:
1. Installer CloudWaste scanner (backend Python)
2. Lancer premier scan DynamoDB (toutes r√©gions)
3. G√©n√©rer rapport initial (baseline costs)
4. Impl√©menter Quick Wins (semaine 1-2)
5. Automation + monitoring continu

---

**Document Version**: 1.0
**Derni√®re Mise √† Jour**: 2024-11-05
**Auteur**: CloudWaste Team
**Contact**: support@cloudwaste.com
